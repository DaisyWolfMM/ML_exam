{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение задачи бинарной классификации на примере датасета про приход покупателя в определенный промежуток времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт библиотек\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import hyperopt\n",
    "import random\n",
    "import itertools\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, StratifiedKFold,RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/ny_train_0901.csv', sep = ';',nrows=40000,encoding = 'cp1251')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ОБработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['customer_id'] \n",
    "#так как берутся два временных промежутка, но используются на них актуальная информация \n",
    "#необходимо первое вхождение покупателя, чтобы все покупатели были уникальны\n",
    "df1=df.groupby('customer_id', as_index =False).nth(0)\n",
    "df=df1\n",
    "df['LCVALBD']=pd.to_datetime(df['LCVALBD'],dayfirst=True)\n",
    "df['LSTKNBDT']=pd.to_datetime(df['LSTKNBDT'],dayfirst=True)\n",
    "df['CALDAYDATE']=pd.to_datetime(df['CALDAYDATE'],dayfirst=True)\n",
    "import datetime\n",
    "df['LCVALBD']=(datetime.datetime.now()-df['LCVALBD']).dt.days\n",
    "df['LSTKNBDT']=(datetime.datetime.now()-df['LSTKNBDT']).dt.days\n",
    "df['CALDAYDATE']=(datetime.datetime.now()-df['CALDAYDATE']).dt.days\n",
    "df['first_purchase_date']=pd.to_datetime(df['first_purchase_date'],dayfirst=True)\n",
    "df['first_purchase_date']=(datetime.datetime.now()-df['first_purchase_date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df.sex)\n",
    "df.sex=le.transform(df.sex)\n",
    "df=pd.DataFrame(data = df, columns = df1.columns)\n",
    "le.fit(df.sms_delivery_index)\n",
    "df.sms_delivery_index=le.transform(df.sms_delivery_index)\n",
    "le.fit(df.LCIDGR)\n",
    "df.LCIDGR=le.transform(df.LCIDGR)\n",
    "le.fit(df.LCSTAT)\n",
    "df.LCSTAT=le.transform(df.LCSTAT)\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сохраним список признаков\n",
    "train_columns = list(df.columns)\n",
    "train_columns=train_columns[:-1]\n",
    "y = df['notbe']\n",
    "X = df[train_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X['customer_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>first_purchase_date</th>\n",
       "      <th>age</th>\n",
       "      <th>customer_month</th>\n",
       "      <th>customer_year</th>\n",
       "      <th>purchase_prefer_visit_time</th>\n",
       "      <th>count_days_since_last_visit</th>\n",
       "      <th>visit_1m_cnt</th>\n",
       "      <th>visit_3m_cnt</th>\n",
       "      <th>visit_6m_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>PAYSCASH</th>\n",
       "      <th>PAYSCRED</th>\n",
       "      <th>PAYSCERT</th>\n",
       "      <th>CHQSKUCOUNT</th>\n",
       "      <th>GOODBONUSSUM</th>\n",
       "      <th>CHEQBONUSSUM</th>\n",
       "      <th>SUMCASD</th>\n",
       "      <th>visit_12m_cnt</th>\n",
       "      <th>count_days_between_visits</th>\n",
       "      <th>mean_check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2515</td>\n",
       "      <td>35.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>617.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>44.0</td>\n",
       "      <td>600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1726</td>\n",
       "      <td>37.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>9</td>\n",
       "      <td>40.0</td>\n",
       "      <td>600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>431</td>\n",
       "      <td>29.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>9</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>857</td>\n",
       "      <td>38.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1861.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>7</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2457</td>\n",
       "      <td>26.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>724.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>8</td>\n",
       "      <td>46.0</td>\n",
       "      <td>700.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex  first_purchase_date   age  customer_month  customer_year  \\\n",
       "0    0                 2515  35.0            81.0            7.0   \n",
       "1    0                 1726  37.0            55.0            5.0   \n",
       "2    1                  431  29.0            13.0            2.0   \n",
       "3    0                  857  38.0            27.0            3.0   \n",
       "4    0                 2457  26.0            79.0            7.0   \n",
       "\n",
       "   purchase_prefer_visit_time  count_days_since_last_visit  visit_1m_cnt  \\\n",
       "0                           2                            2             1   \n",
       "1                           2                           17             2   \n",
       "2                           3                           20             2   \n",
       "3                           2                           13             1   \n",
       "4                           3                           22             1   \n",
       "\n",
       "   visit_3m_cnt  visit_6m_cnt  ...  PAYSCASH  PAYSCRED  PAYSCERT  CHQSKUCOUNT  \\\n",
       "0             1             4  ...     617.0       0.0       0.0          8.0   \n",
       "1             3             5  ...       0.0     602.0       0.0          2.0   \n",
       "2             2             4  ...       0.0     485.0       0.0          4.0   \n",
       "3             2             2  ...       0.0    1861.0       0.0         11.0   \n",
       "4             1             3  ...       0.0     724.0       0.0          6.0   \n",
       "\n",
       "   GOODBONUSSUM  CHEQBONUSSUM  SUMCASD  visit_12m_cnt  \\\n",
       "0          13.0           0.0      0.0              9   \n",
       "1           0.0           0.0      0.9              9   \n",
       "2          39.0           0.0      0.1              9   \n",
       "3         101.0           0.0      0.8              7   \n",
       "4          61.0           0.0      0.2              8   \n",
       "\n",
       "   count_days_between_visits  mean_check  \n",
       "0                       44.0       600.0  \n",
       "1                       40.0       600.0  \n",
       "2                       40.0      1150.0  \n",
       "3                       44.0      1900.0  \n",
       "4                       46.0       700.0  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Распределение зависимой переменной\n",
    "Прежде всего, посмотрим, как распределена зависимая переменная. Из графика видно, что выборка достаточно сбалансирована. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Распределение зависимой переменной')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.value_counts().plot(kind='bar')\n",
    "plt.xlabel('Значение переменной IsGood')\n",
    "plt.ylabel('Количество значений')\n",
    "plt.title('Распределение зависимой переменной')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корреляция между независимыми переменными нам не так интересна, а корреляция независимых переменных с $IsGood$ может быть полезна для дальнейшего анализа (например, выделения релевантных признаков). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count_days_since_last_visit     -0.356848\n",
       "CALDAYDATE                      -0.321856\n",
       "recency                         -0.289468\n",
       "visit_3m_cnt                    -0.209041\n",
       "visit_12m_cnt                   -0.164215\n",
       "frequency                        0.149810\n",
       "card_21d_amt                     0.143937\n",
       "visit_6m_cnt                    -0.133295\n",
       "cnt_days_last_points_increase   -0.128254\n",
       "LSTKNBDT                        -0.113228\n",
       "Name: notbe, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlated_features = ['notbe'] + list(df.corr().loc['notbe'].abs().sort_values(ascending=False).iloc[1:11].index)\n",
    "sub_df = df[correlated_features]\n",
    "sub_df.corr().iloc[0,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что самая большая корреляция с \"количеством дней с последнего посящения\", \"c датой последнего посеящения\", \"c количеством визита за последние 3 и 12 месяцев\", что очень логично. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиваем признаки на зависимые и не зависимые и делим выборку на две части: обучающую и тестовую"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Откладываем 20% данных для тестовой выборки. Валидируемся кросс-валидацией на 6 фолдов на train-наборе\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "smp = SimpleImputer(strategy = 'median', fill_value = 0)\n",
    "x_train = smp.fit_transform(x_train)\n",
    "x_test = smp.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:415: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "iso_forest = IsolationForest(random_state = 42, bootstrap = True)\n",
    "#Если True, отдельные деревья помещаются в случайные подмножества обучающих данных, \n",
    "#выбранных с заменой. предназначенный для улучшения стабильности и точности алгоритмов\n",
    "#Если задан стандартный тренировочный наборD размера n, бэггинг образует m новых\n",
    "#тренировочных наборов D_{i}, каждый размером n′, путём выборки из D равномерно и\n",
    "#с возвратом. При сэмплинге с возвратом некоторые наблюдения могут быть повторены в каждой D_{i}D_{i}.\n",
    "# выбросы ближе к корнб дерева\n",
    "outliers = iso_forest.fit_predict(x_train, y_train)\n",
    "x_train = pd.DataFrame(data = x_train, columns = train_columns[1:])\n",
    "x_train['outliers'] = outliers\n",
    "x_train = x_train[x_train['outliers'] == 1]\n",
    "x_train = x_train.drop('outliers', axis = 1)\n",
    "y_train = y_train[outliers == 1]\n",
    "x_train.replace(-np.inf, 0, inplace = True)\n",
    "x_train.replace(np.inf, 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "# Нормализация\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "columns = df.columns\n",
    "X_columns =columns[:-1]\n",
    "x_test = pd.DataFrame(data = x_test, columns = X_columns[1:])\n",
    "x_train = pd.DataFrame(data = x_train, columns =X_columns[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Постановка задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дано: обучающая выборка --- $\\mathsf{(x_1,\\ldots, x_n) \\subset X}$, $\\mathsf{y_i=y(x_i), ~i=1,\\ldots,n}$ --- известные ответы. \n",
    "\n",
    "Обучающая выборка: $\\mathbb{X}_\\mathsf{n}=\\mathsf{(x_i,y_i)_{i=1}^n},~\\mathsf{x_i}\\in \\mathbb{R}^\\mathsf{p},~ \\mathsf{y_i}\\in \\{-1,+1\\}.$\n",
    "\n",
    "Найти: $\\mathsf{a:X\\to Y}$ --- функцию (decision function), приближающую $\\mathsf{y}$ на всем множестве $\\mathsf{X}$. \n",
    "\n",
    "$\\mathsf{f(x,w)}$ --- разделяющая (дискриминантная) функция, $\\mathsf{w}\\in\\mathbb{R}^\\mathsf{p}$. \n",
    "\n",
    "$\\mathsf{a(x,w)=\\sign f(x,w)}$ --- классификатор. \n",
    "\n",
    "$\\mathsf{f(x,w)}=0$ --- разделяющая поверхность. \n",
    "\n",
    "$\\mathsf{M_i(w)=y_if(x_i,w)}$ --- \\alert{отступ} объекта $\\mathsf{x_i}$. \n",
    "\n",
    "Если $\\mathsf{M_i(w)<0},$ то классификатор ошибается на $\\mathsf{x_i}$.\n",
    "\n",
    "$$Q(\\mathsf{w)=\\sum \\limits_{i=1}^n [M_i(w)<0]}\\le \\tilde Q(\\mathsf{w)=\\sum \\limits_{i=1}^n\\mathcal{L}(M_i(w)) \\to \\min \\limits_{w}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>first_purchase_date</th>\n",
       "      <th>age</th>\n",
       "      <th>customer_month</th>\n",
       "      <th>customer_year</th>\n",
       "      <th>purchase_prefer_visit_time</th>\n",
       "      <th>count_days_since_last_visit</th>\n",
       "      <th>visit_1m_cnt</th>\n",
       "      <th>visit_3m_cnt</th>\n",
       "      <th>visit_6m_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>PAYSCASH</th>\n",
       "      <th>PAYSCRED</th>\n",
       "      <th>PAYSCERT</th>\n",
       "      <th>CHQSKUCOUNT</th>\n",
       "      <th>GOODBONUSSUM</th>\n",
       "      <th>CHEQBONUSSUM</th>\n",
       "      <th>SUMCASD</th>\n",
       "      <th>visit_12m_cnt</th>\n",
       "      <th>count_days_between_visits</th>\n",
       "      <th>mean_check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.057231</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248824</td>\n",
       "      <td>0.258992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.271493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.341200</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.071895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.940449</td>\n",
       "      <td>0.706081</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248824</td>\n",
       "      <td>0.429679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.314480</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.340932</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.159319</td>\n",
       "      <td>0.716216</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248824</td>\n",
       "      <td>0.499976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.361991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.341133</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.137255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.935808</td>\n",
       "      <td>0.777027</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248824</td>\n",
       "      <td>0.345900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.300905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.341066</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.084967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>0.854730</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248824</td>\n",
       "      <td>0.486976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.402715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.341133</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex  first_purchase_date       age  customer_month  customer_year  \\\n",
       "0  1.0             0.057231  0.750000        0.058824       0.000000   \n",
       "1  1.0             0.940449  0.706081        0.941176       0.857143   \n",
       "2  1.0             0.159319  0.716216        0.164706       0.142857   \n",
       "3  0.0             0.935808  0.777027        0.941176       0.857143   \n",
       "4  0.0             0.002707  0.854730        0.011765       0.000000   \n",
       "\n",
       "   purchase_prefer_visit_time  count_days_since_last_visit  visit_1m_cnt  \\\n",
       "0                         1.0                     0.068966          0.25   \n",
       "1                         0.5                     0.137931          0.00   \n",
       "2                         0.5                     0.862069          0.00   \n",
       "3                         0.5                     0.275862          0.00   \n",
       "4                         0.5                     0.275862          0.00   \n",
       "\n",
       "   visit_3m_cnt  visit_6m_cnt  ...  PAYSCASH  PAYSCRED  PAYSCERT  CHQSKUCOUNT  \\\n",
       "0      0.285714         0.375  ...  0.248824  0.258992       0.0         0.00   \n",
       "1      0.142857         0.250  ...  0.248824  0.429679       0.0         0.24   \n",
       "2      0.142857         0.500  ...  0.248824  0.499976       0.0         0.44   \n",
       "3      0.142857         0.250  ...  0.248824  0.345900       0.0         0.16   \n",
       "4      0.142857         0.125  ...  0.248824  0.486976       0.0         0.12   \n",
       "\n",
       "   GOODBONUSSUM  CHEQBONUSSUM   SUMCASD  visit_12m_cnt  \\\n",
       "0      0.271493           0.0  0.341200          0.375   \n",
       "1      0.314480           0.0  0.340932          0.625   \n",
       "2      0.361991           0.0  0.341133          1.000   \n",
       "3      0.300905           0.0  0.341066          0.625   \n",
       "4      0.402715           0.0  0.341133          0.000   \n",
       "\n",
       "   count_days_between_visits  mean_check  \n",
       "0                   0.407407    0.071895  \n",
       "1                   0.777778    0.058824  \n",
       "2                   0.074074    0.137255  \n",
       "3                   0.888889    0.084967  \n",
       "4                   0.000000    0.111111  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отбор признаков\n",
    "\n",
    "\n",
    "### Feature selection и extraction.\n",
    "#### Feature selection:\n",
    "Уменьшает количество признаков. Выбор признаков с наибольшей «важностью» / влиянием на целевую переменную из набора существующих функций. Это может быть сделано с помощью различных методов: например, линейная регрессия, деревья решений.\n",
    "#### Feature extraction: \n",
    "Добавляет признаки, которые вычисляются из других объектов, которые трудно анализировать напрямую / не сопоставим напрямую (например, изображения, временные ряды и т. Д.). В примере временного ряда могут быть использованы некоторые простые функции. пример: длина временного ряда, период, среднее значение, стандартное значение и т. д.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction:\n",
    "Давайте посмотрим как выглядет обработанные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x_train)\n",
    "X_PC = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['PC1', 'PC2'])#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x28b9d3fa470>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_PC.PC1,X_PC.PC2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что классы разделимы.\n",
    "\n",
    "Для визуализации корреляционной матрицы будем использовать \"тепловую карту\" \n",
    "heatmap которая показывает степень корреляции различными цветами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "sns.set(style=\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x28bb7a257f0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(x_train.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корреляционная матрица позволяет понять зависимость между признаками. Не менее важно понять насколько значения целевого признака разделимы в рамках исходных признаков. Для ответа на этот вопрос используется матрица рассеяния (scatter matrix). По диагонали матрицы выводятся гистограммы распределения признаков, другие элементы матрицы показывают гистограмму рассеяния (при этом точки относящиеся к различным значениям целевого признака выделяются разными цветами)."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection \n",
    "Для отбора наиболее информативных признаков воспользуемся критерием Хи-квадрат и методом SelectKBest. Для примера отберем 10 признаков, чтобы был простор для дальнейшей обработки переменных, но в то же время, чтобы не было откровенной лишних признаков.\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_norm = MinMaxScaler().fit_transform(x_train)\n",
    "chi_selector = SelectKBest(chi2, k=10)\n",
    "#y_train=y_train.astype('int')# y тип object, поэтому sklearn не может распознать его тип"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=10, score_func=<function chi2 at 0x0000028B959B70D0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_selector.fit(X_norm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 selected features\n"
     ]
    }
   ],
   "source": [
    "chi_support = chi_selector.get_support()\n",
    "chi_feature = X.loc[:,chi_support].columns.tolist()\n",
    "print(str(len(chi_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_removed = x_train[chi_feature]\n",
    "X_test_removed = x_test[chi_feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того, как будут применены все известные алгоритмы, необходимо сравнить их между собой. Для этого нужно выбрать метрику точности алгоритма. Их существует несколько, каждая имеет плюсы и минусы.\n",
    "\n",
    "1) Accuracy. Пусть после процедуры классификации мы знаем, сколько объектов было определено правильно, а сколько - неправильно. При этом нам важно знать, сколько объектов было классифицировано как $True positive (y=1, \\hat{y}=1), False Positive (y=0, \\hat{y}=1), True Negative (y=0, \\hat{y}=0), False Negative (y=1, \\hat{y}=0)$. \n",
    "\n",
    "Тогда $$accuracy=\\frac{TP+TN}{TP+TN+FN+FP}.$$ \n",
    "\n",
    "Таким образом, точность показывает долю правильно классифицированных объектов.\n",
    "\n",
    "\n",
    "2) Precision - она же точность. Показывает долю объектов, классифицированных как positive, которые на самом деле и есть positive, или $$precision=\\frac{TP}{TP+FP}.$$\n",
    "\n",
    "\n",
    "3) Recall - она же полнота. Показывает долю объектов, которые классифицированы как positive по отношению ко всем объектам этого класса, которые есть в выборке, или $$recall=\\frac{TP}{TP+FN}.$$\n",
    "\n",
    "4) ROC-AUC. Метрика, которая не зависит от баланса классов и вероятности, которую мы приняли как пороговую, чтобы определять, к какому классу принадлежит объект. ROC-AUC показывает зависимость $Precision$ от заданной наперед возможности ошибиться. Таким образом, ROC-кривая строится в координатах $\\frac{FP}{FP+TP}$ по горизонтали и $Precision$ по вертикали. Чем больше площадь под этой кривой, она же $AUC$, тем точнее модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression\n",
    "\n",
    "\n",
    "В качестве аппроксимации пороговой функции потерь берется логарифмическая функция потерь $\\mathcal{L}(\\mathsf{M})=\\log(1+e^{-\\mathsf{M}}).$ \n",
    "\n",
    "Заметим, что во всех методах мы стремимся минимизировать функционал ошибок, каждый раз разный.\n",
    "Для логистической регрессии таким функционалом является\n",
    "\n",
    "$$Q(x,\\bar{y},\\bar{w})=\\sum_{i=1}^{n}\\log(1+exp(-y_i\\bar{w^T}\\bar{x_i})\\to \\min \\limits_{\\bar{w}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании метода логистической регрессии в Python необходимо указать следующие параметры:\n",
    "\n",
    "1) $C=\\frac{1}{\\lambda}$, параметр регуляризации. Чем больше $C$, тем меньше $\\lambda$, тем слабее регуляризация, так как за сложность мы штрафуем меньше. \n",
    "\n",
    "Проблемы: Признаков намного больше, чем объектов; Мультиколлинеарность признаков --- Таким образом, очень много различных векторов дадут близкие значения функционала качества, но при этом коэффициенты могут существенно отличаться. \n",
    "\n",
    "$$\\mathsf{Q(w)=-\\sum\\limits_{i=1}^n [y_i\\log (\\sigma_w(x_i)+(1-y_i))\\log (1-\\sigma_w(x_i))]}$$\n",
    "\n",
    "$\\sigma=\\frac{1}{1+\\exp(- y\\langle x,w \\rangle )}$ (сигмоидная функция)\n",
    "\n",
    "2) Норма, по которой происходит регуляризация. По умолчанию выбрана $L_2$, но можно также выбрать $L_1$ (что соответствует Lasso).\n",
    "\n",
    "Регуляризация в логистической регрессии:\n",
    "$$L2: \\mathsf{Q_\\tau(w)=Q(w)+\\frac{\\lambda}{2}\\sum\\limits_{j=1}^p w_j^2 \\to \\min\\limits_w}$$\n",
    "    \n",
    "$$L1: \\mathsf{Q_\\tau(w)=Q(w)+\\lambda\\sum\\limits_{j=1}^p |w_j| \\to \\min\\limits_w}$$\n",
    "\n",
    "3) Class_weight - поправка на веса. В случае, когда выбрано 'balanced', учитывается, сколько элементов первого и второго классов встретилось в тренировочной выборке. Дальше предполагается, что такой же баланс наблюдается и на тестовой выборке. \n",
    "\n",
    "\n",
    "Параметр $\\mathsf{\\lambda}$ можно подбирать с помощью кросс-валидации. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False,tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1,class_weight=None, random_state=None, solver=’liblinear’,max_iter=100)\n",
    "\t\n",
    "$!$ penalty: ’l1’, ’l2’(default = ’l2’),– вид регуляризации, “lbfgs”и “newton-cg” поддреживают только L2 регуляризвцию.\n",
    "\n",
    "$!$ dual: bool(default = False), прямая или дуальная задача оптимизации. Дуальная формулировка поддерживается только с liblinear. Лучше использовать dual=False, когдаn_samples>n_features.\n",
    "\n",
    "$!$ C: float(default=1.0)– параметр регуляризации, должен быть положительным float. Меньшее С обеспечивает более сильную регуляризацию.\n",
    "\n",
    "$!$ fit_intercept : bool(default = True) – определяет добавлять ли константу к решающей функции.\n",
    "\n",
    "$!$ intercept_scaling : float(default = 1) – полезен, только если используется libliner, если self.fit_intercept=True, то векторx, описывающий объект, становится равным [x,self.intercept_scaling].\n",
    "\n",
    "$!$ class_weight :\n",
    "dict or ‘balanced’(default=None)– веса, ассоциированные с классами, записываются в форме{class_label: weight}. Если параметр не указан, все веса предполагаются равными 1. ’balanced’ использует значения y, чтобы автоматически установить веса обратнопропорционально частотам класса во входных данных.\n",
    "\n",
    "$!$ random_state: \n",
    "int seed, RandomState instance, or None(default) – для инициализации генератора псевдослучайных чисел при перемешивании данных.\n",
    "\n",
    "$!$ solver : ‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’(default =’liblinear’), – алгоритм, который используется для решения проблемы оптимизации.\n",
    "\n",
    "\n",
    "$!$ max_iter : int(default=100) – используется только для newton-cg, sag и lbfgs. Максимальное число итераций.\n",
    "\n",
    "$!$ multiclass: ’ovr’, ’multinimial’. При использовании ’ovr’ используется стратегия OneVsRest для мультиклассовойклассификации. Иначе, минимизируемая функция потерь это multinomial loss, которая учитывает полное распределение вероятностей.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogisticRegression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>roc_auc_train</th>\n",
       "      <td>0.796034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc_test</th>\n",
       "      <td>0.788378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.723452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.720288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.724975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               LogisticRegression\n",
       "roc_auc_train            0.796034\n",
       "roc_auc_test             0.788378\n",
       "accuracy                 0.723452\n",
       "recall                   0.720288\n",
       "precision                0.724975"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score\n",
    "\n",
    "def LogReg(X_train_removed,y_train,X_test_removed,y_test):\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train_removed, y_train)\n",
    "    score_train = logreg.score(X_train_removed, y_train)\n",
    "    score_test = logreg.score(X_test_removed, y_test)\n",
    "    \n",
    "    \n",
    "    predictionsLOGIT = logreg.predict_proba(X_test_removed)[:,1]\n",
    "    predictionsLOGIT_train = logreg.predict_proba(X_train_removed)[:,1]\n",
    "    \n",
    "    roc_auc_train=roc_auc_score(y_train,predictionsLOGIT_train)\n",
    "    roc_auc=roc_auc_score(y_test,predictionsLOGIT)\n",
    "    \n",
    "    log_pred = pd.Series(predictionsLOGIT)\n",
    "    mean_pred=log_pred.mean()\n",
    "    log_pred = log_pred.apply(lambda x: 1 if x >= mean_pred else 0)\n",
    "    \n",
    "    \n",
    "    accuracy=accuracy_score(y_test, log_pred)\n",
    "    recall=recall_score(y_test, log_pred)\n",
    "    precision=precision_score(y_test, log_pred)\n",
    "    data=pd.DataFrame((roc_auc_train,roc_auc,accuracy,recall,precision), \n",
    "                 index = ['roc_auc_train','roc_auc_test','accuracy','recall','precision'],columns = ['LogisticRegression'])    \n",
    "    return(data) \n",
    "LG = LogReg(X_train_removed,y_train,X_test_removed,y_test)\n",
    "LogReg(X_train_removed,y_train,X_test_removed,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Для оптимизации гиперпараметров воспользуемся методом поиска по сетке.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = {\n",
    "    \"C\":np.logspace(-3,3,7), #[1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]\n",
    "    \"penalty\":[\"l1\",\"l2\"],\n",
    "    \"class_weight\":[\"balanced\"]\n",
    "    }\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(random_state=42), parameters, cv=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7356546455853615 0.7239492663516538\n",
      "{'C': 0.1, 'class_weight': 'balanced', 'penalty': 'l1'}\n",
      "Wall time: 11min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf.fit(X_train_removed, y_train)\n",
    "print(clf.score(X_train_removed, y_train), clf.score(X_test_removed, y_test))\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.790990472797623"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionsGS = clf.predict_proba(X_test_removed)[:,1]\n",
    "roc_auc_score(y_test,predictionsGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8000780609722993"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionsGS_train = clf.predict_proba(X_train_removed)[:,1]\n",
    "roc_auc_score(y_train,predictionsGS_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame((roc_auc_score(y_train,predictionsGS_train),roc_auc_score(y_test,predictionsGS)), \n",
    "                 index = ['train','test'],columns = ['LogisticRegression'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же логично сразу проверить деревья решений. Чтобы не использовать точную подстройку параметров дерева, запустим случайный лес, так как он автоматически борется с переобучением за счет усреднения результатов отдельных моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Постановка задачи:\n",
    "\n",
    "Пусть $X\\in\\mathbb{R}^{nxp}$ -матрица данных с p признеаками для n индивидов. $Y=\\{0,1\\}$ - ответы.\n",
    "\n",
    "Идея: разбить пространство признаков, т.е. совокупность всех возможных значений $X_1,...,X_p$ на $J$ непересекающихся областей $R_1,...,R_j$ (многомерных прямоугольников), которые выбираются исходя из условия:\n",
    "\n",
    "$$E=1-\\max_k(\\hat{p}_{jk})->\\min_{R_1,...,R_j}$$\n",
    "\n",
    "где \n",
    "\n",
    "$$\\hat{p}_{jk}=\\frac{1}{|R_j|}\\sum_{x_i\\in R_j}\\mathbb{1}_{(y_i=k)},\\;\\;\\;j=1,...,J$$\n",
    "\n",
    "На практике чаще всего используют два других (информационных) критерия для фиксированного j\n",
    "\n",
    "- $G=\\sum_{k=1}^K\\hat{p}_{jk}(1-\\hat{p}_{jk})$ - индекс Джини\n",
    "\n",
    "- $D=-\\sum_{k=1}^K\\hat{p}_{jk}\\log\\hat{p}_{jk}$ - критерий перекрестной энтропии\n",
    "\n",
    "Предсказание для объекта х:\n",
    "\n",
    "$$f(x)=arg\\max_{k\\in Y}\\hat{p}_{jk}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процесс построения решающих деревьев представляет собой жадный алгоритм, работающий до выполнения\n",
    "критерия останова.\n",
    "\n",
    "Пусть на некотором шаге алгоритма необходимо разбить вершину m, в которой оказалась выборкa $X_m$ ,\n",
    "на две. В качестве условия разбиения используется сравнение j-го признака с порогом t:\n",
    "\n",
    "$$[x^j\\leq t]$$\n",
    "\n",
    "Параметры j и t выбираются исходя из условия минимизации функции ошибки $Q(X_m,j,t)$:\n",
    "\n",
    "$$Q(X_m,j,t)->\\min_{j,t}$$\n",
    "\n",
    "Параметры j и t можно подбирать перебором. Действительно, признаков конечное число, а из всех возможных\n",
    "значений порога t можно рассматривать только те, при которых получаются различные разбиения. Можно\n",
    "показать, что таких значений параметра t столько, сколько различных значений признака $x^j$ на обучающей\n",
    "выборке.\n",
    "\n",
    "После того, как параметры были выбраны, множество $X_m$ объектов из обучающей выборки разбивается\n",
    "на два множества\n",
    "\n",
    "$$X_l=\\{x\\in X_m|[x^j\\leq t]\\},\\;\\;\\;\\;\\;\\;X_r=\\{x\\in X_m|[x^j> t]\\}$$\n",
    "\n",
    "каждое из которых соответствует своей дочерней вершине.\n",
    "\n",
    "Предложенную процедуру можно продолжить для каждой из дочерних вершин: в этом случае дерево\n",
    "будет все больше и больше углубляться. Такой процесс рано или поздно должен остановиться, и очередная\n",
    "дочерняя вершина будет объявлена листком, а не разделена пополам. Этот момент определяется критерием\n",
    "остановки. Существует много различных вариантов критерия остановки:\n",
    "\n",
    "- Если в вершину попал только один объект обучающей выборки или все объекты принадлежат одному\n",
    "классу (в задачах классификации), дальше разбивать не имеет смысла.\n",
    "- Можно также останавливать разбиение, если глубина дерева достигла определенного значения.\n",
    "\n",
    "Рандомизировать процесс построения можно, если в задаче поиска оптимальных параметров выбирать j\n",
    "из случайного подмножества признаков размера q. Оказывается, что этот подход действительно позволяет\n",
    "сделать деревья менее коррелированными.\n",
    "\n",
    "Критерий ошибки записывается следующим образом:\n",
    "\n",
    "$$Q(X_m,j,t)=\\frac{|X_l|}{|X_m|}H(X_l)+\\frac{|X_r|}{|X_m|}H(X_r)$$\n",
    "\n",
    "и состоит из двух слагаемых, каждое из которых соответствует своему листу.\n",
    "Функция H(X) называется критерием информативности.\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Параметры \n",
    "Случайный лес представляет собой композицию деревьев, где на каждом шаге разбиение происходит по признаку из рандомного подмножества признаков размерности $q$.\n",
    "Далее рассмотрим несколько параметров, на которые в первую очередь стоит обратить внимание при построении модели:\n",
    "\n",
    "1) n_estimators - количество деревьев в \"лесу\". Чем выше число, тем лучше, но важно знать, что это сложнее в вычислительном отношении и занимает больше времени для запуска вашего кода.\n",
    "\n",
    "2) max_depth -  максимальная глубина дерева. Если None, то узлы расширяются до тех пор, пока все листья не станут чистыми или пока все листья не будут содержать меньше чем min_samples_split samples.\n",
    "\n",
    "3) min_samples_split - Минимальное количество элементов, попадающих в каждое поддерево. необязательный (по умолчанию = 2)\n",
    "\n",
    "4) min_samples_leaf - Минимальное количество элементов в вершине (в листовом узле) для дальшнейшего разбиения.  необязательный (по умолчанию = 1)\n",
    "\n",
    "5) max_features - Количество функций, которые следует учитывать при поиске лучшего разделения. По умолчанию он соответствует $\\sqrt n$, где $n$-количество признаков, что является оптимальным значением.\n",
    "\n",
    "6) min_weight_fraction_leaf -  Минимальная взвешенная доля общей суммы весов (всех входных выборок), необходимая для конечного узла. Образцы имеют одинаковый вес, когда sample_weight не указан.\n",
    "\n",
    "7) criterion — поскольку у нас теперь задача классификации, то по дефолту выбран критерий \"gini\" (можно выбрать \"entropy\")\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_removed =x_train \n",
    "X_test_removed =x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RandomForestClassifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>roc_auc_train</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc_test</th>\n",
       "      <td>0.867594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.788112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.768523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.799948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               RandomForestClassifier\n",
       "roc_auc_train                1.000000\n",
       "roc_auc_test                 0.867594\n",
       "accuracy                     0.788112\n",
       "recall                       0.768523\n",
       "precision                    0.799948"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def RandForest(X_train_removed,y_train,X_test_removed,y_test):\n",
    "    modelRF = RandomForestClassifier(n_estimators=100, max_features=1, random_state=1)\n",
    "    modelRF.fit(X_train_removed, y_train)\n",
    "    score_train = modelRF.score(X_train_removed, y_train)\n",
    "    score_test =modelRF.score(X_test_removed, y_test)\n",
    "    \n",
    "    predictionsRF = modelRF.predict_proba(X_test_removed)[:,1]\n",
    "    roc_auc=roc_auc_score(y_test,predictionsRF)\n",
    "    \n",
    "    predictionsRF_train = modelRF.predict_proba(X_train_removed)[:,1]\n",
    "    roc_auc_train=roc_auc_score(y_train,predictionsRF_train)\n",
    "    \n",
    "    \n",
    "    rf_pred = pd.Series(predictionsRF)\n",
    "    mean_pred=rf_pred.mean()\n",
    "    rf_pred = rf_pred.apply(lambda x: 1 if x >= mean_pred else 0)\n",
    "    \n",
    "    accuracy=accuracy_score(y_test, rf_pred)\n",
    "    recall=recall_score(y_test, rf_pred)\n",
    "    precision=precision_score(y_test, rf_pred)\n",
    "    \n",
    "    data=pd.DataFrame((roc_auc_train,roc_auc,accuracy,recall,precision), \n",
    "                 index = ['roc_auc_train','roc_auc_test','accuracy','recall','precision'],columns = ['RandomForestClassifier']) \n",
    " \n",
    "    return (data)\n",
    "RF=RandForest(X_train_removed,y_train,X_test_removed,y_test)\n",
    "RandForest(X_train_removed,y_train,X_test_removed,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_rf = {\n",
    "    \"max_depth\": [4,7,10],\n",
    "    \"min_samples_split\": [2,3,4,10], \n",
    "    \"min_samples_leaf\": [2,3,4,10],\n",
    "    \"n_estimators\": [30,60,90],\n",
    "    \"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "clf_rf = GridSearchCV(RandomForestClassifier(random_state=1), parameters_rf, cv=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8632676022976254 0.8270330763491669\n",
      "{'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 90}\n",
      "Wall time: 32min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_rf.fit(x_train, y_train)\n",
    "print(clf_rf.score(X_train_removed, y_train), clf_rf.score(X_test_removed, y_test))\n",
    "print(clf_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9112578579009478"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionsrf = clf_rf.predict_proba(X_test_removed)[:,1]\n",
    "roc_auc_score(y_test,predictionsrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9472817760467459"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionsrf_train = clf_rf.predict_proba(X_train_removed)[:,1]\n",
    "roc_auc_score(y_train,predictionsrf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['RandomForestClassifier'] = (roc_auc_score(y_train,predictionsrf_train),roc_auc_score(y_test,predictionsrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network (Keras)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейронная сеть — это универсальная модель, решающая широкий класс задач регрессии и классификации.\n",
    "\n",
    "###### Нейронная сеть и задача классификации\n",
    "Пусть $\\{(x_i,y_i)\\}^l_{i=1}$ — обучающая выборка в задаче бинарной классификации, $x_i\\in \\mathbb{R}^d$ и $y_i\\in \\{1, −1\\}$ - признаковое описание и метка класса для $i$-го объекта соответственно. Требуется построить разделяющую поверхность\n",
    "\n",
    "$$a(x) -> y \\in \\{−1, 1\\}$$\n",
    "\n",
    "которая отделяет объекты одного класса от объектов другого класса таким образом, что, если $a(x) > 0$, то объект $x$ принадлежит классу $y = +1$, и, если $a(x) ≤ 0$, то $x$ принадлежит классу $y = −1$.\n",
    "\n",
    "Задача выглядит следующим образом:\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Нейронная сеть считается универсальной моделью, так как она способна аппроксимировать любые поверхности. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Двуслойная нейронная сеть\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "\n",
    "- $x$ - исходный объект, \n",
    "\n",
    "- $x_j$ -- соответствующий признак,\n",
    "\n",
    "- $d$ -  количество нейронов во входном слое сети, совпадающее с количеством признаков,\n",
    "\n",
    "- $D$ -  количество нейронов в скрытом слое сети,\n",
    "\n",
    "- $\\sigma^{(1)}, \\sigma^{(2)}$ -- функции активации.\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратное распространение ошибки BackProp\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## В качестве функции активации на скрытом слое сети используется линейная функция. На выходном слое сети используется функция активации sigmoid:\n",
    "\n",
    "$$a(x,w)=\\sigma(w^Tx)=\\frac{1}{1+\\exp(-w^Tx)}$$\n",
    "\n",
    "Эта функция определяет вероятность принадлжености некоторого заданного объекта х к классу у=1 при заданных параметрах w:\n",
    "\n",
    "$$\\sigma(w^Tx)=P(y=1|w,x)$$\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras.layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "\n",
    "- units - Этот параметр является положительным целым числом, которое обозначает выходной размер слоя. Это самый важный параметр, который мы можем установить для этого слоя. Параметр unit фактически определяет размер матрицы весов и вектора смещения (вектор смещения будет того же размера, но матрица весов будет рассчитываться на основе размера входных данных, чтобы произведение точек получило данные, которые имеют выходной размер, ед.)\n",
    "\n",
    "- activation - Этот параметр устанавливает поэлементную функцию активации, которая будет использоваться в плотном слое. По умолчанию мы видим, что для него установлено значение Нет. Это означает, что по умолчанию это линейная активация.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K fold validation\n",
    "\n",
    "classifier = Sequential()\n",
    "    #First Hidden Layer\n",
    "classifier.add(Dense(8, activation='relu', kernel_initializer='random_normal', input_dim=52))#5606\n",
    "    \n",
    "    #Second  Hidden Layer\n",
    "   # classifier.add(Dense(8, activation='relu', kernel_initializer='random_normal'))\n",
    "#Output Layer\n",
    "classifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "#Compiling the neural network\n",
    "classifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16887/16887 [==============================] - ETA: 2:50 - loss: 0.6942 - accuracy: 0.40 - ETA: 4s - loss: 0.6929 - accuracy: 0.5179 - ETA: 2s - loss: 0.6923 - accuracy: 0.55 - ETA: 2s - loss: 0.6906 - accuracy: 0.55 - ETA: 1s - loss: 0.6881 - accuracy: 0.57 - ETA: 1s - loss: 0.6841 - accuracy: 0.58 - ETA: 1s - loss: 0.6788 - accuracy: 0.60 - ETA: 1s - loss: 0.6731 - accuracy: 0.61 - ETA: 1s - loss: 0.6680 - accuracy: 0.62 - ETA: 1s - loss: 0.6622 - accuracy: 0.63 - ETA: 1s - loss: 0.6566 - accuracy: 0.63 - ETA: 1s - loss: 0.6514 - accuracy: 0.64 - ETA: 0s - loss: 0.6455 - accuracy: 0.65 - ETA: 0s - loss: 0.6426 - accuracy: 0.65 - ETA: 0s - loss: 0.6394 - accuracy: 0.65 - ETA: 0s - loss: 0.6357 - accuracy: 0.66 - ETA: 0s - loss: 0.6323 - accuracy: 0.66 - ETA: 0s - loss: 0.6293 - accuracy: 0.66 - ETA: 0s - loss: 0.6249 - accuracy: 0.67 - ETA: 0s - loss: 0.6233 - accuracy: 0.67 - ETA: 0s - loss: 0.6214 - accuracy: 0.67 - ETA: 0s - loss: 0.6197 - accuracy: 0.67 - ETA: 0s - loss: 0.6176 - accuracy: 0.67 - ETA: 0s - loss: 0.6147 - accuracy: 0.67 - ETA: 0s - loss: 0.6119 - accuracy: 0.68 - ETA: 0s - loss: 0.6106 - accuracy: 0.68 - ETA: 0s - loss: 0.6086 - accuracy: 0.68 - ETA: 0s - loss: 0.6081 - accuracy: 0.68 - ETA: 0s - loss: 0.6078 - accuracy: 0.68 - 2s 90us/step - loss: 0.6070 - accuracy: 0.6847\n",
      "Epoch 2/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.4602 - accuracy: 0.80 - ETA: 1s - loss: 0.5571 - accuracy: 0.71 - ETA: 1s - loss: 0.5573 - accuracy: 0.72 - ETA: 1s - loss: 0.5650 - accuracy: 0.71 - ETA: 1s - loss: 0.5681 - accuracy: 0.71 - ETA: 1s - loss: 0.5688 - accuracy: 0.71 - ETA: 1s - loss: 0.5695 - accuracy: 0.71 - ETA: 1s - loss: 0.5710 - accuracy: 0.71 - ETA: 1s - loss: 0.5656 - accuracy: 0.71 - ETA: 0s - loss: 0.5662 - accuracy: 0.71 - ETA: 0s - loss: 0.5650 - accuracy: 0.71 - ETA: 0s - loss: 0.5660 - accuracy: 0.71 - ETA: 0s - loss: 0.5654 - accuracy: 0.71 - ETA: 0s - loss: 0.5629 - accuracy: 0.71 - ETA: 0s - loss: 0.5646 - accuracy: 0.71 - ETA: 0s - loss: 0.5662 - accuracy: 0.71 - ETA: 0s - loss: 0.5648 - accuracy: 0.71 - ETA: 0s - loss: 0.5633 - accuracy: 0.71 - ETA: 0s - loss: 0.5654 - accuracy: 0.71 - ETA: 0s - loss: 0.5658 - accuracy: 0.71 - ETA: 0s - loss: 0.5654 - accuracy: 0.71 - ETA: 0s - loss: 0.5652 - accuracy: 0.71 - ETA: 0s - loss: 0.5655 - accuracy: 0.71 - ETA: 0s - loss: 0.5653 - accuracy: 0.71 - ETA: 0s - loss: 0.5662 - accuracy: 0.71 - ETA: 0s - loss: 0.5657 - accuracy: 0.71 - ETA: 0s - loss: 0.5653 - accuracy: 0.71 - ETA: 0s - loss: 0.5643 - accuracy: 0.71 - ETA: 0s - loss: 0.5638 - accuracy: 0.71 - 1s 84us/step - loss: 0.5641 - accuracy: 0.7193\n",
      "Epoch 3/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.6002 - accuracy: 0.60 - ETA: 1s - loss: 0.5215 - accuracy: 0.73 - ETA: 1s - loss: 0.5476 - accuracy: 0.71 - ETA: 1s - loss: 0.5504 - accuracy: 0.71 - ETA: 1s - loss: 0.5531 - accuracy: 0.72 - ETA: 1s - loss: 0.5487 - accuracy: 0.72 - ETA: 1s - loss: 0.5523 - accuracy: 0.72 - ETA: 1s - loss: 0.5500 - accuracy: 0.72 - ETA: 0s - loss: 0.5494 - accuracy: 0.72 - ETA: 0s - loss: 0.5516 - accuracy: 0.72 - ETA: 0s - loss: 0.5515 - accuracy: 0.72 - ETA: 0s - loss: 0.5522 - accuracy: 0.72 - ETA: 0s - loss: 0.5510 - accuracy: 0.72 - ETA: 0s - loss: 0.5513 - accuracy: 0.73 - ETA: 0s - loss: 0.5514 - accuracy: 0.73 - ETA: 0s - loss: 0.5524 - accuracy: 0.72 - ETA: 0s - loss: 0.5538 - accuracy: 0.72 - ETA: 0s - loss: 0.5512 - accuracy: 0.73 - ETA: 0s - loss: 0.5516 - accuracy: 0.73 - ETA: 0s - loss: 0.5508 - accuracy: 0.73 - ETA: 0s - loss: 0.5511 - accuracy: 0.73 - ETA: 0s - loss: 0.5519 - accuracy: 0.72 - ETA: 0s - loss: 0.5529 - accuracy: 0.72 - ETA: 0s - loss: 0.5526 - accuracy: 0.72 - ETA: 0s - loss: 0.5536 - accuracy: 0.72 - ETA: 0s - loss: 0.5529 - accuracy: 0.72 - ETA: 0s - loss: 0.5530 - accuracy: 0.72 - ETA: 0s - loss: 0.5535 - accuracy: 0.72 - ETA: 0s - loss: 0.5541 - accuracy: 0.72 - 1s 85us/step - loss: 0.5538 - accuracy: 0.7291\n",
      "Epoch 4/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.6171 - accuracy: 0.60 - ETA: 1s - loss: 0.5642 - accuracy: 0.72 - ETA: 1s - loss: 0.5448 - accuracy: 0.73 - ETA: 1s - loss: 0.5501 - accuracy: 0.72 - ETA: 1s - loss: 0.5525 - accuracy: 0.72 - ETA: 1s - loss: 0.5531 - accuracy: 0.72 - ETA: 1s - loss: 0.5523 - accuracy: 0.72 - ETA: 1s - loss: 0.5519 - accuracy: 0.72 - ETA: 1s - loss: 0.5549 - accuracy: 0.72 - ETA: 0s - loss: 0.5540 - accuracy: 0.72 - ETA: 0s - loss: 0.5516 - accuracy: 0.72 - ETA: 0s - loss: 0.5522 - accuracy: 0.72 - ETA: 0s - loss: 0.5487 - accuracy: 0.72 - ETA: 0s - loss: 0.5475 - accuracy: 0.72 - ETA: 0s - loss: 0.5474 - accuracy: 0.72 - ETA: 0s - loss: 0.5462 - accuracy: 0.72 - ETA: 0s - loss: 0.5466 - accuracy: 0.72 - ETA: 0s - loss: 0.5471 - accuracy: 0.72 - ETA: 0s - loss: 0.5458 - accuracy: 0.72 - ETA: 0s - loss: 0.5458 - accuracy: 0.72 - ETA: 0s - loss: 0.5446 - accuracy: 0.73 - ETA: 0s - loss: 0.5447 - accuracy: 0.73 - ETA: 0s - loss: 0.5444 - accuracy: 0.73 - ETA: 0s - loss: 0.5429 - accuracy: 0.73 - ETA: 0s - loss: 0.5437 - accuracy: 0.73 - ETA: 0s - loss: 0.5445 - accuracy: 0.73 - ETA: 0s - loss: 0.5443 - accuracy: 0.73 - ETA: 0s - loss: 0.5451 - accuracy: 0.73 - ETA: 0s - loss: 0.5453 - accuracy: 0.73 - 1s 84us/step - loss: 0.5454 - accuracy: 0.7319\n",
      "Epoch 5/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.4107 - accuracy: 0.90 - ETA: 1s - loss: 0.5540 - accuracy: 0.71 - ETA: 1s - loss: 0.5471 - accuracy: 0.73 - ETA: 1s - loss: 0.5492 - accuracy: 0.73 - ETA: 1s - loss: 0.5510 - accuracy: 0.72 - ETA: 1s - loss: 0.5492 - accuracy: 0.72 - ETA: 1s - loss: 0.5472 - accuracy: 0.72 - ETA: 1s - loss: 0.5423 - accuracy: 0.73 - ETA: 0s - loss: 0.5441 - accuracy: 0.73 - ETA: 0s - loss: 0.5475 - accuracy: 0.72 - ETA: 0s - loss: 0.5512 - accuracy: 0.72 - ETA: 0s - loss: 0.5505 - accuracy: 0.72 - ETA: 0s - loss: 0.5467 - accuracy: 0.72 - ETA: 0s - loss: 0.5444 - accuracy: 0.73 - ETA: 0s - loss: 0.5425 - accuracy: 0.73 - ETA: 0s - loss: 0.5423 - accuracy: 0.73 - ETA: 0s - loss: 0.5421 - accuracy: 0.73 - ETA: 0s - loss: 0.5409 - accuracy: 0.73 - ETA: 0s - loss: 0.5405 - accuracy: 0.73 - ETA: 0s - loss: 0.5412 - accuracy: 0.73 - ETA: 0s - loss: 0.5398 - accuracy: 0.73 - ETA: 0s - loss: 0.5395 - accuracy: 0.73 - ETA: 0s - loss: 0.5385 - accuracy: 0.73 - ETA: 0s - loss: 0.5386 - accuracy: 0.73 - ETA: 0s - loss: 0.5394 - accuracy: 0.73 - ETA: 0s - loss: 0.5381 - accuracy: 0.73 - ETA: 0s - loss: 0.5391 - accuracy: 0.73 - ETA: 0s - loss: 0.5387 - accuracy: 0.73 - ETA: 0s - loss: 0.5381 - accuracy: 0.73 - 1s 84us/step - loss: 0.5379 - accuracy: 0.7373\n",
      "Epoch 6/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.4738 - accuracy: 0.90 - ETA: 1s - loss: 0.5071 - accuracy: 0.75 - ETA: 1s - loss: 0.5201 - accuracy: 0.74 - ETA: 1s - loss: 0.5342 - accuracy: 0.73 - ETA: 1s - loss: 0.5297 - accuracy: 0.73 - ETA: 1s - loss: 0.5316 - accuracy: 0.73 - ETA: 1s - loss: 0.5291 - accuracy: 0.73 - ETA: 1s - loss: 0.5318 - accuracy: 0.74 - ETA: 1s - loss: 0.5327 - accuracy: 0.73 - ETA: 1s - loss: 0.5325 - accuracy: 0.73 - ETA: 0s - loss: 0.5336 - accuracy: 0.73 - ETA: 0s - loss: 0.5330 - accuracy: 0.73 - ETA: 0s - loss: 0.5339 - accuracy: 0.73 - ETA: 0s - loss: 0.5352 - accuracy: 0.73 - ETA: 0s - loss: 0.5346 - accuracy: 0.73 - ETA: 0s - loss: 0.5330 - accuracy: 0.73 - ETA: 0s - loss: 0.5321 - accuracy: 0.74 - ETA: 0s - loss: 0.5306 - accuracy: 0.74 - ETA: 0s - loss: 0.5320 - accuracy: 0.74 - ETA: 0s - loss: 0.5298 - accuracy: 0.74 - ETA: 0s - loss: 0.5296 - accuracy: 0.74 - ETA: 0s - loss: 0.5305 - accuracy: 0.74 - ETA: 0s - loss: 0.5318 - accuracy: 0.74 - ETA: 0s - loss: 0.5327 - accuracy: 0.73 - ETA: 0s - loss: 0.5332 - accuracy: 0.73 - ETA: 0s - loss: 0.5333 - accuracy: 0.73 - ETA: 0s - loss: 0.5325 - accuracy: 0.74 - ETA: 0s - loss: 0.5323 - accuracy: 0.74 - ETA: 0s - loss: 0.5310 - accuracy: 0.74 - 1s 85us/step - loss: 0.5307 - accuracy: 0.7414\n",
      "Epoch 7/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.5293 - accuracy: 0.90 - ETA: 1s - loss: 0.5307 - accuracy: 0.74 - ETA: 1s - loss: 0.5284 - accuracy: 0.75 - ETA: 1s - loss: 0.5166 - accuracy: 0.76 - ETA: 1s - loss: 0.5227 - accuracy: 0.75 - ETA: 1s - loss: 0.5172 - accuracy: 0.75 - ETA: 1s - loss: 0.5177 - accuracy: 0.75 - ETA: 1s - loss: 0.5152 - accuracy: 0.75 - ETA: 0s - loss: 0.5154 - accuracy: 0.75 - ETA: 0s - loss: 0.5169 - accuracy: 0.75 - ETA: 0s - loss: 0.5171 - accuracy: 0.75 - ETA: 0s - loss: 0.5184 - accuracy: 0.75 - ETA: 0s - loss: 0.5179 - accuracy: 0.75 - ETA: 0s - loss: 0.5178 - accuracy: 0.75 - ETA: 0s - loss: 0.5175 - accuracy: 0.75 - ETA: 0s - loss: 0.5181 - accuracy: 0.75 - ETA: 0s - loss: 0.5173 - accuracy: 0.75 - ETA: 0s - loss: 0.5164 - accuracy: 0.75 - ETA: 0s - loss: 0.5166 - accuracy: 0.75 - ETA: 0s - loss: 0.5170 - accuracy: 0.75 - ETA: 0s - loss: 0.5184 - accuracy: 0.75 - ETA: 0s - loss: 0.5207 - accuracy: 0.75 - ETA: 0s - loss: 0.5209 - accuracy: 0.75 - ETA: 0s - loss: 0.5215 - accuracy: 0.74 - ETA: 0s - loss: 0.5213 - accuracy: 0.74 - ETA: 0s - loss: 0.5219 - accuracy: 0.74 - ETA: 0s - loss: 0.5232 - accuracy: 0.74 - ETA: 0s - loss: 0.5232 - accuracy: 0.74 - 1s 82us/step - loss: 0.5234 - accuracy: 0.7470\n",
      "Epoch 8/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.3136 - accuracy: 0.80 - ETA: 1s - loss: 0.5200 - accuracy: 0.73 - ETA: 1s - loss: 0.5096 - accuracy: 0.74 - ETA: 1s - loss: 0.5034 - accuracy: 0.75 - ETA: 1s - loss: 0.5056 - accuracy: 0.75 - ETA: 1s - loss: 0.5086 - accuracy: 0.75 - ETA: 1s - loss: 0.5117 - accuracy: 0.75 - ETA: 1s - loss: 0.5140 - accuracy: 0.75 - ETA: 0s - loss: 0.5149 - accuracy: 0.75 - ETA: 0s - loss: 0.5155 - accuracy: 0.75 - ETA: 0s - loss: 0.5161 - accuracy: 0.75 - ETA: 0s - loss: 0.5163 - accuracy: 0.75 - ETA: 0s - loss: 0.5168 - accuracy: 0.75 - ETA: 0s - loss: 0.5155 - accuracy: 0.75 - ETA: 0s - loss: 0.5171 - accuracy: 0.75 - ETA: 0s - loss: 0.5158 - accuracy: 0.75 - ETA: 0s - loss: 0.5151 - accuracy: 0.75 - ETA: 0s - loss: 0.5162 - accuracy: 0.75 - ETA: 0s - loss: 0.5172 - accuracy: 0.75 - ETA: 0s - loss: 0.5175 - accuracy: 0.75 - ETA: 0s - loss: 0.5169 - accuracy: 0.75 - ETA: 0s - loss: 0.5172 - accuracy: 0.75 - ETA: 0s - loss: 0.5174 - accuracy: 0.75 - ETA: 0s - loss: 0.5173 - accuracy: 0.75 - ETA: 0s - loss: 0.5164 - accuracy: 0.75 - ETA: 0s - loss: 0.5177 - accuracy: 0.75 - ETA: 0s - loss: 0.5179 - accuracy: 0.75 - ETA: 0s - loss: 0.5184 - accuracy: 0.75 - 1s 82us/step - loss: 0.5184 - accuracy: 0.7497\n",
      "Epoch 9/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.5944 - accuracy: 0.90 - ETA: 1s - loss: 0.5079 - accuracy: 0.75 - ETA: 1s - loss: 0.5255 - accuracy: 0.73 - ETA: 1s - loss: 0.5193 - accuracy: 0.74 - ETA: 1s - loss: 0.5134 - accuracy: 0.75 - ETA: 1s - loss: 0.5165 - accuracy: 0.74 - ETA: 1s - loss: 0.5177 - accuracy: 0.74 - ETA: 1s - loss: 0.5158 - accuracy: 0.75 - ETA: 1s - loss: 0.5191 - accuracy: 0.74 - ETA: 0s - loss: 0.5179 - accuracy: 0.74 - ETA: 0s - loss: 0.5145 - accuracy: 0.74 - ETA: 0s - loss: 0.5172 - accuracy: 0.74 - ETA: 0s - loss: 0.5181 - accuracy: 0.74 - ETA: 0s - loss: 0.5172 - accuracy: 0.75 - ETA: 0s - loss: 0.5158 - accuracy: 0.75 - ETA: 0s - loss: 0.5142 - accuracy: 0.75 - ETA: 0s - loss: 0.5122 - accuracy: 0.75 - ETA: 0s - loss: 0.5125 - accuracy: 0.75 - ETA: 0s - loss: 0.5136 - accuracy: 0.75 - ETA: 0s - loss: 0.5129 - accuracy: 0.75 - ETA: 0s - loss: 0.5120 - accuracy: 0.75 - ETA: 0s - loss: 0.5118 - accuracy: 0.75 - ETA: 0s - loss: 0.5116 - accuracy: 0.75 - ETA: 0s - loss: 0.5108 - accuracy: 0.75 - ETA: 0s - loss: 0.5108 - accuracy: 0.75 - ETA: 0s - loss: 0.5112 - accuracy: 0.75 - ETA: 0s - loss: 0.5116 - accuracy: 0.75 - ETA: 0s - loss: 0.5116 - accuracy: 0.75 - ETA: 0s - loss: 0.5120 - accuracy: 0.75 - ETA: 0s - loss: 0.5124 - accuracy: 0.75 - 1s 87us/step - loss: 0.5124 - accuracy: 0.7563\n",
      "Epoch 10/100\n",
      "16887/16887 [==============================] - ETA: 4s - loss: 0.2446 - accuracy: 1.00 - ETA: 1s - loss: 0.5090 - accuracy: 0.74 - ETA: 1s - loss: 0.5169 - accuracy: 0.74 - ETA: 1s - loss: 0.5209 - accuracy: 0.74 - ETA: 1s - loss: 0.5160 - accuracy: 0.74 - ETA: 1s - loss: 0.5211 - accuracy: 0.74 - ETA: 1s - loss: 0.5184 - accuracy: 0.75 - ETA: 1s - loss: 0.5124 - accuracy: 0.75 - ETA: 1s - loss: 0.5076 - accuracy: 0.75 - ETA: 0s - loss: 0.5107 - accuracy: 0.75 - ETA: 0s - loss: 0.5160 - accuracy: 0.75 - ETA: 0s - loss: 0.5170 - accuracy: 0.75 - ETA: 0s - loss: 0.5133 - accuracy: 0.75 - ETA: 0s - loss: 0.5139 - accuracy: 0.75 - ETA: 0s - loss: 0.5136 - accuracy: 0.75 - ETA: 0s - loss: 0.5137 - accuracy: 0.75 - ETA: 0s - loss: 0.5131 - accuracy: 0.75 - ETA: 0s - loss: 0.5122 - accuracy: 0.75 - ETA: 0s - loss: 0.5110 - accuracy: 0.75 - ETA: 0s - loss: 0.5106 - accuracy: 0.75 - ETA: 0s - loss: 0.5099 - accuracy: 0.75 - ETA: 0s - loss: 0.5082 - accuracy: 0.75 - ETA: 0s - loss: 0.5097 - accuracy: 0.75 - ETA: 0s - loss: 0.5093 - accuracy: 0.75 - ETA: 0s - loss: 0.5082 - accuracy: 0.75 - ETA: 0s - loss: 0.5070 - accuracy: 0.75 - ETA: 0s - loss: 0.5081 - accuracy: 0.75 - ETA: 0s - loss: 0.5073 - accuracy: 0.76 - ETA: 0s - loss: 0.5074 - accuracy: 0.75 - 1s 84us/step - loss: 0.5071 - accuracy: 0.7601\n",
      "Epoch 11/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.4899 - accuracy: 0.80 - ETA: 1s - loss: 0.4735 - accuracy: 0.78 - ETA: 1s - loss: 0.4974 - accuracy: 0.76 - ETA: 1s - loss: 0.5032 - accuracy: 0.75 - ETA: 1s - loss: 0.5025 - accuracy: 0.76 - ETA: 1s - loss: 0.5026 - accuracy: 0.76 - ETA: 1s - loss: 0.4997 - accuracy: 0.76 - ETA: 1s - loss: 0.4980 - accuracy: 0.76 - ETA: 0s - loss: 0.4989 - accuracy: 0.76 - ETA: 0s - loss: 0.4992 - accuracy: 0.76 - ETA: 0s - loss: 0.4983 - accuracy: 0.76 - ETA: 0s - loss: 0.4987 - accuracy: 0.76 - ETA: 0s - loss: 0.4979 - accuracy: 0.76 - ETA: 0s - loss: 0.4967 - accuracy: 0.76 - ETA: 0s - loss: 0.4974 - accuracy: 0.76 - ETA: 0s - loss: 0.4962 - accuracy: 0.76 - ETA: 0s - loss: 0.4949 - accuracy: 0.76 - ETA: 0s - loss: 0.4947 - accuracy: 0.76 - ETA: 0s - loss: 0.4969 - accuracy: 0.76 - ETA: 0s - loss: 0.4961 - accuracy: 0.76 - ETA: 0s - loss: 0.4971 - accuracy: 0.76 - ETA: 0s - loss: 0.4978 - accuracy: 0.76 - ETA: 0s - loss: 0.4980 - accuracy: 0.76 - ETA: 0s - loss: 0.4998 - accuracy: 0.76 - ETA: 0s - loss: 0.5009 - accuracy: 0.76 - ETA: 0s - loss: 0.5018 - accuracy: 0.76 - ETA: 0s - loss: 0.5025 - accuracy: 0.76 - ETA: 0s - loss: 0.5018 - accuracy: 0.76 - ETA: 0s - loss: 0.5013 - accuracy: 0.76 - 1s 85us/step - loss: 0.5009 - accuracy: 0.7645\n",
      "Epoch 12/100\n",
      "16887/16887 [==============================] - ETA: 4s - loss: 0.5096 - accuracy: 0.80 - ETA: 1s - loss: 0.4930 - accuracy: 0.77 - ETA: 1s - loss: 0.4992 - accuracy: 0.76 - ETA: 1s - loss: 0.5104 - accuracy: 0.75 - ETA: 1s - loss: 0.5062 - accuracy: 0.76 - ETA: 1s - loss: 0.5092 - accuracy: 0.75 - ETA: 1s - loss: 0.5063 - accuracy: 0.75 - ETA: 1s - loss: 0.5027 - accuracy: 0.76 - ETA: 1s - loss: 0.4988 - accuracy: 0.76 - ETA: 0s - loss: 0.4992 - accuracy: 0.76 - ETA: 0s - loss: 0.5012 - accuracy: 0.76 - ETA: 0s - loss: 0.5003 - accuracy: 0.76 - ETA: 0s - loss: 0.5018 - accuracy: 0.76 - ETA: 0s - loss: 0.5034 - accuracy: 0.76 - ETA: 0s - loss: 0.5016 - accuracy: 0.76 - ETA: 0s - loss: 0.5003 - accuracy: 0.76 - ETA: 0s - loss: 0.4990 - accuracy: 0.76 - ETA: 0s - loss: 0.5002 - accuracy: 0.76 - ETA: 0s - loss: 0.5007 - accuracy: 0.76 - ETA: 0s - loss: 0.4981 - accuracy: 0.76 - ETA: 0s - loss: 0.4983 - accuracy: 0.76 - ETA: 0s - loss: 0.4976 - accuracy: 0.76 - ETA: 0s - loss: 0.4983 - accuracy: 0.76 - ETA: 0s - loss: 0.4981 - accuracy: 0.76 - ETA: 0s - loss: 0.4974 - accuracy: 0.76 - ETA: 0s - loss: 0.4975 - accuracy: 0.76 - ETA: 0s - loss: 0.4976 - accuracy: 0.76 - ETA: 0s - loss: 0.4961 - accuracy: 0.76 - ETA: 0s - loss: 0.4959 - accuracy: 0.76 - 1s 84us/step - loss: 0.4959 - accuracy: 0.7665\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16887/16887 [==============================] - ETA: 5s - loss: 0.3979 - accuracy: 0.90 - ETA: 1s - loss: 0.4863 - accuracy: 0.77 - ETA: 1s - loss: 0.5035 - accuracy: 0.75 - ETA: 1s - loss: 0.4942 - accuracy: 0.76 - ETA: 1s - loss: 0.4936 - accuracy: 0.77 - ETA: 1s - loss: 0.4920 - accuracy: 0.77 - ETA: 1s - loss: 0.4927 - accuracy: 0.77 - ETA: 1s - loss: 0.4911 - accuracy: 0.77 - ETA: 1s - loss: 0.4941 - accuracy: 0.76 - ETA: 0s - loss: 0.4950 - accuracy: 0.76 - ETA: 0s - loss: 0.4950 - accuracy: 0.76 - ETA: 0s - loss: 0.4942 - accuracy: 0.76 - ETA: 0s - loss: 0.4930 - accuracy: 0.76 - ETA: 0s - loss: 0.4950 - accuracy: 0.76 - ETA: 0s - loss: 0.4941 - accuracy: 0.76 - ETA: 0s - loss: 0.4937 - accuracy: 0.76 - ETA: 0s - loss: 0.4936 - accuracy: 0.76 - ETA: 0s - loss: 0.4934 - accuracy: 0.76 - ETA: 0s - loss: 0.4926 - accuracy: 0.77 - ETA: 0s - loss: 0.4910 - accuracy: 0.77 - ETA: 0s - loss: 0.4903 - accuracy: 0.77 - ETA: 0s - loss: 0.4894 - accuracy: 0.77 - ETA: 0s - loss: 0.4903 - accuracy: 0.77 - ETA: 0s - loss: 0.4900 - accuracy: 0.77 - ETA: 0s - loss: 0.4902 - accuracy: 0.77 - ETA: 0s - loss: 0.4910 - accuracy: 0.77 - ETA: 0s - loss: 0.4906 - accuracy: 0.77 - ETA: 0s - loss: 0.4899 - accuracy: 0.77 - ETA: 0s - loss: 0.4895 - accuracy: 0.77 - 1s 85us/step - loss: 0.4896 - accuracy: 0.7736\n",
      "Epoch 14/100\n",
      "16887/16887 [==============================] - ETA: 4s - loss: 0.3546 - accuracy: 0.90 - ETA: 1s - loss: 0.4762 - accuracy: 0.78 - ETA: 1s - loss: 0.4857 - accuracy: 0.77 - ETA: 1s - loss: 0.4902 - accuracy: 0.76 - ETA: 1s - loss: 0.4910 - accuracy: 0.76 - ETA: 1s - loss: 0.4818 - accuracy: 0.77 - ETA: 1s - loss: 0.4843 - accuracy: 0.77 - ETA: 1s - loss: 0.4823 - accuracy: 0.77 - ETA: 1s - loss: 0.4854 - accuracy: 0.77 - ETA: 0s - loss: 0.4811 - accuracy: 0.77 - ETA: 0s - loss: 0.4811 - accuracy: 0.78 - ETA: 0s - loss: 0.4784 - accuracy: 0.78 - ETA: 0s - loss: 0.4793 - accuracy: 0.77 - ETA: 0s - loss: 0.4822 - accuracy: 0.77 - ETA: 0s - loss: 0.4811 - accuracy: 0.78 - ETA: 0s - loss: 0.4822 - accuracy: 0.77 - ETA: 0s - loss: 0.4822 - accuracy: 0.77 - ETA: 0s - loss: 0.4814 - accuracy: 0.77 - ETA: 0s - loss: 0.4789 - accuracy: 0.78 - ETA: 0s - loss: 0.4801 - accuracy: 0.78 - ETA: 0s - loss: 0.4814 - accuracy: 0.77 - ETA: 0s - loss: 0.4820 - accuracy: 0.77 - ETA: 0s - loss: 0.4826 - accuracy: 0.77 - ETA: 0s - loss: 0.4824 - accuracy: 0.77 - ETA: 0s - loss: 0.4822 - accuracy: 0.77 - ETA: 0s - loss: 0.4836 - accuracy: 0.77 - ETA: 0s - loss: 0.4830 - accuracy: 0.77 - ETA: 0s - loss: 0.4832 - accuracy: 0.77 - ETA: 0s - loss: 0.4834 - accuracy: 0.77 - 1s 85us/step - loss: 0.4835 - accuracy: 0.7776\n",
      "Epoch 15/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.5347 - accuracy: 0.60 - ETA: 1s - loss: 0.5002 - accuracy: 0.75 - ETA: 1s - loss: 0.4886 - accuracy: 0.76 - ETA: 1s - loss: 0.4786 - accuracy: 0.78 - ETA: 1s - loss: 0.4835 - accuracy: 0.77 - ETA: 1s - loss: 0.4822 - accuracy: 0.77 - ETA: 1s - loss: 0.4819 - accuracy: 0.77 - ETA: 1s - loss: 0.4803 - accuracy: 0.78 - ETA: 1s - loss: 0.4752 - accuracy: 0.78 - ETA: 0s - loss: 0.4798 - accuracy: 0.78 - ETA: 0s - loss: 0.4799 - accuracy: 0.78 - ETA: 0s - loss: 0.4804 - accuracy: 0.78 - ETA: 0s - loss: 0.4826 - accuracy: 0.77 - ETA: 0s - loss: 0.4828 - accuracy: 0.77 - ETA: 0s - loss: 0.4820 - accuracy: 0.77 - ETA: 0s - loss: 0.4822 - accuracy: 0.77 - ETA: 0s - loss: 0.4809 - accuracy: 0.77 - ETA: 0s - loss: 0.4782 - accuracy: 0.77 - ETA: 0s - loss: 0.4764 - accuracy: 0.78 - ETA: 0s - loss: 0.4766 - accuracy: 0.78 - ETA: 0s - loss: 0.4772 - accuracy: 0.78 - ETA: 0s - loss: 0.4800 - accuracy: 0.77 - ETA: 0s - loss: 0.4791 - accuracy: 0.77 - ETA: 0s - loss: 0.4791 - accuracy: 0.77 - ETA: 0s - loss: 0.4795 - accuracy: 0.77 - ETA: 0s - loss: 0.4787 - accuracy: 0.77 - ETA: 0s - loss: 0.4786 - accuracy: 0.77 - ETA: 0s - loss: 0.4791 - accuracy: 0.77 - ETA: 0s - loss: 0.4790 - accuracy: 0.77 - 1s 84us/step - loss: 0.4790 - accuracy: 0.7789\n",
      "Epoch 16/100\n",
      "16887/16887 [==============================] - ETA: 4s - loss: 0.4171 - accuracy: 0.80 - ETA: 1s - loss: 0.4745 - accuracy: 0.80 - ETA: 1s - loss: 0.4555 - accuracy: 0.80 - ETA: 1s - loss: 0.4677 - accuracy: 0.79 - ETA: 1s - loss: 0.4759 - accuracy: 0.79 - ETA: 1s - loss: 0.4729 - accuracy: 0.78 - ETA: 1s - loss: 0.4710 - accuracy: 0.79 - ETA: 1s - loss: 0.4738 - accuracy: 0.78 - ETA: 1s - loss: 0.4723 - accuracy: 0.78 - ETA: 0s - loss: 0.4708 - accuracy: 0.78 - ETA: 0s - loss: 0.4688 - accuracy: 0.78 - ETA: 0s - loss: 0.4704 - accuracy: 0.78 - ETA: 0s - loss: 0.4706 - accuracy: 0.78 - ETA: 0s - loss: 0.4706 - accuracy: 0.78 - ETA: 0s - loss: 0.4695 - accuracy: 0.78 - ETA: 0s - loss: 0.4713 - accuracy: 0.78 - ETA: 0s - loss: 0.4714 - accuracy: 0.78 - ETA: 0s - loss: 0.4715 - accuracy: 0.78 - ETA: 0s - loss: 0.4741 - accuracy: 0.78 - ETA: 0s - loss: 0.4726 - accuracy: 0.78 - ETA: 0s - loss: 0.4728 - accuracy: 0.78 - ETA: 0s - loss: 0.4729 - accuracy: 0.78 - ETA: 0s - loss: 0.4737 - accuracy: 0.78 - ETA: 0s - loss: 0.4731 - accuracy: 0.78 - ETA: 0s - loss: 0.4723 - accuracy: 0.78 - ETA: 0s - loss: 0.4730 - accuracy: 0.78 - ETA: 0s - loss: 0.4731 - accuracy: 0.78 - ETA: 0s - loss: 0.4721 - accuracy: 0.78 - ETA: 0s - loss: 0.4735 - accuracy: 0.78 - 1s 85us/step - loss: 0.4739 - accuracy: 0.7836\n",
      "Epoch 17/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.6868 - accuracy: 0.80 - ETA: 1s - loss: 0.4506 - accuracy: 0.80 - ETA: 1s - loss: 0.4800 - accuracy: 0.78 - ETA: 1s - loss: 0.4837 - accuracy: 0.78 - ETA: 1s - loss: 0.4828 - accuracy: 0.78 - ETA: 1s - loss: 0.4877 - accuracy: 0.78 - ETA: 1s - loss: 0.4811 - accuracy: 0.78 - ETA: 1s - loss: 0.4754 - accuracy: 0.78 - ETA: 1s - loss: 0.4724 - accuracy: 0.78 - ETA: 0s - loss: 0.4743 - accuracy: 0.78 - ETA: 0s - loss: 0.4750 - accuracy: 0.78 - ETA: 0s - loss: 0.4718 - accuracy: 0.78 - ETA: 0s - loss: 0.4734 - accuracy: 0.78 - ETA: 0s - loss: 0.4738 - accuracy: 0.78 - ETA: 0s - loss: 0.4734 - accuracy: 0.78 - ETA: 0s - loss: 0.4715 - accuracy: 0.78 - ETA: 0s - loss: 0.4704 - accuracy: 0.78 - ETA: 0s - loss: 0.4683 - accuracy: 0.78 - ETA: 0s - loss: 0.4659 - accuracy: 0.79 - ETA: 0s - loss: 0.4645 - accuracy: 0.79 - ETA: 0s - loss: 0.4655 - accuracy: 0.78 - ETA: 0s - loss: 0.4682 - accuracy: 0.78 - ETA: 0s - loss: 0.4699 - accuracy: 0.78 - ETA: 0s - loss: 0.4709 - accuracy: 0.78 - ETA: 0s - loss: 0.4701 - accuracy: 0.78 - ETA: 0s - loss: 0.4693 - accuracy: 0.78 - ETA: 0s - loss: 0.4698 - accuracy: 0.78 - ETA: 0s - loss: 0.4691 - accuracy: 0.78 - ETA: 0s - loss: 0.4705 - accuracy: 0.78 - 1s 85us/step - loss: 0.4705 - accuracy: 0.7851\n",
      "Epoch 18/100\n",
      "16887/16887 [==============================] - ETA: 4s - loss: 0.4033 - accuracy: 0.80 - ETA: 1s - loss: 0.4805 - accuracy: 0.77 - ETA: 1s - loss: 0.4814 - accuracy: 0.78 - ETA: 1s - loss: 0.4682 - accuracy: 0.79 - ETA: 1s - loss: 0.4680 - accuracy: 0.79 - ETA: 1s - loss: 0.4659 - accuracy: 0.79 - ETA: 1s - loss: 0.4661 - accuracy: 0.78 - ETA: 1s - loss: 0.4624 - accuracy: 0.78 - ETA: 1s - loss: 0.4695 - accuracy: 0.78 - ETA: 0s - loss: 0.4681 - accuracy: 0.78 - ETA: 0s - loss: 0.4665 - accuracy: 0.78 - ETA: 0s - loss: 0.4679 - accuracy: 0.78 - ETA: 0s - loss: 0.4660 - accuracy: 0.78 - ETA: 0s - loss: 0.4663 - accuracy: 0.79 - ETA: 0s - loss: 0.4656 - accuracy: 0.79 - ETA: 0s - loss: 0.4648 - accuracy: 0.79 - ETA: 0s - loss: 0.4660 - accuracy: 0.79 - ETA: 0s - loss: 0.4651 - accuracy: 0.79 - ETA: 0s - loss: 0.4657 - accuracy: 0.79 - ETA: 0s - loss: 0.4647 - accuracy: 0.79 - ETA: 0s - loss: 0.4655 - accuracy: 0.79 - ETA: 0s - loss: 0.4649 - accuracy: 0.79 - ETA: 0s - loss: 0.4657 - accuracy: 0.79 - ETA: 0s - loss: 0.4665 - accuracy: 0.79 - ETA: 0s - loss: 0.4653 - accuracy: 0.79 - ETA: 0s - loss: 0.4628 - accuracy: 0.79 - ETA: 0s - loss: 0.4627 - accuracy: 0.79 - ETA: 0s - loss: 0.4643 - accuracy: 0.79 - ETA: 0s - loss: 0.4655 - accuracy: 0.79 - 1s 84us/step - loss: 0.4658 - accuracy: 0.7916\n",
      "Epoch 19/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.3618 - accuracy: 0.90 - ETA: 1s - loss: 0.4658 - accuracy: 0.79 - ETA: 1s - loss: 0.4531 - accuracy: 0.79 - ETA: 1s - loss: 0.4449 - accuracy: 0.79 - ETA: 1s - loss: 0.4502 - accuracy: 0.79 - ETA: 1s - loss: 0.4486 - accuracy: 0.79 - ETA: 1s - loss: 0.4565 - accuracy: 0.79 - ETA: 1s - loss: 0.4665 - accuracy: 0.78 - ETA: 1s - loss: 0.4679 - accuracy: 0.78 - ETA: 0s - loss: 0.4696 - accuracy: 0.78 - ETA: 0s - loss: 0.4719 - accuracy: 0.78 - ETA: 0s - loss: 0.4681 - accuracy: 0.78 - ETA: 0s - loss: 0.4652 - accuracy: 0.78 - ETA: 0s - loss: 0.4646 - accuracy: 0.78 - ETA: 0s - loss: 0.4656 - accuracy: 0.78 - ETA: 0s - loss: 0.4653 - accuracy: 0.79 - ETA: 0s - loss: 0.4636 - accuracy: 0.79 - ETA: 0s - loss: 0.4641 - accuracy: 0.79 - ETA: 0s - loss: 0.4650 - accuracy: 0.79 - ETA: 0s - loss: 0.4630 - accuracy: 0.79 - ETA: 0s - loss: 0.4636 - accuracy: 0.79 - ETA: 0s - loss: 0.4624 - accuracy: 0.79 - ETA: 0s - loss: 0.4620 - accuracy: 0.79 - ETA: 0s - loss: 0.4611 - accuracy: 0.79 - ETA: 0s - loss: 0.4609 - accuracy: 0.79 - ETA: 0s - loss: 0.4621 - accuracy: 0.79 - ETA: 0s - loss: 0.4623 - accuracy: 0.79 - ETA: 0s - loss: 0.4621 - accuracy: 0.79 - ETA: 0s - loss: 0.4618 - accuracy: 0.79 - 1s 85us/step - loss: 0.4621 - accuracy: 0.7922\n",
      "Epoch 20/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.8285 - accuracy: 0.40 - ETA: 1s - loss: 0.4948 - accuracy: 0.77 - ETA: 1s - loss: 0.4860 - accuracy: 0.77 - ETA: 1s - loss: 0.4846 - accuracy: 0.77 - ETA: 1s - loss: 0.4861 - accuracy: 0.77 - ETA: 1s - loss: 0.4815 - accuracy: 0.77 - ETA: 1s - loss: 0.4762 - accuracy: 0.78 - ETA: 1s - loss: 0.4689 - accuracy: 0.78 - ETA: 1s - loss: 0.4633 - accuracy: 0.79 - ETA: 0s - loss: 0.4631 - accuracy: 0.79 - ETA: 0s - loss: 0.4646 - accuracy: 0.79 - ETA: 0s - loss: 0.4612 - accuracy: 0.79 - ETA: 0s - loss: 0.4606 - accuracy: 0.79 - ETA: 0s - loss: 0.4610 - accuracy: 0.79 - ETA: 0s - loss: 0.4609 - accuracy: 0.79 - ETA: 0s - loss: 0.4618 - accuracy: 0.79 - ETA: 0s - loss: 0.4636 - accuracy: 0.79 - ETA: 0s - loss: 0.4652 - accuracy: 0.79 - ETA: 0s - loss: 0.4654 - accuracy: 0.79 - ETA: 0s - loss: 0.4650 - accuracy: 0.78 - ETA: 0s - loss: 0.4642 - accuracy: 0.79 - ETA: 0s - loss: 0.4629 - accuracy: 0.79 - ETA: 0s - loss: 0.4626 - accuracy: 0.79 - ETA: 0s - loss: 0.4636 - accuracy: 0.79 - ETA: 0s - loss: 0.4620 - accuracy: 0.79 - ETA: 0s - loss: 0.4609 - accuracy: 0.79 - ETA: 0s - loss: 0.4609 - accuracy: 0.79 - ETA: 0s - loss: 0.4595 - accuracy: 0.79 - ETA: 0s - loss: 0.4600 - accuracy: 0.79 - 1s 84us/step - loss: 0.4595 - accuracy: 0.7915\n",
      "Epoch 21/100\n",
      "16887/16887 [==============================] - ETA: 4s - loss: 0.4919 - accuracy: 0.70 - ETA: 1s - loss: 0.4602 - accuracy: 0.80 - ETA: 1s - loss: 0.4435 - accuracy: 0.80 - ETA: 1s - loss: 0.4615 - accuracy: 0.79 - ETA: 1s - loss: 0.4529 - accuracy: 0.80 - ETA: 1s - loss: 0.4605 - accuracy: 0.79 - ETA: 1s - loss: 0.4659 - accuracy: 0.78 - ETA: 1s - loss: 0.4686 - accuracy: 0.78 - ETA: 1s - loss: 0.4695 - accuracy: 0.79 - ETA: 0s - loss: 0.4655 - accuracy: 0.79 - ETA: 0s - loss: 0.4629 - accuracy: 0.79 - ETA: 0s - loss: 0.4633 - accuracy: 0.79 - ETA: 0s - loss: 0.4595 - accuracy: 0.79 - ETA: 0s - loss: 0.4574 - accuracy: 0.80 - ETA: 0s - loss: 0.4594 - accuracy: 0.79 - ETA: 0s - loss: 0.4600 - accuracy: 0.79 - ETA: 0s - loss: 0.4606 - accuracy: 0.79 - ETA: 0s - loss: 0.4620 - accuracy: 0.79 - ETA: 0s - loss: 0.4608 - accuracy: 0.79 - ETA: 0s - loss: 0.4583 - accuracy: 0.79 - ETA: 0s - loss: 0.4580 - accuracy: 0.79 - ETA: 0s - loss: 0.4574 - accuracy: 0.79 - ETA: 0s - loss: 0.4564 - accuracy: 0.79 - ETA: 0s - loss: 0.4556 - accuracy: 0.79 - ETA: 0s - loss: 0.4561 - accuracy: 0.79 - ETA: 0s - loss: 0.4555 - accuracy: 0.79 - ETA: 0s - loss: 0.4558 - accuracy: 0.79 - ETA: 0s - loss: 0.4557 - accuracy: 0.79 - ETA: 0s - loss: 0.4563 - accuracy: 0.79 - 1s 85us/step - loss: 0.4561 - accuracy: 0.7966\n",
      "Epoch 22/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.5780 - accuracy: 0.90 - ETA: 1s - loss: 0.4695 - accuracy: 0.78 - ETA: 1s - loss: 0.4682 - accuracy: 0.79 - ETA: 1s - loss: 0.4516 - accuracy: 0.80 - ETA: 1s - loss: 0.4544 - accuracy: 0.79 - ETA: 1s - loss: 0.4495 - accuracy: 0.79 - ETA: 1s - loss: 0.4513 - accuracy: 0.79 - ETA: 1s - loss: 0.4590 - accuracy: 0.79 - ETA: 1s - loss: 0.4598 - accuracy: 0.79 - ETA: 0s - loss: 0.4556 - accuracy: 0.79 - ETA: 0s - loss: 0.4535 - accuracy: 0.79 - ETA: 0s - loss: 0.4520 - accuracy: 0.79 - ETA: 0s - loss: 0.4564 - accuracy: 0.79 - ETA: 0s - loss: 0.4569 - accuracy: 0.79 - ETA: 0s - loss: 0.4586 - accuracy: 0.79 - ETA: 0s - loss: 0.4578 - accuracy: 0.79 - ETA: 0s - loss: 0.4574 - accuracy: 0.79 - ETA: 0s - loss: 0.4586 - accuracy: 0.79 - ETA: 0s - loss: 0.4576 - accuracy: 0.79 - ETA: 0s - loss: 0.4575 - accuracy: 0.79 - ETA: 0s - loss: 0.4572 - accuracy: 0.79 - ETA: 0s - loss: 0.4561 - accuracy: 0.79 - ETA: 0s - loss: 0.4553 - accuracy: 0.79 - ETA: 0s - loss: 0.4560 - accuracy: 0.79 - ETA: 0s - loss: 0.4551 - accuracy: 0.79 - ETA: 0s - loss: 0.4538 - accuracy: 0.79 - ETA: 0s - loss: 0.4533 - accuracy: 0.79 - ETA: 0s - loss: 0.4527 - accuracy: 0.79 - ETA: 0s - loss: 0.4539 - accuracy: 0.79 - 1s 85us/step - loss: 0.4532 - accuracy: 0.7964\n",
      "Epoch 23/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.4730 - accuracy: 0.70 - ETA: 1s - loss: 0.4355 - accuracy: 0.80 - ETA: 1s - loss: 0.4446 - accuracy: 0.79 - ETA: 1s - loss: 0.4408 - accuracy: 0.80 - ETA: 1s - loss: 0.4541 - accuracy: 0.79 - ETA: 1s - loss: 0.4486 - accuracy: 0.79 - ETA: 1s - loss: 0.4494 - accuracy: 0.79 - ETA: 1s - loss: 0.4516 - accuracy: 0.79 - ETA: 1s - loss: 0.4499 - accuracy: 0.80 - ETA: 0s - loss: 0.4490 - accuracy: 0.80 - ETA: 0s - loss: 0.4484 - accuracy: 0.80 - ETA: 0s - loss: 0.4487 - accuracy: 0.80 - ETA: 0s - loss: 0.4479 - accuracy: 0.80 - ETA: 0s - loss: 0.4501 - accuracy: 0.80 - ETA: 0s - loss: 0.4500 - accuracy: 0.80 - ETA: 0s - loss: 0.4505 - accuracy: 0.80 - ETA: 0s - loss: 0.4516 - accuracy: 0.80 - ETA: 0s - loss: 0.4497 - accuracy: 0.80 - ETA: 0s - loss: 0.4486 - accuracy: 0.80 - ETA: 0s - loss: 0.4500 - accuracy: 0.79 - ETA: 0s - loss: 0.4492 - accuracy: 0.80 - ETA: 0s - loss: 0.4498 - accuracy: 0.79 - ETA: 0s - loss: 0.4507 - accuracy: 0.79 - ETA: 0s - loss: 0.4498 - accuracy: 0.79 - ETA: 0s - loss: 0.4500 - accuracy: 0.79 - ETA: 0s - loss: 0.4500 - accuracy: 0.79 - ETA: 0s - loss: 0.4493 - accuracy: 0.79 - ETA: 0s - loss: 0.4497 - accuracy: 0.79 - ETA: 0s - loss: 0.4499 - accuracy: 0.79 - 1s 84us/step - loss: 0.4504 - accuracy: 0.7975\n",
      "Epoch 24/100\n",
      "16887/16887 [==============================] - ETA: 4s - loss: 0.2231 - accuracy: 0.90 - ETA: 1s - loss: 0.4329 - accuracy: 0.81 - ETA: 1s - loss: 0.4368 - accuracy: 0.80 - ETA: 1s - loss: 0.4503 - accuracy: 0.79 - ETA: 1s - loss: 0.4456 - accuracy: 0.79 - ETA: 1s - loss: 0.4492 - accuracy: 0.79 - ETA: 1s - loss: 0.4525 - accuracy: 0.79 - ETA: 1s - loss: 0.4532 - accuracy: 0.79 - ETA: 1s - loss: 0.4549 - accuracy: 0.79 - ETA: 1s - loss: 0.4521 - accuracy: 0.79 - ETA: 1s - loss: 0.4564 - accuracy: 0.79 - ETA: 0s - loss: 0.4564 - accuracy: 0.78 - ETA: 0s - loss: 0.4549 - accuracy: 0.79 - ETA: 0s - loss: 0.4509 - accuracy: 0.79 - ETA: 0s - loss: 0.4496 - accuracy: 0.79 - ETA: 0s - loss: 0.4484 - accuracy: 0.79 - ETA: 0s - loss: 0.4471 - accuracy: 0.79 - ETA: 0s - loss: 0.4493 - accuracy: 0.79 - ETA: 0s - loss: 0.4498 - accuracy: 0.79 - ETA: 0s - loss: 0.4513 - accuracy: 0.79 - ETA: 0s - loss: 0.4499 - accuracy: 0.79 - ETA: 0s - loss: 0.4497 - accuracy: 0.79 - ETA: 0s - loss: 0.4505 - accuracy: 0.79 - ETA: 0s - loss: 0.4475 - accuracy: 0.79 - ETA: 0s - loss: 0.4478 - accuracy: 0.79 - ETA: 0s - loss: 0.4465 - accuracy: 0.79 - ETA: 0s - loss: 0.4476 - accuracy: 0.79 - ETA: 0s - loss: 0.4486 - accuracy: 0.79 - ETA: 0s - loss: 0.4480 - accuracy: 0.79 - 1s 85us/step - loss: 0.4479 - accuracy: 0.7987\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16887/16887 [==============================] - ETA: 3s - loss: 0.5923 - accuracy: 0.60 - ETA: 1s - loss: 0.4652 - accuracy: 0.79 - ETA: 1s - loss: 0.4593 - accuracy: 0.80 - ETA: 1s - loss: 0.4631 - accuracy: 0.79 - ETA: 1s - loss: 0.4582 - accuracy: 0.79 - ETA: 1s - loss: 0.4510 - accuracy: 0.80 - ETA: 1s - loss: 0.4504 - accuracy: 0.80 - ETA: 1s - loss: 0.4527 - accuracy: 0.80 - ETA: 1s - loss: 0.4463 - accuracy: 0.80 - ETA: 0s - loss: 0.4452 - accuracy: 0.80 - ETA: 0s - loss: 0.4479 - accuracy: 0.80 - ETA: 0s - loss: 0.4502 - accuracy: 0.80 - ETA: 0s - loss: 0.4506 - accuracy: 0.80 - ETA: 0s - loss: 0.4523 - accuracy: 0.79 - ETA: 0s - loss: 0.4533 - accuracy: 0.79 - ETA: 0s - loss: 0.4543 - accuracy: 0.79 - ETA: 0s - loss: 0.4529 - accuracy: 0.79 - ETA: 0s - loss: 0.4500 - accuracy: 0.79 - ETA: 0s - loss: 0.4502 - accuracy: 0.79 - ETA: 0s - loss: 0.4484 - accuracy: 0.79 - ETA: 0s - loss: 0.4486 - accuracy: 0.79 - ETA: 0s - loss: 0.4503 - accuracy: 0.79 - ETA: 0s - loss: 0.4492 - accuracy: 0.79 - ETA: 0s - loss: 0.4492 - accuracy: 0.79 - ETA: 0s - loss: 0.4481 - accuracy: 0.79 - ETA: 0s - loss: 0.4487 - accuracy: 0.79 - ETA: 0s - loss: 0.4495 - accuracy: 0.79 - ETA: 0s - loss: 0.4475 - accuracy: 0.80 - ETA: 0s - loss: 0.4476 - accuracy: 0.79 - 1s 84us/step - loss: 0.4472 - accuracy: 0.7992\n",
      "Epoch 26/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.4182 - accuracy: 0.70 - ETA: 1s - loss: 0.4491 - accuracy: 0.79 - ETA: 1s - loss: 0.4472 - accuracy: 0.78 - ETA: 1s - loss: 0.4461 - accuracy: 0.79 - ETA: 1s - loss: 0.4412 - accuracy: 0.79 - ETA: 1s - loss: 0.4472 - accuracy: 0.79 - ETA: 1s - loss: 0.4454 - accuracy: 0.79 - ETA: 1s - loss: 0.4467 - accuracy: 0.79 - ETA: 1s - loss: 0.4470 - accuracy: 0.79 - ETA: 0s - loss: 0.4466 - accuracy: 0.79 - ETA: 0s - loss: 0.4465 - accuracy: 0.79 - ETA: 0s - loss: 0.4460 - accuracy: 0.79 - ETA: 0s - loss: 0.4462 - accuracy: 0.79 - ETA: 0s - loss: 0.4461 - accuracy: 0.79 - ETA: 0s - loss: 0.4446 - accuracy: 0.80 - ETA: 0s - loss: 0.4446 - accuracy: 0.80 - ETA: 0s - loss: 0.4436 - accuracy: 0.80 - ETA: 0s - loss: 0.4426 - accuracy: 0.80 - ETA: 0s - loss: 0.4424 - accuracy: 0.80 - ETA: 0s - loss: 0.4448 - accuracy: 0.80 - ETA: 0s - loss: 0.4439 - accuracy: 0.80 - ETA: 0s - loss: 0.4458 - accuracy: 0.80 - ETA: 0s - loss: 0.4463 - accuracy: 0.80 - ETA: 0s - loss: 0.4465 - accuracy: 0.80 - ETA: 0s - loss: 0.4477 - accuracy: 0.80 - ETA: 0s - loss: 0.4458 - accuracy: 0.80 - ETA: 0s - loss: 0.4467 - accuracy: 0.80 - ETA: 0s - loss: 0.4462 - accuracy: 0.80 - ETA: 0s - loss: 0.4454 - accuracy: 0.80 - 1s 85us/step - loss: 0.4452 - accuracy: 0.8024\n",
      "Epoch 27/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.2726 - accuracy: 0.90 - ETA: 1s - loss: 0.4444 - accuracy: 0.80 - ETA: 1s - loss: 0.4297 - accuracy: 0.80 - ETA: 1s - loss: 0.4278 - accuracy: 0.81 - ETA: 1s - loss: 0.4239 - accuracy: 0.81 - ETA: 1s - loss: 0.4247 - accuracy: 0.81 - ETA: 1s - loss: 0.4316 - accuracy: 0.81 - ETA: 1s - loss: 0.4347 - accuracy: 0.81 - ETA: 1s - loss: 0.4343 - accuracy: 0.81 - ETA: 0s - loss: 0.4390 - accuracy: 0.80 - ETA: 0s - loss: 0.4398 - accuracy: 0.80 - ETA: 0s - loss: 0.4388 - accuracy: 0.80 - ETA: 0s - loss: 0.4401 - accuracy: 0.80 - ETA: 0s - loss: 0.4411 - accuracy: 0.80 - ETA: 0s - loss: 0.4411 - accuracy: 0.80 - ETA: 0s - loss: 0.4414 - accuracy: 0.80 - ETA: 0s - loss: 0.4417 - accuracy: 0.80 - ETA: 0s - loss: 0.4443 - accuracy: 0.80 - ETA: 0s - loss: 0.4445 - accuracy: 0.80 - ETA: 0s - loss: 0.4437 - accuracy: 0.80 - ETA: 0s - loss: 0.4424 - accuracy: 0.80 - ETA: 0s - loss: 0.4413 - accuracy: 0.80 - ETA: 0s - loss: 0.4437 - accuracy: 0.80 - ETA: 0s - loss: 0.4446 - accuracy: 0.80 - ETA: 0s - loss: 0.4442 - accuracy: 0.80 - ETA: 0s - loss: 0.4442 - accuracy: 0.80 - ETA: 0s - loss: 0.4442 - accuracy: 0.80 - ETA: 0s - loss: 0.4439 - accuracy: 0.80 - ETA: 0s - loss: 0.4443 - accuracy: 0.80 - 1s 84us/step - loss: 0.4436 - accuracy: 0.8019\n",
      "Epoch 28/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.8248 - accuracy: 0.60 - ETA: 1s - loss: 0.4387 - accuracy: 0.81 - ETA: 1s - loss: 0.4448 - accuracy: 0.81 - ETA: 1s - loss: 0.4348 - accuracy: 0.81 - ETA: 1s - loss: 0.4402 - accuracy: 0.81 - ETA: 1s - loss: 0.4423 - accuracy: 0.80 - ETA: 1s - loss: 0.4454 - accuracy: 0.80 - ETA: 1s - loss: 0.4489 - accuracy: 0.80 - ETA: 1s - loss: 0.4473 - accuracy: 0.80 - ETA: 0s - loss: 0.4465 - accuracy: 0.80 - ETA: 0s - loss: 0.4438 - accuracy: 0.80 - ETA: 0s - loss: 0.4412 - accuracy: 0.80 - ETA: 0s - loss: 0.4439 - accuracy: 0.80 - ETA: 0s - loss: 0.4411 - accuracy: 0.80 - ETA: 0s - loss: 0.4408 - accuracy: 0.80 - ETA: 0s - loss: 0.4414 - accuracy: 0.80 - ETA: 0s - loss: 0.4406 - accuracy: 0.80 - ETA: 0s - loss: 0.4396 - accuracy: 0.80 - ETA: 0s - loss: 0.4376 - accuracy: 0.80 - ETA: 0s - loss: 0.4390 - accuracy: 0.80 - ETA: 0s - loss: 0.4400 - accuracy: 0.80 - ETA: 0s - loss: 0.4397 - accuracy: 0.80 - ETA: 0s - loss: 0.4404 - accuracy: 0.80 - ETA: 0s - loss: 0.4396 - accuracy: 0.80 - ETA: 0s - loss: 0.4391 - accuracy: 0.80 - ETA: 0s - loss: 0.4404 - accuracy: 0.80 - ETA: 0s - loss: 0.4404 - accuracy: 0.80 - ETA: 0s - loss: 0.4406 - accuracy: 0.80 - ETA: 0s - loss: 0.4411 - accuracy: 0.80 - 1s 84us/step - loss: 0.4414 - accuracy: 0.8022\n",
      "Epoch 29/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.3624 - accuracy: 0.90 - ETA: 1s - loss: 0.4454 - accuracy: 0.79 - ETA: 1s - loss: 0.4283 - accuracy: 0.80 - ETA: 1s - loss: 0.4311 - accuracy: 0.80 - ETA: 1s - loss: 0.4283 - accuracy: 0.80 - ETA: 1s - loss: 0.4234 - accuracy: 0.81 - ETA: 1s - loss: 0.4262 - accuracy: 0.80 - ETA: 1s - loss: 0.4247 - accuracy: 0.81 - ETA: 1s - loss: 0.4249 - accuracy: 0.81 - ETA: 0s - loss: 0.4271 - accuracy: 0.81 - ETA: 0s - loss: 0.4270 - accuracy: 0.81 - ETA: 0s - loss: 0.4303 - accuracy: 0.80 - ETA: 0s - loss: 0.4328 - accuracy: 0.80 - ETA: 0s - loss: 0.4308 - accuracy: 0.80 - ETA: 0s - loss: 0.4326 - accuracy: 0.80 - ETA: 0s - loss: 0.4341 - accuracy: 0.80 - ETA: 0s - loss: 0.4355 - accuracy: 0.80 - ETA: 0s - loss: 0.4355 - accuracy: 0.80 - ETA: 0s - loss: 0.4366 - accuracy: 0.80 - ETA: 0s - loss: 0.4377 - accuracy: 0.80 - ETA: 0s - loss: 0.4394 - accuracy: 0.80 - ETA: 0s - loss: 0.4397 - accuracy: 0.80 - ETA: 0s - loss: 0.4391 - accuracy: 0.80 - ETA: 0s - loss: 0.4404 - accuracy: 0.80 - ETA: 0s - loss: 0.4385 - accuracy: 0.80 - ETA: 0s - loss: 0.4390 - accuracy: 0.80 - ETA: 0s - loss: 0.4389 - accuracy: 0.80 - ETA: 0s - loss: 0.4394 - accuracy: 0.80 - ETA: 0s - loss: 0.4402 - accuracy: 0.80 - 1s 84us/step - loss: 0.4405 - accuracy: 0.8042\n",
      "Epoch 30/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.4852 - accuracy: 0.80 - ETA: 1s - loss: 0.4331 - accuracy: 0.81 - ETA: 1s - loss: 0.4492 - accuracy: 0.79 - ETA: 1s - loss: 0.4381 - accuracy: 0.80 - ETA: 1s - loss: 0.4365 - accuracy: 0.80 - ETA: 1s - loss: 0.4393 - accuracy: 0.79 - ETA: 1s - loss: 0.4391 - accuracy: 0.80 - ETA: 1s - loss: 0.4423 - accuracy: 0.79 - ETA: 1s - loss: 0.4401 - accuracy: 0.79 - ETA: 0s - loss: 0.4394 - accuracy: 0.80 - ETA: 0s - loss: 0.4359 - accuracy: 0.80 - ETA: 0s - loss: 0.4374 - accuracy: 0.80 - ETA: 0s - loss: 0.4393 - accuracy: 0.80 - ETA: 0s - loss: 0.4388 - accuracy: 0.80 - ETA: 0s - loss: 0.4381 - accuracy: 0.80 - ETA: 0s - loss: 0.4390 - accuracy: 0.80 - ETA: 0s - loss: 0.4390 - accuracy: 0.80 - ETA: 0s - loss: 0.4398 - accuracy: 0.80 - ETA: 0s - loss: 0.4403 - accuracy: 0.80 - ETA: 0s - loss: 0.4390 - accuracy: 0.80 - ETA: 0s - loss: 0.4400 - accuracy: 0.80 - ETA: 0s - loss: 0.4395 - accuracy: 0.80 - ETA: 0s - loss: 0.4395 - accuracy: 0.80 - ETA: 0s - loss: 0.4391 - accuracy: 0.80 - ETA: 0s - loss: 0.4399 - accuracy: 0.80 - ETA: 0s - loss: 0.4399 - accuracy: 0.80 - ETA: 0s - loss: 0.4395 - accuracy: 0.80 - ETA: 0s - loss: 0.4388 - accuracy: 0.80 - ETA: 0s - loss: 0.4380 - accuracy: 0.80 - 1s 85us/step - loss: 0.4379 - accuracy: 0.8055\n",
      "Epoch 31/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.2998 - accuracy: 1.00 - ETA: 1s - loss: 0.4361 - accuracy: 0.80 - ETA: 1s - loss: 0.4416 - accuracy: 0.80 - ETA: 1s - loss: 0.4391 - accuracy: 0.80 - ETA: 1s - loss: 0.4372 - accuracy: 0.80 - ETA: 1s - loss: 0.4335 - accuracy: 0.81 - ETA: 1s - loss: 0.4337 - accuracy: 0.81 - ETA: 1s - loss: 0.4351 - accuracy: 0.81 - ETA: 1s - loss: 0.4315 - accuracy: 0.81 - ETA: 0s - loss: 0.4288 - accuracy: 0.81 - ETA: 0s - loss: 0.4276 - accuracy: 0.81 - ETA: 0s - loss: 0.4294 - accuracy: 0.81 - ETA: 0s - loss: 0.4267 - accuracy: 0.81 - ETA: 0s - loss: 0.4258 - accuracy: 0.81 - ETA: 0s - loss: 0.4259 - accuracy: 0.81 - ETA: 0s - loss: 0.4269 - accuracy: 0.81 - ETA: 0s - loss: 0.4287 - accuracy: 0.81 - ETA: 0s - loss: 0.4294 - accuracy: 0.81 - ETA: 0s - loss: 0.4319 - accuracy: 0.81 - ETA: 0s - loss: 0.4319 - accuracy: 0.81 - ETA: 0s - loss: 0.4330 - accuracy: 0.80 - ETA: 0s - loss: 0.4326 - accuracy: 0.80 - ETA: 0s - loss: 0.4335 - accuracy: 0.80 - ETA: 0s - loss: 0.4338 - accuracy: 0.80 - ETA: 0s - loss: 0.4350 - accuracy: 0.80 - ETA: 0s - loss: 0.4347 - accuracy: 0.80 - ETA: 0s - loss: 0.4354 - accuracy: 0.80 - ETA: 0s - loss: 0.4359 - accuracy: 0.80 - ETA: 0s - loss: 0.4363 - accuracy: 0.80 - 1s 85us/step - loss: 0.4371 - accuracy: 0.8055\n",
      "Epoch 32/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.5768 - accuracy: 0.60 - ETA: 1s - loss: 0.4594 - accuracy: 0.78 - ETA: 1s - loss: 0.4303 - accuracy: 0.81 - ETA: 1s - loss: 0.4320 - accuracy: 0.81 - ETA: 1s - loss: 0.4395 - accuracy: 0.80 - ETA: 1s - loss: 0.4366 - accuracy: 0.80 - ETA: 1s - loss: 0.4425 - accuracy: 0.80 - ETA: 1s - loss: 0.4439 - accuracy: 0.80 - ETA: 1s - loss: 0.4394 - accuracy: 0.80 - ETA: 0s - loss: 0.4394 - accuracy: 0.80 - ETA: 0s - loss: 0.4402 - accuracy: 0.80 - ETA: 0s - loss: 0.4386 - accuracy: 0.80 - ETA: 0s - loss: 0.4385 - accuracy: 0.80 - ETA: 0s - loss: 0.4362 - accuracy: 0.80 - ETA: 0s - loss: 0.4373 - accuracy: 0.80 - ETA: 0s - loss: 0.4375 - accuracy: 0.80 - ETA: 0s - loss: 0.4384 - accuracy: 0.80 - ETA: 0s - loss: 0.4383 - accuracy: 0.80 - ETA: 0s - loss: 0.4370 - accuracy: 0.80 - ETA: 0s - loss: 0.4361 - accuracy: 0.80 - ETA: 0s - loss: 0.4357 - accuracy: 0.80 - ETA: 0s - loss: 0.4347 - accuracy: 0.80 - ETA: 0s - loss: 0.4344 - accuracy: 0.80 - ETA: 0s - loss: 0.4349 - accuracy: 0.80 - ETA: 0s - loss: 0.4348 - accuracy: 0.80 - ETA: 0s - loss: 0.4354 - accuracy: 0.80 - ETA: 0s - loss: 0.4356 - accuracy: 0.80 - ETA: 0s - loss: 0.4362 - accuracy: 0.80 - ETA: 0s - loss: 0.4360 - accuracy: 0.80 - 1s 84us/step - loss: 0.4357 - accuracy: 0.8036\n",
      "Epoch 33/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.2603 - accuracy: 0.90 - ETA: 1s - loss: 0.4258 - accuracy: 0.81 - ETA: 1s - loss: 0.4339 - accuracy: 0.80 - ETA: 1s - loss: 0.4316 - accuracy: 0.80 - ETA: 1s - loss: 0.4383 - accuracy: 0.80 - ETA: 1s - loss: 0.4373 - accuracy: 0.80 - ETA: 1s - loss: 0.4411 - accuracy: 0.80 - ETA: 1s - loss: 0.4372 - accuracy: 0.80 - ETA: 1s - loss: 0.4374 - accuracy: 0.80 - ETA: 0s - loss: 0.4362 - accuracy: 0.80 - ETA: 0s - loss: 0.4366 - accuracy: 0.80 - ETA: 0s - loss: 0.4336 - accuracy: 0.80 - ETA: 0s - loss: 0.4344 - accuracy: 0.80 - ETA: 0s - loss: 0.4335 - accuracy: 0.80 - ETA: 0s - loss: 0.4346 - accuracy: 0.80 - ETA: 0s - loss: 0.4345 - accuracy: 0.80 - ETA: 0s - loss: 0.4345 - accuracy: 0.80 - ETA: 0s - loss: 0.4343 - accuracy: 0.80 - ETA: 0s - loss: 0.4313 - accuracy: 0.80 - ETA: 0s - loss: 0.4317 - accuracy: 0.80 - ETA: 0s - loss: 0.4332 - accuracy: 0.80 - ETA: 0s - loss: 0.4338 - accuracy: 0.80 - ETA: 0s - loss: 0.4354 - accuracy: 0.80 - ETA: 0s - loss: 0.4358 - accuracy: 0.80 - ETA: 0s - loss: 0.4345 - accuracy: 0.80 - ETA: 0s - loss: 0.4339 - accuracy: 0.80 - ETA: 0s - loss: 0.4338 - accuracy: 0.80 - ETA: 0s - loss: 0.4341 - accuracy: 0.80 - ETA: 0s - loss: 0.4347 - accuracy: 0.80 - 1s 84us/step - loss: 0.4346 - accuracy: 0.8046\n",
      "Epoch 34/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.1613 - accuracy: 1.00 - ETA: 1s - loss: 0.4271 - accuracy: 0.83 - ETA: 1s - loss: 0.4296 - accuracy: 0.82 - ETA: 1s - loss: 0.4381 - accuracy: 0.81 - ETA: 1s - loss: 0.4398 - accuracy: 0.80 - ETA: 1s - loss: 0.4416 - accuracy: 0.80 - ETA: 1s - loss: 0.4377 - accuracy: 0.80 - ETA: 1s - loss: 0.4334 - accuracy: 0.80 - ETA: 1s - loss: 0.4383 - accuracy: 0.80 - ETA: 0s - loss: 0.4368 - accuracy: 0.80 - ETA: 0s - loss: 0.4319 - accuracy: 0.80 - ETA: 0s - loss: 0.4341 - accuracy: 0.80 - ETA: 0s - loss: 0.4358 - accuracy: 0.80 - ETA: 0s - loss: 0.4355 - accuracy: 0.80 - ETA: 0s - loss: 0.4382 - accuracy: 0.80 - ETA: 0s - loss: 0.4359 - accuracy: 0.80 - ETA: 0s - loss: 0.4357 - accuracy: 0.80 - ETA: 0s - loss: 0.4360 - accuracy: 0.80 - ETA: 0s - loss: 0.4337 - accuracy: 0.80 - ETA: 0s - loss: 0.4332 - accuracy: 0.80 - ETA: 0s - loss: 0.4334 - accuracy: 0.80 - ETA: 0s - loss: 0.4343 - accuracy: 0.80 - ETA: 0s - loss: 0.4342 - accuracy: 0.80 - ETA: 0s - loss: 0.4341 - accuracy: 0.80 - ETA: 0s - loss: 0.4323 - accuracy: 0.80 - ETA: 0s - loss: 0.4315 - accuracy: 0.80 - ETA: 0s - loss: 0.4317 - accuracy: 0.80 - ETA: 0s - loss: 0.4311 - accuracy: 0.80 - ETA: 0s - loss: 0.4325 - accuracy: 0.80 - 1s 85us/step - loss: 0.4332 - accuracy: 0.8065\n",
      "Epoch 35/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.4273 - accuracy: 0.80 - ETA: 1s - loss: 0.4206 - accuracy: 0.81 - ETA: 1s - loss: 0.4054 - accuracy: 0.81 - ETA: 1s - loss: 0.4152 - accuracy: 0.80 - ETA: 1s - loss: 0.4122 - accuracy: 0.81 - ETA: 1s - loss: 0.4160 - accuracy: 0.80 - ETA: 1s - loss: 0.4179 - accuracy: 0.80 - ETA: 1s - loss: 0.4249 - accuracy: 0.80 - ETA: 1s - loss: 0.4213 - accuracy: 0.80 - ETA: 0s - loss: 0.4238 - accuracy: 0.80 - ETA: 0s - loss: 0.4258 - accuracy: 0.80 - ETA: 0s - loss: 0.4267 - accuracy: 0.80 - ETA: 0s - loss: 0.4249 - accuracy: 0.80 - ETA: 0s - loss: 0.4253 - accuracy: 0.80 - ETA: 0s - loss: 0.4259 - accuracy: 0.80 - ETA: 0s - loss: 0.4273 - accuracy: 0.80 - ETA: 0s - loss: 0.4286 - accuracy: 0.80 - ETA: 0s - loss: 0.4298 - accuracy: 0.80 - ETA: 0s - loss: 0.4291 - accuracy: 0.80 - ETA: 0s - loss: 0.4292 - accuracy: 0.80 - ETA: 0s - loss: 0.4299 - accuracy: 0.80 - ETA: 0s - loss: 0.4302 - accuracy: 0.80 - ETA: 0s - loss: 0.4294 - accuracy: 0.80 - ETA: 0s - loss: 0.4306 - accuracy: 0.80 - ETA: 0s - loss: 0.4302 - accuracy: 0.80 - ETA: 0s - loss: 0.4301 - accuracy: 0.80 - ETA: 0s - loss: 0.4305 - accuracy: 0.80 - ETA: 0s - loss: 0.4295 - accuracy: 0.80 - ETA: 0s - loss: 0.4305 - accuracy: 0.80 - 1s 85us/step - loss: 0.4314 - accuracy: 0.8058\n",
      "Epoch 36/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.4493 - accuracy: 0.80 - ETA: 1s - loss: 0.4380 - accuracy: 0.80 - ETA: 1s - loss: 0.4194 - accuracy: 0.81 - ETA: 1s - loss: 0.4258 - accuracy: 0.81 - ETA: 1s - loss: 0.4385 - accuracy: 0.80 - ETA: 1s - loss: 0.4478 - accuracy: 0.79 - ETA: 1s - loss: 0.4475 - accuracy: 0.80 - ETA: 1s - loss: 0.4513 - accuracy: 0.79 - ETA: 1s - loss: 0.4508 - accuracy: 0.79 - ETA: 0s - loss: 0.4479 - accuracy: 0.79 - ETA: 0s - loss: 0.4459 - accuracy: 0.80 - ETA: 0s - loss: 0.4440 - accuracy: 0.80 - ETA: 0s - loss: 0.4414 - accuracy: 0.80 - ETA: 0s - loss: 0.4404 - accuracy: 0.80 - ETA: 0s - loss: 0.4377 - accuracy: 0.80 - ETA: 0s - loss: 0.4389 - accuracy: 0.80 - ETA: 0s - loss: 0.4374 - accuracy: 0.80 - ETA: 0s - loss: 0.4364 - accuracy: 0.80 - ETA: 0s - loss: 0.4346 - accuracy: 0.80 - ETA: 0s - loss: 0.4337 - accuracy: 0.80 - ETA: 0s - loss: 0.4330 - accuracy: 0.80 - ETA: 0s - loss: 0.4328 - accuracy: 0.80 - ETA: 0s - loss: 0.4326 - accuracy: 0.80 - ETA: 0s - loss: 0.4309 - accuracy: 0.80 - ETA: 0s - loss: 0.4316 - accuracy: 0.80 - ETA: 0s - loss: 0.4317 - accuracy: 0.80 - ETA: 0s - loss: 0.4315 - accuracy: 0.80 - ETA: 0s - loss: 0.4311 - accuracy: 0.80 - ETA: 0s - loss: 0.4312 - accuracy: 0.80 - 1s 84us/step - loss: 0.4308 - accuracy: 0.8084\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16887/16887 [==============================] - ETA: 5s - loss: 0.5937 - accuracy: 0.60 - ETA: 1s - loss: 0.4114 - accuracy: 0.82 - ETA: 1s - loss: 0.4184 - accuracy: 0.82 - ETA: 1s - loss: 0.4164 - accuracy: 0.82 - ETA: 1s - loss: 0.4151 - accuracy: 0.82 - ETA: 1s - loss: 0.4177 - accuracy: 0.82 - ETA: 1s - loss: 0.4247 - accuracy: 0.81 - ETA: 1s - loss: 0.4264 - accuracy: 0.81 - ETA: 1s - loss: 0.4259 - accuracy: 0.81 - ETA: 0s - loss: 0.4260 - accuracy: 0.81 - ETA: 0s - loss: 0.4250 - accuracy: 0.81 - ETA: 0s - loss: 0.4250 - accuracy: 0.81 - ETA: 0s - loss: 0.4242 - accuracy: 0.81 - ETA: 0s - loss: 0.4268 - accuracy: 0.81 - ETA: 0s - loss: 0.4259 - accuracy: 0.81 - ETA: 0s - loss: 0.4284 - accuracy: 0.81 - ETA: 0s - loss: 0.4266 - accuracy: 0.81 - ETA: 0s - loss: 0.4286 - accuracy: 0.81 - ETA: 0s - loss: 0.4294 - accuracy: 0.80 - ETA: 0s - loss: 0.4292 - accuracy: 0.80 - ETA: 0s - loss: 0.4295 - accuracy: 0.80 - ETA: 0s - loss: 0.4306 - accuracy: 0.80 - ETA: 0s - loss: 0.4301 - accuracy: 0.80 - ETA: 0s - loss: 0.4292 - accuracy: 0.80 - ETA: 0s - loss: 0.4284 - accuracy: 0.81 - ETA: 0s - loss: 0.4285 - accuracy: 0.81 - ETA: 0s - loss: 0.4273 - accuracy: 0.81 - ETA: 0s - loss: 0.4283 - accuracy: 0.81 - ETA: 0s - loss: 0.4290 - accuracy: 0.81 - 1s 86us/step - loss: 0.4292 - accuracy: 0.8101\n",
      "Epoch 38/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.3444 - accuracy: 0.90 - ETA: 1s - loss: 0.4145 - accuracy: 0.82 - ETA: 1s - loss: 0.4035 - accuracy: 0.82 - ETA: 1s - loss: 0.4173 - accuracy: 0.81 - ETA: 1s - loss: 0.4224 - accuracy: 0.80 - ETA: 1s - loss: 0.4219 - accuracy: 0.81 - ETA: 1s - loss: 0.4274 - accuracy: 0.80 - ETA: 1s - loss: 0.4263 - accuracy: 0.80 - ETA: 1s - loss: 0.4261 - accuracy: 0.80 - ETA: 1s - loss: 0.4254 - accuracy: 0.80 - ETA: 0s - loss: 0.4257 - accuracy: 0.80 - ETA: 0s - loss: 0.4237 - accuracy: 0.80 - ETA: 0s - loss: 0.4256 - accuracy: 0.80 - ETA: 0s - loss: 0.4301 - accuracy: 0.80 - ETA: 0s - loss: 0.4314 - accuracy: 0.80 - ETA: 0s - loss: 0.4291 - accuracy: 0.80 - ETA: 0s - loss: 0.4282 - accuracy: 0.80 - ETA: 0s - loss: 0.4274 - accuracy: 0.80 - ETA: 0s - loss: 0.4274 - accuracy: 0.80 - ETA: 0s - loss: 0.4298 - accuracy: 0.80 - ETA: 0s - loss: 0.4302 - accuracy: 0.80 - ETA: 0s - loss: 0.4298 - accuracy: 0.80 - ETA: 0s - loss: 0.4302 - accuracy: 0.80 - ETA: 0s - loss: 0.4297 - accuracy: 0.80 - ETA: 0s - loss: 0.4314 - accuracy: 0.80 - ETA: 0s - loss: 0.4321 - accuracy: 0.80 - ETA: 0s - loss: 0.4302 - accuracy: 0.80 - ETA: 0s - loss: 0.4295 - accuracy: 0.80 - ETA: 0s - loss: 0.4285 - accuracy: 0.80 - 1s 85us/step - loss: 0.4282 - accuracy: 0.8080\n",
      "Epoch 39/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.4511 - accuracy: 0.70 - ETA: 1s - loss: 0.4002 - accuracy: 0.82 - ETA: 1s - loss: 0.4002 - accuracy: 0.81 - ETA: 1s - loss: 0.3996 - accuracy: 0.82 - ETA: 1s - loss: 0.3977 - accuracy: 0.82 - ETA: 1s - loss: 0.4073 - accuracy: 0.81 - ETA: 1s - loss: 0.4117 - accuracy: 0.81 - ETA: 1s - loss: 0.4137 - accuracy: 0.81 - ETA: 1s - loss: 0.4204 - accuracy: 0.81 - ETA: 0s - loss: 0.4215 - accuracy: 0.81 - ETA: 0s - loss: 0.4251 - accuracy: 0.81 - ETA: 0s - loss: 0.4236 - accuracy: 0.81 - ETA: 0s - loss: 0.4250 - accuracy: 0.81 - ETA: 0s - loss: 0.4235 - accuracy: 0.81 - ETA: 0s - loss: 0.4234 - accuracy: 0.81 - ETA: 0s - loss: 0.4235 - accuracy: 0.81 - ETA: 0s - loss: 0.4238 - accuracy: 0.81 - ETA: 0s - loss: 0.4265 - accuracy: 0.81 - ETA: 0s - loss: 0.4258 - accuracy: 0.81 - ETA: 0s - loss: 0.4246 - accuracy: 0.81 - ETA: 0s - loss: 0.4249 - accuracy: 0.81 - ETA: 0s - loss: 0.4245 - accuracy: 0.81 - ETA: 0s - loss: 0.4231 - accuracy: 0.81 - ETA: 0s - loss: 0.4244 - accuracy: 0.81 - ETA: 0s - loss: 0.4252 - accuracy: 0.81 - ETA: 0s - loss: 0.4245 - accuracy: 0.81 - ETA: 0s - loss: 0.4263 - accuracy: 0.81 - ETA: 0s - loss: 0.4268 - accuracy: 0.80 - ETA: 0s - loss: 0.4276 - accuracy: 0.80 - 1s 85us/step - loss: 0.4272 - accuracy: 0.8092\n",
      "Epoch 40/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.5618 - accuracy: 0.70 - ETA: 1s - loss: 0.4182 - accuracy: 0.81 - ETA: 1s - loss: 0.4124 - accuracy: 0.81 - ETA: 1s - loss: 0.4192 - accuracy: 0.81 - ETA: 1s - loss: 0.4264 - accuracy: 0.80 - ETA: 1s - loss: 0.4193 - accuracy: 0.81 - ETA: 1s - loss: 0.4186 - accuracy: 0.81 - ETA: 1s - loss: 0.4204 - accuracy: 0.81 - ETA: 1s - loss: 0.4223 - accuracy: 0.81 - ETA: 0s - loss: 0.4255 - accuracy: 0.81 - ETA: 0s - loss: 0.4258 - accuracy: 0.81 - ETA: 0s - loss: 0.4265 - accuracy: 0.81 - ETA: 0s - loss: 0.4246 - accuracy: 0.81 - ETA: 0s - loss: 0.4218 - accuracy: 0.81 - ETA: 0s - loss: 0.4214 - accuracy: 0.81 - ETA: 0s - loss: 0.4229 - accuracy: 0.81 - ETA: 0s - loss: 0.4244 - accuracy: 0.81 - ETA: 0s - loss: 0.4243 - accuracy: 0.81 - ETA: 0s - loss: 0.4238 - accuracy: 0.81 - ETA: 0s - loss: 0.4235 - accuracy: 0.81 - ETA: 0s - loss: 0.4245 - accuracy: 0.81 - ETA: 0s - loss: 0.4263 - accuracy: 0.81 - ETA: 0s - loss: 0.4262 - accuracy: 0.81 - ETA: 0s - loss: 0.4257 - accuracy: 0.81 - ETA: 0s - loss: 0.4272 - accuracy: 0.81 - ETA: 0s - loss: 0.4258 - accuracy: 0.81 - ETA: 0s - loss: 0.4263 - accuracy: 0.81 - ETA: 0s - loss: 0.4269 - accuracy: 0.81 - ETA: 0s - loss: 0.4260 - accuracy: 0.81 - 1s 85us/step - loss: 0.4258 - accuracy: 0.8119\n",
      "Epoch 41/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.3343 - accuracy: 0.80 - ETA: 1s - loss: 0.4186 - accuracy: 0.81 - ETA: 1s - loss: 0.4176 - accuracy: 0.81 - ETA: 1s - loss: 0.4289 - accuracy: 0.80 - ETA: 1s - loss: 0.4290 - accuracy: 0.81 - ETA: 1s - loss: 0.4299 - accuracy: 0.80 - ETA: 1s - loss: 0.4286 - accuracy: 0.80 - ETA: 1s - loss: 0.4294 - accuracy: 0.80 - ETA: 1s - loss: 0.4320 - accuracy: 0.80 - ETA: 1s - loss: 0.4279 - accuracy: 0.80 - ETA: 0s - loss: 0.4235 - accuracy: 0.80 - ETA: 0s - loss: 0.4245 - accuracy: 0.80 - ETA: 0s - loss: 0.4251 - accuracy: 0.80 - ETA: 0s - loss: 0.4262 - accuracy: 0.80 - ETA: 0s - loss: 0.4291 - accuracy: 0.80 - ETA: 0s - loss: 0.4306 - accuracy: 0.80 - ETA: 0s - loss: 0.4296 - accuracy: 0.80 - ETA: 0s - loss: 0.4290 - accuracy: 0.80 - ETA: 0s - loss: 0.4290 - accuracy: 0.80 - ETA: 0s - loss: 0.4274 - accuracy: 0.80 - ETA: 0s - loss: 0.4264 - accuracy: 0.81 - ETA: 0s - loss: 0.4274 - accuracy: 0.81 - ETA: 0s - loss: 0.4283 - accuracy: 0.80 - ETA: 0s - loss: 0.4281 - accuracy: 0.80 - ETA: 0s - loss: 0.4278 - accuracy: 0.80 - ETA: 0s - loss: 0.4268 - accuracy: 0.80 - ETA: 0s - loss: 0.4264 - accuracy: 0.80 - ETA: 0s - loss: 0.4258 - accuracy: 0.80 - ETA: 0s - loss: 0.4251 - accuracy: 0.80 - 1s 86us/step - loss: 0.4251 - accuracy: 0.8096\n",
      "Epoch 42/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.7706 - accuracy: 0.70 - ETA: 1s - loss: 0.4013 - accuracy: 0.81 - ETA: 1s - loss: 0.3916 - accuracy: 0.82 - ETA: 1s - loss: 0.3979 - accuracy: 0.82 - ETA: 1s - loss: 0.3960 - accuracy: 0.82 - ETA: 1s - loss: 0.3982 - accuracy: 0.82 - ETA: 1s - loss: 0.4086 - accuracy: 0.81 - ETA: 1s - loss: 0.4049 - accuracy: 0.82 - ETA: 1s - loss: 0.4041 - accuracy: 0.82 - ETA: 0s - loss: 0.4042 - accuracy: 0.82 - ETA: 0s - loss: 0.4055 - accuracy: 0.82 - ETA: 0s - loss: 0.4053 - accuracy: 0.82 - ETA: 0s - loss: 0.4072 - accuracy: 0.82 - ETA: 0s - loss: 0.4086 - accuracy: 0.82 - ETA: 0s - loss: 0.4098 - accuracy: 0.82 - ETA: 0s - loss: 0.4104 - accuracy: 0.82 - ETA: 0s - loss: 0.4098 - accuracy: 0.82 - ETA: 0s - loss: 0.4121 - accuracy: 0.81 - ETA: 0s - loss: 0.4122 - accuracy: 0.81 - ETA: 0s - loss: 0.4115 - accuracy: 0.81 - ETA: 0s - loss: 0.4127 - accuracy: 0.81 - ETA: 0s - loss: 0.4155 - accuracy: 0.81 - ETA: 0s - loss: 0.4178 - accuracy: 0.81 - ETA: 0s - loss: 0.4198 - accuracy: 0.81 - ETA: 0s - loss: 0.4203 - accuracy: 0.81 - ETA: 0s - loss: 0.4199 - accuracy: 0.81 - ETA: 0s - loss: 0.4202 - accuracy: 0.81 - ETA: 0s - loss: 0.4219 - accuracy: 0.81 - ETA: 0s - loss: 0.4226 - accuracy: 0.81 - 1s 86us/step - loss: 0.4232 - accuracy: 0.8120\n",
      "Epoch 43/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.3233 - accuracy: 0.90 - ETA: 1s - loss: 0.4326 - accuracy: 0.81 - ETA: 1s - loss: 0.4236 - accuracy: 0.81 - ETA: 1s - loss: 0.4208 - accuracy: 0.81 - ETA: 1s - loss: 0.4288 - accuracy: 0.81 - ETA: 1s - loss: 0.4251 - accuracy: 0.81 - ETA: 1s - loss: 0.4224 - accuracy: 0.81 - ETA: 1s - loss: 0.4245 - accuracy: 0.81 - ETA: 1s - loss: 0.4219 - accuracy: 0.81 - ETA: 1s - loss: 0.4258 - accuracy: 0.81 - ETA: 0s - loss: 0.4251 - accuracy: 0.81 - ETA: 0s - loss: 0.4224 - accuracy: 0.81 - ETA: 0s - loss: 0.4221 - accuracy: 0.81 - ETA: 0s - loss: 0.4185 - accuracy: 0.81 - ETA: 0s - loss: 0.4195 - accuracy: 0.81 - ETA: 0s - loss: 0.4209 - accuracy: 0.81 - ETA: 0s - loss: 0.4199 - accuracy: 0.81 - ETA: 0s - loss: 0.4213 - accuracy: 0.81 - ETA: 0s - loss: 0.4219 - accuracy: 0.81 - ETA: 0s - loss: 0.4214 - accuracy: 0.81 - ETA: 0s - loss: 0.4216 - accuracy: 0.81 - ETA: 0s - loss: 0.4215 - accuracy: 0.81 - ETA: 0s - loss: 0.4219 - accuracy: 0.81 - ETA: 0s - loss: 0.4224 - accuracy: 0.81 - ETA: 0s - loss: 0.4219 - accuracy: 0.81 - ETA: 0s - loss: 0.4221 - accuracy: 0.81 - ETA: 0s - loss: 0.4218 - accuracy: 0.81 - ETA: 0s - loss: 0.4218 - accuracy: 0.81 - ETA: 0s - loss: 0.4212 - accuracy: 0.81 - ETA: 0s - loss: 0.4228 - accuracy: 0.81 - 1s 87us/step - loss: 0.4227 - accuracy: 0.8125\n",
      "Epoch 44/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.3891 - accuracy: 0.90 - ETA: 1s - loss: 0.4617 - accuracy: 0.79 - ETA: 1s - loss: 0.4374 - accuracy: 0.81 - ETA: 1s - loss: 0.4360 - accuracy: 0.81 - ETA: 1s - loss: 0.4240 - accuracy: 0.81 - ETA: 1s - loss: 0.4278 - accuracy: 0.81 - ETA: 1s - loss: 0.4269 - accuracy: 0.81 - ETA: 1s - loss: 0.4253 - accuracy: 0.81 - ETA: 1s - loss: 0.4192 - accuracy: 0.81 - ETA: 1s - loss: 0.4192 - accuracy: 0.81 - ETA: 0s - loss: 0.4146 - accuracy: 0.82 - ETA: 0s - loss: 0.4150 - accuracy: 0.81 - ETA: 0s - loss: 0.4174 - accuracy: 0.81 - ETA: 0s - loss: 0.4215 - accuracy: 0.81 - ETA: 0s - loss: 0.4194 - accuracy: 0.81 - ETA: 0s - loss: 0.4185 - accuracy: 0.81 - ETA: 0s - loss: 0.4194 - accuracy: 0.81 - ETA: 0s - loss: 0.4210 - accuracy: 0.81 - ETA: 0s - loss: 0.4206 - accuracy: 0.81 - ETA: 0s - loss: 0.4191 - accuracy: 0.81 - ETA: 0s - loss: 0.4201 - accuracy: 0.81 - ETA: 0s - loss: 0.4197 - accuracy: 0.81 - ETA: 0s - loss: 0.4188 - accuracy: 0.81 - ETA: 0s - loss: 0.4199 - accuracy: 0.81 - ETA: 0s - loss: 0.4193 - accuracy: 0.81 - ETA: 0s - loss: 0.4201 - accuracy: 0.81 - ETA: 0s - loss: 0.4207 - accuracy: 0.81 - ETA: 0s - loss: 0.4213 - accuracy: 0.81 - ETA: 0s - loss: 0.4207 - accuracy: 0.81 - ETA: 0s - loss: 0.4211 - accuracy: 0.81 - 1s 87us/step - loss: 0.4212 - accuracy: 0.8146\n",
      "Epoch 45/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.3173 - accuracy: 0.80 - ETA: 1s - loss: 0.4058 - accuracy: 0.81 - ETA: 1s - loss: 0.4146 - accuracy: 0.81 - ETA: 1s - loss: 0.4063 - accuracy: 0.81 - ETA: 1s - loss: 0.4091 - accuracy: 0.81 - ETA: 1s - loss: 0.4152 - accuracy: 0.81 - ETA: 1s - loss: 0.4187 - accuracy: 0.81 - ETA: 1s - loss: 0.4166 - accuracy: 0.81 - ETA: 1s - loss: 0.4142 - accuracy: 0.81 - ETA: 0s - loss: 0.4156 - accuracy: 0.81 - ETA: 0s - loss: 0.4171 - accuracy: 0.81 - ETA: 0s - loss: 0.4157 - accuracy: 0.81 - ETA: 0s - loss: 0.4172 - accuracy: 0.81 - ETA: 0s - loss: 0.4184 - accuracy: 0.81 - ETA: 0s - loss: 0.4200 - accuracy: 0.81 - ETA: 0s - loss: 0.4196 - accuracy: 0.81 - ETA: 0s - loss: 0.4191 - accuracy: 0.81 - ETA: 0s - loss: 0.4181 - accuracy: 0.81 - ETA: 0s - loss: 0.4179 - accuracy: 0.81 - ETA: 0s - loss: 0.4209 - accuracy: 0.81 - ETA: 0s - loss: 0.4204 - accuracy: 0.81 - ETA: 0s - loss: 0.4224 - accuracy: 0.81 - ETA: 0s - loss: 0.4209 - accuracy: 0.81 - ETA: 0s - loss: 0.4211 - accuracy: 0.81 - ETA: 0s - loss: 0.4205 - accuracy: 0.81 - ETA: 0s - loss: 0.4210 - accuracy: 0.81 - ETA: 0s - loss: 0.4224 - accuracy: 0.81 - ETA: 0s - loss: 0.4218 - accuracy: 0.81 - ETA: 0s - loss: 0.4206 - accuracy: 0.81 - 1s 86us/step - loss: 0.4206 - accuracy: 0.8120\n",
      "Epoch 46/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.5832 - accuracy: 0.50 - ETA: 1s - loss: 0.4367 - accuracy: 0.79 - ETA: 1s - loss: 0.4256 - accuracy: 0.80 - ETA: 1s - loss: 0.4239 - accuracy: 0.81 - ETA: 1s - loss: 0.4229 - accuracy: 0.81 - ETA: 1s - loss: 0.4212 - accuracy: 0.80 - ETA: 1s - loss: 0.4238 - accuracy: 0.80 - ETA: 1s - loss: 0.4176 - accuracy: 0.81 - ETA: 1s - loss: 0.4181 - accuracy: 0.81 - ETA: 1s - loss: 0.4162 - accuracy: 0.81 - ETA: 0s - loss: 0.4115 - accuracy: 0.81 - ETA: 0s - loss: 0.4142 - accuracy: 0.81 - ETA: 0s - loss: 0.4144 - accuracy: 0.81 - ETA: 0s - loss: 0.4160 - accuracy: 0.81 - ETA: 0s - loss: 0.4183 - accuracy: 0.81 - ETA: 0s - loss: 0.4169 - accuracy: 0.81 - ETA: 0s - loss: 0.4191 - accuracy: 0.81 - ETA: 0s - loss: 0.4190 - accuracy: 0.81 - ETA: 0s - loss: 0.4193 - accuracy: 0.81 - ETA: 0s - loss: 0.4206 - accuracy: 0.81 - ETA: 0s - loss: 0.4195 - accuracy: 0.81 - ETA: 0s - loss: 0.4185 - accuracy: 0.81 - ETA: 0s - loss: 0.4189 - accuracy: 0.81 - ETA: 0s - loss: 0.4196 - accuracy: 0.81 - ETA: 0s - loss: 0.4196 - accuracy: 0.81 - ETA: 0s - loss: 0.4188 - accuracy: 0.81 - ETA: 0s - loss: 0.4190 - accuracy: 0.81 - ETA: 0s - loss: 0.4188 - accuracy: 0.81 - ETA: 0s - loss: 0.4181 - accuracy: 0.81 - ETA: 0s - loss: 0.4186 - accuracy: 0.81 - 1s 86us/step - loss: 0.4185 - accuracy: 0.8148\n",
      "Epoch 47/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.3815 - accuracy: 0.90 - ETA: 1s - loss: 0.4091 - accuracy: 0.81 - ETA: 1s - loss: 0.4001 - accuracy: 0.83 - ETA: 1s - loss: 0.4170 - accuracy: 0.82 - ETA: 1s - loss: 0.4160 - accuracy: 0.82 - ETA: 1s - loss: 0.4134 - accuracy: 0.82 - ETA: 1s - loss: 0.4135 - accuracy: 0.82 - ETA: 1s - loss: 0.4092 - accuracy: 0.82 - ETA: 1s - loss: 0.4098 - accuracy: 0.82 - ETA: 1s - loss: 0.4130 - accuracy: 0.82 - ETA: 0s - loss: 0.4127 - accuracy: 0.82 - ETA: 0s - loss: 0.4121 - accuracy: 0.82 - ETA: 0s - loss: 0.4127 - accuracy: 0.81 - ETA: 0s - loss: 0.4159 - accuracy: 0.81 - ETA: 0s - loss: 0.4173 - accuracy: 0.81 - ETA: 0s - loss: 0.4179 - accuracy: 0.81 - ETA: 0s - loss: 0.4182 - accuracy: 0.81 - ETA: 0s - loss: 0.4172 - accuracy: 0.81 - ETA: 0s - loss: 0.4162 - accuracy: 0.81 - ETA: 0s - loss: 0.4170 - accuracy: 0.81 - ETA: 0s - loss: 0.4173 - accuracy: 0.81 - ETA: 0s - loss: 0.4159 - accuracy: 0.81 - ETA: 0s - loss: 0.4178 - accuracy: 0.81 - ETA: 0s - loss: 0.4165 - accuracy: 0.81 - ETA: 0s - loss: 0.4165 - accuracy: 0.81 - ETA: 0s - loss: 0.4161 - accuracy: 0.81 - ETA: 0s - loss: 0.4160 - accuracy: 0.81 - ETA: 0s - loss: 0.4161 - accuracy: 0.81 - ETA: 0s - loss: 0.4160 - accuracy: 0.81 - 1s 86us/step - loss: 0.4165 - accuracy: 0.8168\n",
      "Epoch 48/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.8546 - accuracy: 0.50 - ETA: 1s - loss: 0.3887 - accuracy: 0.83 - ETA: 1s - loss: 0.3914 - accuracy: 0.83 - ETA: 1s - loss: 0.4196 - accuracy: 0.80 - ETA: 1s - loss: 0.4176 - accuracy: 0.80 - ETA: 1s - loss: 0.4099 - accuracy: 0.81 - ETA: 1s - loss: 0.4174 - accuracy: 0.80 - ETA: 1s - loss: 0.4140 - accuracy: 0.81 - ETA: 1s - loss: 0.4189 - accuracy: 0.81 - ETA: 1s - loss: 0.4186 - accuracy: 0.81 - ETA: 0s - loss: 0.4186 - accuracy: 0.80 - ETA: 0s - loss: 0.4199 - accuracy: 0.80 - ETA: 0s - loss: 0.4189 - accuracy: 0.80 - ETA: 0s - loss: 0.4189 - accuracy: 0.81 - ETA: 0s - loss: 0.4190 - accuracy: 0.81 - ETA: 0s - loss: 0.4177 - accuracy: 0.81 - ETA: 0s - loss: 0.4173 - accuracy: 0.81 - ETA: 0s - loss: 0.4189 - accuracy: 0.81 - ETA: 0s - loss: 0.4205 - accuracy: 0.81 - ETA: 0s - loss: 0.4189 - accuracy: 0.81 - ETA: 0s - loss: 0.4162 - accuracy: 0.81 - ETA: 0s - loss: 0.4167 - accuracy: 0.81 - ETA: 0s - loss: 0.4162 - accuracy: 0.81 - ETA: 0s - loss: 0.4158 - accuracy: 0.81 - ETA: 0s - loss: 0.4164 - accuracy: 0.81 - ETA: 0s - loss: 0.4147 - accuracy: 0.81 - ETA: 0s - loss: 0.4143 - accuracy: 0.81 - ETA: 0s - loss: 0.4163 - accuracy: 0.81 - ETA: 0s - loss: 0.4155 - accuracy: 0.81 - ETA: 0s - loss: 0.4164 - accuracy: 0.81 - 1s 87us/step - loss: 0.4161 - accuracy: 0.8140\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16887/16887 [==============================] - ETA: 5s - loss: 0.2392 - accuracy: 1.00 - ETA: 1s - loss: 0.4354 - accuracy: 0.78 - ETA: 1s - loss: 0.4340 - accuracy: 0.79 - ETA: 1s - loss: 0.4290 - accuracy: 0.80 - ETA: 1s - loss: 0.4232 - accuracy: 0.81 - ETA: 1s - loss: 0.4242 - accuracy: 0.81 - ETA: 1s - loss: 0.4240 - accuracy: 0.81 - ETA: 1s - loss: 0.4204 - accuracy: 0.81 - ETA: 1s - loss: 0.4225 - accuracy: 0.81 - ETA: 1s - loss: 0.4226 - accuracy: 0.81 - ETA: 0s - loss: 0.4264 - accuracy: 0.81 - ETA: 0s - loss: 0.4261 - accuracy: 0.81 - ETA: 0s - loss: 0.4239 - accuracy: 0.81 - ETA: 0s - loss: 0.4221 - accuracy: 0.81 - ETA: 0s - loss: 0.4228 - accuracy: 0.81 - ETA: 0s - loss: 0.4214 - accuracy: 0.81 - ETA: 0s - loss: 0.4198 - accuracy: 0.81 - ETA: 0s - loss: 0.4196 - accuracy: 0.81 - ETA: 0s - loss: 0.4180 - accuracy: 0.81 - ETA: 0s - loss: 0.4176 - accuracy: 0.81 - ETA: 0s - loss: 0.4166 - accuracy: 0.81 - ETA: 0s - loss: 0.4153 - accuracy: 0.81 - ETA: 0s - loss: 0.4156 - accuracy: 0.81 - ETA: 0s - loss: 0.4158 - accuracy: 0.81 - ETA: 0s - loss: 0.4153 - accuracy: 0.81 - ETA: 0s - loss: 0.4149 - accuracy: 0.81 - ETA: 0s - loss: 0.4143 - accuracy: 0.81 - ETA: 0s - loss: 0.4145 - accuracy: 0.81 - ETA: 0s - loss: 0.4143 - accuracy: 0.81 - ETA: 0s - loss: 0.4140 - accuracy: 0.81 - 1s 88us/step - loss: 0.4139 - accuracy: 0.8160\n",
      "Epoch 50/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.5945 - accuracy: 0.70 - ETA: 1s - loss: 0.4123 - accuracy: 0.84 - ETA: 1s - loss: 0.4184 - accuracy: 0.82 - ETA: 1s - loss: 0.4109 - accuracy: 0.83 - ETA: 1s - loss: 0.4264 - accuracy: 0.81 - ETA: 1s - loss: 0.4286 - accuracy: 0.81 - ETA: 1s - loss: 0.4260 - accuracy: 0.81 - ETA: 1s - loss: 0.4253 - accuracy: 0.81 - ETA: 1s - loss: 0.4201 - accuracy: 0.81 - ETA: 0s - loss: 0.4206 - accuracy: 0.81 - ETA: 0s - loss: 0.4206 - accuracy: 0.81 - ETA: 0s - loss: 0.4207 - accuracy: 0.81 - ETA: 0s - loss: 0.4182 - accuracy: 0.81 - ETA: 0s - loss: 0.4187 - accuracy: 0.81 - ETA: 0s - loss: 0.4191 - accuracy: 0.81 - ETA: 0s - loss: 0.4176 - accuracy: 0.81 - ETA: 0s - loss: 0.4197 - accuracy: 0.81 - ETA: 0s - loss: 0.4196 - accuracy: 0.81 - ETA: 0s - loss: 0.4192 - accuracy: 0.81 - ETA: 0s - loss: 0.4176 - accuracy: 0.81 - ETA: 0s - loss: 0.4169 - accuracy: 0.81 - ETA: 0s - loss: 0.4165 - accuracy: 0.81 - ETA: 0s - loss: 0.4155 - accuracy: 0.81 - ETA: 0s - loss: 0.4158 - accuracy: 0.81 - ETA: 0s - loss: 0.4145 - accuracy: 0.81 - ETA: 0s - loss: 0.4155 - accuracy: 0.81 - ETA: 0s - loss: 0.4146 - accuracy: 0.81 - ETA: 0s - loss: 0.4137 - accuracy: 0.81 - ETA: 0s - loss: 0.4134 - accuracy: 0.81 - 1s 85us/step - loss: 0.4130 - accuracy: 0.8180\n",
      "Epoch 51/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.3429 - accuracy: 0.90 - ETA: 1s - loss: 0.3824 - accuracy: 0.83 - ETA: 1s - loss: 0.4078 - accuracy: 0.81 - ETA: 1s - loss: 0.4195 - accuracy: 0.81 - ETA: 1s - loss: 0.4227 - accuracy: 0.80 - ETA: 1s - loss: 0.4248 - accuracy: 0.80 - ETA: 1s - loss: 0.4180 - accuracy: 0.81 - ETA: 1s - loss: 0.4171 - accuracy: 0.81 - ETA: 1s - loss: 0.4185 - accuracy: 0.81 - ETA: 0s - loss: 0.4135 - accuracy: 0.81 - ETA: 0s - loss: 0.4151 - accuracy: 0.81 - ETA: 0s - loss: 0.4118 - accuracy: 0.81 - ETA: 0s - loss: 0.4126 - accuracy: 0.81 - ETA: 0s - loss: 0.4118 - accuracy: 0.81 - ETA: 0s - loss: 0.4109 - accuracy: 0.82 - ETA: 0s - loss: 0.4122 - accuracy: 0.81 - ETA: 0s - loss: 0.4120 - accuracy: 0.81 - ETA: 0s - loss: 0.4131 - accuracy: 0.81 - ETA: 0s - loss: 0.4128 - accuracy: 0.81 - ETA: 0s - loss: 0.4120 - accuracy: 0.81 - ETA: 0s - loss: 0.4117 - accuracy: 0.81 - ETA: 0s - loss: 0.4126 - accuracy: 0.81 - ETA: 0s - loss: 0.4132 - accuracy: 0.81 - ETA: 0s - loss: 0.4139 - accuracy: 0.81 - ETA: 0s - loss: 0.4149 - accuracy: 0.81 - ETA: 0s - loss: 0.4153 - accuracy: 0.81 - ETA: 0s - loss: 0.4141 - accuracy: 0.81 - ETA: 0s - loss: 0.4127 - accuracy: 0.81 - ETA: 0s - loss: 0.4137 - accuracy: 0.81 - ETA: 0s - loss: 0.4127 - accuracy: 0.81 - 1s 86us/step - loss: 0.4126 - accuracy: 0.8180\n",
      "Epoch 52/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.7231 - accuracy: 0.60 - ETA: 1s - loss: 0.4270 - accuracy: 0.80 - ETA: 1s - loss: 0.4100 - accuracy: 0.81 - ETA: 1s - loss: 0.3997 - accuracy: 0.82 - ETA: 1s - loss: 0.4058 - accuracy: 0.81 - ETA: 1s - loss: 0.4122 - accuracy: 0.81 - ETA: 1s - loss: 0.4097 - accuracy: 0.82 - ETA: 1s - loss: 0.4116 - accuracy: 0.81 - ETA: 1s - loss: 0.4089 - accuracy: 0.81 - ETA: 0s - loss: 0.4079 - accuracy: 0.82 - ETA: 0s - loss: 0.4055 - accuracy: 0.82 - ETA: 0s - loss: 0.4032 - accuracy: 0.82 - ETA: 0s - loss: 0.4032 - accuracy: 0.82 - ETA: 0s - loss: 0.4037 - accuracy: 0.82 - ETA: 0s - loss: 0.4040 - accuracy: 0.82 - ETA: 0s - loss: 0.4057 - accuracy: 0.81 - ETA: 0s - loss: 0.4051 - accuracy: 0.82 - ETA: 0s - loss: 0.4047 - accuracy: 0.82 - ETA: 0s - loss: 0.4064 - accuracy: 0.81 - ETA: 0s - loss: 0.4084 - accuracy: 0.81 - ETA: 0s - loss: 0.4089 - accuracy: 0.81 - ETA: 0s - loss: 0.4082 - accuracy: 0.81 - ETA: 0s - loss: 0.4090 - accuracy: 0.81 - ETA: 0s - loss: 0.4102 - accuracy: 0.81 - ETA: 0s - loss: 0.4110 - accuracy: 0.81 - ETA: 0s - loss: 0.4107 - accuracy: 0.81 - ETA: 0s - loss: 0.4102 - accuracy: 0.81 - ETA: 0s - loss: 0.4098 - accuracy: 0.81 - ETA: 0s - loss: 0.4107 - accuracy: 0.81 - 1s 84us/step - loss: 0.4104 - accuracy: 0.8167\n",
      "Epoch 53/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.5454 - accuracy: 0.80 - ETA: 1s - loss: 0.3981 - accuracy: 0.83 - ETA: 1s - loss: 0.4109 - accuracy: 0.82 - ETA: 1s - loss: 0.4144 - accuracy: 0.82 - ETA: 1s - loss: 0.4083 - accuracy: 0.82 - ETA: 1s - loss: 0.4118 - accuracy: 0.81 - ETA: 1s - loss: 0.4124 - accuracy: 0.81 - ETA: 1s - loss: 0.4105 - accuracy: 0.82 - ETA: 1s - loss: 0.4085 - accuracy: 0.82 - ETA: 0s - loss: 0.4068 - accuracy: 0.82 - ETA: 0s - loss: 0.4059 - accuracy: 0.82 - ETA: 0s - loss: 0.4079 - accuracy: 0.82 - ETA: 0s - loss: 0.4083 - accuracy: 0.82 - ETA: 0s - loss: 0.4074 - accuracy: 0.82 - ETA: 0s - loss: 0.4051 - accuracy: 0.82 - ETA: 0s - loss: 0.4086 - accuracy: 0.82 - ETA: 0s - loss: 0.4099 - accuracy: 0.82 - ETA: 0s - loss: 0.4102 - accuracy: 0.82 - ETA: 0s - loss: 0.4116 - accuracy: 0.82 - ETA: 0s - loss: 0.4098 - accuracy: 0.82 - ETA: 0s - loss: 0.4088 - accuracy: 0.82 - ETA: 0s - loss: 0.4088 - accuracy: 0.82 - ETA: 0s - loss: 0.4089 - accuracy: 0.82 - ETA: 0s - loss: 0.4084 - accuracy: 0.82 - ETA: 0s - loss: 0.4085 - accuracy: 0.82 - ETA: 0s - loss: 0.4088 - accuracy: 0.82 - ETA: 0s - loss: 0.4094 - accuracy: 0.82 - ETA: 0s - loss: 0.4090 - accuracy: 0.82 - ETA: 0s - loss: 0.4084 - accuracy: 0.82 - 1s 85us/step - loss: 0.4096 - accuracy: 0.8203\n",
      "Epoch 54/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.1879 - accuracy: 0.90 - ETA: 1s - loss: 0.4279 - accuracy: 0.82 - ETA: 1s - loss: 0.4110 - accuracy: 0.83 - ETA: 1s - loss: 0.4034 - accuracy: 0.83 - ETA: 1s - loss: 0.4120 - accuracy: 0.82 - ETA: 1s - loss: 0.4071 - accuracy: 0.82 - ETA: 1s - loss: 0.4085 - accuracy: 0.82 - ETA: 1s - loss: 0.4047 - accuracy: 0.82 - ETA: 1s - loss: 0.4065 - accuracy: 0.82 - ETA: 0s - loss: 0.4074 - accuracy: 0.82 - ETA: 0s - loss: 0.4060 - accuracy: 0.82 - ETA: 0s - loss: 0.4072 - accuracy: 0.82 - ETA: 0s - loss: 0.4048 - accuracy: 0.82 - ETA: 0s - loss: 0.4083 - accuracy: 0.82 - ETA: 0s - loss: 0.4076 - accuracy: 0.82 - ETA: 0s - loss: 0.4090 - accuracy: 0.82 - ETA: 0s - loss: 0.4076 - accuracy: 0.82 - ETA: 0s - loss: 0.4088 - accuracy: 0.82 - ETA: 0s - loss: 0.4087 - accuracy: 0.82 - ETA: 0s - loss: 0.4096 - accuracy: 0.82 - ETA: 0s - loss: 0.4090 - accuracy: 0.82 - ETA: 0s - loss: 0.4111 - accuracy: 0.82 - ETA: 0s - loss: 0.4118 - accuracy: 0.82 - ETA: 0s - loss: 0.4100 - accuracy: 0.82 - ETA: 0s - loss: 0.4099 - accuracy: 0.82 - ETA: 0s - loss: 0.4096 - accuracy: 0.82 - ETA: 0s - loss: 0.4099 - accuracy: 0.82 - ETA: 0s - loss: 0.4083 - accuracy: 0.82 - ETA: 0s - loss: 0.4090 - accuracy: 0.82 - 1s 84us/step - loss: 0.4091 - accuracy: 0.8209\n",
      "Epoch 55/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.3010 - accuracy: 0.80 - ETA: 1s - loss: 0.4546 - accuracy: 0.80 - ETA: 1s - loss: 0.4394 - accuracy: 0.81 - ETA: 1s - loss: 0.4369 - accuracy: 0.80 - ETA: 1s - loss: 0.4268 - accuracy: 0.81 - ETA: 1s - loss: 0.4160 - accuracy: 0.81 - ETA: 1s - loss: 0.4145 - accuracy: 0.81 - ETA: 1s - loss: 0.4093 - accuracy: 0.81 - ETA: 1s - loss: 0.4093 - accuracy: 0.81 - ETA: 0s - loss: 0.4083 - accuracy: 0.81 - ETA: 0s - loss: 0.4074 - accuracy: 0.81 - ETA: 0s - loss: 0.4102 - accuracy: 0.81 - ETA: 0s - loss: 0.4088 - accuracy: 0.81 - ETA: 0s - loss: 0.4086 - accuracy: 0.81 - ETA: 0s - loss: 0.4095 - accuracy: 0.81 - ETA: 0s - loss: 0.4107 - accuracy: 0.81 - ETA: 0s - loss: 0.4087 - accuracy: 0.81 - ETA: 0s - loss: 0.4080 - accuracy: 0.81 - ETA: 0s - loss: 0.4109 - accuracy: 0.81 - ETA: 0s - loss: 0.4088 - accuracy: 0.81 - ETA: 0s - loss: 0.4094 - accuracy: 0.81 - ETA: 0s - loss: 0.4091 - accuracy: 0.81 - ETA: 0s - loss: 0.4084 - accuracy: 0.81 - ETA: 0s - loss: 0.4086 - accuracy: 0.81 - ETA: 0s - loss: 0.4081 - accuracy: 0.81 - ETA: 0s - loss: 0.4084 - accuracy: 0.81 - ETA: 0s - loss: 0.4089 - accuracy: 0.81 - ETA: 0s - loss: 0.4090 - accuracy: 0.81 - ETA: 0s - loss: 0.4083 - accuracy: 0.81 - 1s 84us/step - loss: 0.4076 - accuracy: 0.8191\n",
      "Epoch 56/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.3236 - accuracy: 0.90 - ETA: 1s - loss: 0.3634 - accuracy: 0.83 - ETA: 1s - loss: 0.3830 - accuracy: 0.82 - ETA: 1s - loss: 0.3817 - accuracy: 0.82 - ETA: 1s - loss: 0.3918 - accuracy: 0.82 - ETA: 1s - loss: 0.3947 - accuracy: 0.82 - ETA: 1s - loss: 0.3987 - accuracy: 0.82 - ETA: 1s - loss: 0.3980 - accuracy: 0.82 - ETA: 1s - loss: 0.3986 - accuracy: 0.82 - ETA: 0s - loss: 0.4029 - accuracy: 0.81 - ETA: 0s - loss: 0.4023 - accuracy: 0.81 - ETA: 0s - loss: 0.4052 - accuracy: 0.81 - ETA: 0s - loss: 0.4053 - accuracy: 0.81 - ETA: 0s - loss: 0.4010 - accuracy: 0.82 - ETA: 0s - loss: 0.4040 - accuracy: 0.81 - ETA: 0s - loss: 0.4025 - accuracy: 0.82 - ETA: 0s - loss: 0.4043 - accuracy: 0.81 - ETA: 0s - loss: 0.4051 - accuracy: 0.81 - ETA: 0s - loss: 0.4044 - accuracy: 0.81 - ETA: 0s - loss: 0.4056 - accuracy: 0.81 - ETA: 0s - loss: 0.4049 - accuracy: 0.81 - ETA: 0s - loss: 0.4033 - accuracy: 0.82 - ETA: 0s - loss: 0.4032 - accuracy: 0.82 - ETA: 0s - loss: 0.4018 - accuracy: 0.82 - ETA: 0s - loss: 0.4030 - accuracy: 0.82 - ETA: 0s - loss: 0.4049 - accuracy: 0.82 - ETA: 0s - loss: 0.4058 - accuracy: 0.81 - ETA: 0s - loss: 0.4072 - accuracy: 0.81 - ETA: 0s - loss: 0.4072 - accuracy: 0.81 - 1s 84us/step - loss: 0.4074 - accuracy: 0.8193\n",
      "Epoch 57/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.3640 - accuracy: 0.80 - ETA: 1s - loss: 0.3929 - accuracy: 0.81 - ETA: 1s - loss: 0.3906 - accuracy: 0.82 - ETA: 1s - loss: 0.3829 - accuracy: 0.83 - ETA: 1s - loss: 0.3944 - accuracy: 0.82 - ETA: 1s - loss: 0.3916 - accuracy: 0.83 - ETA: 1s - loss: 0.3979 - accuracy: 0.82 - ETA: 1s - loss: 0.3964 - accuracy: 0.82 - ETA: 1s - loss: 0.4001 - accuracy: 0.82 - ETA: 0s - loss: 0.3994 - accuracy: 0.82 - ETA: 0s - loss: 0.3968 - accuracy: 0.82 - ETA: 0s - loss: 0.3948 - accuracy: 0.83 - ETA: 0s - loss: 0.3944 - accuracy: 0.82 - ETA: 0s - loss: 0.3955 - accuracy: 0.82 - ETA: 0s - loss: 0.3975 - accuracy: 0.82 - ETA: 0s - loss: 0.3976 - accuracy: 0.82 - ETA: 0s - loss: 0.3982 - accuracy: 0.82 - ETA: 0s - loss: 0.3993 - accuracy: 0.82 - ETA: 0s - loss: 0.3996 - accuracy: 0.82 - ETA: 0s - loss: 0.4012 - accuracy: 0.82 - ETA: 0s - loss: 0.4024 - accuracy: 0.82 - ETA: 0s - loss: 0.4027 - accuracy: 0.82 - ETA: 0s - loss: 0.4040 - accuracy: 0.82 - ETA: 0s - loss: 0.4036 - accuracy: 0.82 - ETA: 0s - loss: 0.4046 - accuracy: 0.82 - ETA: 0s - loss: 0.4054 - accuracy: 0.82 - ETA: 0s - loss: 0.4065 - accuracy: 0.82 - ETA: 0s - loss: 0.4053 - accuracy: 0.82 - ETA: 0s - loss: 0.4058 - accuracy: 0.82 - 1s 84us/step - loss: 0.4057 - accuracy: 0.8216\n",
      "Epoch 58/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.4092 - accuracy: 0.60 - ETA: 1s - loss: 0.4460 - accuracy: 0.78 - ETA: 1s - loss: 0.4166 - accuracy: 0.82 - ETA: 1s - loss: 0.4049 - accuracy: 0.83 - ETA: 1s - loss: 0.4028 - accuracy: 0.82 - ETA: 1s - loss: 0.4054 - accuracy: 0.82 - ETA: 1s - loss: 0.3996 - accuracy: 0.82 - ETA: 1s - loss: 0.4053 - accuracy: 0.82 - ETA: 1s - loss: 0.4041 - accuracy: 0.82 - ETA: 0s - loss: 0.4006 - accuracy: 0.82 - ETA: 0s - loss: 0.4024 - accuracy: 0.82 - ETA: 0s - loss: 0.4040 - accuracy: 0.82 - ETA: 0s - loss: 0.4009 - accuracy: 0.82 - ETA: 0s - loss: 0.4012 - accuracy: 0.82 - ETA: 0s - loss: 0.3984 - accuracy: 0.82 - ETA: 0s - loss: 0.3986 - accuracy: 0.82 - ETA: 0s - loss: 0.3998 - accuracy: 0.82 - ETA: 0s - loss: 0.3993 - accuracy: 0.82 - ETA: 0s - loss: 0.4008 - accuracy: 0.82 - ETA: 0s - loss: 0.4012 - accuracy: 0.82 - ETA: 0s - loss: 0.4003 - accuracy: 0.82 - ETA: 0s - loss: 0.3996 - accuracy: 0.82 - ETA: 0s - loss: 0.4016 - accuracy: 0.82 - ETA: 0s - loss: 0.4026 - accuracy: 0.82 - ETA: 0s - loss: 0.4034 - accuracy: 0.82 - ETA: 0s - loss: 0.4042 - accuracy: 0.82 - ETA: 0s - loss: 0.4047 - accuracy: 0.82 - ETA: 0s - loss: 0.4051 - accuracy: 0.82 - ETA: 0s - loss: 0.4054 - accuracy: 0.82 - 1s 85us/step - loss: 0.4055 - accuracy: 0.8211\n",
      "Epoch 59/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.1654 - accuracy: 1.00 - ETA: 1s - loss: 0.4027 - accuracy: 0.82 - ETA: 1s - loss: 0.3980 - accuracy: 0.81 - ETA: 1s - loss: 0.4018 - accuracy: 0.82 - ETA: 1s - loss: 0.4026 - accuracy: 0.82 - ETA: 1s - loss: 0.4023 - accuracy: 0.82 - ETA: 1s - loss: 0.3986 - accuracy: 0.82 - ETA: 1s - loss: 0.3955 - accuracy: 0.82 - ETA: 1s - loss: 0.3971 - accuracy: 0.82 - ETA: 1s - loss: 0.3986 - accuracy: 0.82 - ETA: 0s - loss: 0.4004 - accuracy: 0.81 - ETA: 0s - loss: 0.4035 - accuracy: 0.81 - ETA: 0s - loss: 0.4067 - accuracy: 0.81 - ETA: 0s - loss: 0.4071 - accuracy: 0.81 - ETA: 0s - loss: 0.4060 - accuracy: 0.82 - ETA: 0s - loss: 0.4049 - accuracy: 0.82 - ETA: 0s - loss: 0.4056 - accuracy: 0.82 - ETA: 0s - loss: 0.4034 - accuracy: 0.82 - ETA: 0s - loss: 0.4042 - accuracy: 0.82 - ETA: 0s - loss: 0.4034 - accuracy: 0.82 - ETA: 0s - loss: 0.4043 - accuracy: 0.82 - ETA: 0s - loss: 0.4052 - accuracy: 0.82 - ETA: 0s - loss: 0.4057 - accuracy: 0.82 - ETA: 0s - loss: 0.4057 - accuracy: 0.82 - ETA: 0s - loss: 0.4041 - accuracy: 0.82 - ETA: 0s - loss: 0.4040 - accuracy: 0.82 - ETA: 0s - loss: 0.4046 - accuracy: 0.82 - ETA: 0s - loss: 0.4046 - accuracy: 0.82 - ETA: 0s - loss: 0.4038 - accuracy: 0.82 - 1s 86us/step - loss: 0.4036 - accuracy: 0.8218\n",
      "Epoch 60/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.1590 - accuracy: 1.00 - ETA: 1s - loss: 0.3944 - accuracy: 0.83 - ETA: 1s - loss: 0.3931 - accuracy: 0.82 - ETA: 1s - loss: 0.3918 - accuracy: 0.82 - ETA: 1s - loss: 0.4023 - accuracy: 0.81 - ETA: 1s - loss: 0.4072 - accuracy: 0.81 - ETA: 1s - loss: 0.4095 - accuracy: 0.81 - ETA: 1s - loss: 0.4084 - accuracy: 0.81 - ETA: 1s - loss: 0.4042 - accuracy: 0.81 - ETA: 1s - loss: 0.4018 - accuracy: 0.81 - ETA: 0s - loss: 0.4072 - accuracy: 0.81 - ETA: 0s - loss: 0.4070 - accuracy: 0.81 - ETA: 0s - loss: 0.4054 - accuracy: 0.81 - ETA: 0s - loss: 0.4061 - accuracy: 0.81 - ETA: 0s - loss: 0.4075 - accuracy: 0.81 - ETA: 0s - loss: 0.4072 - accuracy: 0.81 - ETA: 0s - loss: 0.4087 - accuracy: 0.81 - ETA: 0s - loss: 0.4084 - accuracy: 0.81 - ETA: 0s - loss: 0.4093 - accuracy: 0.81 - ETA: 0s - loss: 0.4068 - accuracy: 0.81 - ETA: 0s - loss: 0.4049 - accuracy: 0.81 - ETA: 0s - loss: 0.4037 - accuracy: 0.81 - ETA: 0s - loss: 0.4021 - accuracy: 0.81 - ETA: 0s - loss: 0.4025 - accuracy: 0.81 - ETA: 0s - loss: 0.4013 - accuracy: 0.82 - ETA: 0s - loss: 0.4024 - accuracy: 0.81 - ETA: 0s - loss: 0.4035 - accuracy: 0.81 - ETA: 0s - loss: 0.4028 - accuracy: 0.82 - ETA: 0s - loss: 0.4023 - accuracy: 0.82 - ETA: 0s - loss: 0.4029 - accuracy: 0.82 - 1s 87us/step - loss: 0.4028 - accuracy: 0.8208\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16887/16887 [==============================] - ETA: 3s - loss: 0.8457 - accuracy: 0.70 - ETA: 1s - loss: 0.4458 - accuracy: 0.81 - ETA: 1s - loss: 0.4079 - accuracy: 0.83 - ETA: 1s - loss: 0.4151 - accuracy: 0.82 - ETA: 1s - loss: 0.4155 - accuracy: 0.82 - ETA: 1s - loss: 0.4101 - accuracy: 0.82 - ETA: 1s - loss: 0.4115 - accuracy: 0.81 - ETA: 1s - loss: 0.4057 - accuracy: 0.82 - ETA: 1s - loss: 0.4043 - accuracy: 0.82 - ETA: 1s - loss: 0.4038 - accuracy: 0.82 - ETA: 0s - loss: 0.4081 - accuracy: 0.81 - ETA: 0s - loss: 0.4056 - accuracy: 0.82 - ETA: 0s - loss: 0.4062 - accuracy: 0.82 - ETA: 0s - loss: 0.4055 - accuracy: 0.82 - ETA: 0s - loss: 0.4081 - accuracy: 0.81 - ETA: 0s - loss: 0.4084 - accuracy: 0.81 - ETA: 0s - loss: 0.4073 - accuracy: 0.81 - ETA: 0s - loss: 0.4081 - accuracy: 0.81 - ETA: 0s - loss: 0.4063 - accuracy: 0.82 - ETA: 0s - loss: 0.4031 - accuracy: 0.82 - ETA: 0s - loss: 0.4041 - accuracy: 0.82 - ETA: 0s - loss: 0.4038 - accuracy: 0.82 - ETA: 0s - loss: 0.4031 - accuracy: 0.82 - ETA: 0s - loss: 0.4026 - accuracy: 0.82 - ETA: 0s - loss: 0.4029 - accuracy: 0.82 - ETA: 0s - loss: 0.4036 - accuracy: 0.82 - ETA: 0s - loss: 0.4035 - accuracy: 0.82 - ETA: 0s - loss: 0.4036 - accuracy: 0.82 - ETA: 0s - loss: 0.4022 - accuracy: 0.82 - 1s 86us/step - loss: 0.4023 - accuracy: 0.8223\n",
      "Epoch 62/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.3494 - accuracy: 0.80 - ETA: 1s - loss: 0.3676 - accuracy: 0.84 - ETA: 1s - loss: 0.3865 - accuracy: 0.83 - ETA: 1s - loss: 0.3926 - accuracy: 0.83 - ETA: 1s - loss: 0.3934 - accuracy: 0.82 - ETA: 1s - loss: 0.3937 - accuracy: 0.82 - ETA: 1s - loss: 0.4000 - accuracy: 0.82 - ETA: 1s - loss: 0.3978 - accuracy: 0.82 - ETA: 1s - loss: 0.4001 - accuracy: 0.82 - ETA: 1s - loss: 0.4006 - accuracy: 0.82 - ETA: 0s - loss: 0.4010 - accuracy: 0.82 - ETA: 0s - loss: 0.4007 - accuracy: 0.82 - ETA: 0s - loss: 0.4004 - accuracy: 0.82 - ETA: 0s - loss: 0.4003 - accuracy: 0.82 - ETA: 0s - loss: 0.3985 - accuracy: 0.82 - ETA: 0s - loss: 0.4005 - accuracy: 0.82 - ETA: 0s - loss: 0.4006 - accuracy: 0.82 - ETA: 0s - loss: 0.4025 - accuracy: 0.82 - ETA: 0s - loss: 0.4044 - accuracy: 0.81 - ETA: 0s - loss: 0.4023 - accuracy: 0.82 - ETA: 0s - loss: 0.4034 - accuracy: 0.81 - ETA: 0s - loss: 0.4037 - accuracy: 0.81 - ETA: 0s - loss: 0.4040 - accuracy: 0.81 - ETA: 0s - loss: 0.4042 - accuracy: 0.81 - ETA: 0s - loss: 0.4052 - accuracy: 0.81 - ETA: 0s - loss: 0.4053 - accuracy: 0.81 - ETA: 0s - loss: 0.4040 - accuracy: 0.81 - ETA: 0s - loss: 0.4025 - accuracy: 0.82 - ETA: 0s - loss: 0.4035 - accuracy: 0.82 - 1s 86us/step - loss: 0.4036 - accuracy: 0.8206\n",
      "Epoch 63/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.6008 - accuracy: 0.60 - ETA: 1s - loss: 0.3862 - accuracy: 0.83 - ETA: 1s - loss: 0.3959 - accuracy: 0.81 - ETA: 1s - loss: 0.4040 - accuracy: 0.81 - ETA: 1s - loss: 0.3998 - accuracy: 0.82 - ETA: 1s - loss: 0.4013 - accuracy: 0.82 - ETA: 1s - loss: 0.4057 - accuracy: 0.81 - ETA: 1s - loss: 0.4012 - accuracy: 0.82 - ETA: 1s - loss: 0.4010 - accuracy: 0.82 - ETA: 0s - loss: 0.4022 - accuracy: 0.82 - ETA: 0s - loss: 0.4072 - accuracy: 0.81 - ETA: 0s - loss: 0.4073 - accuracy: 0.81 - ETA: 0s - loss: 0.4059 - accuracy: 0.82 - ETA: 0s - loss: 0.4062 - accuracy: 0.81 - ETA: 0s - loss: 0.4045 - accuracy: 0.82 - ETA: 0s - loss: 0.4045 - accuracy: 0.82 - ETA: 0s - loss: 0.4033 - accuracy: 0.82 - ETA: 0s - loss: 0.4039 - accuracy: 0.82 - ETA: 0s - loss: 0.4038 - accuracy: 0.82 - ETA: 0s - loss: 0.4036 - accuracy: 0.82 - ETA: 0s - loss: 0.4018 - accuracy: 0.82 - ETA: 0s - loss: 0.4009 - accuracy: 0.82 - ETA: 0s - loss: 0.4021 - accuracy: 0.82 - ETA: 0s - loss: 0.4025 - accuracy: 0.82 - ETA: 0s - loss: 0.4023 - accuracy: 0.82 - ETA: 0s - loss: 0.4032 - accuracy: 0.82 - ETA: 0s - loss: 0.4026 - accuracy: 0.82 - ETA: 0s - loss: 0.4024 - accuracy: 0.82 - ETA: 0s - loss: 0.4022 - accuracy: 0.82 - 1s 86us/step - loss: 0.4014 - accuracy: 0.8230\n",
      "Epoch 64/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.3108 - accuracy: 0.90 - ETA: 1s - loss: 0.4035 - accuracy: 0.83 - ETA: 1s - loss: 0.4023 - accuracy: 0.83 - ETA: 1s - loss: 0.4114 - accuracy: 0.82 - ETA: 1s - loss: 0.4049 - accuracy: 0.82 - ETA: 1s - loss: 0.4085 - accuracy: 0.82 - ETA: 1s - loss: 0.4064 - accuracy: 0.82 - ETA: 1s - loss: 0.4080 - accuracy: 0.81 - ETA: 1s - loss: 0.4066 - accuracy: 0.81 - ETA: 1s - loss: 0.4052 - accuracy: 0.81 - ETA: 1s - loss: 0.4009 - accuracy: 0.82 - ETA: 0s - loss: 0.4021 - accuracy: 0.82 - ETA: 0s - loss: 0.4027 - accuracy: 0.82 - ETA: 0s - loss: 0.4077 - accuracy: 0.81 - ETA: 0s - loss: 0.4090 - accuracy: 0.81 - ETA: 0s - loss: 0.4063 - accuracy: 0.81 - ETA: 0s - loss: 0.4062 - accuracy: 0.81 - ETA: 0s - loss: 0.4055 - accuracy: 0.81 - ETA: 0s - loss: 0.4043 - accuracy: 0.81 - ETA: 0s - loss: 0.4030 - accuracy: 0.82 - ETA: 0s - loss: 0.4019 - accuracy: 0.82 - ETA: 0s - loss: 0.4011 - accuracy: 0.82 - ETA: 0s - loss: 0.4033 - accuracy: 0.82 - ETA: 0s - loss: 0.4024 - accuracy: 0.82 - ETA: 0s - loss: 0.4023 - accuracy: 0.82 - ETA: 0s - loss: 0.4021 - accuracy: 0.82 - ETA: 0s - loss: 0.4018 - accuracy: 0.82 - ETA: 0s - loss: 0.4013 - accuracy: 0.82 - ETA: 0s - loss: 0.4006 - accuracy: 0.82 - 1s 85us/step - loss: 0.4012 - accuracy: 0.8219\n",
      "Epoch 65/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.2944 - accuracy: 0.90 - ETA: 1s - loss: 0.4154 - accuracy: 0.82 - ETA: 1s - loss: 0.4170 - accuracy: 0.81 - ETA: 1s - loss: 0.4109 - accuracy: 0.82 - ETA: 1s - loss: 0.4201 - accuracy: 0.80 - ETA: 1s - loss: 0.4152 - accuracy: 0.81 - ETA: 1s - loss: 0.4128 - accuracy: 0.81 - ETA: 1s - loss: 0.4112 - accuracy: 0.81 - ETA: 1s - loss: 0.4052 - accuracy: 0.81 - ETA: 1s - loss: 0.4063 - accuracy: 0.81 - ETA: 1s - loss: 0.4054 - accuracy: 0.81 - ETA: 0s - loss: 0.4055 - accuracy: 0.81 - ETA: 0s - loss: 0.4063 - accuracy: 0.81 - ETA: 0s - loss: 0.4063 - accuracy: 0.81 - ETA: 0s - loss: 0.4041 - accuracy: 0.81 - ETA: 0s - loss: 0.4043 - accuracy: 0.81 - ETA: 0s - loss: 0.4033 - accuracy: 0.82 - ETA: 0s - loss: 0.4007 - accuracy: 0.82 - ETA: 0s - loss: 0.4033 - accuracy: 0.82 - ETA: 0s - loss: 0.4025 - accuracy: 0.82 - ETA: 0s - loss: 0.4026 - accuracy: 0.82 - ETA: 0s - loss: 0.4050 - accuracy: 0.81 - ETA: 0s - loss: 0.4055 - accuracy: 0.81 - ETA: 0s - loss: 0.4049 - accuracy: 0.81 - ETA: 0s - loss: 0.4038 - accuracy: 0.82 - ETA: 0s - loss: 0.4028 - accuracy: 0.82 - ETA: 0s - loss: 0.4039 - accuracy: 0.82 - ETA: 0s - loss: 0.4020 - accuracy: 0.82 - ETA: 0s - loss: 0.4008 - accuracy: 0.82 - ETA: 0s - loss: 0.3993 - accuracy: 0.82 - ETA: 0s - loss: 0.4002 - accuracy: 0.82 - 2s 89us/step - loss: 0.4009 - accuracy: 0.8225\n",
      "Epoch 66/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.0978 - accuracy: 1.00 - ETA: 1s - loss: 0.4076 - accuracy: 0.81 - ETA: 1s - loss: 0.4000 - accuracy: 0.82 - ETA: 1s - loss: 0.4121 - accuracy: 0.82 - ETA: 1s - loss: 0.4061 - accuracy: 0.81 - ETA: 1s - loss: 0.4088 - accuracy: 0.81 - ETA: 1s - loss: 0.4072 - accuracy: 0.81 - ETA: 1s - loss: 0.4032 - accuracy: 0.81 - ETA: 1s - loss: 0.4018 - accuracy: 0.82 - ETA: 1s - loss: 0.3982 - accuracy: 0.82 - ETA: 0s - loss: 0.3960 - accuracy: 0.82 - ETA: 0s - loss: 0.3969 - accuracy: 0.82 - ETA: 0s - loss: 0.3964 - accuracy: 0.82 - ETA: 0s - loss: 0.3972 - accuracy: 0.82 - ETA: 0s - loss: 0.4001 - accuracy: 0.82 - ETA: 0s - loss: 0.3997 - accuracy: 0.82 - ETA: 0s - loss: 0.3987 - accuracy: 0.82 - ETA: 0s - loss: 0.3990 - accuracy: 0.82 - ETA: 0s - loss: 0.3988 - accuracy: 0.82 - ETA: 0s - loss: 0.3997 - accuracy: 0.82 - ETA: 0s - loss: 0.4009 - accuracy: 0.82 - ETA: 0s - loss: 0.4007 - accuracy: 0.82 - ETA: 0s - loss: 0.4007 - accuracy: 0.82 - ETA: 0s - loss: 0.3986 - accuracy: 0.82 - ETA: 0s - loss: 0.3998 - accuracy: 0.82 - ETA: 0s - loss: 0.3991 - accuracy: 0.82 - ETA: 0s - loss: 0.3996 - accuracy: 0.82 - ETA: 0s - loss: 0.4001 - accuracy: 0.82 - ETA: 0s - loss: 0.3996 - accuracy: 0.82 - ETA: 0s - loss: 0.3998 - accuracy: 0.82 - 1s 88us/step - loss: 0.4000 - accuracy: 0.8237\n",
      "Epoch 67/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.3781 - accuracy: 0.80 - ETA: 1s - loss: 0.4122 - accuracy: 0.80 - ETA: 1s - loss: 0.3912 - accuracy: 0.82 - ETA: 1s - loss: 0.3863 - accuracy: 0.82 - ETA: 1s - loss: 0.3969 - accuracy: 0.82 - ETA: 1s - loss: 0.3978 - accuracy: 0.82 - ETA: 1s - loss: 0.3940 - accuracy: 0.82 - ETA: 1s - loss: 0.3940 - accuracy: 0.82 - ETA: 1s - loss: 0.3947 - accuracy: 0.82 - ETA: 1s - loss: 0.3931 - accuracy: 0.82 - ETA: 0s - loss: 0.3899 - accuracy: 0.83 - ETA: 0s - loss: 0.3938 - accuracy: 0.82 - ETA: 0s - loss: 0.3926 - accuracy: 0.82 - ETA: 0s - loss: 0.3911 - accuracy: 0.83 - ETA: 0s - loss: 0.3899 - accuracy: 0.83 - ETA: 0s - loss: 0.3921 - accuracy: 0.83 - ETA: 0s - loss: 0.3937 - accuracy: 0.82 - ETA: 0s - loss: 0.3953 - accuracy: 0.82 - ETA: 0s - loss: 0.3962 - accuracy: 0.82 - ETA: 0s - loss: 0.3968 - accuracy: 0.82 - ETA: 0s - loss: 0.3986 - accuracy: 0.82 - ETA: 0s - loss: 0.3978 - accuracy: 0.82 - ETA: 0s - loss: 0.3987 - accuracy: 0.82 - ETA: 0s - loss: 0.3993 - accuracy: 0.82 - ETA: 0s - loss: 0.4000 - accuracy: 0.82 - ETA: 0s - loss: 0.3997 - accuracy: 0.82 - ETA: 0s - loss: 0.3998 - accuracy: 0.82 - ETA: 0s - loss: 0.3982 - accuracy: 0.82 - ETA: 0s - loss: 0.3976 - accuracy: 0.82 - 1s 85us/step - loss: 0.3983 - accuracy: 0.8248\n",
      "Epoch 68/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.2218 - accuracy: 0.90 - ETA: 1s - loss: 0.3778 - accuracy: 0.83 - ETA: 1s - loss: 0.3737 - accuracy: 0.83 - ETA: 1s - loss: 0.3773 - accuracy: 0.82 - ETA: 1s - loss: 0.3922 - accuracy: 0.82 - ETA: 1s - loss: 0.3980 - accuracy: 0.81 - ETA: 1s - loss: 0.3958 - accuracy: 0.82 - ETA: 1s - loss: 0.3957 - accuracy: 0.82 - ETA: 1s - loss: 0.3954 - accuracy: 0.82 - ETA: 1s - loss: 0.3913 - accuracy: 0.82 - ETA: 0s - loss: 0.3948 - accuracy: 0.82 - ETA: 0s - loss: 0.3965 - accuracy: 0.82 - ETA: 0s - loss: 0.3959 - accuracy: 0.82 - ETA: 0s - loss: 0.3970 - accuracy: 0.82 - ETA: 0s - loss: 0.3948 - accuracy: 0.82 - ETA: 0s - loss: 0.3974 - accuracy: 0.82 - ETA: 0s - loss: 0.3964 - accuracy: 0.82 - ETA: 0s - loss: 0.3996 - accuracy: 0.82 - ETA: 0s - loss: 0.4005 - accuracy: 0.82 - ETA: 0s - loss: 0.4009 - accuracy: 0.82 - ETA: 0s - loss: 0.3994 - accuracy: 0.82 - ETA: 0s - loss: 0.3973 - accuracy: 0.82 - ETA: 0s - loss: 0.3962 - accuracy: 0.82 - ETA: 0s - loss: 0.3976 - accuracy: 0.82 - ETA: 0s - loss: 0.3997 - accuracy: 0.82 - ETA: 0s - loss: 0.3983 - accuracy: 0.82 - ETA: 0s - loss: 0.4004 - accuracy: 0.82 - ETA: 0s - loss: 0.4001 - accuracy: 0.82 - ETA: 0s - loss: 0.3997 - accuracy: 0.82 - ETA: 0s - loss: 0.3987 - accuracy: 0.82 - 1s 87us/step - loss: 0.3987 - accuracy: 0.8229\n",
      "Epoch 69/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.4659 - accuracy: 0.80 - ETA: 1s - loss: 0.4121 - accuracy: 0.80 - ETA: 1s - loss: 0.4137 - accuracy: 0.80 - ETA: 1s - loss: 0.4057 - accuracy: 0.82 - ETA: 1s - loss: 0.3995 - accuracy: 0.82 - ETA: 1s - loss: 0.3969 - accuracy: 0.82 - ETA: 1s - loss: 0.4032 - accuracy: 0.82 - ETA: 1s - loss: 0.4009 - accuracy: 0.82 - ETA: 1s - loss: 0.3981 - accuracy: 0.82 - ETA: 1s - loss: 0.4023 - accuracy: 0.82 - ETA: 0s - loss: 0.4025 - accuracy: 0.81 - ETA: 0s - loss: 0.4017 - accuracy: 0.81 - ETA: 0s - loss: 0.4000 - accuracy: 0.82 - ETA: 0s - loss: 0.4025 - accuracy: 0.81 - ETA: 0s - loss: 0.4035 - accuracy: 0.81 - ETA: 0s - loss: 0.4024 - accuracy: 0.82 - ETA: 0s - loss: 0.4030 - accuracy: 0.82 - ETA: 0s - loss: 0.4021 - accuracy: 0.82 - ETA: 0s - loss: 0.4008 - accuracy: 0.82 - ETA: 0s - loss: 0.3999 - accuracy: 0.82 - ETA: 0s - loss: 0.3997 - accuracy: 0.82 - ETA: 0s - loss: 0.3991 - accuracy: 0.82 - ETA: 0s - loss: 0.4000 - accuracy: 0.82 - ETA: 0s - loss: 0.3993 - accuracy: 0.82 - ETA: 0s - loss: 0.3984 - accuracy: 0.82 - ETA: 0s - loss: 0.3979 - accuracy: 0.82 - ETA: 0s - loss: 0.3973 - accuracy: 0.82 - ETA: 0s - loss: 0.3964 - accuracy: 0.82 - ETA: 0s - loss: 0.3988 - accuracy: 0.82 - 1s 86us/step - loss: 0.3985 - accuracy: 0.8241\n",
      "Epoch 70/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.2290 - accuracy: 1.00 - ETA: 1s - loss: 0.4097 - accuracy: 0.81 - ETA: 1s - loss: 0.4215 - accuracy: 0.80 - ETA: 1s - loss: 0.4095 - accuracy: 0.82 - ETA: 1s - loss: 0.4111 - accuracy: 0.81 - ETA: 1s - loss: 0.4116 - accuracy: 0.81 - ETA: 1s - loss: 0.4074 - accuracy: 0.81 - ETA: 1s - loss: 0.4040 - accuracy: 0.81 - ETA: 1s - loss: 0.4013 - accuracy: 0.82 - ETA: 0s - loss: 0.4013 - accuracy: 0.82 - ETA: 0s - loss: 0.3989 - accuracy: 0.82 - ETA: 0s - loss: 0.3973 - accuracy: 0.82 - ETA: 0s - loss: 0.3973 - accuracy: 0.82 - ETA: 0s - loss: 0.3933 - accuracy: 0.82 - ETA: 0s - loss: 0.3955 - accuracy: 0.82 - ETA: 0s - loss: 0.3952 - accuracy: 0.82 - ETA: 0s - loss: 0.3959 - accuracy: 0.82 - ETA: 0s - loss: 0.3974 - accuracy: 0.82 - ETA: 0s - loss: 0.3978 - accuracy: 0.82 - ETA: 0s - loss: 0.3989 - accuracy: 0.82 - ETA: 0s - loss: 0.3987 - accuracy: 0.82 - ETA: 0s - loss: 0.3986 - accuracy: 0.82 - ETA: 0s - loss: 0.3966 - accuracy: 0.82 - ETA: 0s - loss: 0.3974 - accuracy: 0.82 - ETA: 0s - loss: 0.3975 - accuracy: 0.82 - ETA: 0s - loss: 0.3971 - accuracy: 0.82 - ETA: 0s - loss: 0.3970 - accuracy: 0.82 - ETA: 0s - loss: 0.3974 - accuracy: 0.82 - ETA: 0s - loss: 0.3984 - accuracy: 0.82 - 1s 85us/step - loss: 0.3980 - accuracy: 0.8245\n",
      "Epoch 71/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.4894 - accuracy: 0.70 - ETA: 1s - loss: 0.4239 - accuracy: 0.80 - ETA: 1s - loss: 0.4221 - accuracy: 0.79 - ETA: 1s - loss: 0.4108 - accuracy: 0.80 - ETA: 1s - loss: 0.4065 - accuracy: 0.80 - ETA: 1s - loss: 0.4094 - accuracy: 0.81 - ETA: 1s - loss: 0.4103 - accuracy: 0.81 - ETA: 1s - loss: 0.4065 - accuracy: 0.81 - ETA: 1s - loss: 0.4077 - accuracy: 0.81 - ETA: 0s - loss: 0.4089 - accuracy: 0.81 - ETA: 0s - loss: 0.4042 - accuracy: 0.81 - ETA: 0s - loss: 0.4056 - accuracy: 0.81 - ETA: 0s - loss: 0.4033 - accuracy: 0.81 - ETA: 0s - loss: 0.4026 - accuracy: 0.81 - ETA: 0s - loss: 0.4021 - accuracy: 0.81 - ETA: 0s - loss: 0.3997 - accuracy: 0.82 - ETA: 0s - loss: 0.3994 - accuracy: 0.82 - ETA: 0s - loss: 0.4009 - accuracy: 0.82 - ETA: 0s - loss: 0.3994 - accuracy: 0.82 - ETA: 0s - loss: 0.4007 - accuracy: 0.82 - ETA: 0s - loss: 0.3985 - accuracy: 0.82 - ETA: 0s - loss: 0.3981 - accuracy: 0.82 - ETA: 0s - loss: 0.3966 - accuracy: 0.82 - ETA: 0s - loss: 0.3970 - accuracy: 0.82 - ETA: 0s - loss: 0.3963 - accuracy: 0.82 - ETA: 0s - loss: 0.3952 - accuracy: 0.82 - ETA: 0s - loss: 0.3954 - accuracy: 0.82 - ETA: 0s - loss: 0.3954 - accuracy: 0.82 - ETA: 0s - loss: 0.3952 - accuracy: 0.82 - 1s 85us/step - loss: 0.3954 - accuracy: 0.8258\n",
      "Epoch 72/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.2813 - accuracy: 0.80 - ETA: 1s - loss: 0.3823 - accuracy: 0.84 - ETA: 1s - loss: 0.3844 - accuracy: 0.83 - ETA: 1s - loss: 0.3945 - accuracy: 0.82 - ETA: 1s - loss: 0.3870 - accuracy: 0.83 - ETA: 1s - loss: 0.3897 - accuracy: 0.82 - ETA: 1s - loss: 0.3921 - accuracy: 0.83 - ETA: 1s - loss: 0.3923 - accuracy: 0.82 - ETA: 1s - loss: 0.3907 - accuracy: 0.83 - ETA: 0s - loss: 0.3929 - accuracy: 0.82 - ETA: 0s - loss: 0.3912 - accuracy: 0.82 - ETA: 0s - loss: 0.3894 - accuracy: 0.82 - ETA: 0s - loss: 0.3933 - accuracy: 0.82 - ETA: 0s - loss: 0.3938 - accuracy: 0.82 - ETA: 0s - loss: 0.3957 - accuracy: 0.82 - ETA: 0s - loss: 0.3968 - accuracy: 0.82 - ETA: 0s - loss: 0.3958 - accuracy: 0.82 - ETA: 0s - loss: 0.3955 - accuracy: 0.82 - ETA: 0s - loss: 0.3960 - accuracy: 0.82 - ETA: 0s - loss: 0.3958 - accuracy: 0.82 - ETA: 0s - loss: 0.3970 - accuracy: 0.82 - ETA: 0s - loss: 0.3966 - accuracy: 0.82 - ETA: 0s - loss: 0.3973 - accuracy: 0.82 - ETA: 0s - loss: 0.3958 - accuracy: 0.82 - ETA: 0s - loss: 0.3955 - accuracy: 0.82 - ETA: 0s - loss: 0.3959 - accuracy: 0.82 - ETA: 0s - loss: 0.3952 - accuracy: 0.82 - ETA: 0s - loss: 0.3953 - accuracy: 0.82 - ETA: 0s - loss: 0.3962 - accuracy: 0.82 - 1s 86us/step - loss: 0.3968 - accuracy: 0.8260\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16887/16887 [==============================] - ETA: 5s - loss: 0.5435 - accuracy: 0.80 - ETA: 1s - loss: 0.3766 - accuracy: 0.83 - ETA: 1s - loss: 0.3981 - accuracy: 0.82 - ETA: 1s - loss: 0.3999 - accuracy: 0.82 - ETA: 1s - loss: 0.3952 - accuracy: 0.82 - ETA: 1s - loss: 0.3997 - accuracy: 0.81 - ETA: 1s - loss: 0.3953 - accuracy: 0.82 - ETA: 1s - loss: 0.3932 - accuracy: 0.82 - ETA: 1s - loss: 0.3941 - accuracy: 0.82 - ETA: 0s - loss: 0.3993 - accuracy: 0.82 - ETA: 0s - loss: 0.3958 - accuracy: 0.82 - ETA: 0s - loss: 0.3987 - accuracy: 0.82 - ETA: 0s - loss: 0.3969 - accuracy: 0.82 - ETA: 0s - loss: 0.3969 - accuracy: 0.82 - ETA: 0s - loss: 0.3973 - accuracy: 0.82 - ETA: 0s - loss: 0.3970 - accuracy: 0.82 - ETA: 0s - loss: 0.3969 - accuracy: 0.82 - ETA: 0s - loss: 0.3968 - accuracy: 0.82 - ETA: 0s - loss: 0.3968 - accuracy: 0.82 - ETA: 0s - loss: 0.3942 - accuracy: 0.82 - ETA: 0s - loss: 0.3947 - accuracy: 0.82 - ETA: 0s - loss: 0.3949 - accuracy: 0.82 - ETA: 0s - loss: 0.3956 - accuracy: 0.82 - ETA: 0s - loss: 0.3986 - accuracy: 0.82 - ETA: 0s - loss: 0.3978 - accuracy: 0.82 - ETA: 0s - loss: 0.3974 - accuracy: 0.82 - ETA: 0s - loss: 0.3969 - accuracy: 0.82 - ETA: 0s - loss: 0.3970 - accuracy: 0.82 - ETA: 0s - loss: 0.3960 - accuracy: 0.82 - 1s 86us/step - loss: 0.3960 - accuracy: 0.8255\n",
      "Epoch 74/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.2745 - accuracy: 0.90 - ETA: 1s - loss: 0.3510 - accuracy: 0.84 - ETA: 1s - loss: 0.3657 - accuracy: 0.84 - ETA: 1s - loss: 0.3616 - accuracy: 0.84 - ETA: 1s - loss: 0.3806 - accuracy: 0.83 - ETA: 1s - loss: 0.3900 - accuracy: 0.83 - ETA: 1s - loss: 0.3909 - accuracy: 0.83 - ETA: 1s - loss: 0.3900 - accuracy: 0.83 - ETA: 1s - loss: 0.3939 - accuracy: 0.83 - ETA: 1s - loss: 0.3978 - accuracy: 0.82 - ETA: 0s - loss: 0.3958 - accuracy: 0.82 - ETA: 0s - loss: 0.3961 - accuracy: 0.82 - ETA: 0s - loss: 0.3978 - accuracy: 0.82 - ETA: 0s - loss: 0.3975 - accuracy: 0.82 - ETA: 0s - loss: 0.3992 - accuracy: 0.82 - ETA: 0s - loss: 0.3990 - accuracy: 0.82 - ETA: 0s - loss: 0.3993 - accuracy: 0.82 - ETA: 0s - loss: 0.4002 - accuracy: 0.82 - ETA: 0s - loss: 0.3994 - accuracy: 0.82 - ETA: 0s - loss: 0.4005 - accuracy: 0.82 - ETA: 0s - loss: 0.3990 - accuracy: 0.82 - ETA: 0s - loss: 0.4004 - accuracy: 0.82 - ETA: 0s - loss: 0.3988 - accuracy: 0.82 - ETA: 0s - loss: 0.3977 - accuracy: 0.82 - ETA: 0s - loss: 0.3974 - accuracy: 0.82 - ETA: 0s - loss: 0.3962 - accuracy: 0.82 - ETA: 0s - loss: 0.3974 - accuracy: 0.82 - ETA: 0s - loss: 0.3971 - accuracy: 0.82 - ETA: 0s - loss: 0.3951 - accuracy: 0.82 - ETA: 0s - loss: 0.3957 - accuracy: 0.82 - 1s 87us/step - loss: 0.3956 - accuracy: 0.8243\n",
      "Epoch 75/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.2873 - accuracy: 0.90 - ETA: 1s - loss: 0.4290 - accuracy: 0.80 - ETA: 1s - loss: 0.4082 - accuracy: 0.81 - ETA: 1s - loss: 0.3951 - accuracy: 0.82 - ETA: 1s - loss: 0.3952 - accuracy: 0.82 - ETA: 1s - loss: 0.3997 - accuracy: 0.82 - ETA: 1s - loss: 0.4018 - accuracy: 0.82 - ETA: 1s - loss: 0.4010 - accuracy: 0.82 - ETA: 1s - loss: 0.4055 - accuracy: 0.82 - ETA: 0s - loss: 0.4042 - accuracy: 0.82 - ETA: 0s - loss: 0.4030 - accuracy: 0.82 - ETA: 0s - loss: 0.4030 - accuracy: 0.82 - ETA: 0s - loss: 0.4007 - accuracy: 0.82 - ETA: 0s - loss: 0.3998 - accuracy: 0.82 - ETA: 0s - loss: 0.4000 - accuracy: 0.82 - ETA: 0s - loss: 0.3982 - accuracy: 0.82 - ETA: 0s - loss: 0.3981 - accuracy: 0.82 - ETA: 0s - loss: 0.3988 - accuracy: 0.82 - ETA: 0s - loss: 0.3987 - accuracy: 0.82 - ETA: 0s - loss: 0.3984 - accuracy: 0.82 - ETA: 0s - loss: 0.4001 - accuracy: 0.82 - ETA: 0s - loss: 0.4017 - accuracy: 0.82 - ETA: 0s - loss: 0.3997 - accuracy: 0.82 - ETA: 0s - loss: 0.3997 - accuracy: 0.82 - ETA: 0s - loss: 0.3986 - accuracy: 0.82 - ETA: 0s - loss: 0.3972 - accuracy: 0.82 - ETA: 0s - loss: 0.3986 - accuracy: 0.82 - ETA: 0s - loss: 0.3970 - accuracy: 0.82 - ETA: 0s - loss: 0.3958 - accuracy: 0.82 - 1s 86us/step - loss: 0.3951 - accuracy: 0.8264\n",
      "Epoch 76/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.5983 - accuracy: 0.70 - ETA: 1s - loss: 0.4005 - accuracy: 0.81 - ETA: 1s - loss: 0.3856 - accuracy: 0.82 - ETA: 1s - loss: 0.3856 - accuracy: 0.83 - ETA: 1s - loss: 0.3873 - accuracy: 0.83 - ETA: 1s - loss: 0.3921 - accuracy: 0.83 - ETA: 1s - loss: 0.3937 - accuracy: 0.82 - ETA: 1s - loss: 0.3930 - accuracy: 0.82 - ETA: 1s - loss: 0.3979 - accuracy: 0.82 - ETA: 0s - loss: 0.3940 - accuracy: 0.82 - ETA: 0s - loss: 0.3943 - accuracy: 0.82 - ETA: 0s - loss: 0.3919 - accuracy: 0.82 - ETA: 0s - loss: 0.3965 - accuracy: 0.82 - ETA: 0s - loss: 0.3993 - accuracy: 0.82 - ETA: 0s - loss: 0.3974 - accuracy: 0.82 - ETA: 0s - loss: 0.3957 - accuracy: 0.82 - ETA: 0s - loss: 0.3954 - accuracy: 0.82 - ETA: 0s - loss: 0.3948 - accuracy: 0.82 - ETA: 0s - loss: 0.3967 - accuracy: 0.82 - ETA: 0s - loss: 0.3978 - accuracy: 0.82 - ETA: 0s - loss: 0.3984 - accuracy: 0.82 - ETA: 0s - loss: 0.3986 - accuracy: 0.82 - ETA: 0s - loss: 0.3987 - accuracy: 0.82 - ETA: 0s - loss: 0.3978 - accuracy: 0.82 - ETA: 0s - loss: 0.3986 - accuracy: 0.82 - ETA: 0s - loss: 0.3982 - accuracy: 0.82 - ETA: 0s - loss: 0.3979 - accuracy: 0.82 - ETA: 0s - loss: 0.3963 - accuracy: 0.82 - ETA: 0s - loss: 0.3949 - accuracy: 0.82 - 1s 84us/step - loss: 0.3949 - accuracy: 0.8268\n",
      "Epoch 77/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.6050 - accuracy: 0.80 - ETA: 1s - loss: 0.3854 - accuracy: 0.82 - ETA: 1s - loss: 0.3901 - accuracy: 0.81 - ETA: 1s - loss: 0.3850 - accuracy: 0.82 - ETA: 1s - loss: 0.3891 - accuracy: 0.82 - ETA: 1s - loss: 0.3910 - accuracy: 0.82 - ETA: 1s - loss: 0.3868 - accuracy: 0.82 - ETA: 1s - loss: 0.3872 - accuracy: 0.82 - ETA: 1s - loss: 0.3888 - accuracy: 0.82 - ETA: 0s - loss: 0.3922 - accuracy: 0.82 - ETA: 0s - loss: 0.3920 - accuracy: 0.82 - ETA: 0s - loss: 0.3910 - accuracy: 0.82 - ETA: 0s - loss: 0.3933 - accuracy: 0.82 - ETA: 0s - loss: 0.3948 - accuracy: 0.82 - ETA: 0s - loss: 0.3949 - accuracy: 0.82 - ETA: 0s - loss: 0.3935 - accuracy: 0.82 - ETA: 0s - loss: 0.3927 - accuracy: 0.82 - ETA: 0s - loss: 0.3896 - accuracy: 0.82 - ETA: 0s - loss: 0.3931 - accuracy: 0.82 - ETA: 0s - loss: 0.3940 - accuracy: 0.82 - ETA: 0s - loss: 0.3930 - accuracy: 0.82 - ETA: 0s - loss: 0.3928 - accuracy: 0.82 - ETA: 0s - loss: 0.3939 - accuracy: 0.82 - ETA: 0s - loss: 0.3940 - accuracy: 0.82 - ETA: 0s - loss: 0.3952 - accuracy: 0.82 - ETA: 0s - loss: 0.3957 - accuracy: 0.82 - ETA: 0s - loss: 0.3955 - accuracy: 0.82 - ETA: 0s - loss: 0.3951 - accuracy: 0.82 - ETA: 0s - loss: 0.3940 - accuracy: 0.82 - 1s 85us/step - loss: 0.3948 - accuracy: 0.8244\n",
      "Epoch 78/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.1164 - accuracy: 1.00 - ETA: 1s - loss: 0.3867 - accuracy: 0.84 - ETA: 1s - loss: 0.3831 - accuracy: 0.83 - ETA: 1s - loss: 0.3899 - accuracy: 0.83 - ETA: 1s - loss: 0.3980 - accuracy: 0.81 - ETA: 1s - loss: 0.3959 - accuracy: 0.81 - ETA: 1s - loss: 0.3990 - accuracy: 0.81 - ETA: 1s - loss: 0.3997 - accuracy: 0.81 - ETA: 1s - loss: 0.4018 - accuracy: 0.81 - ETA: 1s - loss: 0.4021 - accuracy: 0.82 - ETA: 0s - loss: 0.4042 - accuracy: 0.81 - ETA: 0s - loss: 0.4030 - accuracy: 0.82 - ETA: 0s - loss: 0.4005 - accuracy: 0.82 - ETA: 0s - loss: 0.4000 - accuracy: 0.82 - ETA: 0s - loss: 0.4013 - accuracy: 0.82 - ETA: 0s - loss: 0.4022 - accuracy: 0.82 - ETA: 0s - loss: 0.4011 - accuracy: 0.82 - ETA: 0s - loss: 0.4011 - accuracy: 0.82 - ETA: 0s - loss: 0.3999 - accuracy: 0.82 - ETA: 0s - loss: 0.3981 - accuracy: 0.82 - ETA: 0s - loss: 0.3937 - accuracy: 0.82 - ETA: 0s - loss: 0.3935 - accuracy: 0.82 - ETA: 0s - loss: 0.3923 - accuracy: 0.82 - ETA: 0s - loss: 0.3927 - accuracy: 0.82 - ETA: 0s - loss: 0.3919 - accuracy: 0.82 - ETA: 0s - loss: 0.3920 - accuracy: 0.82 - ETA: 0s - loss: 0.3916 - accuracy: 0.82 - ETA: 0s - loss: 0.3923 - accuracy: 0.82 - ETA: 0s - loss: 0.3932 - accuracy: 0.82 - 1s 85us/step - loss: 0.3941 - accuracy: 0.8246\n",
      "Epoch 79/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.5581 - accuracy: 0.80 - ETA: 1s - loss: 0.4134 - accuracy: 0.82 - ETA: 1s - loss: 0.4015 - accuracy: 0.82 - ETA: 1s - loss: 0.3991 - accuracy: 0.82 - ETA: 1s - loss: 0.3965 - accuracy: 0.82 - ETA: 1s - loss: 0.3977 - accuracy: 0.82 - ETA: 1s - loss: 0.3994 - accuracy: 0.82 - ETA: 1s - loss: 0.3983 - accuracy: 0.82 - ETA: 1s - loss: 0.4016 - accuracy: 0.82 - ETA: 1s - loss: 0.4003 - accuracy: 0.82 - ETA: 0s - loss: 0.3973 - accuracy: 0.82 - ETA: 0s - loss: 0.3987 - accuracy: 0.82 - ETA: 0s - loss: 0.3980 - accuracy: 0.82 - ETA: 0s - loss: 0.3970 - accuracy: 0.82 - ETA: 0s - loss: 0.3955 - accuracy: 0.82 - ETA: 0s - loss: 0.3980 - accuracy: 0.82 - ETA: 0s - loss: 0.3967 - accuracy: 0.82 - ETA: 0s - loss: 0.3944 - accuracy: 0.82 - ETA: 0s - loss: 0.3982 - accuracy: 0.82 - ETA: 0s - loss: 0.3973 - accuracy: 0.82 - ETA: 0s - loss: 0.3961 - accuracy: 0.82 - ETA: 0s - loss: 0.3945 - accuracy: 0.82 - ETA: 0s - loss: 0.3936 - accuracy: 0.82 - ETA: 0s - loss: 0.3923 - accuracy: 0.82 - ETA: 0s - loss: 0.3927 - accuracy: 0.82 - ETA: 0s - loss: 0.3928 - accuracy: 0.82 - ETA: 0s - loss: 0.3916 - accuracy: 0.82 - ETA: 0s - loss: 0.3934 - accuracy: 0.82 - ETA: 0s - loss: 0.3933 - accuracy: 0.82 - ETA: 0s - loss: 0.3936 - accuracy: 0.82 - 1s 87us/step - loss: 0.3939 - accuracy: 0.8271\n",
      "Epoch 80/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.4234 - accuracy: 0.90 - ETA: 1s - loss: 0.3805 - accuracy: 0.82 - ETA: 1s - loss: 0.4024 - accuracy: 0.81 - ETA: 1s - loss: 0.3954 - accuracy: 0.82 - ETA: 1s - loss: 0.4002 - accuracy: 0.82 - ETA: 1s - loss: 0.4008 - accuracy: 0.82 - ETA: 1s - loss: 0.3950 - accuracy: 0.82 - ETA: 1s - loss: 0.3932 - accuracy: 0.82 - ETA: 1s - loss: 0.3917 - accuracy: 0.82 - ETA: 1s - loss: 0.3904 - accuracy: 0.82 - ETA: 0s - loss: 0.3911 - accuracy: 0.83 - ETA: 0s - loss: 0.3886 - accuracy: 0.82 - ETA: 0s - loss: 0.3905 - accuracy: 0.82 - ETA: 0s - loss: 0.3896 - accuracy: 0.83 - ETA: 0s - loss: 0.3891 - accuracy: 0.83 - ETA: 0s - loss: 0.3900 - accuracy: 0.83 - ETA: 0s - loss: 0.3899 - accuracy: 0.83 - ETA: 0s - loss: 0.3915 - accuracy: 0.82 - ETA: 0s - loss: 0.3900 - accuracy: 0.82 - ETA: 0s - loss: 0.3915 - accuracy: 0.82 - ETA: 0s - loss: 0.3911 - accuracy: 0.82 - ETA: 0s - loss: 0.3910 - accuracy: 0.82 - ETA: 0s - loss: 0.3908 - accuracy: 0.82 - ETA: 0s - loss: 0.3912 - accuracy: 0.82 - ETA: 0s - loss: 0.3923 - accuracy: 0.82 - ETA: 0s - loss: 0.3920 - accuracy: 0.82 - ETA: 0s - loss: 0.3932 - accuracy: 0.82 - ETA: 0s - loss: 0.3942 - accuracy: 0.82 - ETA: 0s - loss: 0.3950 - accuracy: 0.82 - ETA: 0s - loss: 0.3939 - accuracy: 0.82 - 1s 87us/step - loss: 0.3939 - accuracy: 0.8262\n",
      "Epoch 81/100\n",
      "16887/16887 [==============================] - ETA: 6s - loss: 0.2543 - accuracy: 1.00 - ETA: 1s - loss: 0.4070 - accuracy: 0.82 - ETA: 1s - loss: 0.4018 - accuracy: 0.82 - ETA: 1s - loss: 0.4125 - accuracy: 0.81 - ETA: 1s - loss: 0.4082 - accuracy: 0.82 - ETA: 1s - loss: 0.4026 - accuracy: 0.82 - ETA: 1s - loss: 0.4010 - accuracy: 0.82 - ETA: 1s - loss: 0.4022 - accuracy: 0.82 - ETA: 1s - loss: 0.3973 - accuracy: 0.82 - ETA: 0s - loss: 0.3958 - accuracy: 0.82 - ETA: 0s - loss: 0.3938 - accuracy: 0.82 - ETA: 0s - loss: 0.3921 - accuracy: 0.82 - ETA: 0s - loss: 0.3917 - accuracy: 0.82 - ETA: 0s - loss: 0.3917 - accuracy: 0.82 - ETA: 0s - loss: 0.3906 - accuracy: 0.82 - ETA: 0s - loss: 0.3896 - accuracy: 0.82 - ETA: 0s - loss: 0.3895 - accuracy: 0.82 - ETA: 0s - loss: 0.3910 - accuracy: 0.82 - ETA: 0s - loss: 0.3897 - accuracy: 0.82 - ETA: 0s - loss: 0.3899 - accuracy: 0.82 - ETA: 0s - loss: 0.3912 - accuracy: 0.82 - ETA: 0s - loss: 0.3901 - accuracy: 0.82 - ETA: 0s - loss: 0.3923 - accuracy: 0.82 - ETA: 0s - loss: 0.3927 - accuracy: 0.82 - ETA: 0s - loss: 0.3931 - accuracy: 0.82 - ETA: 0s - loss: 0.3921 - accuracy: 0.82 - ETA: 0s - loss: 0.3920 - accuracy: 0.82 - ETA: 0s - loss: 0.3927 - accuracy: 0.82 - ETA: 0s - loss: 0.3922 - accuracy: 0.82 - 1s 85us/step - loss: 0.3925 - accuracy: 0.8268\n",
      "Epoch 82/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.3163 - accuracy: 0.90 - ETA: 1s - loss: 0.4023 - accuracy: 0.82 - ETA: 1s - loss: 0.4122 - accuracy: 0.82 - ETA: 1s - loss: 0.4008 - accuracy: 0.82 - ETA: 1s - loss: 0.3972 - accuracy: 0.83 - ETA: 1s - loss: 0.3961 - accuracy: 0.83 - ETA: 1s - loss: 0.3923 - accuracy: 0.83 - ETA: 1s - loss: 0.3951 - accuracy: 0.82 - ETA: 1s - loss: 0.3904 - accuracy: 0.83 - ETA: 0s - loss: 0.3916 - accuracy: 0.82 - ETA: 0s - loss: 0.3946 - accuracy: 0.82 - ETA: 0s - loss: 0.3951 - accuracy: 0.82 - ETA: 0s - loss: 0.3952 - accuracy: 0.82 - ETA: 0s - loss: 0.3937 - accuracy: 0.82 - ETA: 0s - loss: 0.3959 - accuracy: 0.82 - ETA: 0s - loss: 0.3918 - accuracy: 0.82 - ETA: 0s - loss: 0.3897 - accuracy: 0.82 - ETA: 0s - loss: 0.3909 - accuracy: 0.82 - ETA: 0s - loss: 0.3915 - accuracy: 0.82 - ETA: 0s - loss: 0.3916 - accuracy: 0.82 - ETA: 0s - loss: 0.3915 - accuracy: 0.82 - ETA: 0s - loss: 0.3937 - accuracy: 0.82 - ETA: 0s - loss: 0.3937 - accuracy: 0.82 - ETA: 0s - loss: 0.3944 - accuracy: 0.82 - ETA: 0s - loss: 0.3941 - accuracy: 0.82 - ETA: 0s - loss: 0.3949 - accuracy: 0.82 - ETA: 0s - loss: 0.3939 - accuracy: 0.82 - ETA: 0s - loss: 0.3943 - accuracy: 0.82 - ETA: 0s - loss: 0.3941 - accuracy: 0.82 - ETA: 0s - loss: 0.3941 - accuracy: 0.82 - 1s 87us/step - loss: 0.3936 - accuracy: 0.8262\n",
      "Epoch 83/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.5438 - accuracy: 0.60 - ETA: 1s - loss: 0.3942 - accuracy: 0.82 - ETA: 1s - loss: 0.3980 - accuracy: 0.82 - ETA: 1s - loss: 0.3829 - accuracy: 0.83 - ETA: 1s - loss: 0.3950 - accuracy: 0.82 - ETA: 1s - loss: 0.3884 - accuracy: 0.82 - ETA: 1s - loss: 0.3891 - accuracy: 0.82 - ETA: 1s - loss: 0.3866 - accuracy: 0.82 - ETA: 1s - loss: 0.3911 - accuracy: 0.82 - ETA: 0s - loss: 0.3919 - accuracy: 0.82 - ETA: 0s - loss: 0.3926 - accuracy: 0.82 - ETA: 0s - loss: 0.3889 - accuracy: 0.82 - ETA: 0s - loss: 0.3882 - accuracy: 0.82 - ETA: 0s - loss: 0.3868 - accuracy: 0.82 - ETA: 0s - loss: 0.3873 - accuracy: 0.82 - ETA: 0s - loss: 0.3868 - accuracy: 0.82 - ETA: 0s - loss: 0.3861 - accuracy: 0.82 - ETA: 0s - loss: 0.3874 - accuracy: 0.82 - ETA: 0s - loss: 0.3882 - accuracy: 0.82 - ETA: 0s - loss: 0.3868 - accuracy: 0.82 - ETA: 0s - loss: 0.3879 - accuracy: 0.82 - ETA: 0s - loss: 0.3893 - accuracy: 0.82 - ETA: 0s - loss: 0.3901 - accuracy: 0.82 - ETA: 0s - loss: 0.3902 - accuracy: 0.82 - ETA: 0s - loss: 0.3914 - accuracy: 0.82 - ETA: 0s - loss: 0.3922 - accuracy: 0.82 - ETA: 0s - loss: 0.3916 - accuracy: 0.82 - ETA: 0s - loss: 0.3911 - accuracy: 0.82 - ETA: 0s - loss: 0.3911 - accuracy: 0.82 - 1s 86us/step - loss: 0.3919 - accuracy: 0.8261\n",
      "Epoch 84/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.3220 - accuracy: 0.90 - ETA: 1s - loss: 0.3555 - accuracy: 0.85 - ETA: 1s - loss: 0.3691 - accuracy: 0.84 - ETA: 1s - loss: 0.3737 - accuracy: 0.84 - ETA: 1s - loss: 0.3849 - accuracy: 0.83 - ETA: 1s - loss: 0.3815 - accuracy: 0.83 - ETA: 1s - loss: 0.3845 - accuracy: 0.83 - ETA: 1s - loss: 0.3806 - accuracy: 0.83 - ETA: 1s - loss: 0.3791 - accuracy: 0.83 - ETA: 0s - loss: 0.3850 - accuracy: 0.83 - ETA: 0s - loss: 0.3843 - accuracy: 0.83 - ETA: 0s - loss: 0.3830 - accuracy: 0.83 - ETA: 0s - loss: 0.3836 - accuracy: 0.83 - ETA: 0s - loss: 0.3854 - accuracy: 0.83 - ETA: 0s - loss: 0.3845 - accuracy: 0.83 - ETA: 0s - loss: 0.3811 - accuracy: 0.83 - ETA: 0s - loss: 0.3827 - accuracy: 0.83 - ETA: 0s - loss: 0.3863 - accuracy: 0.83 - ETA: 0s - loss: 0.3841 - accuracy: 0.83 - ETA: 0s - loss: 0.3857 - accuracy: 0.83 - ETA: 0s - loss: 0.3853 - accuracy: 0.83 - ETA: 0s - loss: 0.3868 - accuracy: 0.82 - ETA: 0s - loss: 0.3864 - accuracy: 0.82 - ETA: 0s - loss: 0.3875 - accuracy: 0.82 - ETA: 0s - loss: 0.3883 - accuracy: 0.82 - ETA: 0s - loss: 0.3894 - accuracy: 0.82 - ETA: 0s - loss: 0.3898 - accuracy: 0.82 - ETA: 0s - loss: 0.3903 - accuracy: 0.82 - ETA: 0s - loss: 0.3911 - accuracy: 0.82 - ETA: 0s - loss: 0.3920 - accuracy: 0.82 - 1s 86us/step - loss: 0.3918 - accuracy: 0.8286\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16887/16887 [==============================] - ETA: 3s - loss: 0.3213 - accuracy: 0.80 - ETA: 1s - loss: 0.4043 - accuracy: 0.82 - ETA: 1s - loss: 0.3884 - accuracy: 0.83 - ETA: 1s - loss: 0.3869 - accuracy: 0.83 - ETA: 1s - loss: 0.3828 - accuracy: 0.83 - ETA: 1s - loss: 0.3860 - accuracy: 0.83 - ETA: 1s - loss: 0.3888 - accuracy: 0.82 - ETA: 1s - loss: 0.3874 - accuracy: 0.83 - ETA: 1s - loss: 0.3860 - accuracy: 0.83 - ETA: 1s - loss: 0.3880 - accuracy: 0.83 - ETA: 0s - loss: 0.3867 - accuracy: 0.83 - ETA: 0s - loss: 0.3845 - accuracy: 0.83 - ETA: 0s - loss: 0.3853 - accuracy: 0.83 - ETA: 0s - loss: 0.3844 - accuracy: 0.83 - ETA: 0s - loss: 0.3846 - accuracy: 0.83 - ETA: 0s - loss: 0.3858 - accuracy: 0.83 - ETA: 0s - loss: 0.3888 - accuracy: 0.83 - ETA: 0s - loss: 0.3896 - accuracy: 0.83 - ETA: 0s - loss: 0.3880 - accuracy: 0.83 - ETA: 0s - loss: 0.3869 - accuracy: 0.83 - ETA: 0s - loss: 0.3874 - accuracy: 0.83 - ETA: 0s - loss: 0.3857 - accuracy: 0.83 - ETA: 0s - loss: 0.3872 - accuracy: 0.83 - ETA: 0s - loss: 0.3893 - accuracy: 0.83 - ETA: 0s - loss: 0.3919 - accuracy: 0.83 - ETA: 0s - loss: 0.3920 - accuracy: 0.82 - ETA: 0s - loss: 0.3917 - accuracy: 0.83 - ETA: 0s - loss: 0.3912 - accuracy: 0.83 - ETA: 0s - loss: 0.3919 - accuracy: 0.82 - ETA: 0s - loss: 0.3913 - accuracy: 0.83 - 1s 87us/step - loss: 0.3910 - accuracy: 0.8303\n",
      "Epoch 86/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.7194 - accuracy: 0.70 - ETA: 1s - loss: 0.4409 - accuracy: 0.78 - ETA: 1s - loss: 0.4255 - accuracy: 0.79 - ETA: 1s - loss: 0.4092 - accuracy: 0.81 - ETA: 1s - loss: 0.4125 - accuracy: 0.80 - ETA: 1s - loss: 0.4103 - accuracy: 0.81 - ETA: 1s - loss: 0.4079 - accuracy: 0.81 - ETA: 1s - loss: 0.4030 - accuracy: 0.81 - ETA: 1s - loss: 0.3987 - accuracy: 0.81 - ETA: 1s - loss: 0.3973 - accuracy: 0.82 - ETA: 0s - loss: 0.3949 - accuracy: 0.82 - ETA: 0s - loss: 0.3908 - accuracy: 0.82 - ETA: 0s - loss: 0.3928 - accuracy: 0.82 - ETA: 0s - loss: 0.3924 - accuracy: 0.82 - ETA: 0s - loss: 0.3929 - accuracy: 0.82 - ETA: 0s - loss: 0.3962 - accuracy: 0.82 - ETA: 0s - loss: 0.3941 - accuracy: 0.82 - ETA: 0s - loss: 0.3934 - accuracy: 0.82 - ETA: 0s - loss: 0.3943 - accuracy: 0.82 - ETA: 0s - loss: 0.3923 - accuracy: 0.82 - ETA: 0s - loss: 0.3908 - accuracy: 0.82 - ETA: 0s - loss: 0.3916 - accuracy: 0.82 - ETA: 0s - loss: 0.3921 - accuracy: 0.82 - ETA: 0s - loss: 0.3919 - accuracy: 0.82 - ETA: 0s - loss: 0.3930 - accuracy: 0.82 - ETA: 0s - loss: 0.3938 - accuracy: 0.82 - ETA: 0s - loss: 0.3931 - accuracy: 0.82 - ETA: 0s - loss: 0.3922 - accuracy: 0.82 - ETA: 0s - loss: 0.3908 - accuracy: 0.82 - ETA: 0s - loss: 0.3905 - accuracy: 0.82 - 1s 87us/step - loss: 0.3912 - accuracy: 0.8254\n",
      "Epoch 87/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.1144 - accuracy: 1.00 - ETA: 1s - loss: 0.3686 - accuracy: 0.82 - ETA: 1s - loss: 0.4022 - accuracy: 0.81 - ETA: 1s - loss: 0.3903 - accuracy: 0.82 - ETA: 1s - loss: 0.3801 - accuracy: 0.83 - ETA: 1s - loss: 0.3850 - accuracy: 0.82 - ETA: 1s - loss: 0.3835 - accuracy: 0.82 - ETA: 1s - loss: 0.3859 - accuracy: 0.82 - ETA: 1s - loss: 0.3828 - accuracy: 0.83 - ETA: 0s - loss: 0.3846 - accuracy: 0.83 - ETA: 0s - loss: 0.3861 - accuracy: 0.83 - ETA: 0s - loss: 0.3897 - accuracy: 0.82 - ETA: 0s - loss: 0.3861 - accuracy: 0.83 - ETA: 0s - loss: 0.3871 - accuracy: 0.83 - ETA: 0s - loss: 0.3887 - accuracy: 0.82 - ETA: 0s - loss: 0.3871 - accuracy: 0.83 - ETA: 0s - loss: 0.3876 - accuracy: 0.83 - ETA: 0s - loss: 0.3885 - accuracy: 0.83 - ETA: 0s - loss: 0.3894 - accuracy: 0.83 - ETA: 0s - loss: 0.3883 - accuracy: 0.83 - ETA: 0s - loss: 0.3892 - accuracy: 0.83 - ETA: 0s - loss: 0.3889 - accuracy: 0.83 - ETA: 0s - loss: 0.3883 - accuracy: 0.83 - ETA: 0s - loss: 0.3894 - accuracy: 0.83 - ETA: 0s - loss: 0.3907 - accuracy: 0.83 - ETA: 0s - loss: 0.3906 - accuracy: 0.82 - ETA: 0s - loss: 0.3899 - accuracy: 0.82 - ETA: 0s - loss: 0.3900 - accuracy: 0.82 - ETA: 0s - loss: 0.3905 - accuracy: 0.82 - 1s 86us/step - loss: 0.3901 - accuracy: 0.8290\n",
      "Epoch 88/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.3223 - accuracy: 0.80 - ETA: 1s - loss: 0.3950 - accuracy: 0.81 - ETA: 1s - loss: 0.3737 - accuracy: 0.82 - ETA: 1s - loss: 0.3811 - accuracy: 0.82 - ETA: 1s - loss: 0.3847 - accuracy: 0.82 - ETA: 1s - loss: 0.3817 - accuracy: 0.82 - ETA: 1s - loss: 0.3844 - accuracy: 0.82 - ETA: 1s - loss: 0.3829 - accuracy: 0.82 - ETA: 1s - loss: 0.3827 - accuracy: 0.82 - ETA: 1s - loss: 0.3840 - accuracy: 0.82 - ETA: 0s - loss: 0.3842 - accuracy: 0.82 - ETA: 0s - loss: 0.3839 - accuracy: 0.83 - ETA: 0s - loss: 0.3870 - accuracy: 0.82 - ETA: 0s - loss: 0.3850 - accuracy: 0.83 - ETA: 0s - loss: 0.3840 - accuracy: 0.83 - ETA: 0s - loss: 0.3861 - accuracy: 0.83 - ETA: 0s - loss: 0.3871 - accuracy: 0.82 - ETA: 0s - loss: 0.3891 - accuracy: 0.82 - ETA: 0s - loss: 0.3878 - accuracy: 0.82 - ETA: 0s - loss: 0.3893 - accuracy: 0.82 - ETA: 0s - loss: 0.3900 - accuracy: 0.82 - ETA: 0s - loss: 0.3895 - accuracy: 0.82 - ETA: 0s - loss: 0.3896 - accuracy: 0.82 - ETA: 0s - loss: 0.3888 - accuracy: 0.82 - ETA: 0s - loss: 0.3887 - accuracy: 0.83 - ETA: 0s - loss: 0.3876 - accuracy: 0.83 - ETA: 0s - loss: 0.3891 - accuracy: 0.82 - ETA: 0s - loss: 0.3889 - accuracy: 0.82 - ETA: 0s - loss: 0.3899 - accuracy: 0.82 - ETA: 0s - loss: 0.3899 - accuracy: 0.82 - 1s 86us/step - loss: 0.3899 - accuracy: 0.8291\n",
      "Epoch 89/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.3018 - accuracy: 0.80 - ETA: 1s - loss: 0.3732 - accuracy: 0.85 - ETA: 1s - loss: 0.3683 - accuracy: 0.84 - ETA: 1s - loss: 0.3796 - accuracy: 0.83 - ETA: 1s - loss: 0.3906 - accuracy: 0.82 - ETA: 1s - loss: 0.3879 - accuracy: 0.82 - ETA: 1s - loss: 0.3822 - accuracy: 0.83 - ETA: 1s - loss: 0.3807 - accuracy: 0.83 - ETA: 1s - loss: 0.3815 - accuracy: 0.83 - ETA: 0s - loss: 0.3808 - accuracy: 0.83 - ETA: 0s - loss: 0.3752 - accuracy: 0.83 - ETA: 0s - loss: 0.3754 - accuracy: 0.83 - ETA: 0s - loss: 0.3791 - accuracy: 0.83 - ETA: 0s - loss: 0.3803 - accuracy: 0.83 - ETA: 0s - loss: 0.3818 - accuracy: 0.83 - ETA: 0s - loss: 0.3843 - accuracy: 0.83 - ETA: 0s - loss: 0.3844 - accuracy: 0.83 - ETA: 0s - loss: 0.3847 - accuracy: 0.83 - ETA: 0s - loss: 0.3855 - accuracy: 0.83 - ETA: 0s - loss: 0.3873 - accuracy: 0.83 - ETA: 0s - loss: 0.3876 - accuracy: 0.82 - ETA: 0s - loss: 0.3894 - accuracy: 0.82 - ETA: 0s - loss: 0.3884 - accuracy: 0.82 - ETA: 0s - loss: 0.3887 - accuracy: 0.82 - ETA: 0s - loss: 0.3897 - accuracy: 0.82 - ETA: 0s - loss: 0.3896 - accuracy: 0.82 - ETA: 0s - loss: 0.3896 - accuracy: 0.82 - ETA: 0s - loss: 0.3891 - accuracy: 0.82 - ETA: 0s - loss: 0.3909 - accuracy: 0.82 - 1s 86us/step - loss: 0.3907 - accuracy: 0.8256\n",
      "Epoch 90/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.2349 - accuracy: 1.00 - ETA: 1s - loss: 0.3773 - accuracy: 0.83 - ETA: 1s - loss: 0.3808 - accuracy: 0.82 - ETA: 1s - loss: 0.3775 - accuracy: 0.82 - ETA: 1s - loss: 0.3758 - accuracy: 0.83 - ETA: 1s - loss: 0.3801 - accuracy: 0.82 - ETA: 1s - loss: 0.3858 - accuracy: 0.83 - ETA: 1s - loss: 0.3902 - accuracy: 0.82 - ETA: 1s - loss: 0.3917 - accuracy: 0.82 - ETA: 0s - loss: 0.3922 - accuracy: 0.82 - ETA: 0s - loss: 0.3909 - accuracy: 0.82 - ETA: 0s - loss: 0.3894 - accuracy: 0.82 - ETA: 0s - loss: 0.3893 - accuracy: 0.82 - ETA: 0s - loss: 0.3891 - accuracy: 0.82 - ETA: 0s - loss: 0.3899 - accuracy: 0.82 - ETA: 0s - loss: 0.3906 - accuracy: 0.82 - ETA: 0s - loss: 0.3888 - accuracy: 0.82 - ETA: 0s - loss: 0.3898 - accuracy: 0.82 - ETA: 0s - loss: 0.3903 - accuracy: 0.82 - ETA: 0s - loss: 0.3916 - accuracy: 0.82 - ETA: 0s - loss: 0.3914 - accuracy: 0.82 - ETA: 0s - loss: 0.3913 - accuracy: 0.82 - ETA: 0s - loss: 0.3901 - accuracy: 0.82 - ETA: 0s - loss: 0.3879 - accuracy: 0.82 - ETA: 0s - loss: 0.3896 - accuracy: 0.82 - ETA: 0s - loss: 0.3882 - accuracy: 0.82 - ETA: 0s - loss: 0.3884 - accuracy: 0.82 - ETA: 0s - loss: 0.3891 - accuracy: 0.82 - ETA: 0s - loss: 0.3891 - accuracy: 0.82 - 1s 86us/step - loss: 0.3896 - accuracy: 0.8268\n",
      "Epoch 91/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.2219 - accuracy: 0.90 - ETA: 1s - loss: 0.4244 - accuracy: 0.79 - ETA: 1s - loss: 0.4035 - accuracy: 0.81 - ETA: 1s - loss: 0.3998 - accuracy: 0.81 - ETA: 1s - loss: 0.3892 - accuracy: 0.82 - ETA: 1s - loss: 0.3843 - accuracy: 0.82 - ETA: 1s - loss: 0.3941 - accuracy: 0.82 - ETA: 1s - loss: 0.3922 - accuracy: 0.82 - ETA: 1s - loss: 0.3930 - accuracy: 0.82 - ETA: 1s - loss: 0.3943 - accuracy: 0.82 - ETA: 0s - loss: 0.3943 - accuracy: 0.82 - ETA: 0s - loss: 0.3951 - accuracy: 0.82 - ETA: 0s - loss: 0.3969 - accuracy: 0.82 - ETA: 0s - loss: 0.3955 - accuracy: 0.82 - ETA: 0s - loss: 0.3940 - accuracy: 0.82 - ETA: 0s - loss: 0.3936 - accuracy: 0.82 - ETA: 0s - loss: 0.3934 - accuracy: 0.82 - ETA: 0s - loss: 0.3921 - accuracy: 0.82 - ETA: 0s - loss: 0.3915 - accuracy: 0.82 - ETA: 0s - loss: 0.3894 - accuracy: 0.82 - ETA: 0s - loss: 0.3883 - accuracy: 0.83 - ETA: 0s - loss: 0.3882 - accuracy: 0.82 - ETA: 0s - loss: 0.3893 - accuracy: 0.82 - ETA: 0s - loss: 0.3907 - accuracy: 0.82 - ETA: 0s - loss: 0.3916 - accuracy: 0.82 - ETA: 0s - loss: 0.3914 - accuracy: 0.82 - ETA: 0s - loss: 0.3901 - accuracy: 0.82 - ETA: 0s - loss: 0.3900 - accuracy: 0.82 - ETA: 0s - loss: 0.3897 - accuracy: 0.82 - ETA: 0s - loss: 0.3893 - accuracy: 0.82 - 1s 88us/step - loss: 0.3889 - accuracy: 0.8292\n",
      "Epoch 92/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.3318 - accuracy: 0.80 - ETA: 1s - loss: 0.4078 - accuracy: 0.80 - ETA: 1s - loss: 0.3990 - accuracy: 0.81 - ETA: 1s - loss: 0.3954 - accuracy: 0.82 - ETA: 1s - loss: 0.3966 - accuracy: 0.82 - ETA: 1s - loss: 0.3946 - accuracy: 0.82 - ETA: 1s - loss: 0.3956 - accuracy: 0.82 - ETA: 1s - loss: 0.3907 - accuracy: 0.83 - ETA: 1s - loss: 0.3915 - accuracy: 0.82 - ETA: 1s - loss: 0.3887 - accuracy: 0.82 - ETA: 0s - loss: 0.3901 - accuracy: 0.82 - ETA: 0s - loss: 0.3892 - accuracy: 0.82 - ETA: 0s - loss: 0.3896 - accuracy: 0.82 - ETA: 0s - loss: 0.3880 - accuracy: 0.82 - ETA: 0s - loss: 0.3901 - accuracy: 0.82 - ETA: 0s - loss: 0.3897 - accuracy: 0.82 - ETA: 0s - loss: 0.3887 - accuracy: 0.82 - ETA: 0s - loss: 0.3893 - accuracy: 0.82 - ETA: 0s - loss: 0.3897 - accuracy: 0.82 - ETA: 0s - loss: 0.3900 - accuracy: 0.82 - ETA: 0s - loss: 0.3893 - accuracy: 0.82 - ETA: 0s - loss: 0.3875 - accuracy: 0.83 - ETA: 0s - loss: 0.3892 - accuracy: 0.82 - ETA: 0s - loss: 0.3905 - accuracy: 0.82 - ETA: 0s - loss: 0.3906 - accuracy: 0.82 - ETA: 0s - loss: 0.3900 - accuracy: 0.82 - ETA: 0s - loss: 0.3891 - accuracy: 0.82 - ETA: 0s - loss: 0.3892 - accuracy: 0.82 - ETA: 0s - loss: 0.3893 - accuracy: 0.82 - 1s 86us/step - loss: 0.3899 - accuracy: 0.8271\n",
      "Epoch 93/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.3650 - accuracy: 0.80 - ETA: 1s - loss: 0.3664 - accuracy: 0.83 - ETA: 1s - loss: 0.3666 - accuracy: 0.83 - ETA: 1s - loss: 0.3676 - accuracy: 0.83 - ETA: 1s - loss: 0.3775 - accuracy: 0.82 - ETA: 1s - loss: 0.3804 - accuracy: 0.82 - ETA: 1s - loss: 0.3807 - accuracy: 0.82 - ETA: 1s - loss: 0.3731 - accuracy: 0.83 - ETA: 1s - loss: 0.3756 - accuracy: 0.83 - ETA: 0s - loss: 0.3726 - accuracy: 0.83 - ETA: 0s - loss: 0.3760 - accuracy: 0.83 - ETA: 0s - loss: 0.3783 - accuracy: 0.83 - ETA: 0s - loss: 0.3822 - accuracy: 0.82 - ETA: 0s - loss: 0.3804 - accuracy: 0.83 - ETA: 0s - loss: 0.3821 - accuracy: 0.83 - ETA: 0s - loss: 0.3819 - accuracy: 0.83 - ETA: 0s - loss: 0.3812 - accuracy: 0.83 - ETA: 0s - loss: 0.3842 - accuracy: 0.82 - ETA: 0s - loss: 0.3844 - accuracy: 0.82 - ETA: 0s - loss: 0.3851 - accuracy: 0.82 - ETA: 0s - loss: 0.3866 - accuracy: 0.82 - ETA: 0s - loss: 0.3856 - accuracy: 0.82 - ETA: 0s - loss: 0.3861 - accuracy: 0.82 - ETA: 0s - loss: 0.3877 - accuracy: 0.82 - ETA: 0s - loss: 0.3871 - accuracy: 0.82 - ETA: 0s - loss: 0.3870 - accuracy: 0.82 - ETA: 0s - loss: 0.3863 - accuracy: 0.83 - ETA: 0s - loss: 0.3867 - accuracy: 0.82 - ETA: 0s - loss: 0.3882 - accuracy: 0.82 - 1s 86us/step - loss: 0.3881 - accuracy: 0.8290\n",
      "Epoch 94/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.2953 - accuracy: 0.80 - ETA: 1s - loss: 0.3841 - accuracy: 0.83 - ETA: 1s - loss: 0.3861 - accuracy: 0.83 - ETA: 1s - loss: 0.3939 - accuracy: 0.82 - ETA: 1s - loss: 0.4013 - accuracy: 0.81 - ETA: 1s - loss: 0.4024 - accuracy: 0.81 - ETA: 1s - loss: 0.4007 - accuracy: 0.81 - ETA: 1s - loss: 0.3962 - accuracy: 0.82 - ETA: 1s - loss: 0.3977 - accuracy: 0.82 - ETA: 1s - loss: 0.3976 - accuracy: 0.82 - ETA: 0s - loss: 0.3951 - accuracy: 0.82 - ETA: 0s - loss: 0.3954 - accuracy: 0.82 - ETA: 0s - loss: 0.3959 - accuracy: 0.82 - ETA: 0s - loss: 0.3956 - accuracy: 0.82 - ETA: 0s - loss: 0.3944 - accuracy: 0.82 - ETA: 0s - loss: 0.3946 - accuracy: 0.82 - ETA: 0s - loss: 0.3936 - accuracy: 0.82 - ETA: 0s - loss: 0.3942 - accuracy: 0.82 - ETA: 0s - loss: 0.3938 - accuracy: 0.82 - ETA: 0s - loss: 0.3945 - accuracy: 0.82 - ETA: 0s - loss: 0.3929 - accuracy: 0.82 - ETA: 0s - loss: 0.3930 - accuracy: 0.82 - ETA: 0s - loss: 0.3926 - accuracy: 0.82 - ETA: 0s - loss: 0.3918 - accuracy: 0.82 - ETA: 0s - loss: 0.3914 - accuracy: 0.82 - ETA: 0s - loss: 0.3911 - accuracy: 0.82 - ETA: 0s - loss: 0.3900 - accuracy: 0.82 - ETA: 0s - loss: 0.3888 - accuracy: 0.82 - ETA: 0s - loss: 0.3872 - accuracy: 0.82 - ETA: 0s - loss: 0.3873 - accuracy: 0.82 - 1s 86us/step - loss: 0.3873 - accuracy: 0.8288\n",
      "Epoch 95/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.4052 - accuracy: 0.80 - ETA: 1s - loss: 0.3854 - accuracy: 0.83 - ETA: 1s - loss: 0.3874 - accuracy: 0.83 - ETA: 1s - loss: 0.3912 - accuracy: 0.82 - ETA: 1s - loss: 0.4007 - accuracy: 0.82 - ETA: 1s - loss: 0.3928 - accuracy: 0.82 - ETA: 1s - loss: 0.3927 - accuracy: 0.82 - ETA: 1s - loss: 0.3897 - accuracy: 0.82 - ETA: 1s - loss: 0.3912 - accuracy: 0.82 - ETA: 0s - loss: 0.3909 - accuracy: 0.82 - ETA: 0s - loss: 0.3901 - accuracy: 0.82 - ETA: 0s - loss: 0.3893 - accuracy: 0.82 - ETA: 0s - loss: 0.3913 - accuracy: 0.82 - ETA: 0s - loss: 0.3893 - accuracy: 0.82 - ETA: 0s - loss: 0.3898 - accuracy: 0.82 - ETA: 0s - loss: 0.3876 - accuracy: 0.82 - ETA: 0s - loss: 0.3876 - accuracy: 0.82 - ETA: 0s - loss: 0.3863 - accuracy: 0.82 - ETA: 0s - loss: 0.3875 - accuracy: 0.82 - ETA: 0s - loss: 0.3861 - accuracy: 0.82 - ETA: 0s - loss: 0.3871 - accuracy: 0.82 - ETA: 0s - loss: 0.3873 - accuracy: 0.82 - ETA: 0s - loss: 0.3879 - accuracy: 0.82 - ETA: 0s - loss: 0.3864 - accuracy: 0.82 - ETA: 0s - loss: 0.3871 - accuracy: 0.82 - ETA: 0s - loss: 0.3855 - accuracy: 0.82 - ETA: 0s - loss: 0.3858 - accuracy: 0.82 - ETA: 0s - loss: 0.3862 - accuracy: 0.82 - ETA: 0s - loss: 0.3871 - accuracy: 0.82 - 1s 85us/step - loss: 0.3878 - accuracy: 0.8286\n",
      "Epoch 96/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.2044 - accuracy: 0.90 - ETA: 1s - loss: 0.3801 - accuracy: 0.83 - ETA: 1s - loss: 0.3907 - accuracy: 0.83 - ETA: 1s - loss: 0.3958 - accuracy: 0.83 - ETA: 1s - loss: 0.3921 - accuracy: 0.83 - ETA: 1s - loss: 0.3901 - accuracy: 0.83 - ETA: 1s - loss: 0.3859 - accuracy: 0.83 - ETA: 1s - loss: 0.3890 - accuracy: 0.83 - ETA: 1s - loss: 0.3840 - accuracy: 0.83 - ETA: 0s - loss: 0.3856 - accuracy: 0.83 - ETA: 0s - loss: 0.3850 - accuracy: 0.83 - ETA: 0s - loss: 0.3849 - accuracy: 0.83 - ETA: 0s - loss: 0.3863 - accuracy: 0.83 - ETA: 0s - loss: 0.3874 - accuracy: 0.82 - ETA: 0s - loss: 0.3861 - accuracy: 0.82 - ETA: 0s - loss: 0.3848 - accuracy: 0.83 - ETA: 0s - loss: 0.3848 - accuracy: 0.83 - ETA: 0s - loss: 0.3849 - accuracy: 0.83 - ETA: 0s - loss: 0.3862 - accuracy: 0.83 - ETA: 0s - loss: 0.3849 - accuracy: 0.83 - ETA: 0s - loss: 0.3850 - accuracy: 0.83 - ETA: 0s - loss: 0.3884 - accuracy: 0.82 - ETA: 0s - loss: 0.3889 - accuracy: 0.82 - ETA: 0s - loss: 0.3895 - accuracy: 0.82 - ETA: 0s - loss: 0.3896 - accuracy: 0.82 - ETA: 0s - loss: 0.3894 - accuracy: 0.82 - ETA: 0s - loss: 0.3891 - accuracy: 0.82 - ETA: 0s - loss: 0.3883 - accuracy: 0.82 - ETA: 0s - loss: 0.3893 - accuracy: 0.82 - 1s 86us/step - loss: 0.3890 - accuracy: 0.8277\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16887/16887 [==============================] - ETA: 3s - loss: 0.2693 - accuracy: 0.90 - ETA: 1s - loss: 0.3999 - accuracy: 0.82 - ETA: 1s - loss: 0.3965 - accuracy: 0.81 - ETA: 1s - loss: 0.4018 - accuracy: 0.81 - ETA: 1s - loss: 0.3941 - accuracy: 0.81 - ETA: 1s - loss: 0.3901 - accuracy: 0.82 - ETA: 1s - loss: 0.3897 - accuracy: 0.82 - ETA: 1s - loss: 0.3931 - accuracy: 0.82 - ETA: 1s - loss: 0.3933 - accuracy: 0.82 - ETA: 0s - loss: 0.3902 - accuracy: 0.82 - ETA: 0s - loss: 0.3893 - accuracy: 0.82 - ETA: 0s - loss: 0.3889 - accuracy: 0.82 - ETA: 0s - loss: 0.3903 - accuracy: 0.82 - ETA: 0s - loss: 0.3892 - accuracy: 0.82 - ETA: 0s - loss: 0.3878 - accuracy: 0.83 - ETA: 0s - loss: 0.3861 - accuracy: 0.83 - ETA: 0s - loss: 0.3871 - accuracy: 0.83 - ETA: 0s - loss: 0.3873 - accuracy: 0.83 - ETA: 0s - loss: 0.3891 - accuracy: 0.82 - ETA: 0s - loss: 0.3897 - accuracy: 0.82 - ETA: 0s - loss: 0.3916 - accuracy: 0.82 - ETA: 0s - loss: 0.3920 - accuracy: 0.82 - ETA: 0s - loss: 0.3905 - accuracy: 0.82 - ETA: 0s - loss: 0.3903 - accuracy: 0.82 - ETA: 0s - loss: 0.3893 - accuracy: 0.82 - ETA: 0s - loss: 0.3897 - accuracy: 0.82 - ETA: 0s - loss: 0.3899 - accuracy: 0.82 - ETA: 0s - loss: 0.3888 - accuracy: 0.82 - ETA: 0s - loss: 0.3888 - accuracy: 0.82 - 1s 86us/step - loss: 0.3878 - accuracy: 0.8306\n",
      "Epoch 98/100\n",
      "16887/16887 [==============================] - ETA: 3s - loss: 0.5697 - accuracy: 0.80 - ETA: 1s - loss: 0.3755 - accuracy: 0.83 - ETA: 1s - loss: 0.3844 - accuracy: 0.82 - ETA: 1s - loss: 0.3946 - accuracy: 0.81 - ETA: 1s - loss: 0.3859 - accuracy: 0.82 - ETA: 1s - loss: 0.3809 - accuracy: 0.82 - ETA: 1s - loss: 0.3800 - accuracy: 0.83 - ETA: 1s - loss: 0.3843 - accuracy: 0.82 - ETA: 1s - loss: 0.3861 - accuracy: 0.82 - ETA: 0s - loss: 0.3870 - accuracy: 0.82 - ETA: 0s - loss: 0.3898 - accuracy: 0.82 - ETA: 0s - loss: 0.3883 - accuracy: 0.82 - ETA: 0s - loss: 0.3875 - accuracy: 0.82 - ETA: 0s - loss: 0.3886 - accuracy: 0.82 - ETA: 0s - loss: 0.3871 - accuracy: 0.82 - ETA: 0s - loss: 0.3887 - accuracy: 0.82 - ETA: 0s - loss: 0.3881 - accuracy: 0.82 - ETA: 0s - loss: 0.3863 - accuracy: 0.82 - ETA: 0s - loss: 0.3862 - accuracy: 0.82 - ETA: 0s - loss: 0.3855 - accuracy: 0.82 - ETA: 0s - loss: 0.3867 - accuracy: 0.82 - ETA: 0s - loss: 0.3877 - accuracy: 0.82 - ETA: 0s - loss: 0.3893 - accuracy: 0.82 - ETA: 0s - loss: 0.3896 - accuracy: 0.82 - ETA: 0s - loss: 0.3885 - accuracy: 0.82 - ETA: 0s - loss: 0.3885 - accuracy: 0.82 - ETA: 0s - loss: 0.3875 - accuracy: 0.82 - ETA: 0s - loss: 0.3865 - accuracy: 0.82 - ETA: 0s - loss: 0.3874 - accuracy: 0.82 - 1s 86us/step - loss: 0.3873 - accuracy: 0.8289\n",
      "Epoch 99/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.4811 - accuracy: 0.70 - ETA: 1s - loss: 0.4071 - accuracy: 0.80 - ETA: 1s - loss: 0.3876 - accuracy: 0.82 - ETA: 1s - loss: 0.3777 - accuracy: 0.83 - ETA: 1s - loss: 0.3874 - accuracy: 0.82 - ETA: 1s - loss: 0.3881 - accuracy: 0.82 - ETA: 1s - loss: 0.3865 - accuracy: 0.82 - ETA: 1s - loss: 0.3841 - accuracy: 0.82 - ETA: 1s - loss: 0.3825 - accuracy: 0.83 - ETA: 1s - loss: 0.3860 - accuracy: 0.82 - ETA: 0s - loss: 0.3843 - accuracy: 0.83 - ETA: 0s - loss: 0.3836 - accuracy: 0.83 - ETA: 0s - loss: 0.3835 - accuracy: 0.83 - ETA: 0s - loss: 0.3835 - accuracy: 0.83 - ETA: 0s - loss: 0.3848 - accuracy: 0.82 - ETA: 0s - loss: 0.3839 - accuracy: 0.83 - ETA: 0s - loss: 0.3873 - accuracy: 0.82 - ETA: 0s - loss: 0.3903 - accuracy: 0.82 - ETA: 0s - loss: 0.3912 - accuracy: 0.82 - ETA: 0s - loss: 0.3896 - accuracy: 0.82 - ETA: 0s - loss: 0.3919 - accuracy: 0.82 - ETA: 0s - loss: 0.3919 - accuracy: 0.82 - ETA: 0s - loss: 0.3920 - accuracy: 0.82 - ETA: 0s - loss: 0.3916 - accuracy: 0.82 - ETA: 0s - loss: 0.3894 - accuracy: 0.82 - ETA: 0s - loss: 0.3894 - accuracy: 0.82 - ETA: 0s - loss: 0.3874 - accuracy: 0.83 - ETA: 0s - loss: 0.3879 - accuracy: 0.82 - ETA: 0s - loss: 0.3871 - accuracy: 0.83 - 1s 86us/step - loss: 0.3876 - accuracy: 0.8296\n",
      "Epoch 100/100\n",
      "16887/16887 [==============================] - ETA: 5s - loss: 0.3962 - accuracy: 0.80 - ETA: 2s - loss: 0.4406 - accuracy: 0.77 - ETA: 1s - loss: 0.3987 - accuracy: 0.82 - ETA: 1s - loss: 0.3949 - accuracy: 0.82 - ETA: 1s - loss: 0.3914 - accuracy: 0.82 - ETA: 1s - loss: 0.3863 - accuracy: 0.82 - ETA: 1s - loss: 0.3892 - accuracy: 0.82 - ETA: 1s - loss: 0.3864 - accuracy: 0.83 - ETA: 1s - loss: 0.3868 - accuracy: 0.83 - ETA: 1s - loss: 0.3894 - accuracy: 0.82 - ETA: 1s - loss: 0.3875 - accuracy: 0.82 - ETA: 0s - loss: 0.3896 - accuracy: 0.82 - ETA: 0s - loss: 0.3903 - accuracy: 0.82 - ETA: 0s - loss: 0.3931 - accuracy: 0.82 - ETA: 0s - loss: 0.3939 - accuracy: 0.82 - ETA: 0s - loss: 0.3924 - accuracy: 0.82 - ETA: 0s - loss: 0.3933 - accuracy: 0.82 - ETA: 0s - loss: 0.3929 - accuracy: 0.82 - ETA: 0s - loss: 0.3934 - accuracy: 0.82 - ETA: 0s - loss: 0.3915 - accuracy: 0.82 - ETA: 0s - loss: 0.3920 - accuracy: 0.82 - ETA: 0s - loss: 0.3917 - accuracy: 0.82 - ETA: 0s - loss: 0.3925 - accuracy: 0.82 - ETA: 0s - loss: 0.3914 - accuracy: 0.82 - ETA: 0s - loss: 0.3899 - accuracy: 0.82 - ETA: 0s - loss: 0.3899 - accuracy: 0.82 - ETA: 0s - loss: 0.3904 - accuracy: 0.82 - ETA: 0s - loss: 0.3888 - accuracy: 0.82 - ETA: 0s - loss: 0.3880 - accuracy: 0.82 - ETA: 0s - loss: 0.3881 - accuracy: 0.82 - 1s 89us/step - loss: 0.3874 - accuracy: 0.8284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x28a0e379908>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the data to the training dataset\n",
    "classifier.fit(x_train,y_train, batch_size=10, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16887/16887 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 14us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49380437"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model=classifier.evaluate(x_train, y_train)\n",
    "y_pred=classifier.predict(x_test)\n",
    "mean_pred = y_pred.mean()\n",
    "mean_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred =(y_pred>mean_pred)\n",
    "y_pred_t=classifier.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NeuralNetwork</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>roc_auc_train</th>\n",
       "      <td>0.906375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc_test</th>\n",
       "      <td>0.817715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.817707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.787171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.838453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               NeuralNetwork\n",
       "roc_auc_train       0.906375\n",
       "roc_auc_test        0.817715\n",
       "accuracy            0.817707\n",
       "recall              0.787171\n",
       "precision           0.838453"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy=accuracy_score(y_test, y_pred)\n",
    "recall=recall_score(y_test, y_pred)\n",
    "precision=precision_score(y_test, y_pred)\n",
    "roc_auc_train=roc_auc_score(y_train, y_pred_t)\n",
    "roc_auc_test=roc_auc_score(y_test, y_pred)\n",
    "data_NN=pd.DataFrame((roc_auc_train,roc_auc_test,accuracy,recall,precision), \n",
    "                index = ['roc_auc_train','roc_auc_test','accuracy','recall','precision'],columns = ['NeuralNetwork']) \n",
    "data_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизаторы"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adagrad - обнавляет параметры с оглядкой на то, насколько типичный признак он описывает. Хронит для каждого параметра в виде G =$\\sum ()^2$ его обнавлений. Хотим уменьшить количества обновлений для элементов. \n",
    "\n",
    "- (-) может привести к большой сумме и обновления будут редко.\n",
    "- (+) нет необходимости точно подбирать lr\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSprop: мы всё так же собираемся обновлять меньше веса, которые слишком часто обновляются, но вместо полной суммы обновлений, будем использовать усреднённый по истории квадрат градиента.\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adadelta: отличается от RMSprop тем, что мы добавляем в числитель стабилизирующий член пропорциональный  $RMS$ от $\\Delta\\theta_t$. На шаге $t$ мы ещё не знаем значение $RMS[\\Delta \\theta]_{t}$, поэтому обновление параметров происходит в три этапа, а не в два: сначала накапливаем квадрат градиента, затем обновляем $\\theta$, после чего обновляем $RMS[\\Delta \\theta]$.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam: Он сочетает в себе и идею накопления движения и идею более слабого обновления весов для типичных признаков. \n",
    "![image.png](attachment:image.png)\n",
    "Легко заметить, что это уже знакомый нам $E[g^2]_t$, так что по сути тут нет отличий от RMSProp.\n",
    "\n",
    "Важное отличие состоит в начальной калибровке \\inline m_t и \\inline v_t: \n",
    "\n",
    "та же проблема что и в RMSprop: если задать нулевое начальное значение, то они будут долго накапливаться, особенно при большом окне накопления $(0<< \\beta_1<1, 0<< \\beta_2 <1)$, а какие-то изначальные значения — это ещё два гиперпараметра. Никто не хочет ещё два гиперпараметра, так что мы искусственно увеличиваем  $m_t$ и $v_t$ на первых шагах (примерно  $0 <t<10$ для $ m_t и $0 <t <1000$ для $v_t$)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подбор параметров GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout #метод решения проблемы переобучения в нейронных сетях\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_NN ={'batch_size':[4,8,10,16,32,64],\n",
    "'nb_epoch':[50,100,150],\n",
    "'optimizer':['rmsprop','SGD','adam','Adagrad','Adadelta'],#\n",
    "'units':[4,8,13]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifierGS_Drop(optimizer, units):\n",
    "    classifier = Sequential()\n",
    "    #First Hidden Layer\n",
    "    classifier.add(Dense(units=units, activation='relu', kernel_initializer='random_normal', input_dim=52)\n",
    "    classifier.add(Dropout(rate=0.2))\n",
    "    #Second  Hidden Layer\n",
    "    #classifier.add(Dense(units=units, activation='relu', kernel_initializer='random_normal'))\n",
    "#Output Layer\n",
    "    classifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "#Compiling the neural network\n",
    "    classifier.compile(optimizer=optimizer,loss='binary_crossentropy', metrics =['accuracy'])\n",
    "    return classifier\n",
    "classifierGS_Drop = KerasClassifier(build_fn=build_classifierGS_Drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  EarlyStopping \n",
    "Слишком много эпох может привести к переобучению учебного набора данных, в то время как слишком мало может привести к модели недостаточного соответствия. Ранняя остановка - это метод, который позволяет указать произвольно большое количество периодов обучения и прекратить обучение, как только производительность модели перестает улучшаться в наборе проверенных данных.\n",
    "\n",
    "Функции обратного вызова вызываются после каждой эпохи обучения с помощью параметра callbacks, передаваемого методам fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "earlystop = EarlyStopping(monitor = 'val_loss',#контролируемое значение\n",
    "                          min_delta = 0,#минимальное изменение контролируемого значения.\n",
    "                          patience = 3,# количество эпох без улучшений, после которых обучение будет остановлено\n",
    "                          verbose = 1,#Чтобы обнаружить эпоху обучения, в которой обучение было остановлено,\n",
    "                          restore_best_weights = True)# сохранить лучшие веса после остановки"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Основная идея слоя Dropout заключается в следующем: вместо обучения одной нейронной сети мы обучаем несколько нейронных сетей, а затем усредняем полученные результаты.\n",
    "\n",
    "С помощью исключения из сети нейронов с вероятностью 𝑝 мы получаем сети для обучения. Таким образом, вероятность того, что нейрон останется в сети, составляет 1 − 𝑝.\n",
    "\n",
    "Выбрасывание нейрона из сети означает, что при любых входных данных или параметрах\n",
    "он возвращает 0.\n",
    "\n",
    "Выброшенные нейроны перестают вносить свой вклад в процесс обучения ни на одном\n",
    "из этапов алгоритма, поэтому исключение хотя бы одного нейрона из сети равносильно обучению новой нейронной сети.\n",
    "\n",
    "Другими словами Dropout предотвращает взаимоадаптацию нейронов на этапе обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:35 - loss: 0.6980 - accuracy: 0.50 - ETA: 9s - loss: 0.6942 - accuracy: 0.5147 - ETA: 6s - loss: 0.6929 - accuracy: 0.52 - ETA: 4s - loss: 0.6931 - accuracy: 0.51 - ETA: 4s - loss: 0.6927 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.52 - ETA: 3s - loss: 0.6924 - accuracy: 0.52 - ETA: 3s - loss: 0.6921 - accuracy: 0.52 - ETA: 3s - loss: 0.6919 - accuracy: 0.53 - ETA: 3s - loss: 0.6916 - accuracy: 0.53 - ETA: 3s - loss: 0.6917 - accuracy: 0.53 - ETA: 2s - loss: 0.6912 - accuracy: 0.53 - ETA: 2s - loss: 0.6910 - accuracy: 0.54 - ETA: 2s - loss: 0.6899 - accuracy: 0.55 - ETA: 2s - loss: 0.6894 - accuracy: 0.55 - ETA: 2s - loss: 0.6892 - accuracy: 0.55 - ETA: 2s - loss: 0.6885 - accuracy: 0.55 - ETA: 2s - loss: 0.6878 - accuracy: 0.56 - ETA: 2s - loss: 0.6870 - accuracy: 0.56 - ETA: 2s - loss: 0.6865 - accuracy: 0.56 - ETA: 2s - loss: 0.6858 - accuracy: 0.57 - ETA: 2s - loss: 0.6849 - accuracy: 0.57 - ETA: 2s - loss: 0.6847 - accuracy: 0.57 - ETA: 2s - loss: 0.6840 - accuracy: 0.57 - ETA: 2s - loss: 0.6832 - accuracy: 0.57 - ETA: 1s - loss: 0.6830 - accuracy: 0.57 - ETA: 1s - loss: 0.6826 - accuracy: 0.57 - ETA: 1s - loss: 0.6818 - accuracy: 0.58 - ETA: 1s - loss: 0.6815 - accuracy: 0.58 - ETA: 1s - loss: 0.6807 - accuracy: 0.58 - ETA: 1s - loss: 0.6795 - accuracy: 0.58 - ETA: 1s - loss: 0.6788 - accuracy: 0.58 - ETA: 1s - loss: 0.6781 - accuracy: 0.58 - ETA: 1s - loss: 0.6771 - accuracy: 0.58 - ETA: 1s - loss: 0.6770 - accuracy: 0.58 - ETA: 1s - loss: 0.6763 - accuracy: 0.58 - ETA: 1s - loss: 0.6757 - accuracy: 0.58 - ETA: 1s - loss: 0.6760 - accuracy: 0.58 - ETA: 1s - loss: 0.6752 - accuracy: 0.58 - ETA: 1s - loss: 0.6744 - accuracy: 0.58 - ETA: 1s - loss: 0.6741 - accuracy: 0.59 - ETA: 1s - loss: 0.6734 - accuracy: 0.59 - ETA: 0s - loss: 0.6719 - accuracy: 0.59 - ETA: 0s - loss: 0.6713 - accuracy: 0.59 - ETA: 0s - loss: 0.6707 - accuracy: 0.59 - ETA: 0s - loss: 0.6706 - accuracy: 0.59 - ETA: 0s - loss: 0.6695 - accuracy: 0.59 - ETA: 0s - loss: 0.6688 - accuracy: 0.59 - ETA: 0s - loss: 0.6680 - accuracy: 0.59 - ETA: 0s - loss: 0.6676 - accuracy: 0.59 - ETA: 0s - loss: 0.6674 - accuracy: 0.60 - ETA: 0s - loss: 0.6666 - accuracy: 0.60 - ETA: 0s - loss: 0.6659 - accuracy: 0.60 - ETA: 0s - loss: 0.6648 - accuracy: 0.60 - ETA: 0s - loss: 0.6638 - accuracy: 0.60 - ETA: 0s - loss: 0.6632 - accuracy: 0.60 - ETA: 0s - loss: 0.6628 - accuracy: 0.60 - ETA: 0s - loss: 0.6621 - accuracy: 0.60 - ETA: 0s - loss: 0.6616 - accuracy: 0.60 - ETA: 0s - loss: 0.6613 - accuracy: 0.60 - 3s 261us/step - loss: 0.6612 - accuracy: 0.6083 - val_loss: 0.6121 - val_accuracy: 0.7081\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 109us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:45 - loss: 0.6933 - accuracy: 0.50 - ETA: 9s - loss: 0.6924 - accuracy: 0.5509 - ETA: 6s - loss: 0.6928 - accuracy: 0.53 - ETA: 5s - loss: 0.6917 - accuracy: 0.54 - ETA: 4s - loss: 0.6912 - accuracy: 0.54 - ETA: 4s - loss: 0.6912 - accuracy: 0.53 - ETA: 3s - loss: 0.6922 - accuracy: 0.52 - ETA: 3s - loss: 0.6922 - accuracy: 0.51 - ETA: 3s - loss: 0.6924 - accuracy: 0.51 - ETA: 3s - loss: 0.6922 - accuracy: 0.52 - ETA: 3s - loss: 0.6918 - accuracy: 0.53 - ETA: 3s - loss: 0.6914 - accuracy: 0.52 - ETA: 2s - loss: 0.6908 - accuracy: 0.52 - ETA: 2s - loss: 0.6904 - accuracy: 0.52 - ETA: 2s - loss: 0.6902 - accuracy: 0.52 - ETA: 2s - loss: 0.6897 - accuracy: 0.53 - ETA: 2s - loss: 0.6894 - accuracy: 0.53 - ETA: 2s - loss: 0.6891 - accuracy: 0.53 - ETA: 2s - loss: 0.6888 - accuracy: 0.54 - ETA: 2s - loss: 0.6883 - accuracy: 0.54 - ETA: 2s - loss: 0.6873 - accuracy: 0.54 - ETA: 2s - loss: 0.6864 - accuracy: 0.55 - ETA: 2s - loss: 0.6863 - accuracy: 0.55 - ETA: 2s - loss: 0.6859 - accuracy: 0.55 - ETA: 2s - loss: 0.6857 - accuracy: 0.55 - ETA: 1s - loss: 0.6851 - accuracy: 0.56 - ETA: 1s - loss: 0.6846 - accuracy: 0.56 - ETA: 1s - loss: 0.6840 - accuracy: 0.56 - ETA: 1s - loss: 0.6837 - accuracy: 0.56 - ETA: 1s - loss: 0.6831 - accuracy: 0.56 - ETA: 1s - loss: 0.6821 - accuracy: 0.56 - ETA: 1s - loss: 0.6812 - accuracy: 0.57 - ETA: 1s - loss: 0.6807 - accuracy: 0.57 - ETA: 1s - loss: 0.6803 - accuracy: 0.57 - ETA: 1s - loss: 0.6795 - accuracy: 0.57 - ETA: 1s - loss: 0.6791 - accuracy: 0.57 - ETA: 1s - loss: 0.6786 - accuracy: 0.57 - ETA: 1s - loss: 0.6776 - accuracy: 0.58 - ETA: 1s - loss: 0.6770 - accuracy: 0.58 - ETA: 1s - loss: 0.6768 - accuracy: 0.58 - ETA: 1s - loss: 0.6758 - accuracy: 0.58 - ETA: 1s - loss: 0.6760 - accuracy: 0.58 - ETA: 0s - loss: 0.6748 - accuracy: 0.58 - ETA: 0s - loss: 0.6745 - accuracy: 0.58 - ETA: 0s - loss: 0.6738 - accuracy: 0.59 - ETA: 0s - loss: 0.6732 - accuracy: 0.59 - ETA: 0s - loss: 0.6728 - accuracy: 0.59 - ETA: 0s - loss: 0.6725 - accuracy: 0.59 - ETA: 0s - loss: 0.6721 - accuracy: 0.59 - ETA: 0s - loss: 0.6715 - accuracy: 0.59 - ETA: 0s - loss: 0.6715 - accuracy: 0.59 - ETA: 0s - loss: 0.6708 - accuracy: 0.60 - ETA: 0s - loss: 0.6704 - accuracy: 0.60 - ETA: 0s - loss: 0.6699 - accuracy: 0.60 - ETA: 0s - loss: 0.6700 - accuracy: 0.60 - ETA: 0s - loss: 0.6700 - accuracy: 0.60 - ETA: 0s - loss: 0.6694 - accuracy: 0.60 - ETA: 0s - loss: 0.6688 - accuracy: 0.60 - ETA: 0s - loss: 0.6685 - accuracy: 0.60 - ETA: 0s - loss: 0.6678 - accuracy: 0.60 - ETA: 0s - loss: 0.6673 - accuracy: 0.61 - 3s 266us/step - loss: 0.6670 - accuracy: 0.6117 - val_loss: 0.6259 - val_accuracy: 0.7202\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 114us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:54 - loss: 0.6804 - accuracy: 1.00 - ETA: 9s - loss: 0.6923 - accuracy: 0.5318 - ETA: 6s - loss: 0.6934 - accuracy: 0.50 - ETA: 5s - loss: 0.6911 - accuracy: 0.52 - ETA: 4s - loss: 0.6906 - accuracy: 0.53 - ETA: 4s - loss: 0.6909 - accuracy: 0.52 - ETA: 3s - loss: 0.6909 - accuracy: 0.51 - ETA: 3s - loss: 0.6909 - accuracy: 0.52 - ETA: 3s - loss: 0.6904 - accuracy: 0.53 - ETA: 3s - loss: 0.6901 - accuracy: 0.53 - ETA: 3s - loss: 0.6898 - accuracy: 0.53 - ETA: 3s - loss: 0.6893 - accuracy: 0.53 - ETA: 3s - loss: 0.6891 - accuracy: 0.54 - ETA: 2s - loss: 0.6882 - accuracy: 0.54 - ETA: 2s - loss: 0.6873 - accuracy: 0.54 - ETA: 2s - loss: 0.6868 - accuracy: 0.55 - ETA: 2s - loss: 0.6861 - accuracy: 0.55 - ETA: 2s - loss: 0.6855 - accuracy: 0.55 - ETA: 2s - loss: 0.6853 - accuracy: 0.55 - ETA: 2s - loss: 0.6842 - accuracy: 0.56 - ETA: 2s - loss: 0.6835 - accuracy: 0.56 - ETA: 2s - loss: 0.6828 - accuracy: 0.56 - ETA: 2s - loss: 0.6811 - accuracy: 0.56 - ETA: 2s - loss: 0.6807 - accuracy: 0.56 - ETA: 2s - loss: 0.6798 - accuracy: 0.56 - ETA: 1s - loss: 0.6789 - accuracy: 0.56 - ETA: 1s - loss: 0.6777 - accuracy: 0.57 - ETA: 1s - loss: 0.6765 - accuracy: 0.57 - ETA: 1s - loss: 0.6757 - accuracy: 0.57 - ETA: 1s - loss: 0.6746 - accuracy: 0.57 - ETA: 1s - loss: 0.6739 - accuracy: 0.57 - ETA: 1s - loss: 0.6733 - accuracy: 0.58 - ETA: 1s - loss: 0.6726 - accuracy: 0.58 - ETA: 1s - loss: 0.6711 - accuracy: 0.58 - ETA: 1s - loss: 0.6709 - accuracy: 0.58 - ETA: 1s - loss: 0.6708 - accuracy: 0.58 - ETA: 1s - loss: 0.6704 - accuracy: 0.58 - ETA: 1s - loss: 0.6695 - accuracy: 0.58 - ETA: 1s - loss: 0.6692 - accuracy: 0.59 - ETA: 1s - loss: 0.6678 - accuracy: 0.59 - ETA: 1s - loss: 0.6665 - accuracy: 0.59 - ETA: 1s - loss: 0.6666 - accuracy: 0.59 - ETA: 1s - loss: 0.6662 - accuracy: 0.59 - ETA: 0s - loss: 0.6647 - accuracy: 0.59 - ETA: 0s - loss: 0.6644 - accuracy: 0.59 - ETA: 0s - loss: 0.6634 - accuracy: 0.59 - ETA: 0s - loss: 0.6627 - accuracy: 0.59 - ETA: 0s - loss: 0.6622 - accuracy: 0.59 - ETA: 0s - loss: 0.6609 - accuracy: 0.59 - ETA: 0s - loss: 0.6599 - accuracy: 0.60 - ETA: 0s - loss: 0.6596 - accuracy: 0.60 - ETA: 0s - loss: 0.6586 - accuracy: 0.60 - ETA: 0s - loss: 0.6580 - accuracy: 0.60 - ETA: 0s - loss: 0.6580 - accuracy: 0.60 - ETA: 0s - loss: 0.6575 - accuracy: 0.60 - ETA: 0s - loss: 0.6568 - accuracy: 0.60 - ETA: 0s - loss: 0.6564 - accuracy: 0.60 - ETA: 0s - loss: 0.6554 - accuracy: 0.60 - ETA: 0s - loss: 0.6558 - accuracy: 0.60 - ETA: 0s - loss: 0.6554 - accuracy: 0.60 - ETA: 0s - loss: 0.6556 - accuracy: 0.60 - ETA: 0s - loss: 0.6556 - accuracy: 0.60 - 3s 270us/step - loss: 0.6556 - accuracy: 0.6024 - val_loss: 0.6062 - val_accuracy: 0.7202\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 106us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:38 - loss: 0.6912 - accuracy: 1.00 - ETA: 8s - loss: 0.6930 - accuracy: 0.5089 - ETA: 5s - loss: 0.6928 - accuracy: 0.53 - ETA: 4s - loss: 0.6922 - accuracy: 0.55 - ETA: 4s - loss: 0.6920 - accuracy: 0.56 - ETA: 3s - loss: 0.6919 - accuracy: 0.55 - ETA: 3s - loss: 0.6922 - accuracy: 0.54 - ETA: 3s - loss: 0.6922 - accuracy: 0.54 - ETA: 3s - loss: 0.6920 - accuracy: 0.54 - ETA: 3s - loss: 0.6916 - accuracy: 0.54 - ETA: 3s - loss: 0.6916 - accuracy: 0.54 - ETA: 2s - loss: 0.6908 - accuracy: 0.55 - ETA: 2s - loss: 0.6908 - accuracy: 0.55 - ETA: 2s - loss: 0.6904 - accuracy: 0.55 - ETA: 2s - loss: 0.6900 - accuracy: 0.55 - ETA: 2s - loss: 0.6892 - accuracy: 0.55 - ETA: 2s - loss: 0.6886 - accuracy: 0.55 - ETA: 2s - loss: 0.6879 - accuracy: 0.56 - ETA: 2s - loss: 0.6872 - accuracy: 0.56 - ETA: 2s - loss: 0.6865 - accuracy: 0.56 - ETA: 2s - loss: 0.6854 - accuracy: 0.57 - ETA: 2s - loss: 0.6848 - accuracy: 0.56 - ETA: 2s - loss: 0.6842 - accuracy: 0.57 - ETA: 2s - loss: 0.6835 - accuracy: 0.57 - ETA: 1s - loss: 0.6830 - accuracy: 0.57 - ETA: 1s - loss: 0.6823 - accuracy: 0.57 - ETA: 1s - loss: 0.6815 - accuracy: 0.57 - ETA: 1s - loss: 0.6803 - accuracy: 0.58 - ETA: 1s - loss: 0.6795 - accuracy: 0.58 - ETA: 1s - loss: 0.6788 - accuracy: 0.58 - ETA: 1s - loss: 0.6780 - accuracy: 0.58 - ETA: 1s - loss: 0.6771 - accuracy: 0.59 - ETA: 1s - loss: 0.6760 - accuracy: 0.59 - ETA: 1s - loss: 0.6751 - accuracy: 0.59 - ETA: 1s - loss: 0.6743 - accuracy: 0.59 - ETA: 1s - loss: 0.6736 - accuracy: 0.59 - ETA: 1s - loss: 0.6723 - accuracy: 0.60 - ETA: 1s - loss: 0.6718 - accuracy: 0.60 - ETA: 1s - loss: 0.6708 - accuracy: 0.60 - ETA: 1s - loss: 0.6703 - accuracy: 0.60 - ETA: 1s - loss: 0.6700 - accuracy: 0.60 - ETA: 1s - loss: 0.6698 - accuracy: 0.60 - ETA: 0s - loss: 0.6689 - accuracy: 0.60 - ETA: 0s - loss: 0.6682 - accuracy: 0.61 - ETA: 0s - loss: 0.6676 - accuracy: 0.61 - ETA: 0s - loss: 0.6668 - accuracy: 0.61 - ETA: 0s - loss: 0.6666 - accuracy: 0.61 - ETA: 0s - loss: 0.6666 - accuracy: 0.61 - ETA: 0s - loss: 0.6658 - accuracy: 0.61 - ETA: 0s - loss: 0.6648 - accuracy: 0.61 - ETA: 0s - loss: 0.6645 - accuracy: 0.61 - ETA: 0s - loss: 0.6641 - accuracy: 0.61 - ETA: 0s - loss: 0.6634 - accuracy: 0.61 - ETA: 0s - loss: 0.6625 - accuracy: 0.61 - ETA: 0s - loss: 0.6619 - accuracy: 0.61 - ETA: 0s - loss: 0.6612 - accuracy: 0.62 - ETA: 0s - loss: 0.6611 - accuracy: 0.62 - ETA: 0s - loss: 0.6606 - accuracy: 0.62 - ETA: 0s - loss: 0.6600 - accuracy: 0.62 - ETA: 0s - loss: 0.6596 - accuracy: 0.62 - ETA: 0s - loss: 0.6585 - accuracy: 0.62 - ETA: 0s - loss: 0.6580 - accuracy: 0.62 - 3s 269us/step - loss: 0.6578 - accuracy: 0.6262 - val_loss: 0.6023 - val_accuracy: 0.7131\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 107us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 5:38 - loss: 0.6947 - accuracy: 0.50 - ETA: 9s - loss: 0.6934 - accuracy: 0.5330 - ETA: 6s - loss: 0.6936 - accuracy: 0.50 - ETA: 5s - loss: 0.6931 - accuracy: 0.52 - ETA: 4s - loss: 0.6933 - accuracy: 0.51 - ETA: 4s - loss: 0.6933 - accuracy: 0.50 - ETA: 3s - loss: 0.6934 - accuracy: 0.50 - ETA: 3s - loss: 0.6932 - accuracy: 0.51 - ETA: 3s - loss: 0.6930 - accuracy: 0.52 - ETA: 3s - loss: 0.6929 - accuracy: 0.52 - ETA: 3s - loss: 0.6927 - accuracy: 0.52 - ETA: 2s - loss: 0.6926 - accuracy: 0.52 - ETA: 2s - loss: 0.6924 - accuracy: 0.53 - ETA: 2s - loss: 0.6925 - accuracy: 0.53 - ETA: 2s - loss: 0.6922 - accuracy: 0.53 - ETA: 2s - loss: 0.6922 - accuracy: 0.53 - ETA: 2s - loss: 0.6918 - accuracy: 0.53 - ETA: 2s - loss: 0.6916 - accuracy: 0.53 - ETA: 2s - loss: 0.6910 - accuracy: 0.54 - ETA: 2s - loss: 0.6911 - accuracy: 0.54 - ETA: 2s - loss: 0.6906 - accuracy: 0.54 - ETA: 2s - loss: 0.6903 - accuracy: 0.54 - ETA: 2s - loss: 0.6899 - accuracy: 0.54 - ETA: 2s - loss: 0.6895 - accuracy: 0.54 - ETA: 1s - loss: 0.6889 - accuracy: 0.54 - ETA: 1s - loss: 0.6885 - accuracy: 0.54 - ETA: 1s - loss: 0.6881 - accuracy: 0.55 - ETA: 1s - loss: 0.6879 - accuracy: 0.55 - ETA: 1s - loss: 0.6874 - accuracy: 0.55 - ETA: 1s - loss: 0.6869 - accuracy: 0.55 - ETA: 1s - loss: 0.6861 - accuracy: 0.55 - ETA: 1s - loss: 0.6858 - accuracy: 0.55 - ETA: 1s - loss: 0.6851 - accuracy: 0.55 - ETA: 1s - loss: 0.6845 - accuracy: 0.55 - ETA: 1s - loss: 0.6840 - accuracy: 0.55 - ETA: 1s - loss: 0.6837 - accuracy: 0.55 - ETA: 1s - loss: 0.6835 - accuracy: 0.56 - ETA: 1s - loss: 0.6834 - accuracy: 0.55 - ETA: 1s - loss: 0.6827 - accuracy: 0.56 - ETA: 1s - loss: 0.6820 - accuracy: 0.56 - ETA: 0s - loss: 0.6817 - accuracy: 0.56 - ETA: 0s - loss: 0.6809 - accuracy: 0.56 - ETA: 0s - loss: 0.6800 - accuracy: 0.56 - ETA: 0s - loss: 0.6798 - accuracy: 0.56 - ETA: 0s - loss: 0.6794 - accuracy: 0.56 - ETA: 0s - loss: 0.6790 - accuracy: 0.56 - ETA: 0s - loss: 0.6786 - accuracy: 0.56 - ETA: 0s - loss: 0.6784 - accuracy: 0.56 - ETA: 0s - loss: 0.6774 - accuracy: 0.56 - ETA: 0s - loss: 0.6770 - accuracy: 0.56 - ETA: 0s - loss: 0.6767 - accuracy: 0.56 - ETA: 0s - loss: 0.6765 - accuracy: 0.56 - ETA: 0s - loss: 0.6761 - accuracy: 0.57 - ETA: 0s - loss: 0.6755 - accuracy: 0.57 - ETA: 0s - loss: 0.6750 - accuracy: 0.57 - ETA: 0s - loss: 0.6744 - accuracy: 0.57 - ETA: 0s - loss: 0.6741 - accuracy: 0.57 - ETA: 0s - loss: 0.6737 - accuracy: 0.57 - ETA: 0s - loss: 0.6728 - accuracy: 0.57 - 3s 259us/step - loss: 0.6726 - accuracy: 0.5736 - val_loss: 0.6296 - val_accuracy: 0.7152\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 112us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:29 - loss: 0.6868 - accuracy: 0.75 - ETA: 9s - loss: 0.6924 - accuracy: 0.5236 - ETA: 5s - loss: 0.6917 - accuracy: 0.52 - ETA: 4s - loss: 0.6917 - accuracy: 0.52 - ETA: 4s - loss: 0.6920 - accuracy: 0.52 - ETA: 3s - loss: 0.6917 - accuracy: 0.53 - ETA: 3s - loss: 0.6915 - accuracy: 0.53 - ETA: 3s - loss: 0.6909 - accuracy: 0.54 - ETA: 3s - loss: 0.6908 - accuracy: 0.54 - ETA: 3s - loss: 0.6902 - accuracy: 0.54 - ETA: 3s - loss: 0.6899 - accuracy: 0.55 - ETA: 3s - loss: 0.6896 - accuracy: 0.54 - ETA: 2s - loss: 0.6892 - accuracy: 0.55 - ETA: 2s - loss: 0.6886 - accuracy: 0.55 - ETA: 2s - loss: 0.6879 - accuracy: 0.55 - ETA: 2s - loss: 0.6874 - accuracy: 0.55 - ETA: 2s - loss: 0.6868 - accuracy: 0.55 - ETA: 2s - loss: 0.6857 - accuracy: 0.55 - ETA: 2s - loss: 0.6848 - accuracy: 0.56 - ETA: 2s - loss: 0.6845 - accuracy: 0.56 - ETA: 2s - loss: 0.6840 - accuracy: 0.56 - ETA: 2s - loss: 0.6834 - accuracy: 0.56 - ETA: 2s - loss: 0.6825 - accuracy: 0.56 - ETA: 2s - loss: 0.6817 - accuracy: 0.56 - ETA: 2s - loss: 0.6804 - accuracy: 0.57 - ETA: 2s - loss: 0.6793 - accuracy: 0.57 - ETA: 1s - loss: 0.6788 - accuracy: 0.57 - ETA: 1s - loss: 0.6779 - accuracy: 0.57 - ETA: 1s - loss: 0.6778 - accuracy: 0.57 - ETA: 1s - loss: 0.6771 - accuracy: 0.57 - ETA: 1s - loss: 0.6767 - accuracy: 0.57 - ETA: 1s - loss: 0.6754 - accuracy: 0.57 - ETA: 1s - loss: 0.6746 - accuracy: 0.57 - ETA: 1s - loss: 0.6737 - accuracy: 0.58 - ETA: 1s - loss: 0.6727 - accuracy: 0.58 - ETA: 1s - loss: 0.6719 - accuracy: 0.58 - ETA: 1s - loss: 0.6709 - accuracy: 0.58 - ETA: 1s - loss: 0.6696 - accuracy: 0.58 - ETA: 1s - loss: 0.6690 - accuracy: 0.58 - ETA: 1s - loss: 0.6683 - accuracy: 0.58 - ETA: 1s - loss: 0.6674 - accuracy: 0.59 - ETA: 1s - loss: 0.6666 - accuracy: 0.59 - ETA: 1s - loss: 0.6652 - accuracy: 0.59 - ETA: 0s - loss: 0.6656 - accuracy: 0.59 - ETA: 0s - loss: 0.6648 - accuracy: 0.59 - ETA: 0s - loss: 0.6649 - accuracy: 0.59 - ETA: 0s - loss: 0.6636 - accuracy: 0.59 - ETA: 0s - loss: 0.6631 - accuracy: 0.59 - ETA: 0s - loss: 0.6624 - accuracy: 0.59 - ETA: 0s - loss: 0.6620 - accuracy: 0.59 - ETA: 0s - loss: 0.6609 - accuracy: 0.59 - ETA: 0s - loss: 0.6604 - accuracy: 0.60 - ETA: 0s - loss: 0.6598 - accuracy: 0.60 - ETA: 0s - loss: 0.6591 - accuracy: 0.60 - ETA: 0s - loss: 0.6587 - accuracy: 0.60 - ETA: 0s - loss: 0.6583 - accuracy: 0.60 - ETA: 0s - loss: 0.6575 - accuracy: 0.60 - ETA: 0s - loss: 0.6570 - accuracy: 0.60 - ETA: 0s - loss: 0.6564 - accuracy: 0.60 - ETA: 0s - loss: 0.6556 - accuracy: 0.60 - ETA: 0s - loss: 0.6550 - accuracy: 0.60 - ETA: 0s - loss: 0.6547 - accuracy: 0.60 - 3s 271us/step - loss: 0.6539 - accuracy: 0.6093 - val_loss: 0.6029 - val_accuracy: 0.7159\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:38 - loss: 0.6891 - accuracy: 0.50 - ETA: 9s - loss: 0.6932 - accuracy: 0.5139 - ETA: 5s - loss: 0.6927 - accuracy: 0.51 - ETA: 4s - loss: 0.6925 - accuracy: 0.51 - ETA: 4s - loss: 0.6924 - accuracy: 0.52 - ETA: 3s - loss: 0.6919 - accuracy: 0.52 - ETA: 3s - loss: 0.6924 - accuracy: 0.51 - ETA: 3s - loss: 0.6922 - accuracy: 0.51 - ETA: 3s - loss: 0.6920 - accuracy: 0.51 - ETA: 3s - loss: 0.6916 - accuracy: 0.51 - ETA: 2s - loss: 0.6910 - accuracy: 0.52 - ETA: 2s - loss: 0.6898 - accuracy: 0.52 - ETA: 2s - loss: 0.6897 - accuracy: 0.53 - ETA: 2s - loss: 0.6895 - accuracy: 0.53 - ETA: 2s - loss: 0.6885 - accuracy: 0.54 - ETA: 2s - loss: 0.6874 - accuracy: 0.54 - ETA: 2s - loss: 0.6869 - accuracy: 0.54 - ETA: 2s - loss: 0.6864 - accuracy: 0.55 - ETA: 2s - loss: 0.6856 - accuracy: 0.55 - ETA: 2s - loss: 0.6852 - accuracy: 0.55 - ETA: 2s - loss: 0.6840 - accuracy: 0.55 - ETA: 2s - loss: 0.6831 - accuracy: 0.56 - ETA: 2s - loss: 0.6824 - accuracy: 0.56 - ETA: 2s - loss: 0.6816 - accuracy: 0.56 - ETA: 1s - loss: 0.6806 - accuracy: 0.56 - ETA: 1s - loss: 0.6796 - accuracy: 0.57 - ETA: 1s - loss: 0.6790 - accuracy: 0.57 - ETA: 1s - loss: 0.6787 - accuracy: 0.57 - ETA: 1s - loss: 0.6778 - accuracy: 0.57 - ETA: 1s - loss: 0.6770 - accuracy: 0.57 - ETA: 1s - loss: 0.6762 - accuracy: 0.58 - ETA: 1s - loss: 0.6753 - accuracy: 0.58 - ETA: 1s - loss: 0.6749 - accuracy: 0.58 - ETA: 1s - loss: 0.6735 - accuracy: 0.58 - ETA: 1s - loss: 0.6735 - accuracy: 0.58 - ETA: 1s - loss: 0.6732 - accuracy: 0.58 - ETA: 1s - loss: 0.6719 - accuracy: 0.58 - ETA: 1s - loss: 0.6721 - accuracy: 0.58 - ETA: 1s - loss: 0.6716 - accuracy: 0.58 - ETA: 1s - loss: 0.6711 - accuracy: 0.59 - ETA: 1s - loss: 0.6703 - accuracy: 0.59 - ETA: 0s - loss: 0.6689 - accuracy: 0.59 - ETA: 0s - loss: 0.6685 - accuracy: 0.59 - ETA: 0s - loss: 0.6670 - accuracy: 0.59 - ETA: 0s - loss: 0.6660 - accuracy: 0.59 - ETA: 0s - loss: 0.6648 - accuracy: 0.60 - ETA: 0s - loss: 0.6637 - accuracy: 0.60 - ETA: 0s - loss: 0.6638 - accuracy: 0.60 - ETA: 0s - loss: 0.6631 - accuracy: 0.60 - ETA: 0s - loss: 0.6628 - accuracy: 0.60 - ETA: 0s - loss: 0.6625 - accuracy: 0.60 - ETA: 0s - loss: 0.6612 - accuracy: 0.60 - ETA: 0s - loss: 0.6598 - accuracy: 0.60 - ETA: 0s - loss: 0.6586 - accuracy: 0.61 - ETA: 0s - loss: 0.6571 - accuracy: 0.61 - ETA: 0s - loss: 0.6558 - accuracy: 0.61 - ETA: 0s - loss: 0.6557 - accuracy: 0.61 - ETA: 0s - loss: 0.6555 - accuracy: 0.61 - ETA: 0s - loss: 0.6549 - accuracy: 0.61 - ETA: 0s - loss: 0.6543 - accuracy: 0.61 - 3s 260us/step - loss: 0.6543 - accuracy: 0.6141 - val_loss: 0.5948 - val_accuracy: 0.7173\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 104us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:32 - loss: 0.6963 - accuracy: 0.75 - ETA: 8s - loss: 0.6929 - accuracy: 0.5000 - ETA: 5s - loss: 0.6922 - accuracy: 0.50 - ETA: 4s - loss: 0.6916 - accuracy: 0.52 - ETA: 4s - loss: 0.6921 - accuracy: 0.51 - ETA: 3s - loss: 0.6913 - accuracy: 0.52 - ETA: 3s - loss: 0.6906 - accuracy: 0.52 - ETA: 3s - loss: 0.6904 - accuracy: 0.52 - ETA: 3s - loss: 0.6896 - accuracy: 0.53 - ETA: 3s - loss: 0.6889 - accuracy: 0.53 - ETA: 3s - loss: 0.6896 - accuracy: 0.53 - ETA: 2s - loss: 0.6894 - accuracy: 0.53 - ETA: 2s - loss: 0.6886 - accuracy: 0.54 - ETA: 2s - loss: 0.6881 - accuracy: 0.54 - ETA: 2s - loss: 0.6875 - accuracy: 0.54 - ETA: 2s - loss: 0.6862 - accuracy: 0.55 - ETA: 2s - loss: 0.6855 - accuracy: 0.55 - ETA: 2s - loss: 0.6846 - accuracy: 0.56 - ETA: 2s - loss: 0.6834 - accuracy: 0.56 - ETA: 2s - loss: 0.6826 - accuracy: 0.56 - ETA: 2s - loss: 0.6816 - accuracy: 0.56 - ETA: 2s - loss: 0.6807 - accuracy: 0.56 - ETA: 2s - loss: 0.6801 - accuracy: 0.57 - ETA: 2s - loss: 0.6787 - accuracy: 0.57 - ETA: 2s - loss: 0.6782 - accuracy: 0.57 - ETA: 1s - loss: 0.6771 - accuracy: 0.58 - ETA: 1s - loss: 0.6759 - accuracy: 0.58 - ETA: 1s - loss: 0.6752 - accuracy: 0.58 - ETA: 1s - loss: 0.6742 - accuracy: 0.58 - ETA: 1s - loss: 0.6733 - accuracy: 0.58 - ETA: 1s - loss: 0.6725 - accuracy: 0.58 - ETA: 1s - loss: 0.6711 - accuracy: 0.59 - ETA: 1s - loss: 0.6693 - accuracy: 0.59 - ETA: 1s - loss: 0.6685 - accuracy: 0.59 - ETA: 1s - loss: 0.6672 - accuracy: 0.59 - ETA: 1s - loss: 0.6667 - accuracy: 0.59 - ETA: 1s - loss: 0.6662 - accuracy: 0.60 - ETA: 1s - loss: 0.6647 - accuracy: 0.60 - ETA: 1s - loss: 0.6638 - accuracy: 0.60 - ETA: 1s - loss: 0.6622 - accuracy: 0.60 - ETA: 1s - loss: 0.6618 - accuracy: 0.60 - ETA: 1s - loss: 0.6614 - accuracy: 0.60 - ETA: 0s - loss: 0.6605 - accuracy: 0.60 - ETA: 0s - loss: 0.6600 - accuracy: 0.61 - ETA: 0s - loss: 0.6590 - accuracy: 0.61 - ETA: 0s - loss: 0.6583 - accuracy: 0.61 - ETA: 0s - loss: 0.6571 - accuracy: 0.61 - ETA: 0s - loss: 0.6567 - accuracy: 0.61 - ETA: 0s - loss: 0.6562 - accuracy: 0.61 - ETA: 0s - loss: 0.6558 - accuracy: 0.61 - ETA: 0s - loss: 0.6547 - accuracy: 0.62 - ETA: 0s - loss: 0.6534 - accuracy: 0.62 - ETA: 0s - loss: 0.6533 - accuracy: 0.62 - ETA: 0s - loss: 0.6530 - accuracy: 0.62 - ETA: 0s - loss: 0.6521 - accuracy: 0.62 - ETA: 0s - loss: 0.6515 - accuracy: 0.62 - ETA: 0s - loss: 0.6502 - accuracy: 0.62 - ETA: 0s - loss: 0.6497 - accuracy: 0.62 - ETA: 0s - loss: 0.6493 - accuracy: 0.62 - ETA: 0s - loss: 0.6491 - accuracy: 0.62 - ETA: 0s - loss: 0.6487 - accuracy: 0.62 - 3s 265us/step - loss: 0.6486 - accuracy: 0.6293 - val_loss: 0.5931 - val_accuracy: 0.7180\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 6:10 - loss: 0.6988 - accuracy: 0.25 - ETA: 9s - loss: 0.6931 - accuracy: 0.5741 - ETA: 6s - loss: 0.6938 - accuracy: 0.52 - ETA: 4s - loss: 0.6934 - accuracy: 0.52 - ETA: 4s - loss: 0.6927 - accuracy: 0.53 - ETA: 3s - loss: 0.6926 - accuracy: 0.53 - ETA: 3s - loss: 0.6920 - accuracy: 0.54 - ETA: 3s - loss: 0.6918 - accuracy: 0.54 - ETA: 3s - loss: 0.6910 - accuracy: 0.55 - ETA: 3s - loss: 0.6905 - accuracy: 0.55 - ETA: 3s - loss: 0.6895 - accuracy: 0.55 - ETA: 2s - loss: 0.6890 - accuracy: 0.56 - ETA: 2s - loss: 0.6882 - accuracy: 0.56 - ETA: 2s - loss: 0.6871 - accuracy: 0.56 - ETA: 2s - loss: 0.6863 - accuracy: 0.57 - ETA: 2s - loss: 0.6851 - accuracy: 0.57 - ETA: 2s - loss: 0.6841 - accuracy: 0.58 - ETA: 2s - loss: 0.6830 - accuracy: 0.58 - ETA: 2s - loss: 0.6819 - accuracy: 0.58 - ETA: 2s - loss: 0.6808 - accuracy: 0.58 - ETA: 2s - loss: 0.6797 - accuracy: 0.59 - ETA: 2s - loss: 0.6785 - accuracy: 0.59 - ETA: 2s - loss: 0.6777 - accuracy: 0.59 - ETA: 1s - loss: 0.6763 - accuracy: 0.59 - ETA: 1s - loss: 0.6745 - accuracy: 0.60 - ETA: 1s - loss: 0.6739 - accuracy: 0.60 - ETA: 1s - loss: 0.6723 - accuracy: 0.60 - ETA: 1s - loss: 0.6704 - accuracy: 0.60 - ETA: 1s - loss: 0.6694 - accuracy: 0.60 - ETA: 1s - loss: 0.6676 - accuracy: 0.61 - ETA: 1s - loss: 0.6664 - accuracy: 0.61 - ETA: 1s - loss: 0.6655 - accuracy: 0.61 - ETA: 1s - loss: 0.6646 - accuracy: 0.61 - ETA: 1s - loss: 0.6629 - accuracy: 0.61 - ETA: 1s - loss: 0.6620 - accuracy: 0.61 - ETA: 1s - loss: 0.6613 - accuracy: 0.61 - ETA: 1s - loss: 0.6597 - accuracy: 0.62 - ETA: 1s - loss: 0.6589 - accuracy: 0.62 - ETA: 1s - loss: 0.6580 - accuracy: 0.62 - ETA: 1s - loss: 0.6565 - accuracy: 0.62 - ETA: 0s - loss: 0.6555 - accuracy: 0.62 - ETA: 0s - loss: 0.6552 - accuracy: 0.62 - ETA: 0s - loss: 0.6541 - accuracy: 0.62 - ETA: 0s - loss: 0.6541 - accuracy: 0.62 - ETA: 0s - loss: 0.6538 - accuracy: 0.63 - ETA: 0s - loss: 0.6526 - accuracy: 0.63 - ETA: 0s - loss: 0.6514 - accuracy: 0.63 - ETA: 0s - loss: 0.6506 - accuracy: 0.63 - ETA: 0s - loss: 0.6502 - accuracy: 0.63 - ETA: 0s - loss: 0.6490 - accuracy: 0.63 - ETA: 0s - loss: 0.6475 - accuracy: 0.64 - ETA: 0s - loss: 0.6476 - accuracy: 0.64 - ETA: 0s - loss: 0.6469 - accuracy: 0.64 - ETA: 0s - loss: 0.6459 - accuracy: 0.64 - ETA: 0s - loss: 0.6450 - accuracy: 0.64 - ETA: 0s - loss: 0.6445 - accuracy: 0.64 - ETA: 0s - loss: 0.6445 - accuracy: 0.64 - ETA: 0s - loss: 0.6444 - accuracy: 0.64 - ETA: 0s - loss: 0.6441 - accuracy: 0.64 - 3s 260us/step - loss: 0.6436 - accuracy: 0.6448 - val_loss: 0.5874 - val_accuracy: 0.7166\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:35 - loss: 0.6957 - accuracy: 0.0000e+ - ETA: 9s - loss: 0.6938 - accuracy: 0.5288     - ETA: 6s - loss: 0.6933 - accuracy: 0.54 - ETA: 4s - loss: 0.6930 - accuracy: 0.54 - ETA: 4s - loss: 0.6930 - accuracy: 0.53 - ETA: 4s - loss: 0.6926 - accuracy: 0.54 - ETA: 3s - loss: 0.6917 - accuracy: 0.54 - ETA: 3s - loss: 0.6917 - accuracy: 0.54 - ETA: 3s - loss: 0.6915 - accuracy: 0.54 - ETA: 3s - loss: 0.6904 - accuracy: 0.55 - ETA: 3s - loss: 0.6897 - accuracy: 0.55 - ETA: 3s - loss: 0.6896 - accuracy: 0.54 - ETA: 2s - loss: 0.6887 - accuracy: 0.55 - ETA: 2s - loss: 0.6879 - accuracy: 0.55 - ETA: 2s - loss: 0.6870 - accuracy: 0.55 - ETA: 2s - loss: 0.6863 - accuracy: 0.55 - ETA: 2s - loss: 0.6858 - accuracy: 0.55 - ETA: 2s - loss: 0.6847 - accuracy: 0.56 - ETA: 2s - loss: 0.6841 - accuracy: 0.56 - ETA: 2s - loss: 0.6836 - accuracy: 0.56 - ETA: 2s - loss: 0.6826 - accuracy: 0.56 - ETA: 2s - loss: 0.6824 - accuracy: 0.57 - ETA: 2s - loss: 0.6811 - accuracy: 0.57 - ETA: 2s - loss: 0.6798 - accuracy: 0.57 - ETA: 2s - loss: 0.6791 - accuracy: 0.57 - ETA: 2s - loss: 0.6773 - accuracy: 0.58 - ETA: 1s - loss: 0.6764 - accuracy: 0.58 - ETA: 1s - loss: 0.6760 - accuracy: 0.58 - ETA: 1s - loss: 0.6745 - accuracy: 0.58 - ETA: 1s - loss: 0.6726 - accuracy: 0.58 - ETA: 1s - loss: 0.6725 - accuracy: 0.59 - ETA: 1s - loss: 0.6711 - accuracy: 0.59 - ETA: 1s - loss: 0.6701 - accuracy: 0.59 - ETA: 1s - loss: 0.6694 - accuracy: 0.59 - ETA: 1s - loss: 0.6680 - accuracy: 0.60 - ETA: 1s - loss: 0.6667 - accuracy: 0.60 - ETA: 1s - loss: 0.6662 - accuracy: 0.60 - ETA: 1s - loss: 0.6645 - accuracy: 0.60 - ETA: 1s - loss: 0.6634 - accuracy: 0.60 - ETA: 1s - loss: 0.6617 - accuracy: 0.61 - ETA: 1s - loss: 0.6609 - accuracy: 0.61 - ETA: 1s - loss: 0.6592 - accuracy: 0.61 - ETA: 1s - loss: 0.6589 - accuracy: 0.61 - ETA: 1s - loss: 0.6582 - accuracy: 0.61 - ETA: 0s - loss: 0.6571 - accuracy: 0.61 - ETA: 0s - loss: 0.6555 - accuracy: 0.62 - ETA: 0s - loss: 0.6556 - accuracy: 0.62 - ETA: 0s - loss: 0.6549 - accuracy: 0.62 - ETA: 0s - loss: 0.6540 - accuracy: 0.62 - ETA: 0s - loss: 0.6530 - accuracy: 0.62 - ETA: 0s - loss: 0.6519 - accuracy: 0.62 - ETA: 0s - loss: 0.6513 - accuracy: 0.62 - ETA: 0s - loss: 0.6507 - accuracy: 0.62 - ETA: 0s - loss: 0.6501 - accuracy: 0.62 - ETA: 0s - loss: 0.6496 - accuracy: 0.63 - ETA: 0s - loss: 0.6489 - accuracy: 0.63 - ETA: 0s - loss: 0.6476 - accuracy: 0.63 - ETA: 0s - loss: 0.6463 - accuracy: 0.63 - ETA: 0s - loss: 0.6461 - accuracy: 0.63 - ETA: 0s - loss: 0.6463 - accuracy: 0.63 - ETA: 0s - loss: 0.6459 - accuracy: 0.63 - ETA: 0s - loss: 0.6452 - accuracy: 0.63 - ETA: 0s - loss: 0.6450 - accuracy: 0.63 - 3s 274us/step - loss: 0.6452 - accuracy: 0.6371 - val_loss: 0.5962 - val_accuracy: 0.7173\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:32 - loss: 0.6894 - accuracy: 1.00 - ETA: 8s - loss: 0.6942 - accuracy: 0.4491 - ETA: 5s - loss: 0.6939 - accuracy: 0.47 - ETA: 4s - loss: 0.6936 - accuracy: 0.49 - ETA: 4s - loss: 0.6931 - accuracy: 0.50 - ETA: 3s - loss: 0.6932 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 3s - loss: 0.6921 - accuracy: 0.51 - ETA: 3s - loss: 0.6921 - accuracy: 0.51 - ETA: 3s - loss: 0.6917 - accuracy: 0.52 - ETA: 3s - loss: 0.6913 - accuracy: 0.52 - ETA: 2s - loss: 0.6906 - accuracy: 0.53 - ETA: 2s - loss: 0.6900 - accuracy: 0.53 - ETA: 2s - loss: 0.6891 - accuracy: 0.53 - ETA: 2s - loss: 0.6887 - accuracy: 0.53 - ETA: 2s - loss: 0.6876 - accuracy: 0.54 - ETA: 2s - loss: 0.6871 - accuracy: 0.54 - ETA: 2s - loss: 0.6862 - accuracy: 0.55 - ETA: 2s - loss: 0.6855 - accuracy: 0.55 - ETA: 2s - loss: 0.6847 - accuracy: 0.56 - ETA: 2s - loss: 0.6840 - accuracy: 0.56 - ETA: 2s - loss: 0.6830 - accuracy: 0.56 - ETA: 2s - loss: 0.6821 - accuracy: 0.56 - ETA: 2s - loss: 0.6807 - accuracy: 0.57 - ETA: 1s - loss: 0.6802 - accuracy: 0.57 - ETA: 1s - loss: 0.6793 - accuracy: 0.57 - ETA: 1s - loss: 0.6787 - accuracy: 0.57 - ETA: 1s - loss: 0.6771 - accuracy: 0.58 - ETA: 1s - loss: 0.6757 - accuracy: 0.58 - ETA: 1s - loss: 0.6743 - accuracy: 0.58 - ETA: 1s - loss: 0.6728 - accuracy: 0.59 - ETA: 1s - loss: 0.6711 - accuracy: 0.59 - ETA: 1s - loss: 0.6708 - accuracy: 0.59 - ETA: 1s - loss: 0.6699 - accuracy: 0.59 - ETA: 1s - loss: 0.6690 - accuracy: 0.60 - ETA: 1s - loss: 0.6691 - accuracy: 0.60 - ETA: 1s - loss: 0.6677 - accuracy: 0.60 - ETA: 1s - loss: 0.6677 - accuracy: 0.60 - ETA: 1s - loss: 0.6668 - accuracy: 0.60 - ETA: 1s - loss: 0.6665 - accuracy: 0.60 - ETA: 1s - loss: 0.6656 - accuracy: 0.60 - ETA: 0s - loss: 0.6651 - accuracy: 0.60 - ETA: 0s - loss: 0.6638 - accuracy: 0.61 - ETA: 0s - loss: 0.6630 - accuracy: 0.61 - ETA: 0s - loss: 0.6616 - accuracy: 0.61 - ETA: 0s - loss: 0.6609 - accuracy: 0.61 - ETA: 0s - loss: 0.6603 - accuracy: 0.61 - ETA: 0s - loss: 0.6592 - accuracy: 0.61 - ETA: 0s - loss: 0.6586 - accuracy: 0.61 - ETA: 0s - loss: 0.6572 - accuracy: 0.61 - ETA: 0s - loss: 0.6561 - accuracy: 0.62 - ETA: 0s - loss: 0.6554 - accuracy: 0.62 - ETA: 0s - loss: 0.6550 - accuracy: 0.62 - ETA: 0s - loss: 0.6540 - accuracy: 0.62 - ETA: 0s - loss: 0.6530 - accuracy: 0.62 - ETA: 0s - loss: 0.6527 - accuracy: 0.62 - ETA: 0s - loss: 0.6518 - accuracy: 0.62 - ETA: 0s - loss: 0.6510 - accuracy: 0.62 - ETA: 0s - loss: 0.6507 - accuracy: 0.62 - ETA: 0s - loss: 0.6498 - accuracy: 0.62 - ETA: 0s - loss: 0.6493 - accuracy: 0.62 - 3s 266us/step - loss: 0.6487 - accuracy: 0.6306 - val_loss: 0.5923 - val_accuracy: 0.7188\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:42 - loss: 0.6940 - accuracy: 0.50 - ETA: 9s - loss: 0.6931 - accuracy: 0.5566 - ETA: 5s - loss: 0.6930 - accuracy: 0.53 - ETA: 4s - loss: 0.6935 - accuracy: 0.50 - ETA: 4s - loss: 0.6931 - accuracy: 0.51 - ETA: 3s - loss: 0.6924 - accuracy: 0.51 - ETA: 3s - loss: 0.6918 - accuracy: 0.52 - ETA: 3s - loss: 0.6910 - accuracy: 0.52 - ETA: 3s - loss: 0.6907 - accuracy: 0.52 - ETA: 3s - loss: 0.6901 - accuracy: 0.52 - ETA: 3s - loss: 0.6898 - accuracy: 0.52 - ETA: 3s - loss: 0.6898 - accuracy: 0.52 - ETA: 2s - loss: 0.6893 - accuracy: 0.53 - ETA: 2s - loss: 0.6896 - accuracy: 0.52 - ETA: 2s - loss: 0.6891 - accuracy: 0.53 - ETA: 2s - loss: 0.6882 - accuracy: 0.54 - ETA: 2s - loss: 0.6874 - accuracy: 0.54 - ETA: 2s - loss: 0.6866 - accuracy: 0.54 - ETA: 2s - loss: 0.6857 - accuracy: 0.54 - ETA: 2s - loss: 0.6861 - accuracy: 0.54 - ETA: 2s - loss: 0.6851 - accuracy: 0.55 - ETA: 2s - loss: 0.6850 - accuracy: 0.55 - ETA: 2s - loss: 0.6838 - accuracy: 0.55 - ETA: 2s - loss: 0.6826 - accuracy: 0.56 - ETA: 2s - loss: 0.6823 - accuracy: 0.56 - ETA: 2s - loss: 0.6815 - accuracy: 0.56 - ETA: 1s - loss: 0.6803 - accuracy: 0.56 - ETA: 1s - loss: 0.6797 - accuracy: 0.56 - ETA: 1s - loss: 0.6791 - accuracy: 0.56 - ETA: 1s - loss: 0.6787 - accuracy: 0.57 - ETA: 1s - loss: 0.6776 - accuracy: 0.57 - ETA: 1s - loss: 0.6764 - accuracy: 0.57 - ETA: 1s - loss: 0.6754 - accuracy: 0.57 - ETA: 1s - loss: 0.6740 - accuracy: 0.58 - ETA: 1s - loss: 0.6726 - accuracy: 0.58 - ETA: 1s - loss: 0.6723 - accuracy: 0.58 - ETA: 1s - loss: 0.6708 - accuracy: 0.58 - ETA: 1s - loss: 0.6708 - accuracy: 0.58 - ETA: 1s - loss: 0.6705 - accuracy: 0.58 - ETA: 1s - loss: 0.6700 - accuracy: 0.59 - ETA: 1s - loss: 0.6687 - accuracy: 0.59 - ETA: 1s - loss: 0.6673 - accuracy: 0.59 - ETA: 1s - loss: 0.6666 - accuracy: 0.59 - ETA: 0s - loss: 0.6660 - accuracy: 0.59 - ETA: 0s - loss: 0.6655 - accuracy: 0.59 - ETA: 0s - loss: 0.6641 - accuracy: 0.60 - ETA: 0s - loss: 0.6637 - accuracy: 0.60 - ETA: 0s - loss: 0.6633 - accuracy: 0.60 - ETA: 0s - loss: 0.6623 - accuracy: 0.60 - ETA: 0s - loss: 0.6616 - accuracy: 0.60 - ETA: 0s - loss: 0.6604 - accuracy: 0.60 - ETA: 0s - loss: 0.6597 - accuracy: 0.60 - ETA: 0s - loss: 0.6594 - accuracy: 0.60 - ETA: 0s - loss: 0.6588 - accuracy: 0.60 - ETA: 0s - loss: 0.6583 - accuracy: 0.61 - ETA: 0s - loss: 0.6578 - accuracy: 0.61 - ETA: 0s - loss: 0.6568 - accuracy: 0.61 - ETA: 0s - loss: 0.6564 - accuracy: 0.61 - ETA: 0s - loss: 0.6557 - accuracy: 0.61 - ETA: 0s - loss: 0.6553 - accuracy: 0.61 - ETA: 0s - loss: 0.6541 - accuracy: 0.61 - ETA: 0s - loss: 0.6535 - accuracy: 0.61 - 3s 271us/step - loss: 0.6534 - accuracy: 0.6195 - val_loss: 0.6004 - val_accuracy: 0.7109\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 100us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 5:32 - loss: 0.6994 - accuracy: 0.50 - ETA: 8s - loss: 0.6944 - accuracy: 0.5227 - ETA: 5s - loss: 0.6917 - accuracy: 0.57 - ETA: 4s - loss: 0.6921 - accuracy: 0.55 - ETA: 4s - loss: 0.6915 - accuracy: 0.55 - ETA: 3s - loss: 0.6899 - accuracy: 0.56 - ETA: 3s - loss: 0.6894 - accuracy: 0.56 - ETA: 3s - loss: 0.6896 - accuracy: 0.55 - ETA: 3s - loss: 0.6882 - accuracy: 0.56 - ETA: 3s - loss: 0.6870 - accuracy: 0.57 - ETA: 3s - loss: 0.6860 - accuracy: 0.58 - ETA: 2s - loss: 0.6839 - accuracy: 0.58 - ETA: 2s - loss: 0.6828 - accuracy: 0.59 - ETA: 2s - loss: 0.6808 - accuracy: 0.59 - ETA: 2s - loss: 0.6800 - accuracy: 0.59 - ETA: 2s - loss: 0.6792 - accuracy: 0.59 - ETA: 2s - loss: 0.6787 - accuracy: 0.59 - ETA: 2s - loss: 0.6774 - accuracy: 0.59 - ETA: 2s - loss: 0.6752 - accuracy: 0.60 - ETA: 2s - loss: 0.6749 - accuracy: 0.60 - ETA: 2s - loss: 0.6728 - accuracy: 0.60 - ETA: 2s - loss: 0.6708 - accuracy: 0.61 - ETA: 2s - loss: 0.6703 - accuracy: 0.61 - ETA: 2s - loss: 0.6693 - accuracy: 0.61 - ETA: 2s - loss: 0.6674 - accuracy: 0.61 - ETA: 2s - loss: 0.6661 - accuracy: 0.61 - ETA: 1s - loss: 0.6647 - accuracy: 0.61 - ETA: 1s - loss: 0.6625 - accuracy: 0.62 - ETA: 1s - loss: 0.6622 - accuracy: 0.62 - ETA: 1s - loss: 0.6603 - accuracy: 0.62 - ETA: 1s - loss: 0.6586 - accuracy: 0.62 - ETA: 1s - loss: 0.6570 - accuracy: 0.62 - ETA: 1s - loss: 0.6555 - accuracy: 0.63 - ETA: 1s - loss: 0.6536 - accuracy: 0.63 - ETA: 1s - loss: 0.6530 - accuracy: 0.63 - ETA: 1s - loss: 0.6511 - accuracy: 0.63 - ETA: 1s - loss: 0.6502 - accuracy: 0.63 - ETA: 1s - loss: 0.6486 - accuracy: 0.63 - ETA: 1s - loss: 0.6474 - accuracy: 0.64 - ETA: 1s - loss: 0.6475 - accuracy: 0.64 - ETA: 1s - loss: 0.6459 - accuracy: 0.64 - ETA: 1s - loss: 0.6457 - accuracy: 0.64 - ETA: 1s - loss: 0.6455 - accuracy: 0.64 - ETA: 0s - loss: 0.6448 - accuracy: 0.64 - ETA: 0s - loss: 0.6437 - accuracy: 0.64 - ETA: 0s - loss: 0.6430 - accuracy: 0.64 - ETA: 0s - loss: 0.6429 - accuracy: 0.64 - ETA: 0s - loss: 0.6422 - accuracy: 0.64 - ETA: 0s - loss: 0.6426 - accuracy: 0.64 - ETA: 0s - loss: 0.6420 - accuracy: 0.64 - ETA: 0s - loss: 0.6418 - accuracy: 0.64 - ETA: 0s - loss: 0.6419 - accuracy: 0.64 - ETA: 0s - loss: 0.6414 - accuracy: 0.65 - ETA: 0s - loss: 0.6398 - accuracy: 0.65 - ETA: 0s - loss: 0.6388 - accuracy: 0.65 - ETA: 0s - loss: 0.6383 - accuracy: 0.65 - ETA: 0s - loss: 0.6376 - accuracy: 0.65 - ETA: 0s - loss: 0.6361 - accuracy: 0.65 - ETA: 0s - loss: 0.6364 - accuracy: 0.65 - ETA: 0s - loss: 0.6359 - accuracy: 0.65 - ETA: 0s - loss: 0.6344 - accuracy: 0.65 - ETA: 0s - loss: 0.6343 - accuracy: 0.65 - 3s 273us/step - loss: 0.6330 - accuracy: 0.6597 - val_loss: 0.5726 - val_accuracy: 0.7223\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 100us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:37 - loss: 0.6915 - accuracy: 0.75 - ETA: 8s - loss: 0.6906 - accuracy: 0.5364 - ETA: 5s - loss: 0.6912 - accuracy: 0.52 - ETA: 4s - loss: 0.6916 - accuracy: 0.52 - ETA: 4s - loss: 0.6908 - accuracy: 0.52 - ETA: 3s - loss: 0.6898 - accuracy: 0.52 - ETA: 3s - loss: 0.6893 - accuracy: 0.53 - ETA: 3s - loss: 0.6881 - accuracy: 0.54 - ETA: 3s - loss: 0.6881 - accuracy: 0.54 - ETA: 3s - loss: 0.6864 - accuracy: 0.55 - ETA: 3s - loss: 0.6846 - accuracy: 0.56 - ETA: 2s - loss: 0.6829 - accuracy: 0.57 - ETA: 2s - loss: 0.6814 - accuracy: 0.57 - ETA: 2s - loss: 0.6807 - accuracy: 0.57 - ETA: 2s - loss: 0.6805 - accuracy: 0.58 - ETA: 2s - loss: 0.6793 - accuracy: 0.58 - ETA: 2s - loss: 0.6778 - accuracy: 0.58 - ETA: 2s - loss: 0.6761 - accuracy: 0.59 - ETA: 2s - loss: 0.6736 - accuracy: 0.59 - ETA: 2s - loss: 0.6720 - accuracy: 0.60 - ETA: 2s - loss: 0.6704 - accuracy: 0.60 - ETA: 2s - loss: 0.6694 - accuracy: 0.60 - ETA: 2s - loss: 0.6685 - accuracy: 0.60 - ETA: 2s - loss: 0.6680 - accuracy: 0.60 - ETA: 1s - loss: 0.6656 - accuracy: 0.61 - ETA: 1s - loss: 0.6628 - accuracy: 0.61 - ETA: 1s - loss: 0.6610 - accuracy: 0.62 - ETA: 1s - loss: 0.6595 - accuracy: 0.62 - ETA: 1s - loss: 0.6575 - accuracy: 0.62 - ETA: 1s - loss: 0.6566 - accuracy: 0.62 - ETA: 1s - loss: 0.6560 - accuracy: 0.62 - ETA: 1s - loss: 0.6551 - accuracy: 0.63 - ETA: 1s - loss: 0.6540 - accuracy: 0.63 - ETA: 1s - loss: 0.6534 - accuracy: 0.63 - ETA: 1s - loss: 0.6520 - accuracy: 0.63 - ETA: 1s - loss: 0.6507 - accuracy: 0.63 - ETA: 1s - loss: 0.6509 - accuracy: 0.63 - ETA: 1s - loss: 0.6506 - accuracy: 0.63 - ETA: 1s - loss: 0.6492 - accuracy: 0.63 - ETA: 1s - loss: 0.6489 - accuracy: 0.64 - ETA: 1s - loss: 0.6489 - accuracy: 0.63 - ETA: 1s - loss: 0.6474 - accuracy: 0.64 - ETA: 0s - loss: 0.6466 - accuracy: 0.64 - ETA: 0s - loss: 0.6459 - accuracy: 0.64 - ETA: 0s - loss: 0.6446 - accuracy: 0.64 - ETA: 0s - loss: 0.6441 - accuracy: 0.64 - ETA: 0s - loss: 0.6436 - accuracy: 0.64 - ETA: 0s - loss: 0.6443 - accuracy: 0.64 - ETA: 0s - loss: 0.6434 - accuracy: 0.64 - ETA: 0s - loss: 0.6425 - accuracy: 0.65 - ETA: 0s - loss: 0.6427 - accuracy: 0.65 - ETA: 0s - loss: 0.6420 - accuracy: 0.65 - ETA: 0s - loss: 0.6415 - accuracy: 0.65 - ETA: 0s - loss: 0.6410 - accuracy: 0.65 - ETA: 0s - loss: 0.6405 - accuracy: 0.65 - ETA: 0s - loss: 0.6402 - accuracy: 0.65 - ETA: 0s - loss: 0.6393 - accuracy: 0.65 - ETA: 0s - loss: 0.6386 - accuracy: 0.65 - ETA: 0s - loss: 0.6376 - accuracy: 0.65 - ETA: 0s - loss: 0.6370 - accuracy: 0.65 - ETA: 0s - loss: 0.6359 - accuracy: 0.65 - 3s 267us/step - loss: 0.6355 - accuracy: 0.6594 - val_loss: 0.5767 - val_accuracy: 0.7266\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 110us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:48 - loss: 0.6989 - accuracy: 0.50 - ETA: 9s - loss: 0.6933 - accuracy: 0.5463 - ETA: 6s - loss: 0.6936 - accuracy: 0.52 - ETA: 5s - loss: 0.6926 - accuracy: 0.52 - ETA: 4s - loss: 0.6921 - accuracy: 0.52 - ETA: 4s - loss: 0.6912 - accuracy: 0.54 - ETA: 3s - loss: 0.6899 - accuracy: 0.54 - ETA: 3s - loss: 0.6899 - accuracy: 0.54 - ETA: 3s - loss: 0.6887 - accuracy: 0.54 - ETA: 3s - loss: 0.6890 - accuracy: 0.54 - ETA: 3s - loss: 0.6880 - accuracy: 0.55 - ETA: 3s - loss: 0.6874 - accuracy: 0.55 - ETA: 2s - loss: 0.6846 - accuracy: 0.56 - ETA: 2s - loss: 0.6842 - accuracy: 0.56 - ETA: 2s - loss: 0.6831 - accuracy: 0.57 - ETA: 2s - loss: 0.6819 - accuracy: 0.57 - ETA: 2s - loss: 0.6808 - accuracy: 0.58 - ETA: 2s - loss: 0.6795 - accuracy: 0.58 - ETA: 2s - loss: 0.6783 - accuracy: 0.58 - ETA: 2s - loss: 0.6774 - accuracy: 0.59 - ETA: 2s - loss: 0.6768 - accuracy: 0.59 - ETA: 2s - loss: 0.6761 - accuracy: 0.59 - ETA: 2s - loss: 0.6747 - accuracy: 0.60 - ETA: 2s - loss: 0.6732 - accuracy: 0.60 - ETA: 1s - loss: 0.6712 - accuracy: 0.60 - ETA: 1s - loss: 0.6696 - accuracy: 0.61 - ETA: 1s - loss: 0.6678 - accuracy: 0.61 - ETA: 1s - loss: 0.6669 - accuracy: 0.61 - ETA: 1s - loss: 0.6666 - accuracy: 0.61 - ETA: 1s - loss: 0.6653 - accuracy: 0.61 - ETA: 1s - loss: 0.6642 - accuracy: 0.61 - ETA: 1s - loss: 0.6625 - accuracy: 0.62 - ETA: 1s - loss: 0.6612 - accuracy: 0.62 - ETA: 1s - loss: 0.6609 - accuracy: 0.62 - ETA: 1s - loss: 0.6593 - accuracy: 0.62 - ETA: 1s - loss: 0.6577 - accuracy: 0.62 - ETA: 1s - loss: 0.6575 - accuracy: 0.62 - ETA: 1s - loss: 0.6567 - accuracy: 0.62 - ETA: 1s - loss: 0.6554 - accuracy: 0.63 - ETA: 1s - loss: 0.6546 - accuracy: 0.63 - ETA: 1s - loss: 0.6536 - accuracy: 0.63 - ETA: 1s - loss: 0.6524 - accuracy: 0.63 - ETA: 0s - loss: 0.6509 - accuracy: 0.63 - ETA: 0s - loss: 0.6501 - accuracy: 0.63 - ETA: 0s - loss: 0.6500 - accuracy: 0.63 - ETA: 0s - loss: 0.6492 - accuracy: 0.63 - ETA: 0s - loss: 0.6477 - accuracy: 0.64 - ETA: 0s - loss: 0.6471 - accuracy: 0.64 - ETA: 0s - loss: 0.6463 - accuracy: 0.64 - ETA: 0s - loss: 0.6462 - accuracy: 0.64 - ETA: 0s - loss: 0.6455 - accuracy: 0.64 - ETA: 0s - loss: 0.6444 - accuracy: 0.64 - ETA: 0s - loss: 0.6440 - accuracy: 0.64 - ETA: 0s - loss: 0.6427 - accuracy: 0.64 - ETA: 0s - loss: 0.6417 - accuracy: 0.64 - ETA: 0s - loss: 0.6418 - accuracy: 0.64 - ETA: 0s - loss: 0.6409 - accuracy: 0.64 - ETA: 0s - loss: 0.6409 - accuracy: 0.64 - ETA: 0s - loss: 0.6409 - accuracy: 0.64 - ETA: 0s - loss: 0.6403 - accuracy: 0.65 - ETA: 0s - loss: 0.6393 - accuracy: 0.65 - 3s 266us/step - loss: 0.6393 - accuracy: 0.6513 - val_loss: 0.5791 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 108us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:26 - loss: 0.6906 - accuracy: 0.25 - ETA: 8s - loss: 0.6930 - accuracy: 0.5425 - ETA: 5s - loss: 0.6932 - accuracy: 0.51 - ETA: 4s - loss: 0.6923 - accuracy: 0.54 - ETA: 4s - loss: 0.6920 - accuracy: 0.54 - ETA: 3s - loss: 0.6918 - accuracy: 0.53 - ETA: 3s - loss: 0.6909 - accuracy: 0.54 - ETA: 3s - loss: 0.6903 - accuracy: 0.54 - ETA: 3s - loss: 0.6897 - accuracy: 0.55 - ETA: 3s - loss: 0.6891 - accuracy: 0.55 - ETA: 3s - loss: 0.6882 - accuracy: 0.56 - ETA: 3s - loss: 0.6871 - accuracy: 0.56 - ETA: 3s - loss: 0.6860 - accuracy: 0.56 - ETA: 2s - loss: 0.6852 - accuracy: 0.57 - ETA: 2s - loss: 0.6836 - accuracy: 0.57 - ETA: 2s - loss: 0.6829 - accuracy: 0.57 - ETA: 2s - loss: 0.6822 - accuracy: 0.58 - ETA: 2s - loss: 0.6806 - accuracy: 0.58 - ETA: 2s - loss: 0.6795 - accuracy: 0.59 - ETA: 2s - loss: 0.6780 - accuracy: 0.59 - ETA: 2s - loss: 0.6761 - accuracy: 0.59 - ETA: 2s - loss: 0.6754 - accuracy: 0.60 - ETA: 2s - loss: 0.6742 - accuracy: 0.60 - ETA: 2s - loss: 0.6735 - accuracy: 0.60 - ETA: 2s - loss: 0.6722 - accuracy: 0.60 - ETA: 1s - loss: 0.6706 - accuracy: 0.60 - ETA: 1s - loss: 0.6701 - accuracy: 0.60 - ETA: 1s - loss: 0.6690 - accuracy: 0.60 - ETA: 1s - loss: 0.6683 - accuracy: 0.61 - ETA: 1s - loss: 0.6682 - accuracy: 0.61 - ETA: 1s - loss: 0.6660 - accuracy: 0.61 - ETA: 1s - loss: 0.6652 - accuracy: 0.61 - ETA: 1s - loss: 0.6650 - accuracy: 0.61 - ETA: 1s - loss: 0.6631 - accuracy: 0.61 - ETA: 1s - loss: 0.6617 - accuracy: 0.62 - ETA: 1s - loss: 0.6606 - accuracy: 0.62 - ETA: 1s - loss: 0.6595 - accuracy: 0.62 - ETA: 1s - loss: 0.6584 - accuracy: 0.62 - ETA: 1s - loss: 0.6563 - accuracy: 0.62 - ETA: 1s - loss: 0.6557 - accuracy: 0.63 - ETA: 1s - loss: 0.6547 - accuracy: 0.63 - ETA: 1s - loss: 0.6535 - accuracy: 0.63 - ETA: 1s - loss: 0.6518 - accuracy: 0.63 - ETA: 1s - loss: 0.6513 - accuracy: 0.63 - ETA: 0s - loss: 0.6501 - accuracy: 0.63 - ETA: 0s - loss: 0.6493 - accuracy: 0.63 - ETA: 0s - loss: 0.6491 - accuracy: 0.63 - ETA: 0s - loss: 0.6475 - accuracy: 0.63 - ETA: 0s - loss: 0.6463 - accuracy: 0.64 - ETA: 0s - loss: 0.6458 - accuracy: 0.64 - ETA: 0s - loss: 0.6458 - accuracy: 0.64 - ETA: 0s - loss: 0.6461 - accuracy: 0.64 - ETA: 0s - loss: 0.6454 - accuracy: 0.64 - ETA: 0s - loss: 0.6455 - accuracy: 0.64 - ETA: 0s - loss: 0.6451 - accuracy: 0.64 - ETA: 0s - loss: 0.6452 - accuracy: 0.64 - ETA: 0s - loss: 0.6450 - accuracy: 0.64 - ETA: 0s - loss: 0.6441 - accuracy: 0.64 - ETA: 0s - loss: 0.6438 - accuracy: 0.64 - ETA: 0s - loss: 0.6433 - accuracy: 0.64 - ETA: 0s - loss: 0.6418 - accuracy: 0.64 - ETA: 0s - loss: 0.6413 - accuracy: 0.64 - 3s 270us/step - loss: 0.6410 - accuracy: 0.6480 - val_loss: 0.5825 - val_accuracy: 0.7124\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 107us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 5:48 - loss: 0.6940 - accuracy: 0.75 - ETA: 9s - loss: 0.6900 - accuracy: 0.5648 - ETA: 6s - loss: 0.6893 - accuracy: 0.56 - ETA: 4s - loss: 0.6884 - accuracy: 0.55 - ETA: 4s - loss: 0.6886 - accuracy: 0.55 - ETA: 3s - loss: 0.6897 - accuracy: 0.54 - ETA: 3s - loss: 0.6898 - accuracy: 0.53 - ETA: 3s - loss: 0.6898 - accuracy: 0.53 - ETA: 3s - loss: 0.6888 - accuracy: 0.54 - ETA: 3s - loss: 0.6878 - accuracy: 0.54 - ETA: 3s - loss: 0.6872 - accuracy: 0.54 - ETA: 2s - loss: 0.6871 - accuracy: 0.55 - ETA: 2s - loss: 0.6852 - accuracy: 0.55 - ETA: 2s - loss: 0.6841 - accuracy: 0.56 - ETA: 2s - loss: 0.6823 - accuracy: 0.56 - ETA: 2s - loss: 0.6804 - accuracy: 0.57 - ETA: 2s - loss: 0.6795 - accuracy: 0.57 - ETA: 2s - loss: 0.6789 - accuracy: 0.58 - ETA: 2s - loss: 0.6781 - accuracy: 0.58 - ETA: 2s - loss: 0.6764 - accuracy: 0.58 - ETA: 2s - loss: 0.6749 - accuracy: 0.58 - ETA: 2s - loss: 0.6738 - accuracy: 0.59 - ETA: 2s - loss: 0.6729 - accuracy: 0.59 - ETA: 2s - loss: 0.6719 - accuracy: 0.59 - ETA: 2s - loss: 0.6713 - accuracy: 0.59 - ETA: 1s - loss: 0.6702 - accuracy: 0.60 - ETA: 1s - loss: 0.6687 - accuracy: 0.60 - ETA: 1s - loss: 0.6676 - accuracy: 0.60 - ETA: 1s - loss: 0.6656 - accuracy: 0.60 - ETA: 1s - loss: 0.6646 - accuracy: 0.60 - ETA: 1s - loss: 0.6641 - accuracy: 0.60 - ETA: 1s - loss: 0.6632 - accuracy: 0.61 - ETA: 1s - loss: 0.6615 - accuracy: 0.61 - ETA: 1s - loss: 0.6608 - accuracy: 0.61 - ETA: 1s - loss: 0.6598 - accuracy: 0.61 - ETA: 1s - loss: 0.6592 - accuracy: 0.61 - ETA: 1s - loss: 0.6589 - accuracy: 0.61 - ETA: 1s - loss: 0.6580 - accuracy: 0.61 - ETA: 1s - loss: 0.6570 - accuracy: 0.62 - ETA: 1s - loss: 0.6558 - accuracy: 0.62 - ETA: 1s - loss: 0.6548 - accuracy: 0.62 - ETA: 1s - loss: 0.6541 - accuracy: 0.62 - ETA: 1s - loss: 0.6527 - accuracy: 0.62 - ETA: 1s - loss: 0.6519 - accuracy: 0.63 - ETA: 1s - loss: 0.6516 - accuracy: 0.62 - ETA: 0s - loss: 0.6501 - accuracy: 0.63 - ETA: 0s - loss: 0.6495 - accuracy: 0.63 - ETA: 0s - loss: 0.6483 - accuracy: 0.63 - ETA: 0s - loss: 0.6482 - accuracy: 0.63 - ETA: 0s - loss: 0.6473 - accuracy: 0.63 - ETA: 0s - loss: 0.6478 - accuracy: 0.63 - ETA: 0s - loss: 0.6470 - accuracy: 0.63 - ETA: 0s - loss: 0.6459 - accuracy: 0.63 - ETA: 0s - loss: 0.6454 - accuracy: 0.63 - ETA: 0s - loss: 0.6444 - accuracy: 0.64 - ETA: 0s - loss: 0.6435 - accuracy: 0.64 - ETA: 0s - loss: 0.6425 - accuracy: 0.64 - ETA: 0s - loss: 0.6406 - accuracy: 0.64 - ETA: 0s - loss: 0.6404 - accuracy: 0.64 - ETA: 0s - loss: 0.6393 - accuracy: 0.64 - ETA: 0s - loss: 0.6386 - accuracy: 0.64 - ETA: 0s - loss: 0.6382 - accuracy: 0.64 - ETA: 0s - loss: 0.6373 - accuracy: 0.65 - 3s 274us/step - loss: 0.6360 - accuracy: 0.6514 - val_loss: 0.5751 - val_accuracy: 0.7166\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:38 - loss: 0.7022 - accuracy: 0.50 - ETA: 9s - loss: 0.6951 - accuracy: 0.4706 - ETA: 6s - loss: 0.6939 - accuracy: 0.49 - ETA: 5s - loss: 0.6938 - accuracy: 0.50 - ETA: 4s - loss: 0.6935 - accuracy: 0.49 - ETA: 4s - loss: 0.6932 - accuracy: 0.50 - ETA: 3s - loss: 0.6924 - accuracy: 0.50 - ETA: 3s - loss: 0.6911 - accuracy: 0.51 - ETA: 3s - loss: 0.6905 - accuracy: 0.52 - ETA: 3s - loss: 0.6896 - accuracy: 0.53 - ETA: 3s - loss: 0.6881 - accuracy: 0.54 - ETA: 3s - loss: 0.6869 - accuracy: 0.55 - ETA: 2s - loss: 0.6860 - accuracy: 0.56 - ETA: 2s - loss: 0.6846 - accuracy: 0.56 - ETA: 2s - loss: 0.6835 - accuracy: 0.56 - ETA: 2s - loss: 0.6824 - accuracy: 0.57 - ETA: 2s - loss: 0.6798 - accuracy: 0.58 - ETA: 2s - loss: 0.6773 - accuracy: 0.58 - ETA: 2s - loss: 0.6767 - accuracy: 0.58 - ETA: 2s - loss: 0.6757 - accuracy: 0.58 - ETA: 2s - loss: 0.6742 - accuracy: 0.59 - ETA: 2s - loss: 0.6736 - accuracy: 0.59 - ETA: 2s - loss: 0.6720 - accuracy: 0.59 - ETA: 2s - loss: 0.6699 - accuracy: 0.60 - ETA: 2s - loss: 0.6681 - accuracy: 0.60 - ETA: 2s - loss: 0.6675 - accuracy: 0.60 - ETA: 1s - loss: 0.6657 - accuracy: 0.61 - ETA: 1s - loss: 0.6648 - accuracy: 0.61 - ETA: 1s - loss: 0.6631 - accuracy: 0.61 - ETA: 1s - loss: 0.6616 - accuracy: 0.61 - ETA: 1s - loss: 0.6610 - accuracy: 0.61 - ETA: 1s - loss: 0.6603 - accuracy: 0.61 - ETA: 1s - loss: 0.6593 - accuracy: 0.62 - ETA: 1s - loss: 0.6591 - accuracy: 0.62 - ETA: 1s - loss: 0.6579 - accuracy: 0.62 - ETA: 1s - loss: 0.6574 - accuracy: 0.62 - ETA: 1s - loss: 0.6565 - accuracy: 0.62 - ETA: 1s - loss: 0.6553 - accuracy: 0.62 - ETA: 1s - loss: 0.6551 - accuracy: 0.62 - ETA: 1s - loss: 0.6544 - accuracy: 0.63 - ETA: 1s - loss: 0.6534 - accuracy: 0.63 - ETA: 1s - loss: 0.6529 - accuracy: 0.63 - ETA: 1s - loss: 0.6523 - accuracy: 0.63 - ETA: 1s - loss: 0.6505 - accuracy: 0.63 - ETA: 0s - loss: 0.6501 - accuracy: 0.63 - ETA: 0s - loss: 0.6489 - accuracy: 0.64 - ETA: 0s - loss: 0.6479 - accuracy: 0.64 - ETA: 0s - loss: 0.6470 - accuracy: 0.64 - ETA: 0s - loss: 0.6464 - accuracy: 0.64 - ETA: 0s - loss: 0.6459 - accuracy: 0.64 - ETA: 0s - loss: 0.6449 - accuracy: 0.64 - ETA: 0s - loss: 0.6446 - accuracy: 0.64 - ETA: 0s - loss: 0.6438 - accuracy: 0.64 - ETA: 0s - loss: 0.6428 - accuracy: 0.64 - ETA: 0s - loss: 0.6422 - accuracy: 0.65 - ETA: 0s - loss: 0.6410 - accuracy: 0.65 - ETA: 0s - loss: 0.6410 - accuracy: 0.65 - ETA: 0s - loss: 0.6395 - accuracy: 0.65 - ETA: 0s - loss: 0.6393 - accuracy: 0.65 - ETA: 0s - loss: 0.6382 - accuracy: 0.65 - ETA: 0s - loss: 0.6375 - accuracy: 0.65 - ETA: 0s - loss: 0.6374 - accuracy: 0.65 - ETA: 0s - loss: 0.6365 - accuracy: 0.65 - 3s 273us/step - loss: 0.6364 - accuracy: 0.6571 - val_loss: 0.5926 - val_accuracy: 0.6861\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 104us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 4:45 - loss: 0.6905 - accuracy: 0.75 - ETA: 7s - loss: 0.6937 - accuracy: 0.4821 - ETA: 5s - loss: 0.6937 - accuracy: 0.50 - ETA: 4s - loss: 0.6938 - accuracy: 0.50 - ETA: 3s - loss: 0.6937 - accuracy: 0.50 - ETA: 3s - loss: 0.6938 - accuracy: 0.49 - ETA: 3s - loss: 0.6938 - accuracy: 0.49 - ETA: 3s - loss: 0.6938 - accuracy: 0.50 - ETA: 3s - loss: 0.6936 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 2s - loss: 0.6937 - accuracy: 0.50 - ETA: 2s - loss: 0.6937 - accuracy: 0.50 - ETA: 2s - loss: 0.6936 - accuracy: 0.50 - ETA: 2s - loss: 0.6937 - accuracy: 0.50 - ETA: 2s - loss: 0.6936 - accuracy: 0.50 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.50 - ETA: 2s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6934 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6934 - accuracy: 0.50 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.50 - ETA: 1s - loss: 0.6932 - accuracy: 0.50 - ETA: 1s - loss: 0.6931 - accuracy: 0.50 - ETA: 1s - loss: 0.6932 - accuracy: 0.50 - ETA: 1s - loss: 0.6931 - accuracy: 0.50 - ETA: 1s - loss: 0.6930 - accuracy: 0.50 - ETA: 1s - loss: 0.6930 - accuracy: 0.50 - ETA: 1s - loss: 0.6930 - accuracy: 0.50 - ETA: 1s - loss: 0.6930 - accuracy: 0.50 - ETA: 0s - loss: 0.6928 - accuracy: 0.50 - ETA: 0s - loss: 0.6927 - accuracy: 0.50 - ETA: 0s - loss: 0.6926 - accuracy: 0.50 - ETA: 0s - loss: 0.6926 - accuracy: 0.50 - ETA: 0s - loss: 0.6924 - accuracy: 0.50 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - ETA: 0s - loss: 0.6918 - accuracy: 0.51 - ETA: 0s - loss: 0.6919 - accuracy: 0.50 - ETA: 0s - loss: 0.6917 - accuracy: 0.51 - ETA: 0s - loss: 0.6917 - accuracy: 0.51 - ETA: 0s - loss: 0.6916 - accuracy: 0.51 - ETA: 0s - loss: 0.6916 - accuracy: 0.51 - ETA: 0s - loss: 0.6914 - accuracy: 0.51 - ETA: 0s - loss: 0.6913 - accuracy: 0.51 - ETA: 0s - loss: 0.6912 - accuracy: 0.51 - ETA: 0s - loss: 0.6910 - accuracy: 0.51 - ETA: 0s - loss: 0.6908 - accuracy: 0.51 - 3s 255us/step - loss: 0.6908 - accuracy: 0.5122 - val_loss: 0.6792 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 110us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 4:48 - loss: 0.6936 - accuracy: 0.50 - ETA: 7s - loss: 0.6942 - accuracy: 0.4649 - ETA: 5s - loss: 0.6943 - accuracy: 0.46 - ETA: 4s - loss: 0.6940 - accuracy: 0.47 - ETA: 3s - loss: 0.6939 - accuracy: 0.48 - ETA: 3s - loss: 0.6937 - accuracy: 0.49 - ETA: 3s - loss: 0.6934 - accuracy: 0.50 - ETA: 3s - loss: 0.6932 - accuracy: 0.50 - ETA: 3s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.50 - ETA: 2s - loss: 0.6934 - accuracy: 0.50 - ETA: 2s - loss: 0.6934 - accuracy: 0.50 - ETA: 2s - loss: 0.6934 - accuracy: 0.50 - ETA: 2s - loss: 0.6933 - accuracy: 0.50 - ETA: 2s - loss: 0.6934 - accuracy: 0.50 - ETA: 2s - loss: 0.6934 - accuracy: 0.50 - ETA: 2s - loss: 0.6934 - accuracy: 0.50 - ETA: 2s - loss: 0.6934 - accuracy: 0.50 - ETA: 2s - loss: 0.6934 - accuracy: 0.50 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6929 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6929 - accuracy: 0.52 - ETA: 1s - loss: 0.6929 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6927 - accuracy: 0.53 - ETA: 1s - loss: 0.6926 - accuracy: 0.53 - ETA: 1s - loss: 0.6926 - accuracy: 0.53 - ETA: 1s - loss: 0.6925 - accuracy: 0.53 - ETA: 1s - loss: 0.6923 - accuracy: 0.53 - ETA: 0s - loss: 0.6921 - accuracy: 0.53 - ETA: 0s - loss: 0.6920 - accuracy: 0.53 - ETA: 0s - loss: 0.6918 - accuracy: 0.53 - ETA: 0s - loss: 0.6916 - accuracy: 0.54 - ETA: 0s - loss: 0.6915 - accuracy: 0.54 - ETA: 0s - loss: 0.6914 - accuracy: 0.54 - ETA: 0s - loss: 0.6912 - accuracy: 0.54 - ETA: 0s - loss: 0.6910 - accuracy: 0.54 - ETA: 0s - loss: 0.6907 - accuracy: 0.54 - ETA: 0s - loss: 0.6908 - accuracy: 0.54 - ETA: 0s - loss: 0.6906 - accuracy: 0.54 - ETA: 0s - loss: 0.6905 - accuracy: 0.54 - ETA: 0s - loss: 0.6904 - accuracy: 0.54 - ETA: 0s - loss: 0.6904 - accuracy: 0.54 - ETA: 0s - loss: 0.6903 - accuracy: 0.54 - ETA: 0s - loss: 0.6901 - accuracy: 0.55 - ETA: 0s - loss: 0.6897 - accuracy: 0.55 - ETA: 0s - loss: 0.6896 - accuracy: 0.55 - ETA: 0s - loss: 0.6893 - accuracy: 0.55 - 3s 254us/step - loss: 0.6892 - accuracy: 0.5543 - val_loss: 0.6745 - val_accuracy: 0.6271\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:04 - loss: 0.6926 - accuracy: 0.25 - ETA: 8s - loss: 0.6933 - accuracy: 0.5370 - ETA: 5s - loss: 0.6931 - accuracy: 0.52 - ETA: 4s - loss: 0.6933 - accuracy: 0.51 - ETA: 4s - loss: 0.6933 - accuracy: 0.52 - ETA: 3s - loss: 0.6926 - accuracy: 0.53 - ETA: 3s - loss: 0.6926 - accuracy: 0.53 - ETA: 3s - loss: 0.6924 - accuracy: 0.53 - ETA: 3s - loss: 0.6922 - accuracy: 0.53 - ETA: 3s - loss: 0.6925 - accuracy: 0.53 - ETA: 2s - loss: 0.6923 - accuracy: 0.53 - ETA: 2s - loss: 0.6923 - accuracy: 0.52 - ETA: 2s - loss: 0.6921 - accuracy: 0.53 - ETA: 2s - loss: 0.6920 - accuracy: 0.53 - ETA: 2s - loss: 0.6920 - accuracy: 0.53 - ETA: 2s - loss: 0.6918 - accuracy: 0.53 - ETA: 2s - loss: 0.6919 - accuracy: 0.52 - ETA: 2s - loss: 0.6921 - accuracy: 0.52 - ETA: 2s - loss: 0.6921 - accuracy: 0.52 - ETA: 2s - loss: 0.6922 - accuracy: 0.52 - ETA: 2s - loss: 0.6923 - accuracy: 0.52 - ETA: 2s - loss: 0.6923 - accuracy: 0.52 - ETA: 2s - loss: 0.6922 - accuracy: 0.52 - ETA: 2s - loss: 0.6922 - accuracy: 0.52 - ETA: 2s - loss: 0.6919 - accuracy: 0.53 - ETA: 1s - loss: 0.6917 - accuracy: 0.53 - ETA: 1s - loss: 0.6917 - accuracy: 0.53 - ETA: 1s - loss: 0.6917 - accuracy: 0.53 - ETA: 1s - loss: 0.6916 - accuracy: 0.53 - ETA: 1s - loss: 0.6916 - accuracy: 0.53 - ETA: 1s - loss: 0.6915 - accuracy: 0.53 - ETA: 1s - loss: 0.6914 - accuracy: 0.53 - ETA: 1s - loss: 0.6913 - accuracy: 0.53 - ETA: 1s - loss: 0.6911 - accuracy: 0.53 - ETA: 1s - loss: 0.6909 - accuracy: 0.54 - ETA: 1s - loss: 0.6907 - accuracy: 0.54 - ETA: 1s - loss: 0.6904 - accuracy: 0.54 - ETA: 1s - loss: 0.6904 - accuracy: 0.54 - ETA: 1s - loss: 0.6905 - accuracy: 0.54 - ETA: 1s - loss: 0.6901 - accuracy: 0.54 - ETA: 1s - loss: 0.6899 - accuracy: 0.54 - ETA: 1s - loss: 0.6899 - accuracy: 0.54 - ETA: 0s - loss: 0.6897 - accuracy: 0.54 - ETA: 0s - loss: 0.6895 - accuracy: 0.54 - ETA: 0s - loss: 0.6893 - accuracy: 0.54 - ETA: 0s - loss: 0.6891 - accuracy: 0.54 - ETA: 0s - loss: 0.6888 - accuracy: 0.54 - ETA: 0s - loss: 0.6886 - accuracy: 0.54 - ETA: 0s - loss: 0.6884 - accuracy: 0.54 - ETA: 0s - loss: 0.6881 - accuracy: 0.54 - ETA: 0s - loss: 0.6878 - accuracy: 0.55 - ETA: 0s - loss: 0.6876 - accuracy: 0.55 - ETA: 0s - loss: 0.6871 - accuracy: 0.55 - ETA: 0s - loss: 0.6869 - accuracy: 0.55 - ETA: 0s - loss: 0.6865 - accuracy: 0.55 - ETA: 0s - loss: 0.6864 - accuracy: 0.55 - ETA: 0s - loss: 0.6860 - accuracy: 0.55 - ETA: 0s - loss: 0.6856 - accuracy: 0.55 - ETA: 0s - loss: 0.6852 - accuracy: 0.55 - 3s 259us/step - loss: 0.6848 - accuracy: 0.5558 - val_loss: 0.6560 - val_accuracy: 0.7017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 99us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:00 - loss: 0.6967 - accuracy: 0.25 - ETA: 8s - loss: 0.6937 - accuracy: 0.4955 - ETA: 5s - loss: 0.6938 - accuracy: 0.50 - ETA: 4s - loss: 0.6938 - accuracy: 0.50 - ETA: 3s - loss: 0.6937 - accuracy: 0.51 - ETA: 3s - loss: 0.6938 - accuracy: 0.50 - ETA: 3s - loss: 0.6937 - accuracy: 0.51 - ETA: 3s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.52 - ETA: 2s - loss: 0.6934 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6931 - accuracy: 0.51 - ETA: 2s - loss: 0.6928 - accuracy: 0.52 - ETA: 2s - loss: 0.6929 - accuracy: 0.51 - ETA: 2s - loss: 0.6931 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6929 - accuracy: 0.51 - ETA: 2s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6924 - accuracy: 0.51 - ETA: 1s - loss: 0.6924 - accuracy: 0.51 - ETA: 1s - loss: 0.6922 - accuracy: 0.51 - ETA: 1s - loss: 0.6922 - accuracy: 0.51 - ETA: 1s - loss: 0.6919 - accuracy: 0.51 - ETA: 1s - loss: 0.6917 - accuracy: 0.51 - ETA: 1s - loss: 0.6917 - accuracy: 0.51 - ETA: 1s - loss: 0.6917 - accuracy: 0.51 - ETA: 1s - loss: 0.6914 - accuracy: 0.51 - ETA: 1s - loss: 0.6912 - accuracy: 0.51 - ETA: 1s - loss: 0.6910 - accuracy: 0.52 - ETA: 1s - loss: 0.6911 - accuracy: 0.51 - ETA: 1s - loss: 0.6910 - accuracy: 0.51 - ETA: 1s - loss: 0.6909 - accuracy: 0.52 - ETA: 1s - loss: 0.6907 - accuracy: 0.52 - ETA: 1s - loss: 0.6905 - accuracy: 0.52 - ETA: 1s - loss: 0.6903 - accuracy: 0.52 - ETA: 0s - loss: 0.6903 - accuracy: 0.52 - ETA: 0s - loss: 0.6900 - accuracy: 0.52 - ETA: 0s - loss: 0.6901 - accuracy: 0.52 - ETA: 0s - loss: 0.6896 - accuracy: 0.52 - ETA: 0s - loss: 0.6894 - accuracy: 0.52 - ETA: 0s - loss: 0.6891 - accuracy: 0.52 - ETA: 0s - loss: 0.6887 - accuracy: 0.52 - ETA: 0s - loss: 0.6882 - accuracy: 0.52 - ETA: 0s - loss: 0.6882 - accuracy: 0.52 - ETA: 0s - loss: 0.6882 - accuracy: 0.52 - ETA: 0s - loss: 0.6881 - accuracy: 0.52 - ETA: 0s - loss: 0.6877 - accuracy: 0.53 - ETA: 0s - loss: 0.6875 - accuracy: 0.53 - ETA: 0s - loss: 0.6873 - accuracy: 0.53 - ETA: 0s - loss: 0.6872 - accuracy: 0.53 - ETA: 0s - loss: 0.6868 - accuracy: 0.53 - ETA: 0s - loss: 0.6866 - accuracy: 0.53 - ETA: 0s - loss: 0.6863 - accuracy: 0.53 - ETA: 0s - loss: 0.6860 - accuracy: 0.53 - 3s 250us/step - loss: 0.6857 - accuracy: 0.5379 - val_loss: 0.6611 - val_accuracy: 0.6726\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 110us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:57 - loss: 0.6926 - accuracy: 0.75 - ETA: 9s - loss: 0.6939 - accuracy: 0.5150 - ETA: 5s - loss: 0.6938 - accuracy: 0.51 - ETA: 4s - loss: 0.6938 - accuracy: 0.50 - ETA: 4s - loss: 0.6939 - accuracy: 0.50 - ETA: 3s - loss: 0.6938 - accuracy: 0.51 - ETA: 3s - loss: 0.6938 - accuracy: 0.51 - ETA: 3s - loss: 0.6933 - accuracy: 0.52 - ETA: 3s - loss: 0.6934 - accuracy: 0.52 - ETA: 3s - loss: 0.6934 - accuracy: 0.51 - ETA: 3s - loss: 0.6933 - accuracy: 0.52 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.51 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6919 - accuracy: 0.53 - ETA: 0s - loss: 0.6918 - accuracy: 0.53 - ETA: 0s - loss: 0.6917 - accuracy: 0.53 - ETA: 0s - loss: 0.6916 - accuracy: 0.53 - 3s 252us/step - loss: 0.6917 - accuracy: 0.5330 - val_loss: 0.6824 - val_accuracy: 0.6115\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 104us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:54 - loss: 0.6920 - accuracy: 0.75 - ETA: 8s - loss: 0.6930 - accuracy: 0.5591 - ETA: 5s - loss: 0.6933 - accuracy: 0.53 - ETA: 4s - loss: 0.6939 - accuracy: 0.50 - ETA: 3s - loss: 0.6938 - accuracy: 0.50 - ETA: 3s - loss: 0.6937 - accuracy: 0.51 - ETA: 3s - loss: 0.6937 - accuracy: 0.51 - ETA: 3s - loss: 0.6937 - accuracy: 0.51 - ETA: 3s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6937 - accuracy: 0.50 - ETA: 2s - loss: 0.6937 - accuracy: 0.50 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6937 - accuracy: 0.50 - ETA: 2s - loss: 0.6936 - accuracy: 0.50 - ETA: 2s - loss: 0.6936 - accuracy: 0.50 - ETA: 2s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6934 - accuracy: 0.50 - ETA: 1s - loss: 0.6933 - accuracy: 0.50 - ETA: 1s - loss: 0.6933 - accuracy: 0.50 - ETA: 1s - loss: 0.6931 - accuracy: 0.50 - ETA: 1s - loss: 0.6930 - accuracy: 0.50 - ETA: 1s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6930 - accuracy: 0.50 - ETA: 0s - loss: 0.6930 - accuracy: 0.50 - ETA: 0s - loss: 0.6928 - accuracy: 0.50 - ETA: 0s - loss: 0.6928 - accuracy: 0.50 - ETA: 0s - loss: 0.6927 - accuracy: 0.50 - ETA: 0s - loss: 0.6926 - accuracy: 0.50 - ETA: 0s - loss: 0.6925 - accuracy: 0.50 - ETA: 0s - loss: 0.6925 - accuracy: 0.50 - ETA: 0s - loss: 0.6924 - accuracy: 0.50 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6919 - accuracy: 0.51 - ETA: 0s - loss: 0.6918 - accuracy: 0.51 - ETA: 0s - loss: 0.6916 - accuracy: 0.51 - ETA: 0s - loss: 0.6916 - accuracy: 0.51 - ETA: 0s - loss: 0.6914 - accuracy: 0.51 - 3s 256us/step - loss: 0.6913 - accuracy: 0.5126 - val_loss: 0.6824 - val_accuracy: 0.5376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 4:51 - loss: 0.6981 - accuracy: 0.50 - ETA: 7s - loss: 0.6952 - accuracy: 0.4353 - ETA: 5s - loss: 0.6945 - accuracy: 0.46 - ETA: 4s - loss: 0.6945 - accuracy: 0.47 - ETA: 3s - loss: 0.6946 - accuracy: 0.48 - ETA: 3s - loss: 0.6947 - accuracy: 0.48 - ETA: 3s - loss: 0.6949 - accuracy: 0.48 - ETA: 3s - loss: 0.6948 - accuracy: 0.48 - ETA: 3s - loss: 0.6947 - accuracy: 0.48 - ETA: 3s - loss: 0.6949 - accuracy: 0.47 - ETA: 3s - loss: 0.6948 - accuracy: 0.47 - ETA: 2s - loss: 0.6948 - accuracy: 0.47 - ETA: 2s - loss: 0.6948 - accuracy: 0.47 - ETA: 2s - loss: 0.6947 - accuracy: 0.47 - ETA: 2s - loss: 0.6948 - accuracy: 0.47 - ETA: 2s - loss: 0.6947 - accuracy: 0.47 - ETA: 2s - loss: 0.6947 - accuracy: 0.47 - ETA: 2s - loss: 0.6947 - accuracy: 0.47 - ETA: 2s - loss: 0.6946 - accuracy: 0.48 - ETA: 2s - loss: 0.6945 - accuracy: 0.48 - ETA: 2s - loss: 0.6945 - accuracy: 0.48 - ETA: 2s - loss: 0.6944 - accuracy: 0.48 - ETA: 1s - loss: 0.6943 - accuracy: 0.48 - ETA: 1s - loss: 0.6944 - accuracy: 0.48 - ETA: 1s - loss: 0.6943 - accuracy: 0.49 - ETA: 1s - loss: 0.6943 - accuracy: 0.49 - ETA: 1s - loss: 0.6941 - accuracy: 0.49 - ETA: 1s - loss: 0.6942 - accuracy: 0.49 - ETA: 1s - loss: 0.6942 - accuracy: 0.49 - ETA: 1s - loss: 0.6942 - accuracy: 0.49 - ETA: 1s - loss: 0.6942 - accuracy: 0.49 - ETA: 1s - loss: 0.6942 - accuracy: 0.49 - ETA: 1s - loss: 0.6941 - accuracy: 0.49 - ETA: 1s - loss: 0.6940 - accuracy: 0.50 - ETA: 1s - loss: 0.6939 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - 3s 251us/step - loss: 0.6935 - accuracy: 0.5066 - val_loss: 0.6902 - val_accuracy: 0.5369\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 110us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:19 - loss: 0.6967 - accuracy: 0.25 - ETA: 8s - loss: 0.6948 - accuracy: 0.4387 - ETA: 6s - loss: 0.6946 - accuracy: 0.45 - ETA: 5s - loss: 0.6942 - accuracy: 0.48 - ETA: 4s - loss: 0.6939 - accuracy: 0.49 - ETA: 4s - loss: 0.6944 - accuracy: 0.49 - ETA: 3s - loss: 0.6943 - accuracy: 0.49 - ETA: 3s - loss: 0.6945 - accuracy: 0.49 - ETA: 3s - loss: 0.6944 - accuracy: 0.50 - ETA: 3s - loss: 0.6943 - accuracy: 0.50 - ETA: 3s - loss: 0.6942 - accuracy: 0.50 - ETA: 3s - loss: 0.6940 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6940 - accuracy: 0.50 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6931 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - ETA: 0s - loss: 0.6919 - accuracy: 0.51 - ETA: 0s - loss: 0.6916 - accuracy: 0.52 - ETA: 0s - loss: 0.6916 - accuracy: 0.52 - ETA: 0s - loss: 0.6914 - accuracy: 0.52 - ETA: 0s - loss: 0.6914 - accuracy: 0.52 - ETA: 0s - loss: 0.6913 - accuracy: 0.52 - ETA: 0s - loss: 0.6912 - accuracy: 0.52 - 3s 261us/step - loss: 0.6911 - accuracy: 0.5234 - val_loss: 0.6784 - val_accuracy: 0.5703\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 108us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 4:51 - loss: 0.6939 - accuracy: 0.75 - ETA: 7s - loss: 0.6942 - accuracy: 0.5179 - ETA: 5s - loss: 0.6936 - accuracy: 0.54 - ETA: 4s - loss: 0.6941 - accuracy: 0.51 - ETA: 3s - loss: 0.6943 - accuracy: 0.51 - ETA: 3s - loss: 0.6943 - accuracy: 0.50 - ETA: 3s - loss: 0.6944 - accuracy: 0.50 - ETA: 3s - loss: 0.6944 - accuracy: 0.50 - ETA: 2s - loss: 0.6944 - accuracy: 0.50 - ETA: 2s - loss: 0.6943 - accuracy: 0.50 - ETA: 2s - loss: 0.6943 - accuracy: 0.51 - ETA: 2s - loss: 0.6942 - accuracy: 0.51 - ETA: 2s - loss: 0.6942 - accuracy: 0.51 - ETA: 2s - loss: 0.6942 - accuracy: 0.51 - ETA: 2s - loss: 0.6940 - accuracy: 0.52 - ETA: 2s - loss: 0.6941 - accuracy: 0.51 - ETA: 2s - loss: 0.6940 - accuracy: 0.51 - ETA: 2s - loss: 0.6939 - accuracy: 0.52 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.52 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.52 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6922 - accuracy: 0.52 - ETA: 1s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6918 - accuracy: 0.52 - ETA: 0s - loss: 0.6919 - accuracy: 0.52 - ETA: 0s - loss: 0.6917 - accuracy: 0.52 - ETA: 0s - loss: 0.6916 - accuracy: 0.53 - ETA: 0s - loss: 0.6913 - accuracy: 0.53 - ETA: 0s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6909 - accuracy: 0.53 - ETA: 0s - loss: 0.6909 - accuracy: 0.53 - ETA: 0s - loss: 0.6907 - accuracy: 0.53 - ETA: 0s - loss: 0.6905 - accuracy: 0.53 - ETA: 0s - loss: 0.6903 - accuracy: 0.53 - ETA: 0s - loss: 0.6900 - accuracy: 0.53 - ETA: 0s - loss: 0.6899 - accuracy: 0.53 - ETA: 0s - loss: 0.6898 - accuracy: 0.53 - ETA: 0s - loss: 0.6895 - accuracy: 0.54 - ETA: 0s - loss: 0.6892 - accuracy: 0.54 - ETA: 0s - loss: 0.6890 - accuracy: 0.54 - ETA: 0s - loss: 0.6887 - accuracy: 0.54 - 3s 256us/step - loss: 0.6886 - accuracy: 0.5452 - val_loss: 0.6662 - val_accuracy: 0.7010\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 122us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:48 - loss: 0.7008 - accuracy: 0.0000e+ - ETA: 7s - loss: 0.6946 - accuracy: 0.4732     - ETA: 5s - loss: 0.6943 - accuracy: 0.50 - ETA: 4s - loss: 0.6942 - accuracy: 0.50 - ETA: 3s - loss: 0.6943 - accuracy: 0.50 - ETA: 3s - loss: 0.6944 - accuracy: 0.49 - ETA: 3s - loss: 0.6942 - accuracy: 0.51 - ETA: 3s - loss: 0.6939 - accuracy: 0.51 - ETA: 3s - loss: 0.6940 - accuracy: 0.51 - ETA: 2s - loss: 0.6940 - accuracy: 0.50 - ETA: 2s - loss: 0.6940 - accuracy: 0.50 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6939 - accuracy: 0.50 - ETA: 2s - loss: 0.6939 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 2s - loss: 0.6937 - accuracy: 0.50 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6924 - accuracy: 0.51 - ETA: 1s - loss: 0.6924 - accuracy: 0.51 - ETA: 1s - loss: 0.6918 - accuracy: 0.51 - ETA: 1s - loss: 0.6917 - accuracy: 0.51 - ETA: 1s - loss: 0.6914 - accuracy: 0.51 - ETA: 1s - loss: 0.6914 - accuracy: 0.51 - ETA: 1s - loss: 0.6913 - accuracy: 0.51 - ETA: 1s - loss: 0.6910 - accuracy: 0.51 - ETA: 1s - loss: 0.6906 - accuracy: 0.51 - ETA: 1s - loss: 0.6906 - accuracy: 0.52 - ETA: 1s - loss: 0.6905 - accuracy: 0.52 - ETA: 1s - loss: 0.6901 - accuracy: 0.52 - ETA: 1s - loss: 0.6897 - accuracy: 0.52 - ETA: 0s - loss: 0.6896 - accuracy: 0.52 - ETA: 0s - loss: 0.6894 - accuracy: 0.52 - ETA: 0s - loss: 0.6891 - accuracy: 0.52 - ETA: 0s - loss: 0.6889 - accuracy: 0.52 - ETA: 0s - loss: 0.6887 - accuracy: 0.53 - ETA: 0s - loss: 0.6884 - accuracy: 0.53 - ETA: 0s - loss: 0.6882 - accuracy: 0.53 - ETA: 0s - loss: 0.6880 - accuracy: 0.53 - ETA: 0s - loss: 0.6877 - accuracy: 0.53 - ETA: 0s - loss: 0.6874 - accuracy: 0.53 - ETA: 0s - loss: 0.6871 - accuracy: 0.53 - ETA: 0s - loss: 0.6865 - accuracy: 0.54 - ETA: 0s - loss: 0.6863 - accuracy: 0.54 - ETA: 0s - loss: 0.6860 - accuracy: 0.54 - ETA: 0s - loss: 0.6856 - accuracy: 0.54 - ETA: 0s - loss: 0.6851 - accuracy: 0.54 - ETA: 0s - loss: 0.6848 - accuracy: 0.54 - ETA: 0s - loss: 0.6845 - accuracy: 0.54 - ETA: 0s - loss: 0.6840 - accuracy: 0.55 - 3s 257us/step - loss: 0.6838 - accuracy: 0.5512 - val_loss: 0.6514 - val_accuracy: 0.6804\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 108us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:45 - loss: 0.6964 - accuracy: 0.0000e+ - ETA: 7s - loss: 0.6944 - accuracy: 0.4777     - ETA: 5s - loss: 0.6946 - accuracy: 0.48 - ETA: 4s - loss: 0.6948 - accuracy: 0.46 - ETA: 3s - loss: 0.6948 - accuracy: 0.47 - ETA: 3s - loss: 0.6947 - accuracy: 0.48 - ETA: 3s - loss: 0.6946 - accuracy: 0.48 - ETA: 3s - loss: 0.6945 - accuracy: 0.49 - ETA: 2s - loss: 0.6946 - accuracy: 0.49 - ETA: 2s - loss: 0.6944 - accuracy: 0.50 - ETA: 2s - loss: 0.6943 - accuracy: 0.50 - ETA: 2s - loss: 0.6942 - accuracy: 0.50 - ETA: 2s - loss: 0.6940 - accuracy: 0.50 - ETA: 2s - loss: 0.6942 - accuracy: 0.50 - ETA: 2s - loss: 0.6942 - accuracy: 0.50 - ETA: 2s - loss: 0.6943 - accuracy: 0.50 - ETA: 2s - loss: 0.6943 - accuracy: 0.50 - ETA: 2s - loss: 0.6943 - accuracy: 0.50 - ETA: 2s - loss: 0.6941 - accuracy: 0.50 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.53 - ETA: 0s - loss: 0.6922 - accuracy: 0.53 - ETA: 0s - loss: 0.6921 - accuracy: 0.53 - ETA: 0s - loss: 0.6921 - accuracy: 0.53 - 3s 248us/step - loss: 0.6920 - accuracy: 0.5341 - val_loss: 0.6845 - val_accuracy: 0.6868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:48 - loss: 0.6941 - accuracy: 0.50 - ETA: 7s - loss: 0.6941 - accuracy: 0.5395 - ETA: 5s - loss: 0.6943 - accuracy: 0.51 - ETA: 4s - loss: 0.6943 - accuracy: 0.51 - ETA: 3s - loss: 0.6944 - accuracy: 0.50 - ETA: 3s - loss: 0.6942 - accuracy: 0.51 - ETA: 3s - loss: 0.6942 - accuracy: 0.51 - ETA: 3s - loss: 0.6942 - accuracy: 0.51 - ETA: 3s - loss: 0.6937 - accuracy: 0.52 - ETA: 2s - loss: 0.6940 - accuracy: 0.51 - ETA: 2s - loss: 0.6940 - accuracy: 0.51 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 2s - loss: 0.6937 - accuracy: 0.52 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.52 - ETA: 2s - loss: 0.6934 - accuracy: 0.52 - ETA: 2s - loss: 0.6936 - accuracy: 0.52 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.52 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - 3s 255us/step - loss: 0.6933 - accuracy: 0.5156 - val_loss: 0.6895 - val_accuracy: 0.5739\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 4:48 - loss: 0.6882 - accuracy: 0.75 - ETA: 8s - loss: 0.6943 - accuracy: 0.4630 - ETA: 5s - loss: 0.6936 - accuracy: 0.50 - ETA: 4s - loss: 0.6943 - accuracy: 0.50 - ETA: 3s - loss: 0.6943 - accuracy: 0.50 - ETA: 3s - loss: 0.6939 - accuracy: 0.50 - ETA: 3s - loss: 0.6932 - accuracy: 0.51 - ETA: 3s - loss: 0.6935 - accuracy: 0.51 - ETA: 3s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6929 - accuracy: 0.51 - ETA: 2s - loss: 0.6928 - accuracy: 0.52 - ETA: 2s - loss: 0.6925 - accuracy: 0.52 - ETA: 2s - loss: 0.6923 - accuracy: 0.52 - ETA: 2s - loss: 0.6920 - accuracy: 0.52 - ETA: 2s - loss: 0.6921 - accuracy: 0.52 - ETA: 2s - loss: 0.6922 - accuracy: 0.52 - ETA: 2s - loss: 0.6920 - accuracy: 0.52 - ETA: 2s - loss: 0.6920 - accuracy: 0.52 - ETA: 2s - loss: 0.6920 - accuracy: 0.52 - ETA: 2s - loss: 0.6920 - accuracy: 0.52 - ETA: 2s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6916 - accuracy: 0.53 - ETA: 1s - loss: 0.6914 - accuracy: 0.53 - ETA: 1s - loss: 0.6913 - accuracy: 0.53 - ETA: 1s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6909 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 1s - loss: 0.6903 - accuracy: 0.54 - ETA: 1s - loss: 0.6903 - accuracy: 0.54 - ETA: 1s - loss: 0.6900 - accuracy: 0.54 - ETA: 1s - loss: 0.6897 - accuracy: 0.54 - ETA: 1s - loss: 0.6895 - accuracy: 0.54 - ETA: 1s - loss: 0.6891 - accuracy: 0.54 - ETA: 1s - loss: 0.6887 - accuracy: 0.54 - ETA: 1s - loss: 0.6885 - accuracy: 0.55 - ETA: 1s - loss: 0.6882 - accuracy: 0.55 - ETA: 1s - loss: 0.6877 - accuracy: 0.55 - ETA: 1s - loss: 0.6874 - accuracy: 0.55 - ETA: 1s - loss: 0.6871 - accuracy: 0.55 - ETA: 1s - loss: 0.6865 - accuracy: 0.55 - ETA: 0s - loss: 0.6860 - accuracy: 0.56 - ETA: 0s - loss: 0.6859 - accuracy: 0.56 - ETA: 0s - loss: 0.6854 - accuracy: 0.56 - ETA: 0s - loss: 0.6847 - accuracy: 0.56 - ETA: 0s - loss: 0.6843 - accuracy: 0.56 - ETA: 0s - loss: 0.6839 - accuracy: 0.56 - ETA: 0s - loss: 0.6830 - accuracy: 0.57 - ETA: 0s - loss: 0.6825 - accuracy: 0.57 - ETA: 0s - loss: 0.6816 - accuracy: 0.57 - ETA: 0s - loss: 0.6811 - accuracy: 0.57 - ETA: 0s - loss: 0.6805 - accuracy: 0.57 - ETA: 0s - loss: 0.6800 - accuracy: 0.57 - ETA: 0s - loss: 0.6798 - accuracy: 0.57 - ETA: 0s - loss: 0.6794 - accuracy: 0.57 - ETA: 0s - loss: 0.6791 - accuracy: 0.57 - ETA: 0s - loss: 0.6786 - accuracy: 0.57 - ETA: 0s - loss: 0.6783 - accuracy: 0.58 - ETA: 0s - loss: 0.6778 - accuracy: 0.58 - 3s 258us/step - loss: 0.6772 - accuracy: 0.5817 - val_loss: 0.6334 - val_accuracy: 0.6960\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 4:51 - loss: 0.6964 - accuracy: 0.25 - ETA: 8s - loss: 0.6958 - accuracy: 0.4773 - ETA: 5s - loss: 0.6953 - accuracy: 0.50 - ETA: 4s - loss: 0.6947 - accuracy: 0.51 - ETA: 3s - loss: 0.6952 - accuracy: 0.50 - ETA: 3s - loss: 0.6945 - accuracy: 0.51 - ETA: 3s - loss: 0.6945 - accuracy: 0.51 - ETA: 3s - loss: 0.6942 - accuracy: 0.52 - ETA: 3s - loss: 0.6941 - accuracy: 0.52 - ETA: 2s - loss: 0.6943 - accuracy: 0.52 - ETA: 2s - loss: 0.6944 - accuracy: 0.51 - ETA: 2s - loss: 0.6946 - accuracy: 0.51 - ETA: 2s - loss: 0.6946 - accuracy: 0.51 - ETA: 2s - loss: 0.6946 - accuracy: 0.52 - ETA: 2s - loss: 0.6943 - accuracy: 0.52 - ETA: 2s - loss: 0.6943 - accuracy: 0.52 - ETA: 2s - loss: 0.6945 - accuracy: 0.51 - ETA: 2s - loss: 0.6945 - accuracy: 0.51 - ETA: 2s - loss: 0.6946 - accuracy: 0.51 - ETA: 2s - loss: 0.6945 - accuracy: 0.51 - ETA: 2s - loss: 0.6942 - accuracy: 0.52 - ETA: 1s - loss: 0.6943 - accuracy: 0.51 - ETA: 1s - loss: 0.6942 - accuracy: 0.52 - ETA: 1s - loss: 0.6943 - accuracy: 0.51 - ETA: 1s - loss: 0.6944 - accuracy: 0.51 - ETA: 1s - loss: 0.6943 - accuracy: 0.51 - ETA: 1s - loss: 0.6943 - accuracy: 0.51 - ETA: 1s - loss: 0.6943 - accuracy: 0.51 - ETA: 1s - loss: 0.6941 - accuracy: 0.51 - ETA: 1s - loss: 0.6943 - accuracy: 0.51 - ETA: 1s - loss: 0.6943 - accuracy: 0.51 - ETA: 1s - loss: 0.6940 - accuracy: 0.52 - ETA: 1s - loss: 0.6938 - accuracy: 0.52 - ETA: 1s - loss: 0.6936 - accuracy: 0.52 - ETA: 1s - loss: 0.6937 - accuracy: 0.52 - ETA: 1s - loss: 0.6938 - accuracy: 0.52 - ETA: 1s - loss: 0.6938 - accuracy: 0.52 - ETA: 1s - loss: 0.6938 - accuracy: 0.52 - ETA: 1s - loss: 0.6937 - accuracy: 0.52 - ETA: 1s - loss: 0.6936 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.52 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6924 - accuracy: 0.53 - ETA: 0s - loss: 0.6923 - accuracy: 0.53 - ETA: 0s - loss: 0.6921 - accuracy: 0.53 - ETA: 0s - loss: 0.6920 - accuracy: 0.53 - ETA: 0s - loss: 0.6918 - accuracy: 0.53 - ETA: 0s - loss: 0.6917 - accuracy: 0.53 - 3s 256us/step - loss: 0.6917 - accuracy: 0.5366 - val_loss: 0.6810 - val_accuracy: 0.6442\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:13 - loss: 0.7010 - accuracy: 0.0000e+ - ETA: 8s - loss: 0.6949 - accuracy: 0.4911     - ETA: 5s - loss: 0.6952 - accuracy: 0.47 - ETA: 4s - loss: 0.6951 - accuracy: 0.49 - ETA: 4s - loss: 0.6951 - accuracy: 0.49 - ETA: 3s - loss: 0.6949 - accuracy: 0.50 - ETA: 3s - loss: 0.6947 - accuracy: 0.51 - ETA: 3s - loss: 0.6951 - accuracy: 0.50 - ETA: 3s - loss: 0.6952 - accuracy: 0.50 - ETA: 3s - loss: 0.6951 - accuracy: 0.50 - ETA: 2s - loss: 0.6951 - accuracy: 0.50 - ETA: 2s - loss: 0.6951 - accuracy: 0.50 - ETA: 2s - loss: 0.6951 - accuracy: 0.50 - ETA: 2s - loss: 0.6950 - accuracy: 0.50 - ETA: 2s - loss: 0.6950 - accuracy: 0.51 - ETA: 2s - loss: 0.6949 - accuracy: 0.51 - ETA: 2s - loss: 0.6949 - accuracy: 0.51 - ETA: 2s - loss: 0.6949 - accuracy: 0.51 - ETA: 2s - loss: 0.6948 - accuracy: 0.51 - ETA: 2s - loss: 0.6949 - accuracy: 0.51 - ETA: 2s - loss: 0.6948 - accuracy: 0.51 - ETA: 2s - loss: 0.6947 - accuracy: 0.51 - ETA: 1s - loss: 0.6944 - accuracy: 0.51 - ETA: 1s - loss: 0.6945 - accuracy: 0.51 - ETA: 1s - loss: 0.6943 - accuracy: 0.51 - ETA: 1s - loss: 0.6942 - accuracy: 0.51 - ETA: 1s - loss: 0.6941 - accuracy: 0.51 - ETA: 1s - loss: 0.6941 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.52 - ETA: 1s - loss: 0.6938 - accuracy: 0.52 - ETA: 1s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.52 - ETA: 1s - loss: 0.6937 - accuracy: 0.52 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.52 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6922 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6919 - accuracy: 0.52 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - ETA: 0s - loss: 0.6916 - accuracy: 0.53 - ETA: 0s - loss: 0.6915 - accuracy: 0.53 - ETA: 0s - loss: 0.6914 - accuracy: 0.53 - ETA: 0s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6908 - accuracy: 0.53 - ETA: 0s - loss: 0.6906 - accuracy: 0.53 - 3s 257us/step - loss: 0.6904 - accuracy: 0.5347 - val_loss: 0.6729 - val_accuracy: 0.5788\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 111us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:57 - loss: 0.6873 - accuracy: 1.00 - ETA: 8s - loss: 0.6931 - accuracy: 0.5318 - ETA: 5s - loss: 0.6926 - accuracy: 0.53 - ETA: 4s - loss: 0.6931 - accuracy: 0.52 - ETA: 4s - loss: 0.6934 - accuracy: 0.52 - ETA: 3s - loss: 0.6934 - accuracy: 0.52 - ETA: 3s - loss: 0.6931 - accuracy: 0.52 - ETA: 3s - loss: 0.6930 - accuracy: 0.52 - ETA: 3s - loss: 0.6932 - accuracy: 0.52 - ETA: 3s - loss: 0.6928 - accuracy: 0.52 - ETA: 2s - loss: 0.6929 - accuracy: 0.52 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 2s - loss: 0.6932 - accuracy: 0.52 - ETA: 2s - loss: 0.6930 - accuracy: 0.52 - ETA: 2s - loss: 0.6927 - accuracy: 0.52 - ETA: 2s - loss: 0.6926 - accuracy: 0.52 - ETA: 2s - loss: 0.6926 - accuracy: 0.52 - ETA: 2s - loss: 0.6924 - accuracy: 0.53 - ETA: 2s - loss: 0.6923 - accuracy: 0.53 - ETA: 2s - loss: 0.6923 - accuracy: 0.53 - ETA: 2s - loss: 0.6921 - accuracy: 0.53 - ETA: 2s - loss: 0.6917 - accuracy: 0.53 - ETA: 2s - loss: 0.6916 - accuracy: 0.53 - ETA: 1s - loss: 0.6913 - accuracy: 0.54 - ETA: 1s - loss: 0.6908 - accuracy: 0.54 - ETA: 1s - loss: 0.6905 - accuracy: 0.54 - ETA: 1s - loss: 0.6902 - accuracy: 0.54 - ETA: 1s - loss: 0.6900 - accuracy: 0.54 - ETA: 1s - loss: 0.6898 - accuracy: 0.54 - ETA: 1s - loss: 0.6893 - accuracy: 0.54 - ETA: 1s - loss: 0.6888 - accuracy: 0.55 - ETA: 1s - loss: 0.6886 - accuracy: 0.55 - ETA: 1s - loss: 0.6882 - accuracy: 0.55 - ETA: 1s - loss: 0.6878 - accuracy: 0.55 - ETA: 1s - loss: 0.6876 - accuracy: 0.55 - ETA: 1s - loss: 0.6873 - accuracy: 0.55 - ETA: 1s - loss: 0.6871 - accuracy: 0.55 - ETA: 1s - loss: 0.6871 - accuracy: 0.55 - ETA: 1s - loss: 0.6868 - accuracy: 0.55 - ETA: 0s - loss: 0.6864 - accuracy: 0.55 - ETA: 0s - loss: 0.6860 - accuracy: 0.56 - ETA: 0s - loss: 0.6856 - accuracy: 0.56 - ETA: 0s - loss: 0.6852 - accuracy: 0.56 - ETA: 0s - loss: 0.6848 - accuracy: 0.56 - ETA: 0s - loss: 0.6844 - accuracy: 0.56 - ETA: 0s - loss: 0.6833 - accuracy: 0.56 - ETA: 0s - loss: 0.6831 - accuracy: 0.56 - ETA: 0s - loss: 0.6824 - accuracy: 0.56 - ETA: 0s - loss: 0.6820 - accuracy: 0.56 - ETA: 0s - loss: 0.6815 - accuracy: 0.56 - ETA: 0s - loss: 0.6812 - accuracy: 0.56 - ETA: 0s - loss: 0.6805 - accuracy: 0.57 - ETA: 0s - loss: 0.6800 - accuracy: 0.57 - ETA: 0s - loss: 0.6796 - accuracy: 0.57 - ETA: 0s - loss: 0.6787 - accuracy: 0.57 - ETA: 0s - loss: 0.6782 - accuracy: 0.57 - ETA: 0s - loss: 0.6775 - accuracy: 0.57 - ETA: 0s - loss: 0.6774 - accuracy: 0.57 - 3s 256us/step - loss: 0.6772 - accuracy: 0.5759 - val_loss: 0.6369 - val_accuracy: 0.7067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 106us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:51 - loss: 0.6910 - accuracy: 0.75 - ETA: 8s - loss: 0.6945 - accuracy: 0.5182 - ETA: 5s - loss: 0.6948 - accuracy: 0.50 - ETA: 4s - loss: 0.6947 - accuracy: 0.50 - ETA: 3s - loss: 0.6940 - accuracy: 0.52 - ETA: 3s - loss: 0.6936 - accuracy: 0.52 - ETA: 3s - loss: 0.6938 - accuracy: 0.51 - ETA: 3s - loss: 0.6936 - accuracy: 0.52 - ETA: 3s - loss: 0.6937 - accuracy: 0.52 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.52 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 2s - loss: 0.6929 - accuracy: 0.52 - ETA: 2s - loss: 0.6930 - accuracy: 0.52 - ETA: 2s - loss: 0.6929 - accuracy: 0.52 - ETA: 2s - loss: 0.6928 - accuracy: 0.52 - ETA: 2s - loss: 0.6919 - accuracy: 0.52 - ETA: 2s - loss: 0.6920 - accuracy: 0.52 - ETA: 2s - loss: 0.6912 - accuracy: 0.52 - ETA: 2s - loss: 0.6912 - accuracy: 0.52 - ETA: 1s - loss: 0.6906 - accuracy: 0.52 - ETA: 1s - loss: 0.6911 - accuracy: 0.52 - ETA: 1s - loss: 0.6911 - accuracy: 0.52 - ETA: 1s - loss: 0.6911 - accuracy: 0.52 - ETA: 1s - loss: 0.6908 - accuracy: 0.52 - ETA: 1s - loss: 0.6909 - accuracy: 0.52 - ETA: 1s - loss: 0.6908 - accuracy: 0.52 - ETA: 1s - loss: 0.6904 - accuracy: 0.52 - ETA: 1s - loss: 0.6905 - accuracy: 0.52 - ETA: 1s - loss: 0.6902 - accuracy: 0.53 - ETA: 1s - loss: 0.6900 - accuracy: 0.53 - ETA: 1s - loss: 0.6898 - accuracy: 0.53 - ETA: 1s - loss: 0.6896 - accuracy: 0.53 - ETA: 1s - loss: 0.6893 - accuracy: 0.54 - ETA: 1s - loss: 0.6893 - accuracy: 0.54 - ETA: 1s - loss: 0.6890 - accuracy: 0.54 - ETA: 1s - loss: 0.6891 - accuracy: 0.54 - ETA: 1s - loss: 0.6888 - accuracy: 0.54 - ETA: 0s - loss: 0.6887 - accuracy: 0.54 - ETA: 0s - loss: 0.6883 - accuracy: 0.54 - ETA: 0s - loss: 0.6881 - accuracy: 0.54 - ETA: 0s - loss: 0.6879 - accuracy: 0.55 - ETA: 0s - loss: 0.6876 - accuracy: 0.55 - ETA: 0s - loss: 0.6872 - accuracy: 0.55 - ETA: 0s - loss: 0.6870 - accuracy: 0.55 - ETA: 0s - loss: 0.6866 - accuracy: 0.55 - ETA: 0s - loss: 0.6863 - accuracy: 0.55 - ETA: 0s - loss: 0.6857 - accuracy: 0.56 - ETA: 0s - loss: 0.6854 - accuracy: 0.56 - ETA: 0s - loss: 0.6852 - accuracy: 0.56 - ETA: 0s - loss: 0.6849 - accuracy: 0.56 - ETA: 0s - loss: 0.6844 - accuracy: 0.56 - ETA: 0s - loss: 0.6842 - accuracy: 0.56 - ETA: 0s - loss: 0.6836 - accuracy: 0.56 - ETA: 0s - loss: 0.6833 - accuracy: 0.56 - ETA: 0s - loss: 0.6828 - accuracy: 0.57 - ETA: 0s - loss: 0.6821 - accuracy: 0.57 - 3s 252us/step - loss: 0.6820 - accuracy: 0.5725 - val_loss: 0.6494 - val_accuracy: 0.7109\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 109us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:51 - loss: 0.6905 - accuracy: 0.50 - ETA: 8s - loss: 0.6967 - accuracy: 0.4500 - ETA: 5s - loss: 0.6959 - accuracy: 0.45 - ETA: 4s - loss: 0.6953 - accuracy: 0.48 - ETA: 3s - loss: 0.6955 - accuracy: 0.48 - ETA: 3s - loss: 0.6954 - accuracy: 0.49 - ETA: 3s - loss: 0.6953 - accuracy: 0.50 - ETA: 3s - loss: 0.6951 - accuracy: 0.50 - ETA: 3s - loss: 0.6951 - accuracy: 0.50 - ETA: 2s - loss: 0.6949 - accuracy: 0.50 - ETA: 2s - loss: 0.6947 - accuracy: 0.51 - ETA: 2s - loss: 0.6944 - accuracy: 0.51 - ETA: 2s - loss: 0.6943 - accuracy: 0.51 - ETA: 2s - loss: 0.6944 - accuracy: 0.51 - ETA: 2s - loss: 0.6944 - accuracy: 0.51 - ETA: 2s - loss: 0.6943 - accuracy: 0.51 - ETA: 2s - loss: 0.6941 - accuracy: 0.51 - ETA: 2s - loss: 0.6940 - accuracy: 0.51 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.52 - ETA: 1s - loss: 0.6935 - accuracy: 0.52 - ETA: 1s - loss: 0.6935 - accuracy: 0.52 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6927 - accuracy: 0.53 - ETA: 1s - loss: 0.6926 - accuracy: 0.53 - ETA: 1s - loss: 0.6923 - accuracy: 0.53 - ETA: 1s - loss: 0.6921 - accuracy: 0.53 - ETA: 1s - loss: 0.6919 - accuracy: 0.53 - ETA: 1s - loss: 0.6915 - accuracy: 0.53 - ETA: 1s - loss: 0.6913 - accuracy: 0.53 - ETA: 1s - loss: 0.6911 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 1s - loss: 0.6905 - accuracy: 0.54 - ETA: 1s - loss: 0.6905 - accuracy: 0.54 - ETA: 1s - loss: 0.6903 - accuracy: 0.54 - ETA: 0s - loss: 0.6901 - accuracy: 0.54 - ETA: 0s - loss: 0.6898 - accuracy: 0.54 - ETA: 0s - loss: 0.6896 - accuracy: 0.54 - ETA: 0s - loss: 0.6892 - accuracy: 0.54 - ETA: 0s - loss: 0.6886 - accuracy: 0.54 - ETA: 0s - loss: 0.6885 - accuracy: 0.54 - ETA: 0s - loss: 0.6883 - accuracy: 0.54 - ETA: 0s - loss: 0.6880 - accuracy: 0.55 - ETA: 0s - loss: 0.6876 - accuracy: 0.55 - ETA: 0s - loss: 0.6872 - accuracy: 0.55 - ETA: 0s - loss: 0.6867 - accuracy: 0.55 - ETA: 0s - loss: 0.6864 - accuracy: 0.55 - ETA: 0s - loss: 0.6861 - accuracy: 0.55 - ETA: 0s - loss: 0.6858 - accuracy: 0.55 - ETA: 0s - loss: 0.6855 - accuracy: 0.55 - ETA: 0s - loss: 0.6851 - accuracy: 0.56 - ETA: 0s - loss: 0.6848 - accuracy: 0.56 - ETA: 0s - loss: 0.6841 - accuracy: 0.56 - ETA: 0s - loss: 0.6837 - accuracy: 0.56 - 3s 256us/step - loss: 0.6834 - accuracy: 0.5643 - val_loss: 0.6530 - val_accuracy: 0.6903\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:20 - loss: 0.6933 - accuracy: 0.25 - ETA: 9s - loss: 0.6936 - accuracy: 0.4858 - ETA: 6s - loss: 0.6934 - accuracy: 0.50 - ETA: 5s - loss: 0.6935 - accuracy: 0.49 - ETA: 4s - loss: 0.6935 - accuracy: 0.49 - ETA: 4s - loss: 0.6935 - accuracy: 0.48 - ETA: 3s - loss: 0.6934 - accuracy: 0.48 - ETA: 3s - loss: 0.6934 - accuracy: 0.49 - ETA: 3s - loss: 0.6933 - accuracy: 0.49 - ETA: 3s - loss: 0.6933 - accuracy: 0.49 - ETA: 3s - loss: 0.6932 - accuracy: 0.49 - ETA: 3s - loss: 0.6931 - accuracy: 0.49 - ETA: 3s - loss: 0.6927 - accuracy: 0.49 - ETA: 2s - loss: 0.6922 - accuracy: 0.50 - ETA: 2s - loss: 0.6913 - accuracy: 0.50 - ETA: 2s - loss: 0.6913 - accuracy: 0.50 - ETA: 2s - loss: 0.6911 - accuracy: 0.50 - ETA: 2s - loss: 0.6911 - accuracy: 0.50 - ETA: 2s - loss: 0.6913 - accuracy: 0.50 - ETA: 2s - loss: 0.6909 - accuracy: 0.51 - ETA: 2s - loss: 0.6903 - accuracy: 0.51 - ETA: 2s - loss: 0.6894 - accuracy: 0.51 - ETA: 2s - loss: 0.6893 - accuracy: 0.51 - ETA: 2s - loss: 0.6883 - accuracy: 0.51 - ETA: 2s - loss: 0.6870 - accuracy: 0.52 - ETA: 2s - loss: 0.6868 - accuracy: 0.52 - ETA: 2s - loss: 0.6860 - accuracy: 0.52 - ETA: 1s - loss: 0.6859 - accuracy: 0.52 - ETA: 1s - loss: 0.6845 - accuracy: 0.53 - ETA: 1s - loss: 0.6837 - accuracy: 0.53 - ETA: 1s - loss: 0.6834 - accuracy: 0.53 - ETA: 1s - loss: 0.6830 - accuracy: 0.53 - ETA: 1s - loss: 0.6825 - accuracy: 0.53 - ETA: 1s - loss: 0.6822 - accuracy: 0.53 - ETA: 1s - loss: 0.6818 - accuracy: 0.54 - ETA: 1s - loss: 0.6807 - accuracy: 0.54 - ETA: 1s - loss: 0.6802 - accuracy: 0.54 - ETA: 1s - loss: 0.6800 - accuracy: 0.54 - ETA: 1s - loss: 0.6795 - accuracy: 0.54 - ETA: 1s - loss: 0.6792 - accuracy: 0.54 - ETA: 1s - loss: 0.6782 - accuracy: 0.55 - ETA: 1s - loss: 0.6773 - accuracy: 0.55 - ETA: 1s - loss: 0.6768 - accuracy: 0.55 - ETA: 1s - loss: 0.6768 - accuracy: 0.55 - ETA: 1s - loss: 0.6758 - accuracy: 0.55 - ETA: 0s - loss: 0.6754 - accuracy: 0.56 - ETA: 0s - loss: 0.6747 - accuracy: 0.56 - ETA: 0s - loss: 0.6748 - accuracy: 0.56 - ETA: 0s - loss: 0.6746 - accuracy: 0.56 - ETA: 0s - loss: 0.6743 - accuracy: 0.56 - ETA: 0s - loss: 0.6741 - accuracy: 0.56 - ETA: 0s - loss: 0.6734 - accuracy: 0.56 - ETA: 0s - loss: 0.6731 - accuracy: 0.57 - ETA: 0s - loss: 0.6721 - accuracy: 0.57 - ETA: 0s - loss: 0.6716 - accuracy: 0.57 - ETA: 0s - loss: 0.6718 - accuracy: 0.57 - ETA: 0s - loss: 0.6716 - accuracy: 0.57 - ETA: 0s - loss: 0.6715 - accuracy: 0.57 - ETA: 0s - loss: 0.6714 - accuracy: 0.57 - ETA: 0s - loss: 0.6713 - accuracy: 0.57 - ETA: 0s - loss: 0.6711 - accuracy: 0.57 - ETA: 0s - loss: 0.6710 - accuracy: 0.58 - ETA: 0s - loss: 0.6706 - accuracy: 0.58 - ETA: 0s - loss: 0.6705 - accuracy: 0.58 - ETA: 0s - loss: 0.6703 - accuracy: 0.58 - 4s 281us/step - loss: 0.6703 - accuracy: 0.5841 - val_loss: 0.6326 - val_accuracy: 0.7131\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 106us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:20 - loss: 0.6924 - accuracy: 0.50 - ETA: 10s - loss: 0.6940 - accuracy: 0.4231 - ETA: 6s - loss: 0.6936 - accuracy: 0.487 - ETA: 5s - loss: 0.6934 - accuracy: 0.51 - ETA: 4s - loss: 0.6931 - accuracy: 0.52 - ETA: 4s - loss: 0.6927 - accuracy: 0.53 - ETA: 4s - loss: 0.6924 - accuracy: 0.53 - ETA: 3s - loss: 0.6919 - accuracy: 0.54 - ETA: 3s - loss: 0.6916 - accuracy: 0.53 - ETA: 3s - loss: 0.6908 - accuracy: 0.54 - ETA: 3s - loss: 0.6903 - accuracy: 0.54 - ETA: 3s - loss: 0.6896 - accuracy: 0.55 - ETA: 3s - loss: 0.6888 - accuracy: 0.55 - ETA: 3s - loss: 0.6883 - accuracy: 0.55 - ETA: 2s - loss: 0.6880 - accuracy: 0.55 - ETA: 2s - loss: 0.6873 - accuracy: 0.56 - ETA: 2s - loss: 0.6865 - accuracy: 0.56 - ETA: 2s - loss: 0.6858 - accuracy: 0.56 - ETA: 2s - loss: 0.6848 - accuracy: 0.57 - ETA: 2s - loss: 0.6836 - accuracy: 0.58 - ETA: 2s - loss: 0.6826 - accuracy: 0.58 - ETA: 2s - loss: 0.6817 - accuracy: 0.59 - ETA: 2s - loss: 0.6811 - accuracy: 0.59 - ETA: 2s - loss: 0.6796 - accuracy: 0.59 - ETA: 2s - loss: 0.6786 - accuracy: 0.59 - ETA: 2s - loss: 0.6786 - accuracy: 0.59 - ETA: 2s - loss: 0.6778 - accuracy: 0.60 - ETA: 2s - loss: 0.6772 - accuracy: 0.60 - ETA: 2s - loss: 0.6765 - accuracy: 0.60 - ETA: 1s - loss: 0.6764 - accuracy: 0.60 - ETA: 1s - loss: 0.6754 - accuracy: 0.60 - ETA: 1s - loss: 0.6751 - accuracy: 0.60 - ETA: 1s - loss: 0.6750 - accuracy: 0.60 - ETA: 1s - loss: 0.6743 - accuracy: 0.61 - ETA: 1s - loss: 0.6739 - accuracy: 0.61 - ETA: 1s - loss: 0.6728 - accuracy: 0.61 - ETA: 1s - loss: 0.6720 - accuracy: 0.61 - ETA: 1s - loss: 0.6711 - accuracy: 0.61 - ETA: 1s - loss: 0.6704 - accuracy: 0.61 - ETA: 1s - loss: 0.6699 - accuracy: 0.61 - ETA: 1s - loss: 0.6691 - accuracy: 0.61 - ETA: 1s - loss: 0.6689 - accuracy: 0.61 - ETA: 1s - loss: 0.6684 - accuracy: 0.62 - ETA: 1s - loss: 0.6679 - accuracy: 0.62 - ETA: 1s - loss: 0.6677 - accuracy: 0.62 - ETA: 1s - loss: 0.6672 - accuracy: 0.62 - ETA: 1s - loss: 0.6667 - accuracy: 0.62 - ETA: 0s - loss: 0.6665 - accuracy: 0.62 - ETA: 0s - loss: 0.6665 - accuracy: 0.62 - ETA: 0s - loss: 0.6655 - accuracy: 0.62 - ETA: 0s - loss: 0.6652 - accuracy: 0.62 - ETA: 0s - loss: 0.6642 - accuracy: 0.62 - ETA: 0s - loss: 0.6640 - accuracy: 0.62 - ETA: 0s - loss: 0.6640 - accuracy: 0.63 - ETA: 0s - loss: 0.6635 - accuracy: 0.63 - ETA: 0s - loss: 0.6631 - accuracy: 0.63 - ETA: 0s - loss: 0.6626 - accuracy: 0.63 - ETA: 0s - loss: 0.6616 - accuracy: 0.63 - ETA: 0s - loss: 0.6608 - accuracy: 0.63 - ETA: 0s - loss: 0.6604 - accuracy: 0.63 - ETA: 0s - loss: 0.6597 - accuracy: 0.63 - ETA: 0s - loss: 0.6590 - accuracy: 0.63 - ETA: 0s - loss: 0.6587 - accuracy: 0.63 - ETA: 0s - loss: 0.6583 - accuracy: 0.64 - ETA: 0s - loss: 0.6577 - accuracy: 0.64 - ETA: 0s - loss: 0.6578 - accuracy: 0.64 - 4s 289us/step - loss: 0.6571 - accuracy: 0.6419 - val_loss: 0.6094 - val_accuracy: 0.7202\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 109us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 6:20 - loss: 0.6936 - accuracy: 0.75 - ETA: 12s - loss: 0.6937 - accuracy: 0.4583 - ETA: 7s - loss: 0.6933 - accuracy: 0.508 - ETA: 5s - loss: 0.6936 - accuracy: 0.48 - ETA: 4s - loss: 0.6933 - accuracy: 0.50 - ETA: 4s - loss: 0.6935 - accuracy: 0.49 - ETA: 4s - loss: 0.6928 - accuracy: 0.50 - ETA: 3s - loss: 0.6923 - accuracy: 0.51 - ETA: 3s - loss: 0.6921 - accuracy: 0.51 - ETA: 3s - loss: 0.6914 - accuracy: 0.51 - ETA: 3s - loss: 0.6913 - accuracy: 0.51 - ETA: 3s - loss: 0.6907 - accuracy: 0.51 - ETA: 3s - loss: 0.6912 - accuracy: 0.51 - ETA: 2s - loss: 0.6912 - accuracy: 0.50 - ETA: 2s - loss: 0.6903 - accuracy: 0.51 - ETA: 2s - loss: 0.6899 - accuracy: 0.51 - ETA: 2s - loss: 0.6897 - accuracy: 0.51 - ETA: 2s - loss: 0.6893 - accuracy: 0.51 - ETA: 2s - loss: 0.6886 - accuracy: 0.52 - ETA: 2s - loss: 0.6880 - accuracy: 0.52 - ETA: 2s - loss: 0.6876 - accuracy: 0.52 - ETA: 2s - loss: 0.6868 - accuracy: 0.52 - ETA: 2s - loss: 0.6858 - accuracy: 0.53 - ETA: 2s - loss: 0.6855 - accuracy: 0.53 - ETA: 2s - loss: 0.6844 - accuracy: 0.53 - ETA: 1s - loss: 0.6837 - accuracy: 0.53 - ETA: 1s - loss: 0.6827 - accuracy: 0.54 - ETA: 1s - loss: 0.6828 - accuracy: 0.54 - ETA: 1s - loss: 0.6819 - accuracy: 0.54 - ETA: 1s - loss: 0.6815 - accuracy: 0.54 - ETA: 1s - loss: 0.6812 - accuracy: 0.54 - ETA: 1s - loss: 0.6799 - accuracy: 0.55 - ETA: 1s - loss: 0.6799 - accuracy: 0.55 - ETA: 1s - loss: 0.6788 - accuracy: 0.55 - ETA: 1s - loss: 0.6782 - accuracy: 0.56 - ETA: 1s - loss: 0.6777 - accuracy: 0.56 - ETA: 1s - loss: 0.6777 - accuracy: 0.56 - ETA: 1s - loss: 0.6770 - accuracy: 0.56 - ETA: 1s - loss: 0.6763 - accuracy: 0.56 - ETA: 1s - loss: 0.6757 - accuracy: 0.56 - ETA: 1s - loss: 0.6753 - accuracy: 0.56 - ETA: 1s - loss: 0.6753 - accuracy: 0.57 - ETA: 0s - loss: 0.6744 - accuracy: 0.57 - ETA: 0s - loss: 0.6735 - accuracy: 0.57 - ETA: 0s - loss: 0.6723 - accuracy: 0.57 - ETA: 0s - loss: 0.6720 - accuracy: 0.57 - ETA: 0s - loss: 0.6717 - accuracy: 0.57 - ETA: 0s - loss: 0.6713 - accuracy: 0.58 - ETA: 0s - loss: 0.6706 - accuracy: 0.58 - ETA: 0s - loss: 0.6707 - accuracy: 0.58 - ETA: 0s - loss: 0.6702 - accuracy: 0.58 - ETA: 0s - loss: 0.6700 - accuracy: 0.58 - ETA: 0s - loss: 0.6697 - accuracy: 0.58 - ETA: 0s - loss: 0.6696 - accuracy: 0.58 - ETA: 0s - loss: 0.6693 - accuracy: 0.58 - ETA: 0s - loss: 0.6690 - accuracy: 0.59 - ETA: 0s - loss: 0.6689 - accuracy: 0.59 - ETA: 0s - loss: 0.6683 - accuracy: 0.59 - ETA: 0s - loss: 0.6678 - accuracy: 0.59 - ETA: 0s - loss: 0.6679 - accuracy: 0.59 - ETA: 0s - loss: 0.6672 - accuracy: 0.59 - 3s 269us/step - loss: 0.6672 - accuracy: 0.5953 - val_loss: 0.6279 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:13 - loss: 0.6913 - accuracy: 0.75 - ETA: 10s - loss: 0.6929 - accuracy: 0.5000 - ETA: 7s - loss: 0.6927 - accuracy: 0.510 - ETA: 5s - loss: 0.6921 - accuracy: 0.51 - ETA: 4s - loss: 0.6910 - accuracy: 0.53 - ETA: 4s - loss: 0.6913 - accuracy: 0.52 - ETA: 4s - loss: 0.6908 - accuracy: 0.52 - ETA: 3s - loss: 0.6908 - accuracy: 0.52 - ETA: 3s - loss: 0.6900 - accuracy: 0.52 - ETA: 3s - loss: 0.6893 - accuracy: 0.53 - ETA: 3s - loss: 0.6887 - accuracy: 0.52 - ETA: 3s - loss: 0.6869 - accuracy: 0.53 - ETA: 3s - loss: 0.6857 - accuracy: 0.53 - ETA: 2s - loss: 0.6848 - accuracy: 0.53 - ETA: 2s - loss: 0.6833 - accuracy: 0.54 - ETA: 2s - loss: 0.6816 - accuracy: 0.55 - ETA: 2s - loss: 0.6813 - accuracy: 0.55 - ETA: 2s - loss: 0.6807 - accuracy: 0.55 - ETA: 2s - loss: 0.6799 - accuracy: 0.55 - ETA: 2s - loss: 0.6789 - accuracy: 0.55 - ETA: 2s - loss: 0.6775 - accuracy: 0.56 - ETA: 2s - loss: 0.6766 - accuracy: 0.56 - ETA: 2s - loss: 0.6739 - accuracy: 0.57 - ETA: 2s - loss: 0.6721 - accuracy: 0.57 - ETA: 1s - loss: 0.6715 - accuracy: 0.57 - ETA: 1s - loss: 0.6709 - accuracy: 0.57 - ETA: 1s - loss: 0.6694 - accuracy: 0.57 - ETA: 1s - loss: 0.6688 - accuracy: 0.57 - ETA: 1s - loss: 0.6680 - accuracy: 0.57 - ETA: 1s - loss: 0.6670 - accuracy: 0.57 - ETA: 1s - loss: 0.6662 - accuracy: 0.57 - ETA: 1s - loss: 0.6650 - accuracy: 0.58 - ETA: 1s - loss: 0.6632 - accuracy: 0.58 - ETA: 1s - loss: 0.6617 - accuracy: 0.58 - ETA: 1s - loss: 0.6607 - accuracy: 0.58 - ETA: 1s - loss: 0.6607 - accuracy: 0.58 - ETA: 1s - loss: 0.6604 - accuracy: 0.59 - ETA: 1s - loss: 0.6594 - accuracy: 0.59 - ETA: 1s - loss: 0.6586 - accuracy: 0.59 - ETA: 1s - loss: 0.6581 - accuracy: 0.59 - ETA: 1s - loss: 0.6579 - accuracy: 0.59 - ETA: 0s - loss: 0.6573 - accuracy: 0.59 - ETA: 0s - loss: 0.6571 - accuracy: 0.59 - ETA: 0s - loss: 0.6570 - accuracy: 0.59 - ETA: 0s - loss: 0.6558 - accuracy: 0.59 - ETA: 0s - loss: 0.6555 - accuracy: 0.59 - ETA: 0s - loss: 0.6550 - accuracy: 0.59 - ETA: 0s - loss: 0.6545 - accuracy: 0.60 - ETA: 0s - loss: 0.6537 - accuracy: 0.60 - ETA: 0s - loss: 0.6534 - accuracy: 0.60 - ETA: 0s - loss: 0.6531 - accuracy: 0.60 - ETA: 0s - loss: 0.6522 - accuracy: 0.60 - ETA: 0s - loss: 0.6520 - accuracy: 0.60 - ETA: 0s - loss: 0.6515 - accuracy: 0.60 - ETA: 0s - loss: 0.6503 - accuracy: 0.60 - ETA: 0s - loss: 0.6499 - accuracy: 0.60 - ETA: 0s - loss: 0.6493 - accuracy: 0.60 - ETA: 0s - loss: 0.6495 - accuracy: 0.60 - ETA: 0s - loss: 0.6494 - accuracy: 0.60 - ETA: 0s - loss: 0.6493 - accuracy: 0.60 - ETA: 0s - loss: 0.6493 - accuracy: 0.60 - 3s 267us/step - loss: 0.6495 - accuracy: 0.6081 - val_loss: 0.6039 - val_accuracy: 0.7202\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 110us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:58 - loss: 0.6987 - accuracy: 0.25 - ETA: 10s - loss: 0.6921 - accuracy: 0.5913 - ETA: 6s - loss: 0.6928 - accuracy: 0.549 - ETA: 5s - loss: 0.6928 - accuracy: 0.54 - ETA: 5s - loss: 0.6929 - accuracy: 0.53 - ETA: 4s - loss: 0.6926 - accuracy: 0.54 - ETA: 4s - loss: 0.6925 - accuracy: 0.54 - ETA: 4s - loss: 0.6920 - accuracy: 0.54 - ETA: 3s - loss: 0.6908 - accuracy: 0.55 - ETA: 3s - loss: 0.6901 - accuracy: 0.55 - ETA: 3s - loss: 0.6899 - accuracy: 0.55 - ETA: 3s - loss: 0.6892 - accuracy: 0.55 - ETA: 3s - loss: 0.6875 - accuracy: 0.56 - ETA: 3s - loss: 0.6872 - accuracy: 0.56 - ETA: 2s - loss: 0.6857 - accuracy: 0.56 - ETA: 2s - loss: 0.6850 - accuracy: 0.56 - ETA: 2s - loss: 0.6836 - accuracy: 0.57 - ETA: 2s - loss: 0.6826 - accuracy: 0.57 - ETA: 2s - loss: 0.6823 - accuracy: 0.57 - ETA: 2s - loss: 0.6806 - accuracy: 0.57 - ETA: 2s - loss: 0.6801 - accuracy: 0.57 - ETA: 2s - loss: 0.6794 - accuracy: 0.57 - ETA: 2s - loss: 0.6795 - accuracy: 0.57 - ETA: 2s - loss: 0.6790 - accuracy: 0.58 - ETA: 2s - loss: 0.6780 - accuracy: 0.58 - ETA: 2s - loss: 0.6769 - accuracy: 0.58 - ETA: 2s - loss: 0.6757 - accuracy: 0.58 - ETA: 1s - loss: 0.6749 - accuracy: 0.58 - ETA: 1s - loss: 0.6748 - accuracy: 0.59 - ETA: 1s - loss: 0.6741 - accuracy: 0.59 - ETA: 1s - loss: 0.6732 - accuracy: 0.59 - ETA: 1s - loss: 0.6722 - accuracy: 0.59 - ETA: 1s - loss: 0.6714 - accuracy: 0.59 - ETA: 1s - loss: 0.6701 - accuracy: 0.59 - ETA: 1s - loss: 0.6693 - accuracy: 0.60 - ETA: 1s - loss: 0.6687 - accuracy: 0.60 - ETA: 1s - loss: 0.6674 - accuracy: 0.60 - ETA: 1s - loss: 0.6670 - accuracy: 0.60 - ETA: 1s - loss: 0.6667 - accuracy: 0.60 - ETA: 1s - loss: 0.6655 - accuracy: 0.60 - ETA: 1s - loss: 0.6645 - accuracy: 0.60 - ETA: 1s - loss: 0.6632 - accuracy: 0.61 - ETA: 1s - loss: 0.6619 - accuracy: 0.61 - ETA: 0s - loss: 0.6614 - accuracy: 0.61 - ETA: 0s - loss: 0.6607 - accuracy: 0.61 - ETA: 0s - loss: 0.6597 - accuracy: 0.61 - ETA: 0s - loss: 0.6590 - accuracy: 0.61 - ETA: 0s - loss: 0.6587 - accuracy: 0.61 - ETA: 0s - loss: 0.6585 - accuracy: 0.61 - ETA: 0s - loss: 0.6591 - accuracy: 0.61 - ETA: 0s - loss: 0.6582 - accuracy: 0.61 - ETA: 0s - loss: 0.6573 - accuracy: 0.62 - ETA: 0s - loss: 0.6570 - accuracy: 0.62 - ETA: 0s - loss: 0.6559 - accuracy: 0.62 - ETA: 0s - loss: 0.6550 - accuracy: 0.62 - ETA: 0s - loss: 0.6547 - accuracy: 0.62 - ETA: 0s - loss: 0.6539 - accuracy: 0.62 - ETA: 0s - loss: 0.6536 - accuracy: 0.62 - ETA: 0s - loss: 0.6528 - accuracy: 0.62 - ETA: 0s - loss: 0.6522 - accuracy: 0.62 - ETA: 0s - loss: 0.6522 - accuracy: 0.62 - ETA: 0s - loss: 0.6512 - accuracy: 0.62 - 3s 271us/step - loss: 0.6513 - accuracy: 0.6296 - val_loss: 0.5937 - val_accuracy: 0.7216\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 107us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:20 - loss: 0.6938 - accuracy: 0.25 - ETA: 10s - loss: 0.6932 - accuracy: 0.5577 - ETA: 6s - loss: 0.6930 - accuracy: 0.519 - ETA: 5s - loss: 0.6925 - accuracy: 0.52 - ETA: 4s - loss: 0.6925 - accuracy: 0.51 - ETA: 4s - loss: 0.6913 - accuracy: 0.52 - ETA: 4s - loss: 0.6916 - accuracy: 0.52 - ETA: 3s - loss: 0.6912 - accuracy: 0.52 - ETA: 3s - loss: 0.6913 - accuracy: 0.52 - ETA: 3s - loss: 0.6904 - accuracy: 0.53 - ETA: 3s - loss: 0.6900 - accuracy: 0.53 - ETA: 3s - loss: 0.6899 - accuracy: 0.53 - ETA: 3s - loss: 0.6893 - accuracy: 0.53 - ETA: 3s - loss: 0.6896 - accuracy: 0.53 - ETA: 2s - loss: 0.6889 - accuracy: 0.53 - ETA: 2s - loss: 0.6879 - accuracy: 0.53 - ETA: 2s - loss: 0.6875 - accuracy: 0.53 - ETA: 2s - loss: 0.6875 - accuracy: 0.53 - ETA: 2s - loss: 0.6869 - accuracy: 0.54 - ETA: 2s - loss: 0.6868 - accuracy: 0.54 - ETA: 2s - loss: 0.6858 - accuracy: 0.54 - ETA: 2s - loss: 0.6859 - accuracy: 0.54 - ETA: 2s - loss: 0.6854 - accuracy: 0.55 - ETA: 2s - loss: 0.6849 - accuracy: 0.55 - ETA: 2s - loss: 0.6844 - accuracy: 0.55 - ETA: 2s - loss: 0.6839 - accuracy: 0.55 - ETA: 2s - loss: 0.6829 - accuracy: 0.56 - ETA: 2s - loss: 0.6822 - accuracy: 0.56 - ETA: 1s - loss: 0.6815 - accuracy: 0.56 - ETA: 1s - loss: 0.6809 - accuracy: 0.57 - ETA: 1s - loss: 0.6809 - accuracy: 0.57 - ETA: 1s - loss: 0.6806 - accuracy: 0.57 - ETA: 1s - loss: 0.6790 - accuracy: 0.57 - ETA: 1s - loss: 0.6789 - accuracy: 0.57 - ETA: 1s - loss: 0.6789 - accuracy: 0.57 - ETA: 1s - loss: 0.6781 - accuracy: 0.57 - ETA: 1s - loss: 0.6773 - accuracy: 0.58 - ETA: 1s - loss: 0.6768 - accuracy: 0.58 - ETA: 1s - loss: 0.6762 - accuracy: 0.58 - ETA: 1s - loss: 0.6757 - accuracy: 0.58 - ETA: 1s - loss: 0.6755 - accuracy: 0.58 - ETA: 1s - loss: 0.6751 - accuracy: 0.58 - ETA: 1s - loss: 0.6745 - accuracy: 0.59 - ETA: 1s - loss: 0.6744 - accuracy: 0.59 - ETA: 1s - loss: 0.6741 - accuracy: 0.59 - ETA: 0s - loss: 0.6734 - accuracy: 0.59 - ETA: 0s - loss: 0.6724 - accuracy: 0.59 - ETA: 0s - loss: 0.6723 - accuracy: 0.59 - ETA: 0s - loss: 0.6723 - accuracy: 0.59 - ETA: 0s - loss: 0.6718 - accuracy: 0.59 - ETA: 0s - loss: 0.6708 - accuracy: 0.60 - ETA: 0s - loss: 0.6705 - accuracy: 0.60 - ETA: 0s - loss: 0.6700 - accuracy: 0.60 - ETA: 0s - loss: 0.6696 - accuracy: 0.60 - ETA: 0s - loss: 0.6691 - accuracy: 0.60 - ETA: 0s - loss: 0.6693 - accuracy: 0.60 - ETA: 0s - loss: 0.6688 - accuracy: 0.60 - ETA: 0s - loss: 0.6679 - accuracy: 0.60 - ETA: 0s - loss: 0.6679 - accuracy: 0.60 - ETA: 0s - loss: 0.6675 - accuracy: 0.60 - ETA: 0s - loss: 0.6672 - accuracy: 0.61 - ETA: 0s - loss: 0.6671 - accuracy: 0.61 - ETA: 0s - loss: 0.6668 - accuracy: 0.61 - 3s 276us/step - loss: 0.6670 - accuracy: 0.6109 - val_loss: 0.6342 - val_accuracy: 0.7244\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 6:13 - loss: 0.6956 - accuracy: 0.0000e+ - ETA: 9s - loss: 0.6933 - accuracy: 0.5139     - ETA: 6s - loss: 0.6930 - accuracy: 0.51 - ETA: 5s - loss: 0.6935 - accuracy: 0.51 - ETA: 4s - loss: 0.6928 - accuracy: 0.52 - ETA: 4s - loss: 0.6923 - accuracy: 0.51 - ETA: 3s - loss: 0.6914 - accuracy: 0.52 - ETA: 3s - loss: 0.6913 - accuracy: 0.52 - ETA: 3s - loss: 0.6908 - accuracy: 0.52 - ETA: 3s - loss: 0.6897 - accuracy: 0.53 - ETA: 3s - loss: 0.6889 - accuracy: 0.53 - ETA: 2s - loss: 0.6877 - accuracy: 0.54 - ETA: 2s - loss: 0.6873 - accuracy: 0.54 - ETA: 2s - loss: 0.6853 - accuracy: 0.55 - ETA: 2s - loss: 0.6845 - accuracy: 0.55 - ETA: 2s - loss: 0.6831 - accuracy: 0.56 - ETA: 2s - loss: 0.6816 - accuracy: 0.56 - ETA: 2s - loss: 0.6801 - accuracy: 0.56 - ETA: 2s - loss: 0.6793 - accuracy: 0.56 - ETA: 2s - loss: 0.6781 - accuracy: 0.57 - ETA: 2s - loss: 0.6770 - accuracy: 0.57 - ETA: 2s - loss: 0.6757 - accuracy: 0.57 - ETA: 2s - loss: 0.6740 - accuracy: 0.58 - ETA: 2s - loss: 0.6720 - accuracy: 0.58 - ETA: 1s - loss: 0.6709 - accuracy: 0.58 - ETA: 1s - loss: 0.6686 - accuracy: 0.59 - ETA: 1s - loss: 0.6683 - accuracy: 0.59 - ETA: 1s - loss: 0.6667 - accuracy: 0.59 - ETA: 1s - loss: 0.6655 - accuracy: 0.60 - ETA: 1s - loss: 0.6641 - accuracy: 0.60 - ETA: 1s - loss: 0.6629 - accuracy: 0.60 - ETA: 1s - loss: 0.6624 - accuracy: 0.60 - ETA: 1s - loss: 0.6614 - accuracy: 0.60 - ETA: 1s - loss: 0.6602 - accuracy: 0.61 - ETA: 1s - loss: 0.6596 - accuracy: 0.61 - ETA: 1s - loss: 0.6593 - accuracy: 0.61 - ETA: 1s - loss: 0.6578 - accuracy: 0.61 - ETA: 1s - loss: 0.6562 - accuracy: 0.61 - ETA: 1s - loss: 0.6556 - accuracy: 0.61 - ETA: 1s - loss: 0.6548 - accuracy: 0.61 - ETA: 1s - loss: 0.6543 - accuracy: 0.62 - ETA: 0s - loss: 0.6539 - accuracy: 0.62 - ETA: 0s - loss: 0.6535 - accuracy: 0.62 - ETA: 0s - loss: 0.6526 - accuracy: 0.62 - ETA: 0s - loss: 0.6513 - accuracy: 0.62 - ETA: 0s - loss: 0.6502 - accuracy: 0.62 - ETA: 0s - loss: 0.6487 - accuracy: 0.62 - ETA: 0s - loss: 0.6483 - accuracy: 0.62 - ETA: 0s - loss: 0.6475 - accuracy: 0.62 - ETA: 0s - loss: 0.6474 - accuracy: 0.63 - ETA: 0s - loss: 0.6464 - accuracy: 0.63 - ETA: 0s - loss: 0.6460 - accuracy: 0.63 - ETA: 0s - loss: 0.6457 - accuracy: 0.63 - ETA: 0s - loss: 0.6447 - accuracy: 0.63 - ETA: 0s - loss: 0.6446 - accuracy: 0.63 - ETA: 0s - loss: 0.6436 - accuracy: 0.63 - ETA: 0s - loss: 0.6429 - accuracy: 0.63 - ETA: 0s - loss: 0.6428 - accuracy: 0.63 - ETA: 0s - loss: 0.6416 - accuracy: 0.63 - ETA: 0s - loss: 0.6415 - accuracy: 0.63 - ETA: 0s - loss: 0.6408 - accuracy: 0.63 - 3s 266us/step - loss: 0.6405 - accuracy: 0.6402 - val_loss: 0.5890 - val_accuracy: 0.7074\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 113us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:29 - loss: 0.6878 - accuracy: 1.00 - ETA: 12s - loss: 0.6939 - accuracy: 0.4826 - ETA: 7s - loss: 0.6938 - accuracy: 0.505 - ETA: 5s - loss: 0.6932 - accuracy: 0.51 - ETA: 4s - loss: 0.6926 - accuracy: 0.51 - ETA: 4s - loss: 0.6925 - accuracy: 0.51 - ETA: 4s - loss: 0.6916 - accuracy: 0.52 - ETA: 4s - loss: 0.6920 - accuracy: 0.52 - ETA: 3s - loss: 0.6914 - accuracy: 0.53 - ETA: 3s - loss: 0.6907 - accuracy: 0.53 - ETA: 3s - loss: 0.6894 - accuracy: 0.53 - ETA: 3s - loss: 0.6889 - accuracy: 0.53 - ETA: 3s - loss: 0.6876 - accuracy: 0.54 - ETA: 3s - loss: 0.6869 - accuracy: 0.54 - ETA: 3s - loss: 0.6862 - accuracy: 0.54 - ETA: 2s - loss: 0.6846 - accuracy: 0.54 - ETA: 2s - loss: 0.6840 - accuracy: 0.54 - ETA: 2s - loss: 0.6832 - accuracy: 0.55 - ETA: 2s - loss: 0.6828 - accuracy: 0.55 - ETA: 2s - loss: 0.6811 - accuracy: 0.55 - ETA: 2s - loss: 0.6788 - accuracy: 0.56 - ETA: 2s - loss: 0.6778 - accuracy: 0.56 - ETA: 2s - loss: 0.6763 - accuracy: 0.57 - ETA: 2s - loss: 0.6747 - accuracy: 0.57 - ETA: 2s - loss: 0.6743 - accuracy: 0.57 - ETA: 2s - loss: 0.6741 - accuracy: 0.57 - ETA: 2s - loss: 0.6731 - accuracy: 0.58 - ETA: 2s - loss: 0.6724 - accuracy: 0.58 - ETA: 1s - loss: 0.6701 - accuracy: 0.58 - ETA: 1s - loss: 0.6681 - accuracy: 0.59 - ETA: 1s - loss: 0.6685 - accuracy: 0.59 - ETA: 1s - loss: 0.6674 - accuracy: 0.59 - ETA: 1s - loss: 0.6660 - accuracy: 0.59 - ETA: 1s - loss: 0.6654 - accuracy: 0.59 - ETA: 1s - loss: 0.6640 - accuracy: 0.60 - ETA: 1s - loss: 0.6631 - accuracy: 0.60 - ETA: 1s - loss: 0.6615 - accuracy: 0.60 - ETA: 1s - loss: 0.6609 - accuracy: 0.60 - ETA: 1s - loss: 0.6595 - accuracy: 0.60 - ETA: 1s - loss: 0.6586 - accuracy: 0.61 - ETA: 1s - loss: 0.6579 - accuracy: 0.61 - ETA: 1s - loss: 0.6570 - accuracy: 0.61 - ETA: 1s - loss: 0.6557 - accuracy: 0.61 - ETA: 1s - loss: 0.6546 - accuracy: 0.61 - ETA: 1s - loss: 0.6538 - accuracy: 0.62 - ETA: 0s - loss: 0.6540 - accuracy: 0.62 - ETA: 0s - loss: 0.6535 - accuracy: 0.62 - ETA: 0s - loss: 0.6528 - accuracy: 0.62 - ETA: 0s - loss: 0.6516 - accuracy: 0.62 - ETA: 0s - loss: 0.6505 - accuracy: 0.62 - ETA: 0s - loss: 0.6498 - accuracy: 0.62 - ETA: 0s - loss: 0.6494 - accuracy: 0.62 - ETA: 0s - loss: 0.6487 - accuracy: 0.62 - ETA: 0s - loss: 0.6475 - accuracy: 0.62 - ETA: 0s - loss: 0.6471 - accuracy: 0.63 - ETA: 0s - loss: 0.6466 - accuracy: 0.63 - ETA: 0s - loss: 0.6466 - accuracy: 0.63 - ETA: 0s - loss: 0.6458 - accuracy: 0.63 - ETA: 0s - loss: 0.6457 - accuracy: 0.63 - ETA: 0s - loss: 0.6446 - accuracy: 0.63 - ETA: 0s - loss: 0.6439 - accuracy: 0.63 - ETA: 0s - loss: 0.6430 - accuracy: 0.63 - ETA: 0s - loss: 0.6426 - accuracy: 0.63 - 4s 276us/step - loss: 0.6423 - accuracy: 0.6377 - val_loss: 0.5885 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 111us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:29 - loss: 0.6958 - accuracy: 0.50 - ETA: 10s - loss: 0.6943 - accuracy: 0.4858 - ETA: 6s - loss: 0.6942 - accuracy: 0.490 - ETA: 5s - loss: 0.6939 - accuracy: 0.49 - ETA: 4s - loss: 0.6939 - accuracy: 0.48 - ETA: 4s - loss: 0.6932 - accuracy: 0.50 - ETA: 3s - loss: 0.6932 - accuracy: 0.50 - ETA: 3s - loss: 0.6919 - accuracy: 0.52 - ETA: 3s - loss: 0.6919 - accuracy: 0.51 - ETA: 3s - loss: 0.6910 - accuracy: 0.52 - ETA: 3s - loss: 0.6912 - accuracy: 0.52 - ETA: 3s - loss: 0.6904 - accuracy: 0.52 - ETA: 3s - loss: 0.6891 - accuracy: 0.53 - ETA: 2s - loss: 0.6876 - accuracy: 0.54 - ETA: 2s - loss: 0.6872 - accuracy: 0.54 - ETA: 2s - loss: 0.6858 - accuracy: 0.54 - ETA: 2s - loss: 0.6848 - accuracy: 0.54 - ETA: 2s - loss: 0.6840 - accuracy: 0.54 - ETA: 2s - loss: 0.6826 - accuracy: 0.55 - ETA: 2s - loss: 0.6816 - accuracy: 0.55 - ETA: 2s - loss: 0.6810 - accuracy: 0.55 - ETA: 2s - loss: 0.6795 - accuracy: 0.55 - ETA: 2s - loss: 0.6776 - accuracy: 0.56 - ETA: 2s - loss: 0.6765 - accuracy: 0.56 - ETA: 2s - loss: 0.6754 - accuracy: 0.56 - ETA: 2s - loss: 0.6737 - accuracy: 0.57 - ETA: 2s - loss: 0.6736 - accuracy: 0.57 - ETA: 1s - loss: 0.6735 - accuracy: 0.57 - ETA: 1s - loss: 0.6726 - accuracy: 0.57 - ETA: 1s - loss: 0.6713 - accuracy: 0.57 - ETA: 1s - loss: 0.6697 - accuracy: 0.58 - ETA: 1s - loss: 0.6684 - accuracy: 0.58 - ETA: 1s - loss: 0.6669 - accuracy: 0.58 - ETA: 1s - loss: 0.6655 - accuracy: 0.58 - ETA: 1s - loss: 0.6650 - accuracy: 0.58 - ETA: 1s - loss: 0.6642 - accuracy: 0.59 - ETA: 1s - loss: 0.6635 - accuracy: 0.59 - ETA: 1s - loss: 0.6630 - accuracy: 0.59 - ETA: 1s - loss: 0.6614 - accuracy: 0.59 - ETA: 1s - loss: 0.6607 - accuracy: 0.59 - ETA: 1s - loss: 0.6601 - accuracy: 0.59 - ETA: 1s - loss: 0.6591 - accuracy: 0.59 - ETA: 1s - loss: 0.6578 - accuracy: 0.60 - ETA: 1s - loss: 0.6568 - accuracy: 0.60 - ETA: 0s - loss: 0.6556 - accuracy: 0.60 - ETA: 0s - loss: 0.6546 - accuracy: 0.60 - ETA: 0s - loss: 0.6542 - accuracy: 0.60 - ETA: 0s - loss: 0.6537 - accuracy: 0.60 - ETA: 0s - loss: 0.6529 - accuracy: 0.60 - ETA: 0s - loss: 0.6523 - accuracy: 0.60 - ETA: 0s - loss: 0.6515 - accuracy: 0.60 - ETA: 0s - loss: 0.6510 - accuracy: 0.61 - ETA: 0s - loss: 0.6504 - accuracy: 0.61 - ETA: 0s - loss: 0.6502 - accuracy: 0.61 - ETA: 0s - loss: 0.6498 - accuracy: 0.61 - ETA: 0s - loss: 0.6496 - accuracy: 0.61 - ETA: 0s - loss: 0.6485 - accuracy: 0.61 - ETA: 0s - loss: 0.6469 - accuracy: 0.61 - ETA: 0s - loss: 0.6465 - accuracy: 0.61 - ETA: 0s - loss: 0.6458 - accuracy: 0.61 - ETA: 0s - loss: 0.6450 - accuracy: 0.61 - ETA: 0s - loss: 0.6453 - accuracy: 0.61 - ETA: 0s - loss: 0.6448 - accuracy: 0.61 - 4s 277us/step - loss: 0.6445 - accuracy: 0.6194 - val_loss: 0.5997 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:26 - loss: 0.6945 - accuracy: 0.50 - ETA: 10s - loss: 0.6916 - accuracy: 0.5900 - ETA: 6s - loss: 0.6911 - accuracy: 0.566 - ETA: 5s - loss: 0.6893 - accuracy: 0.55 - ETA: 4s - loss: 0.6905 - accuracy: 0.54 - ETA: 4s - loss: 0.6911 - accuracy: 0.53 - ETA: 3s - loss: 0.6898 - accuracy: 0.54 - ETA: 3s - loss: 0.6882 - accuracy: 0.55 - ETA: 3s - loss: 0.6873 - accuracy: 0.55 - ETA: 3s - loss: 0.6868 - accuracy: 0.55 - ETA: 3s - loss: 0.6871 - accuracy: 0.54 - ETA: 3s - loss: 0.6854 - accuracy: 0.55 - ETA: 3s - loss: 0.6832 - accuracy: 0.56 - ETA: 2s - loss: 0.6827 - accuracy: 0.56 - ETA: 2s - loss: 0.6816 - accuracy: 0.56 - ETA: 2s - loss: 0.6802 - accuracy: 0.56 - ETA: 2s - loss: 0.6786 - accuracy: 0.57 - ETA: 2s - loss: 0.6777 - accuracy: 0.57 - ETA: 2s - loss: 0.6778 - accuracy: 0.57 - ETA: 2s - loss: 0.6759 - accuracy: 0.57 - ETA: 2s - loss: 0.6763 - accuracy: 0.57 - ETA: 2s - loss: 0.6746 - accuracy: 0.58 - ETA: 2s - loss: 0.6733 - accuracy: 0.58 - ETA: 2s - loss: 0.6730 - accuracy: 0.58 - ETA: 2s - loss: 0.6725 - accuracy: 0.58 - ETA: 2s - loss: 0.6715 - accuracy: 0.59 - ETA: 2s - loss: 0.6703 - accuracy: 0.59 - ETA: 1s - loss: 0.6698 - accuracy: 0.59 - ETA: 1s - loss: 0.6692 - accuracy: 0.59 - ETA: 1s - loss: 0.6679 - accuracy: 0.60 - ETA: 1s - loss: 0.6674 - accuracy: 0.60 - ETA: 1s - loss: 0.6660 - accuracy: 0.60 - ETA: 1s - loss: 0.6645 - accuracy: 0.60 - ETA: 1s - loss: 0.6641 - accuracy: 0.61 - ETA: 1s - loss: 0.6637 - accuracy: 0.61 - ETA: 1s - loss: 0.6622 - accuracy: 0.61 - ETA: 1s - loss: 0.6614 - accuracy: 0.61 - ETA: 1s - loss: 0.6611 - accuracy: 0.61 - ETA: 1s - loss: 0.6604 - accuracy: 0.61 - ETA: 1s - loss: 0.6589 - accuracy: 0.62 - ETA: 1s - loss: 0.6587 - accuracy: 0.62 - ETA: 1s - loss: 0.6569 - accuracy: 0.62 - ETA: 1s - loss: 0.6562 - accuracy: 0.62 - ETA: 1s - loss: 0.6560 - accuracy: 0.62 - ETA: 1s - loss: 0.6544 - accuracy: 0.62 - ETA: 0s - loss: 0.6540 - accuracy: 0.62 - ETA: 0s - loss: 0.6529 - accuracy: 0.63 - ETA: 0s - loss: 0.6523 - accuracy: 0.63 - ETA: 0s - loss: 0.6513 - accuracy: 0.63 - ETA: 0s - loss: 0.6506 - accuracy: 0.63 - ETA: 0s - loss: 0.6508 - accuracy: 0.63 - ETA: 0s - loss: 0.6499 - accuracy: 0.63 - ETA: 0s - loss: 0.6497 - accuracy: 0.63 - ETA: 0s - loss: 0.6484 - accuracy: 0.64 - ETA: 0s - loss: 0.6474 - accuracy: 0.64 - ETA: 0s - loss: 0.6464 - accuracy: 0.64 - ETA: 0s - loss: 0.6454 - accuracy: 0.64 - ETA: 0s - loss: 0.6443 - accuracy: 0.64 - ETA: 0s - loss: 0.6435 - accuracy: 0.64 - ETA: 0s - loss: 0.6431 - accuracy: 0.64 - ETA: 0s - loss: 0.6426 - accuracy: 0.64 - ETA: 0s - loss: 0.6430 - accuracy: 0.64 - ETA: 0s - loss: 0.6424 - accuracy: 0.64 - ETA: 0s - loss: 0.6427 - accuracy: 0.64 - 4s 280us/step - loss: 0.6422 - accuracy: 0.6493 - val_loss: 0.5934 - val_accuracy: 0.7216\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 111us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 6:16 - loss: 0.6966 - accuracy: 0.50 - ETA: 10s - loss: 0.6937 - accuracy: 0.5240 - ETA: 6s - loss: 0.6931 - accuracy: 0.528 - ETA: 5s - loss: 0.6934 - accuracy: 0.51 - ETA: 4s - loss: 0.6934 - accuracy: 0.51 - ETA: 4s - loss: 0.6926 - accuracy: 0.52 - ETA: 4s - loss: 0.6922 - accuracy: 0.52 - ETA: 3s - loss: 0.6912 - accuracy: 0.52 - ETA: 3s - loss: 0.6912 - accuracy: 0.52 - ETA: 3s - loss: 0.6904 - accuracy: 0.53 - ETA: 3s - loss: 0.6901 - accuracy: 0.53 - ETA: 3s - loss: 0.6893 - accuracy: 0.54 - ETA: 3s - loss: 0.6886 - accuracy: 0.53 - ETA: 2s - loss: 0.6867 - accuracy: 0.54 - ETA: 2s - loss: 0.6859 - accuracy: 0.54 - ETA: 2s - loss: 0.6847 - accuracy: 0.55 - ETA: 2s - loss: 0.6841 - accuracy: 0.55 - ETA: 2s - loss: 0.6827 - accuracy: 0.55 - ETA: 2s - loss: 0.6817 - accuracy: 0.56 - ETA: 2s - loss: 0.6802 - accuracy: 0.56 - ETA: 2s - loss: 0.6792 - accuracy: 0.57 - ETA: 2s - loss: 0.6785 - accuracy: 0.57 - ETA: 2s - loss: 0.6765 - accuracy: 0.57 - ETA: 2s - loss: 0.6752 - accuracy: 0.58 - ETA: 2s - loss: 0.6745 - accuracy: 0.58 - ETA: 2s - loss: 0.6729 - accuracy: 0.58 - ETA: 2s - loss: 0.6722 - accuracy: 0.59 - ETA: 2s - loss: 0.6713 - accuracy: 0.59 - ETA: 1s - loss: 0.6707 - accuracy: 0.59 - ETA: 1s - loss: 0.6704 - accuracy: 0.59 - ETA: 1s - loss: 0.6693 - accuracy: 0.59 - ETA: 1s - loss: 0.6678 - accuracy: 0.59 - ETA: 1s - loss: 0.6672 - accuracy: 0.59 - ETA: 1s - loss: 0.6661 - accuracy: 0.60 - ETA: 1s - loss: 0.6649 - accuracy: 0.60 - ETA: 1s - loss: 0.6648 - accuracy: 0.60 - ETA: 1s - loss: 0.6640 - accuracy: 0.60 - ETA: 1s - loss: 0.6631 - accuracy: 0.60 - ETA: 1s - loss: 0.6621 - accuracy: 0.60 - ETA: 1s - loss: 0.6607 - accuracy: 0.61 - ETA: 1s - loss: 0.6593 - accuracy: 0.61 - ETA: 1s - loss: 0.6578 - accuracy: 0.61 - ETA: 1s - loss: 0.6566 - accuracy: 0.61 - ETA: 1s - loss: 0.6565 - accuracy: 0.61 - ETA: 1s - loss: 0.6556 - accuracy: 0.61 - ETA: 1s - loss: 0.6547 - accuracy: 0.62 - ETA: 0s - loss: 0.6538 - accuracy: 0.62 - ETA: 0s - loss: 0.6531 - accuracy: 0.62 - ETA: 0s - loss: 0.6530 - accuracy: 0.62 - ETA: 0s - loss: 0.6527 - accuracy: 0.62 - ETA: 0s - loss: 0.6525 - accuracy: 0.62 - ETA: 0s - loss: 0.6517 - accuracy: 0.62 - ETA: 0s - loss: 0.6511 - accuracy: 0.62 - ETA: 0s - loss: 0.6509 - accuracy: 0.62 - ETA: 0s - loss: 0.6497 - accuracy: 0.63 - ETA: 0s - loss: 0.6493 - accuracy: 0.63 - ETA: 0s - loss: 0.6485 - accuracy: 0.63 - ETA: 0s - loss: 0.6482 - accuracy: 0.63 - ETA: 0s - loss: 0.6477 - accuracy: 0.63 - ETA: 0s - loss: 0.6471 - accuracy: 0.63 - ETA: 0s - loss: 0.6466 - accuracy: 0.63 - ETA: 0s - loss: 0.6459 - accuracy: 0.63 - ETA: 0s - loss: 0.6449 - accuracy: 0.64 - ETA: 0s - loss: 0.6436 - accuracy: 0.64 - 4s 280us/step - loss: 0.6427 - accuracy: 0.6430 - val_loss: 0.5904 - val_accuracy: 0.7173\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:10 - loss: 0.6922 - accuracy: 0.50 - ETA: 10s - loss: 0.6940 - accuracy: 0.4856 - ETA: 6s - loss: 0.6937 - accuracy: 0.509 - ETA: 5s - loss: 0.6926 - accuracy: 0.52 - ETA: 4s - loss: 0.6929 - accuracy: 0.51 - ETA: 4s - loss: 0.6927 - accuracy: 0.52 - ETA: 3s - loss: 0.6921 - accuracy: 0.52 - ETA: 3s - loss: 0.6918 - accuracy: 0.53 - ETA: 3s - loss: 0.6914 - accuracy: 0.53 - ETA: 3s - loss: 0.6910 - accuracy: 0.53 - ETA: 3s - loss: 0.6895 - accuracy: 0.54 - ETA: 3s - loss: 0.6892 - accuracy: 0.54 - ETA: 3s - loss: 0.6889 - accuracy: 0.54 - ETA: 3s - loss: 0.6884 - accuracy: 0.54 - ETA: 2s - loss: 0.6880 - accuracy: 0.54 - ETA: 2s - loss: 0.6872 - accuracy: 0.55 - ETA: 2s - loss: 0.6859 - accuracy: 0.55 - ETA: 2s - loss: 0.6849 - accuracy: 0.56 - ETA: 2s - loss: 0.6832 - accuracy: 0.56 - ETA: 2s - loss: 0.6824 - accuracy: 0.56 - ETA: 2s - loss: 0.6805 - accuracy: 0.57 - ETA: 2s - loss: 0.6775 - accuracy: 0.57 - ETA: 2s - loss: 0.6755 - accuracy: 0.58 - ETA: 2s - loss: 0.6744 - accuracy: 0.58 - ETA: 2s - loss: 0.6733 - accuracy: 0.58 - ETA: 2s - loss: 0.6723 - accuracy: 0.58 - ETA: 2s - loss: 0.6719 - accuracy: 0.58 - ETA: 2s - loss: 0.6703 - accuracy: 0.59 - ETA: 1s - loss: 0.6696 - accuracy: 0.59 - ETA: 1s - loss: 0.6690 - accuracy: 0.59 - ETA: 1s - loss: 0.6687 - accuracy: 0.59 - ETA: 1s - loss: 0.6673 - accuracy: 0.59 - ETA: 1s - loss: 0.6661 - accuracy: 0.60 - ETA: 1s - loss: 0.6657 - accuracy: 0.60 - ETA: 1s - loss: 0.6647 - accuracy: 0.60 - ETA: 1s - loss: 0.6631 - accuracy: 0.60 - ETA: 1s - loss: 0.6614 - accuracy: 0.60 - ETA: 1s - loss: 0.6612 - accuracy: 0.60 - ETA: 1s - loss: 0.6605 - accuracy: 0.60 - ETA: 1s - loss: 0.6597 - accuracy: 0.60 - ETA: 1s - loss: 0.6590 - accuracy: 0.60 - ETA: 1s - loss: 0.6578 - accuracy: 0.61 - ETA: 1s - loss: 0.6571 - accuracy: 0.61 - ETA: 1s - loss: 0.6572 - accuracy: 0.61 - ETA: 1s - loss: 0.6563 - accuracy: 0.61 - ETA: 1s - loss: 0.6549 - accuracy: 0.61 - ETA: 0s - loss: 0.6546 - accuracy: 0.61 - ETA: 0s - loss: 0.6538 - accuracy: 0.61 - ETA: 0s - loss: 0.6533 - accuracy: 0.61 - ETA: 0s - loss: 0.6522 - accuracy: 0.61 - ETA: 0s - loss: 0.6514 - accuracy: 0.62 - ETA: 0s - loss: 0.6520 - accuracy: 0.61 - ETA: 0s - loss: 0.6512 - accuracy: 0.62 - ETA: 0s - loss: 0.6514 - accuracy: 0.62 - ETA: 0s - loss: 0.6507 - accuracy: 0.62 - ETA: 0s - loss: 0.6508 - accuracy: 0.62 - ETA: 0s - loss: 0.6500 - accuracy: 0.62 - ETA: 0s - loss: 0.6497 - accuracy: 0.62 - ETA: 0s - loss: 0.6491 - accuracy: 0.62 - ETA: 0s - loss: 0.6491 - accuracy: 0.62 - ETA: 0s - loss: 0.6485 - accuracy: 0.62 - ETA: 0s - loss: 0.6480 - accuracy: 0.62 - ETA: 0s - loss: 0.6476 - accuracy: 0.62 - ETA: 0s - loss: 0.6466 - accuracy: 0.62 - ETA: 0s - loss: 0.6463 - accuracy: 0.62 - 4s 282us/step - loss: 0.6460 - accuracy: 0.6276 - val_loss: 0.5996 - val_accuracy: 0.7031\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:13 - loss: 0.7011 - accuracy: 0.0000e+ - ETA: 10s - loss: 0.6938 - accuracy: 0.5000     - ETA: 6s - loss: 0.6934 - accuracy: 0.512 - ETA: 5s - loss: 0.6916 - accuracy: 0.53 - ETA: 4s - loss: 0.6936 - accuracy: 0.51 - ETA: 4s - loss: 0.6927 - accuracy: 0.51 - ETA: 3s - loss: 0.6910 - accuracy: 0.52 - ETA: 3s - loss: 0.6902 - accuracy: 0.52 - ETA: 3s - loss: 0.6902 - accuracy: 0.53 - ETA: 3s - loss: 0.6886 - accuracy: 0.53 - ETA: 3s - loss: 0.6888 - accuracy: 0.53 - ETA: 3s - loss: 0.6881 - accuracy: 0.54 - ETA: 2s - loss: 0.6868 - accuracy: 0.54 - ETA: 2s - loss: 0.6840 - accuracy: 0.55 - ETA: 2s - loss: 0.6830 - accuracy: 0.54 - ETA: 2s - loss: 0.6813 - accuracy: 0.55 - ETA: 2s - loss: 0.6792 - accuracy: 0.55 - ETA: 2s - loss: 0.6773 - accuracy: 0.56 - ETA: 2s - loss: 0.6763 - accuracy: 0.56 - ETA: 2s - loss: 0.6747 - accuracy: 0.57 - ETA: 2s - loss: 0.6732 - accuracy: 0.58 - ETA: 2s - loss: 0.6711 - accuracy: 0.58 - ETA: 2s - loss: 0.6703 - accuracy: 0.58 - ETA: 2s - loss: 0.6698 - accuracy: 0.58 - ETA: 2s - loss: 0.6679 - accuracy: 0.59 - ETA: 2s - loss: 0.6674 - accuracy: 0.59 - ETA: 1s - loss: 0.6663 - accuracy: 0.59 - ETA: 1s - loss: 0.6651 - accuracy: 0.59 - ETA: 1s - loss: 0.6632 - accuracy: 0.60 - ETA: 1s - loss: 0.6622 - accuracy: 0.60 - ETA: 1s - loss: 0.6617 - accuracy: 0.60 - ETA: 1s - loss: 0.6621 - accuracy: 0.60 - ETA: 1s - loss: 0.6613 - accuracy: 0.60 - ETA: 1s - loss: 0.6593 - accuracy: 0.61 - ETA: 1s - loss: 0.6582 - accuracy: 0.61 - ETA: 1s - loss: 0.6575 - accuracy: 0.61 - ETA: 1s - loss: 0.6558 - accuracy: 0.61 - ETA: 1s - loss: 0.6545 - accuracy: 0.62 - ETA: 1s - loss: 0.6545 - accuracy: 0.62 - ETA: 1s - loss: 0.6533 - accuracy: 0.62 - ETA: 1s - loss: 0.6516 - accuracy: 0.62 - ETA: 1s - loss: 0.6513 - accuracy: 0.62 - ETA: 1s - loss: 0.6506 - accuracy: 0.63 - ETA: 1s - loss: 0.6498 - accuracy: 0.63 - ETA: 0s - loss: 0.6487 - accuracy: 0.63 - ETA: 0s - loss: 0.6477 - accuracy: 0.63 - ETA: 0s - loss: 0.6473 - accuracy: 0.63 - ETA: 0s - loss: 0.6467 - accuracy: 0.63 - ETA: 0s - loss: 0.6460 - accuracy: 0.63 - ETA: 0s - loss: 0.6455 - accuracy: 0.63 - ETA: 0s - loss: 0.6448 - accuracy: 0.63 - ETA: 0s - loss: 0.6442 - accuracy: 0.64 - ETA: 0s - loss: 0.6448 - accuracy: 0.64 - ETA: 0s - loss: 0.6447 - accuracy: 0.64 - ETA: 0s - loss: 0.6450 - accuracy: 0.64 - ETA: 0s - loss: 0.6446 - accuracy: 0.64 - ETA: 0s - loss: 0.6438 - accuracy: 0.64 - ETA: 0s - loss: 0.6433 - accuracy: 0.64 - ETA: 0s - loss: 0.6419 - accuracy: 0.64 - ETA: 0s - loss: 0.6415 - accuracy: 0.64 - ETA: 0s - loss: 0.6410 - accuracy: 0.64 - ETA: 0s - loss: 0.6409 - accuracy: 0.64 - 3s 272us/step - loss: 0.6408 - accuracy: 0.6481 - val_loss: 0.5853 - val_accuracy: 0.7223\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 104us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:13 - loss: 0.7045 - accuracy: 0.25 - ETA: 10s - loss: 0.6955 - accuracy: 0.4471 - ETA: 6s - loss: 0.6951 - accuracy: 0.470 - ETA: 5s - loss: 0.6936 - accuracy: 0.50 - ETA: 4s - loss: 0.6927 - accuracy: 0.51 - ETA: 4s - loss: 0.6914 - accuracy: 0.52 - ETA: 4s - loss: 0.6890 - accuracy: 0.54 - ETA: 3s - loss: 0.6872 - accuracy: 0.55 - ETA: 3s - loss: 0.6853 - accuracy: 0.56 - ETA: 3s - loss: 0.6839 - accuracy: 0.57 - ETA: 3s - loss: 0.6822 - accuracy: 0.57 - ETA: 3s - loss: 0.6795 - accuracy: 0.58 - ETA: 3s - loss: 0.6776 - accuracy: 0.58 - ETA: 3s - loss: 0.6750 - accuracy: 0.59 - ETA: 2s - loss: 0.6704 - accuracy: 0.60 - ETA: 2s - loss: 0.6685 - accuracy: 0.60 - ETA: 2s - loss: 0.6658 - accuracy: 0.60 - ETA: 2s - loss: 0.6649 - accuracy: 0.61 - ETA: 2s - loss: 0.6632 - accuracy: 0.61 - ETA: 2s - loss: 0.6618 - accuracy: 0.61 - ETA: 2s - loss: 0.6601 - accuracy: 0.62 - ETA: 2s - loss: 0.6581 - accuracy: 0.62 - ETA: 2s - loss: 0.6546 - accuracy: 0.63 - ETA: 2s - loss: 0.6530 - accuracy: 0.63 - ETA: 2s - loss: 0.6532 - accuracy: 0.63 - ETA: 2s - loss: 0.6529 - accuracy: 0.63 - ETA: 2s - loss: 0.6510 - accuracy: 0.63 - ETA: 2s - loss: 0.6497 - accuracy: 0.64 - ETA: 2s - loss: 0.6495 - accuracy: 0.63 - ETA: 1s - loss: 0.6483 - accuracy: 0.64 - ETA: 1s - loss: 0.6472 - accuracy: 0.64 - ETA: 1s - loss: 0.6456 - accuracy: 0.64 - ETA: 1s - loss: 0.6454 - accuracy: 0.64 - ETA: 1s - loss: 0.6441 - accuracy: 0.64 - ETA: 1s - loss: 0.6426 - accuracy: 0.64 - ETA: 1s - loss: 0.6414 - accuracy: 0.65 - ETA: 1s - loss: 0.6399 - accuracy: 0.65 - ETA: 1s - loss: 0.6389 - accuracy: 0.65 - ETA: 1s - loss: 0.6378 - accuracy: 0.65 - ETA: 1s - loss: 0.6380 - accuracy: 0.65 - ETA: 1s - loss: 0.6365 - accuracy: 0.65 - ETA: 1s - loss: 0.6366 - accuracy: 0.65 - ETA: 1s - loss: 0.6353 - accuracy: 0.65 - ETA: 1s - loss: 0.6347 - accuracy: 0.65 - ETA: 1s - loss: 0.6346 - accuracy: 0.65 - ETA: 1s - loss: 0.6346 - accuracy: 0.65 - ETA: 1s - loss: 0.6348 - accuracy: 0.65 - ETA: 1s - loss: 0.6339 - accuracy: 0.65 - ETA: 1s - loss: 0.6337 - accuracy: 0.65 - ETA: 0s - loss: 0.6329 - accuracy: 0.66 - ETA: 0s - loss: 0.6318 - accuracy: 0.66 - ETA: 0s - loss: 0.6303 - accuracy: 0.66 - ETA: 0s - loss: 0.6296 - accuracy: 0.66 - ETA: 0s - loss: 0.6285 - accuracy: 0.66 - ETA: 0s - loss: 0.6290 - accuracy: 0.66 - ETA: 0s - loss: 0.6289 - accuracy: 0.66 - ETA: 0s - loss: 0.6285 - accuracy: 0.66 - ETA: 0s - loss: 0.6284 - accuracy: 0.66 - ETA: 0s - loss: 0.6284 - accuracy: 0.66 - ETA: 0s - loss: 0.6288 - accuracy: 0.66 - ETA: 0s - loss: 0.6291 - accuracy: 0.66 - ETA: 0s - loss: 0.6293 - accuracy: 0.66 - ETA: 0s - loss: 0.6286 - accuracy: 0.66 - ETA: 0s - loss: 0.6277 - accuracy: 0.66 - ETA: 0s - loss: 0.6272 - accuracy: 0.66 - ETA: 0s - loss: 0.6266 - accuracy: 0.66 - 4s 287us/step - loss: 0.6263 - accuracy: 0.6694 - val_loss: 0.5790 - val_accuracy: 0.7209\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 6:04 - loss: 0.6949 - accuracy: 0.75 - ETA: 9s - loss: 0.6924 - accuracy: 0.5529 - ETA: 6s - loss: 0.6911 - accuracy: 0.55 - ETA: 5s - loss: 0.6915 - accuracy: 0.53 - ETA: 4s - loss: 0.6908 - accuracy: 0.53 - ETA: 4s - loss: 0.6900 - accuracy: 0.54 - ETA: 3s - loss: 0.6892 - accuracy: 0.54 - ETA: 3s - loss: 0.6890 - accuracy: 0.54 - ETA: 3s - loss: 0.6881 - accuracy: 0.54 - ETA: 3s - loss: 0.6872 - accuracy: 0.55 - ETA: 3s - loss: 0.6863 - accuracy: 0.56 - ETA: 3s - loss: 0.6852 - accuracy: 0.56 - ETA: 2s - loss: 0.6839 - accuracy: 0.56 - ETA: 2s - loss: 0.6835 - accuracy: 0.56 - ETA: 2s - loss: 0.6821 - accuracy: 0.57 - ETA: 2s - loss: 0.6815 - accuracy: 0.57 - ETA: 2s - loss: 0.6798 - accuracy: 0.58 - ETA: 2s - loss: 0.6783 - accuracy: 0.58 - ETA: 2s - loss: 0.6779 - accuracy: 0.58 - ETA: 2s - loss: 0.6776 - accuracy: 0.58 - ETA: 2s - loss: 0.6770 - accuracy: 0.58 - ETA: 2s - loss: 0.6760 - accuracy: 0.59 - ETA: 2s - loss: 0.6738 - accuracy: 0.59 - ETA: 2s - loss: 0.6732 - accuracy: 0.59 - ETA: 2s - loss: 0.6711 - accuracy: 0.60 - ETA: 2s - loss: 0.6698 - accuracy: 0.60 - ETA: 2s - loss: 0.6685 - accuracy: 0.60 - ETA: 1s - loss: 0.6663 - accuracy: 0.61 - ETA: 1s - loss: 0.6648 - accuracy: 0.61 - ETA: 1s - loss: 0.6627 - accuracy: 0.61 - ETA: 1s - loss: 0.6618 - accuracy: 0.61 - ETA: 1s - loss: 0.6610 - accuracy: 0.62 - ETA: 1s - loss: 0.6609 - accuracy: 0.62 - ETA: 1s - loss: 0.6590 - accuracy: 0.62 - ETA: 1s - loss: 0.6578 - accuracy: 0.62 - ETA: 1s - loss: 0.6548 - accuracy: 0.62 - ETA: 1s - loss: 0.6546 - accuracy: 0.63 - ETA: 1s - loss: 0.6529 - accuracy: 0.63 - ETA: 1s - loss: 0.6515 - accuracy: 0.63 - ETA: 1s - loss: 0.6508 - accuracy: 0.63 - ETA: 1s - loss: 0.6490 - accuracy: 0.63 - ETA: 1s - loss: 0.6480 - accuracy: 0.63 - ETA: 1s - loss: 0.6472 - accuracy: 0.64 - ETA: 1s - loss: 0.6471 - accuracy: 0.64 - ETA: 1s - loss: 0.6461 - accuracy: 0.64 - ETA: 0s - loss: 0.6453 - accuracy: 0.64 - ETA: 0s - loss: 0.6447 - accuracy: 0.64 - ETA: 0s - loss: 0.6438 - accuracy: 0.64 - ETA: 0s - loss: 0.6429 - accuracy: 0.64 - ETA: 0s - loss: 0.6424 - accuracy: 0.64 - ETA: 0s - loss: 0.6412 - accuracy: 0.64 - ETA: 0s - loss: 0.6398 - accuracy: 0.65 - ETA: 0s - loss: 0.6385 - accuracy: 0.65 - ETA: 0s - loss: 0.6384 - accuracy: 0.65 - ETA: 0s - loss: 0.6369 - accuracy: 0.65 - ETA: 0s - loss: 0.6369 - accuracy: 0.65 - ETA: 0s - loss: 0.6364 - accuracy: 0.65 - ETA: 0s - loss: 0.6357 - accuracy: 0.65 - ETA: 0s - loss: 0.6352 - accuracy: 0.65 - ETA: 0s - loss: 0.6341 - accuracy: 0.65 - ETA: 0s - loss: 0.6335 - accuracy: 0.65 - ETA: 0s - loss: 0.6328 - accuracy: 0.65 - ETA: 0s - loss: 0.6321 - accuracy: 0.65 - ETA: 0s - loss: 0.6325 - accuracy: 0.65 - 4s 278us/step - loss: 0.6325 - accuracy: 0.6581 - val_loss: 0.5865 - val_accuracy: 0.7081\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:13 - loss: 0.6983 - accuracy: 0.25 - ETA: 10s - loss: 0.6953 - accuracy: 0.4650 - ETA: 6s - loss: 0.6932 - accuracy: 0.522 - ETA: 5s - loss: 0.6920 - accuracy: 0.52 - ETA: 4s - loss: 0.6903 - accuracy: 0.54 - ETA: 4s - loss: 0.6896 - accuracy: 0.54 - ETA: 3s - loss: 0.6875 - accuracy: 0.55 - ETA: 3s - loss: 0.6869 - accuracy: 0.55 - ETA: 3s - loss: 0.6860 - accuracy: 0.56 - ETA: 3s - loss: 0.6850 - accuracy: 0.57 - ETA: 3s - loss: 0.6832 - accuracy: 0.57 - ETA: 3s - loss: 0.6803 - accuracy: 0.58 - ETA: 3s - loss: 0.6781 - accuracy: 0.58 - ETA: 2s - loss: 0.6765 - accuracy: 0.58 - ETA: 2s - loss: 0.6766 - accuracy: 0.58 - ETA: 2s - loss: 0.6746 - accuracy: 0.59 - ETA: 2s - loss: 0.6720 - accuracy: 0.59 - ETA: 2s - loss: 0.6689 - accuracy: 0.60 - ETA: 2s - loss: 0.6684 - accuracy: 0.60 - ETA: 2s - loss: 0.6663 - accuracy: 0.61 - ETA: 2s - loss: 0.6654 - accuracy: 0.61 - ETA: 2s - loss: 0.6627 - accuracy: 0.61 - ETA: 2s - loss: 0.6615 - accuracy: 0.62 - ETA: 2s - loss: 0.6596 - accuracy: 0.62 - ETA: 2s - loss: 0.6590 - accuracy: 0.62 - ETA: 2s - loss: 0.6583 - accuracy: 0.62 - ETA: 2s - loss: 0.6570 - accuracy: 0.63 - ETA: 2s - loss: 0.6553 - accuracy: 0.63 - ETA: 1s - loss: 0.6543 - accuracy: 0.63 - ETA: 1s - loss: 0.6525 - accuracy: 0.63 - ETA: 1s - loss: 0.6523 - accuracy: 0.63 - ETA: 1s - loss: 0.6513 - accuracy: 0.63 - ETA: 1s - loss: 0.6507 - accuracy: 0.63 - ETA: 1s - loss: 0.6502 - accuracy: 0.64 - ETA: 1s - loss: 0.6491 - accuracy: 0.64 - ETA: 1s - loss: 0.6474 - accuracy: 0.64 - ETA: 1s - loss: 0.6448 - accuracy: 0.64 - ETA: 1s - loss: 0.6439 - accuracy: 0.64 - ETA: 1s - loss: 0.6427 - accuracy: 0.65 - ETA: 1s - loss: 0.6427 - accuracy: 0.65 - ETA: 1s - loss: 0.6410 - accuracy: 0.65 - ETA: 1s - loss: 0.6404 - accuracy: 0.65 - ETA: 1s - loss: 0.6401 - accuracy: 0.65 - ETA: 1s - loss: 0.6392 - accuracy: 0.65 - ETA: 1s - loss: 0.6377 - accuracy: 0.65 - ETA: 0s - loss: 0.6364 - accuracy: 0.65 - ETA: 0s - loss: 0.6366 - accuracy: 0.65 - ETA: 0s - loss: 0.6359 - accuracy: 0.65 - ETA: 0s - loss: 0.6357 - accuracy: 0.65 - ETA: 0s - loss: 0.6346 - accuracy: 0.65 - ETA: 0s - loss: 0.6340 - accuracy: 0.66 - ETA: 0s - loss: 0.6340 - accuracy: 0.66 - ETA: 0s - loss: 0.6336 - accuracy: 0.66 - ETA: 0s - loss: 0.6328 - accuracy: 0.66 - ETA: 0s - loss: 0.6324 - accuracy: 0.66 - ETA: 0s - loss: 0.6311 - accuracy: 0.66 - ETA: 0s - loss: 0.6303 - accuracy: 0.66 - ETA: 0s - loss: 0.6294 - accuracy: 0.66 - ETA: 0s - loss: 0.6293 - accuracy: 0.66 - ETA: 0s - loss: 0.6297 - accuracy: 0.66 - ETA: 0s - loss: 0.6288 - accuracy: 0.66 - ETA: 0s - loss: 0.6279 - accuracy: 0.66 - ETA: 0s - loss: 0.6278 - accuracy: 0.66 - ETA: 0s - loss: 0.6272 - accuracy: 0.66 - 4s 279us/step - loss: 0.6267 - accuracy: 0.6690 - val_loss: 0.5750 - val_accuracy: 0.7180\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 111us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:16 - loss: 0.6930 - accuracy: 0.50 - ETA: 10s - loss: 0.6941 - accuracy: 0.5000 - ETA: 6s - loss: 0.6938 - accuracy: 0.507 - ETA: 5s - loss: 0.6931 - accuracy: 0.52 - ETA: 4s - loss: 0.6919 - accuracy: 0.54 - ETA: 4s - loss: 0.6919 - accuracy: 0.53 - ETA: 4s - loss: 0.6906 - accuracy: 0.53 - ETA: 3s - loss: 0.6906 - accuracy: 0.53 - ETA: 3s - loss: 0.6897 - accuracy: 0.54 - ETA: 3s - loss: 0.6896 - accuracy: 0.54 - ETA: 3s - loss: 0.6889 - accuracy: 0.54 - ETA: 3s - loss: 0.6874 - accuracy: 0.55 - ETA: 3s - loss: 0.6861 - accuracy: 0.55 - ETA: 3s - loss: 0.6852 - accuracy: 0.56 - ETA: 2s - loss: 0.6839 - accuracy: 0.56 - ETA: 2s - loss: 0.6822 - accuracy: 0.57 - ETA: 2s - loss: 0.6805 - accuracy: 0.57 - ETA: 2s - loss: 0.6792 - accuracy: 0.57 - ETA: 2s - loss: 0.6775 - accuracy: 0.58 - ETA: 2s - loss: 0.6754 - accuracy: 0.58 - ETA: 2s - loss: 0.6728 - accuracy: 0.59 - ETA: 2s - loss: 0.6715 - accuracy: 0.59 - ETA: 2s - loss: 0.6692 - accuracy: 0.59 - ETA: 2s - loss: 0.6682 - accuracy: 0.59 - ETA: 2s - loss: 0.6679 - accuracy: 0.60 - ETA: 2s - loss: 0.6666 - accuracy: 0.60 - ETA: 2s - loss: 0.6661 - accuracy: 0.60 - ETA: 2s - loss: 0.6648 - accuracy: 0.60 - ETA: 2s - loss: 0.6638 - accuracy: 0.60 - ETA: 1s - loss: 0.6647 - accuracy: 0.60 - ETA: 1s - loss: 0.6631 - accuracy: 0.60 - ETA: 1s - loss: 0.6622 - accuracy: 0.60 - ETA: 1s - loss: 0.6615 - accuracy: 0.60 - ETA: 1s - loss: 0.6613 - accuracy: 0.60 - ETA: 1s - loss: 0.6606 - accuracy: 0.61 - ETA: 1s - loss: 0.6602 - accuracy: 0.61 - ETA: 1s - loss: 0.6596 - accuracy: 0.61 - ETA: 1s - loss: 0.6582 - accuracy: 0.61 - ETA: 1s - loss: 0.6572 - accuracy: 0.61 - ETA: 1s - loss: 0.6557 - accuracy: 0.61 - ETA: 1s - loss: 0.6549 - accuracy: 0.61 - ETA: 1s - loss: 0.6535 - accuracy: 0.62 - ETA: 1s - loss: 0.6524 - accuracy: 0.62 - ETA: 1s - loss: 0.6525 - accuracy: 0.62 - ETA: 1s - loss: 0.6517 - accuracy: 0.62 - ETA: 1s - loss: 0.6511 - accuracy: 0.62 - ETA: 1s - loss: 0.6499 - accuracy: 0.62 - ETA: 0s - loss: 0.6490 - accuracy: 0.62 - ETA: 0s - loss: 0.6483 - accuracy: 0.63 - ETA: 0s - loss: 0.6485 - accuracy: 0.63 - ETA: 0s - loss: 0.6481 - accuracy: 0.63 - ETA: 0s - loss: 0.6474 - accuracy: 0.63 - ETA: 0s - loss: 0.6465 - accuracy: 0.63 - ETA: 0s - loss: 0.6455 - accuracy: 0.63 - ETA: 0s - loss: 0.6451 - accuracy: 0.63 - ETA: 0s - loss: 0.6436 - accuracy: 0.63 - ETA: 0s - loss: 0.6433 - accuracy: 0.63 - ETA: 0s - loss: 0.6424 - accuracy: 0.64 - ETA: 0s - loss: 0.6421 - accuracy: 0.64 - ETA: 0s - loss: 0.6420 - accuracy: 0.64 - ETA: 0s - loss: 0.6412 - accuracy: 0.64 - ETA: 0s - loss: 0.6402 - accuracy: 0.64 - ETA: 0s - loss: 0.6395 - accuracy: 0.64 - ETA: 0s - loss: 0.6390 - accuracy: 0.64 - ETA: 0s - loss: 0.6387 - accuracy: 0.64 - 4s 284us/step - loss: 0.6384 - accuracy: 0.6459 - val_loss: 0.5853 - val_accuracy: 0.7180\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 106us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:13 - loss: 0.7013 - accuracy: 0.50 - ETA: 9s - loss: 0.6954 - accuracy: 0.5045 - ETA: 6s - loss: 0.6952 - accuracy: 0.48 - ETA: 5s - loss: 0.6945 - accuracy: 0.50 - ETA: 4s - loss: 0.6922 - accuracy: 0.53 - ETA: 4s - loss: 0.6916 - accuracy: 0.53 - ETA: 3s - loss: 0.6914 - accuracy: 0.53 - ETA: 3s - loss: 0.6904 - accuracy: 0.54 - ETA: 3s - loss: 0.6895 - accuracy: 0.55 - ETA: 3s - loss: 0.6882 - accuracy: 0.55 - ETA: 3s - loss: 0.6864 - accuracy: 0.56 - ETA: 3s - loss: 0.6855 - accuracy: 0.56 - ETA: 3s - loss: 0.6849 - accuracy: 0.56 - ETA: 2s - loss: 0.6830 - accuracy: 0.57 - ETA: 2s - loss: 0.6816 - accuracy: 0.57 - ETA: 2s - loss: 0.6797 - accuracy: 0.58 - ETA: 2s - loss: 0.6784 - accuracy: 0.58 - ETA: 2s - loss: 0.6770 - accuracy: 0.58 - ETA: 2s - loss: 0.6748 - accuracy: 0.59 - ETA: 2s - loss: 0.6723 - accuracy: 0.60 - ETA: 2s - loss: 0.6705 - accuracy: 0.60 - ETA: 2s - loss: 0.6694 - accuracy: 0.60 - ETA: 2s - loss: 0.6675 - accuracy: 0.60 - ETA: 2s - loss: 0.6658 - accuracy: 0.61 - ETA: 2s - loss: 0.6642 - accuracy: 0.61 - ETA: 1s - loss: 0.6627 - accuracy: 0.61 - ETA: 1s - loss: 0.6620 - accuracy: 0.61 - ETA: 1s - loss: 0.6603 - accuracy: 0.61 - ETA: 1s - loss: 0.6595 - accuracy: 0.61 - ETA: 1s - loss: 0.6590 - accuracy: 0.61 - ETA: 1s - loss: 0.6568 - accuracy: 0.62 - ETA: 1s - loss: 0.6547 - accuracy: 0.62 - ETA: 1s - loss: 0.6534 - accuracy: 0.62 - ETA: 1s - loss: 0.6512 - accuracy: 0.62 - ETA: 1s - loss: 0.6487 - accuracy: 0.63 - ETA: 1s - loss: 0.6486 - accuracy: 0.63 - ETA: 1s - loss: 0.6483 - accuracy: 0.63 - ETA: 1s - loss: 0.6465 - accuracy: 0.63 - ETA: 1s - loss: 0.6452 - accuracy: 0.63 - ETA: 1s - loss: 0.6437 - accuracy: 0.64 - ETA: 1s - loss: 0.6430 - accuracy: 0.64 - ETA: 1s - loss: 0.6429 - accuracy: 0.64 - ETA: 0s - loss: 0.6432 - accuracy: 0.64 - ETA: 0s - loss: 0.6419 - accuracy: 0.64 - ETA: 0s - loss: 0.6412 - accuracy: 0.64 - ETA: 0s - loss: 0.6400 - accuracy: 0.64 - ETA: 0s - loss: 0.6386 - accuracy: 0.64 - ETA: 0s - loss: 0.6368 - accuracy: 0.64 - ETA: 0s - loss: 0.6369 - accuracy: 0.64 - ETA: 0s - loss: 0.6360 - accuracy: 0.65 - ETA: 0s - loss: 0.6359 - accuracy: 0.65 - ETA: 0s - loss: 0.6351 - accuracy: 0.65 - ETA: 0s - loss: 0.6346 - accuracy: 0.65 - ETA: 0s - loss: 0.6339 - accuracy: 0.65 - ETA: 0s - loss: 0.6342 - accuracy: 0.65 - ETA: 0s - loss: 0.6335 - accuracy: 0.65 - ETA: 0s - loss: 0.6323 - accuracy: 0.65 - ETA: 0s - loss: 0.6322 - accuracy: 0.65 - ETA: 0s - loss: 0.6312 - accuracy: 0.65 - ETA: 0s - loss: 0.6302 - accuracy: 0.65 - 3s 264us/step - loss: 0.6306 - accuracy: 0.6580 - val_loss: 0.5934 - val_accuracy: 0.6889\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 5:10 - loss: 0.6788 - accuracy: 0.75 - ETA: 8s - loss: 0.6952 - accuracy: 0.5136 - ETA: 5s - loss: 0.6928 - accuracy: 0.51 - ETA: 4s - loss: 0.6929 - accuracy: 0.51 - ETA: 4s - loss: 0.6896 - accuracy: 0.52 - ETA: 3s - loss: 0.6903 - accuracy: 0.51 - ETA: 3s - loss: 0.6906 - accuracy: 0.51 - ETA: 3s - loss: 0.6904 - accuracy: 0.50 - ETA: 3s - loss: 0.6900 - accuracy: 0.50 - ETA: 3s - loss: 0.6894 - accuracy: 0.51 - ETA: 2s - loss: 0.6888 - accuracy: 0.51 - ETA: 2s - loss: 0.6885 - accuracy: 0.51 - ETA: 2s - loss: 0.6887 - accuracy: 0.51 - ETA: 2s - loss: 0.6885 - accuracy: 0.51 - ETA: 2s - loss: 0.6879 - accuracy: 0.52 - ETA: 2s - loss: 0.6875 - accuracy: 0.52 - ETA: 2s - loss: 0.6872 - accuracy: 0.52 - ETA: 2s - loss: 0.6863 - accuracy: 0.53 - ETA: 2s - loss: 0.6862 - accuracy: 0.53 - ETA: 2s - loss: 0.6856 - accuracy: 0.53 - ETA: 2s - loss: 0.6852 - accuracy: 0.53 - ETA: 2s - loss: 0.6852 - accuracy: 0.53 - ETA: 2s - loss: 0.6849 - accuracy: 0.53 - ETA: 1s - loss: 0.6843 - accuracy: 0.53 - ETA: 1s - loss: 0.6841 - accuracy: 0.53 - ETA: 1s - loss: 0.6837 - accuracy: 0.54 - ETA: 1s - loss: 0.6837 - accuracy: 0.54 - ETA: 1s - loss: 0.6833 - accuracy: 0.54 - ETA: 1s - loss: 0.6828 - accuracy: 0.54 - ETA: 1s - loss: 0.6829 - accuracy: 0.54 - ETA: 1s - loss: 0.6823 - accuracy: 0.54 - ETA: 1s - loss: 0.6822 - accuracy: 0.55 - ETA: 1s - loss: 0.6815 - accuracy: 0.55 - ETA: 1s - loss: 0.6816 - accuracy: 0.55 - ETA: 1s - loss: 0.6816 - accuracy: 0.55 - ETA: 1s - loss: 0.6812 - accuracy: 0.55 - ETA: 1s - loss: 0.6808 - accuracy: 0.55 - ETA: 1s - loss: 0.6806 - accuracy: 0.55 - ETA: 1s - loss: 0.6806 - accuracy: 0.55 - ETA: 1s - loss: 0.6803 - accuracy: 0.56 - ETA: 1s - loss: 0.6801 - accuracy: 0.56 - ETA: 0s - loss: 0.6798 - accuracy: 0.56 - ETA: 0s - loss: 0.6793 - accuracy: 0.56 - ETA: 0s - loss: 0.6791 - accuracy: 0.56 - ETA: 0s - loss: 0.6789 - accuracy: 0.57 - ETA: 0s - loss: 0.6783 - accuracy: 0.57 - ETA: 0s - loss: 0.6782 - accuracy: 0.57 - ETA: 0s - loss: 0.6780 - accuracy: 0.57 - ETA: 0s - loss: 0.6780 - accuracy: 0.57 - ETA: 0s - loss: 0.6779 - accuracy: 0.57 - ETA: 0s - loss: 0.6775 - accuracy: 0.58 - ETA: 0s - loss: 0.6775 - accuracy: 0.58 - ETA: 0s - loss: 0.6773 - accuracy: 0.58 - ETA: 0s - loss: 0.6771 - accuracy: 0.58 - ETA: 0s - loss: 0.6766 - accuracy: 0.58 - ETA: 0s - loss: 0.6761 - accuracy: 0.58 - ETA: 0s - loss: 0.6757 - accuracy: 0.58 - ETA: 0s - loss: 0.6755 - accuracy: 0.58 - ETA: 0s - loss: 0.6752 - accuracy: 0.58 - 3s 257us/step - loss: 0.6749 - accuracy: 0.5895 - val_loss: 0.6550 - val_accuracy: 0.6165\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:23 - loss: 0.6905 - accuracy: 0.50 - ETA: 8s - loss: 0.6927 - accuracy: 0.5636 - ETA: 5s - loss: 0.6919 - accuracy: 0.53 - ETA: 4s - loss: 0.6920 - accuracy: 0.52 - ETA: 4s - loss: 0.6922 - accuracy: 0.52 - ETA: 3s - loss: 0.6917 - accuracy: 0.52 - ETA: 3s - loss: 0.6911 - accuracy: 0.52 - ETA: 3s - loss: 0.6910 - accuracy: 0.51 - ETA: 3s - loss: 0.6900 - accuracy: 0.52 - ETA: 3s - loss: 0.6895 - accuracy: 0.52 - ETA: 2s - loss: 0.6893 - accuracy: 0.52 - ETA: 2s - loss: 0.6892 - accuracy: 0.52 - ETA: 2s - loss: 0.6895 - accuracy: 0.52 - ETA: 2s - loss: 0.6892 - accuracy: 0.52 - ETA: 2s - loss: 0.6885 - accuracy: 0.53 - ETA: 2s - loss: 0.6889 - accuracy: 0.52 - ETA: 2s - loss: 0.6894 - accuracy: 0.52 - ETA: 2s - loss: 0.6892 - accuracy: 0.52 - ETA: 2s - loss: 0.6890 - accuracy: 0.52 - ETA: 2s - loss: 0.6890 - accuracy: 0.52 - ETA: 2s - loss: 0.6890 - accuracy: 0.52 - ETA: 2s - loss: 0.6889 - accuracy: 0.52 - ETA: 2s - loss: 0.6885 - accuracy: 0.52 - ETA: 1s - loss: 0.6886 - accuracy: 0.52 - ETA: 1s - loss: 0.6887 - accuracy: 0.52 - ETA: 1s - loss: 0.6887 - accuracy: 0.51 - ETA: 1s - loss: 0.6887 - accuracy: 0.51 - ETA: 1s - loss: 0.6887 - accuracy: 0.51 - ETA: 1s - loss: 0.6886 - accuracy: 0.51 - ETA: 1s - loss: 0.6886 - accuracy: 0.51 - ETA: 1s - loss: 0.6885 - accuracy: 0.51 - ETA: 1s - loss: 0.6885 - accuracy: 0.51 - ETA: 1s - loss: 0.6886 - accuracy: 0.51 - ETA: 1s - loss: 0.6885 - accuracy: 0.51 - ETA: 1s - loss: 0.6881 - accuracy: 0.51 - ETA: 1s - loss: 0.6878 - accuracy: 0.51 - ETA: 1s - loss: 0.6878 - accuracy: 0.51 - ETA: 1s - loss: 0.6878 - accuracy: 0.51 - ETA: 1s - loss: 0.6877 - accuracy: 0.51 - ETA: 1s - loss: 0.6876 - accuracy: 0.51 - ETA: 0s - loss: 0.6875 - accuracy: 0.51 - ETA: 0s - loss: 0.6873 - accuracy: 0.51 - ETA: 0s - loss: 0.6873 - accuracy: 0.51 - ETA: 0s - loss: 0.6868 - accuracy: 0.51 - ETA: 0s - loss: 0.6869 - accuracy: 0.51 - ETA: 0s - loss: 0.6866 - accuracy: 0.51 - ETA: 0s - loss: 0.6864 - accuracy: 0.51 - ETA: 0s - loss: 0.6865 - accuracy: 0.51 - ETA: 0s - loss: 0.6863 - accuracy: 0.51 - ETA: 0s - loss: 0.6861 - accuracy: 0.51 - ETA: 0s - loss: 0.6860 - accuracy: 0.51 - ETA: 0s - loss: 0.6860 - accuracy: 0.51 - ETA: 0s - loss: 0.6859 - accuracy: 0.51 - ETA: 0s - loss: 0.6858 - accuracy: 0.51 - ETA: 0s - loss: 0.6857 - accuracy: 0.51 - ETA: 0s - loss: 0.6857 - accuracy: 0.51 - ETA: 0s - loss: 0.6853 - accuracy: 0.51 - ETA: 0s - loss: 0.6852 - accuracy: 0.51 - 3s 255us/step - loss: 0.6851 - accuracy: 0.5148 - val_loss: 0.6720 - val_accuracy: 0.5604\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:04 - loss: 0.6915 - accuracy: 0.75 - ETA: 8s - loss: 0.6918 - accuracy: 0.5636 - ETA: 5s - loss: 0.6921 - accuracy: 0.55 - ETA: 4s - loss: 0.6926 - accuracy: 0.53 - ETA: 4s - loss: 0.6923 - accuracy: 0.53 - ETA: 3s - loss: 0.6918 - accuracy: 0.54 - ETA: 3s - loss: 0.6911 - accuracy: 0.54 - ETA: 3s - loss: 0.6903 - accuracy: 0.54 - ETA: 3s - loss: 0.6893 - accuracy: 0.56 - ETA: 3s - loss: 0.6896 - accuracy: 0.56 - ETA: 2s - loss: 0.6890 - accuracy: 0.56 - ETA: 2s - loss: 0.6884 - accuracy: 0.56 - ETA: 2s - loss: 0.6874 - accuracy: 0.57 - ETA: 2s - loss: 0.6873 - accuracy: 0.57 - ETA: 2s - loss: 0.6864 - accuracy: 0.57 - ETA: 2s - loss: 0.6861 - accuracy: 0.57 - ETA: 2s - loss: 0.6858 - accuracy: 0.57 - ETA: 2s - loss: 0.6854 - accuracy: 0.57 - ETA: 2s - loss: 0.6853 - accuracy: 0.57 - ETA: 2s - loss: 0.6846 - accuracy: 0.57 - ETA: 2s - loss: 0.6846 - accuracy: 0.57 - ETA: 2s - loss: 0.6842 - accuracy: 0.57 - ETA: 1s - loss: 0.6837 - accuracy: 0.57 - ETA: 1s - loss: 0.6834 - accuracy: 0.58 - ETA: 1s - loss: 0.6828 - accuracy: 0.58 - ETA: 1s - loss: 0.6824 - accuracy: 0.58 - ETA: 1s - loss: 0.6823 - accuracy: 0.58 - ETA: 1s - loss: 0.6818 - accuracy: 0.58 - ETA: 1s - loss: 0.6817 - accuracy: 0.58 - ETA: 1s - loss: 0.6816 - accuracy: 0.58 - ETA: 1s - loss: 0.6814 - accuracy: 0.58 - ETA: 1s - loss: 0.6810 - accuracy: 0.58 - ETA: 1s - loss: 0.6810 - accuracy: 0.58 - ETA: 1s - loss: 0.6808 - accuracy: 0.58 - ETA: 1s - loss: 0.6806 - accuracy: 0.58 - ETA: 1s - loss: 0.6802 - accuracy: 0.58 - ETA: 1s - loss: 0.6800 - accuracy: 0.58 - ETA: 1s - loss: 0.6795 - accuracy: 0.58 - ETA: 1s - loss: 0.6793 - accuracy: 0.58 - ETA: 1s - loss: 0.6791 - accuracy: 0.58 - ETA: 0s - loss: 0.6789 - accuracy: 0.58 - ETA: 0s - loss: 0.6787 - accuracy: 0.58 - ETA: 0s - loss: 0.6783 - accuracy: 0.59 - ETA: 0s - loss: 0.6779 - accuracy: 0.59 - ETA: 0s - loss: 0.6775 - accuracy: 0.59 - ETA: 0s - loss: 0.6774 - accuracy: 0.59 - ETA: 0s - loss: 0.6773 - accuracy: 0.59 - ETA: 0s - loss: 0.6767 - accuracy: 0.59 - ETA: 0s - loss: 0.6766 - accuracy: 0.59 - ETA: 0s - loss: 0.6764 - accuracy: 0.59 - ETA: 0s - loss: 0.6761 - accuracy: 0.59 - ETA: 0s - loss: 0.6759 - accuracy: 0.59 - ETA: 0s - loss: 0.6754 - accuracy: 0.59 - ETA: 0s - loss: 0.6751 - accuracy: 0.59 - ETA: 0s - loss: 0.6748 - accuracy: 0.59 - ETA: 0s - loss: 0.6747 - accuracy: 0.59 - ETA: 0s - loss: 0.6747 - accuracy: 0.59 - ETA: 0s - loss: 0.6745 - accuracy: 0.59 - ETA: 0s - loss: 0.6743 - accuracy: 0.59 - 3s 256us/step - loss: 0.6743 - accuracy: 0.5951 - val_loss: 0.6546 - val_accuracy: 0.7060\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:13 - loss: 0.6936 - accuracy: 0.75 - ETA: 8s - loss: 0.6953 - accuracy: 0.5482 - ETA: 5s - loss: 0.6928 - accuracy: 0.53 - ETA: 4s - loss: 0.6936 - accuracy: 0.52 - ETA: 3s - loss: 0.6930 - accuracy: 0.51 - ETA: 3s - loss: 0.6929 - accuracy: 0.50 - ETA: 3s - loss: 0.6925 - accuracy: 0.51 - ETA: 3s - loss: 0.6918 - accuracy: 0.52 - ETA: 3s - loss: 0.6917 - accuracy: 0.52 - ETA: 2s - loss: 0.6916 - accuracy: 0.52 - ETA: 2s - loss: 0.6913 - accuracy: 0.52 - ETA: 2s - loss: 0.6912 - accuracy: 0.52 - ETA: 2s - loss: 0.6910 - accuracy: 0.52 - ETA: 2s - loss: 0.6909 - accuracy: 0.51 - ETA: 2s - loss: 0.6906 - accuracy: 0.51 - ETA: 2s - loss: 0.6905 - accuracy: 0.51 - ETA: 2s - loss: 0.6905 - accuracy: 0.51 - ETA: 2s - loss: 0.6903 - accuracy: 0.52 - ETA: 2s - loss: 0.6901 - accuracy: 0.52 - ETA: 2s - loss: 0.6901 - accuracy: 0.51 - ETA: 2s - loss: 0.6899 - accuracy: 0.51 - ETA: 1s - loss: 0.6895 - accuracy: 0.52 - ETA: 1s - loss: 0.6894 - accuracy: 0.52 - ETA: 1s - loss: 0.6891 - accuracy: 0.52 - ETA: 1s - loss: 0.6891 - accuracy: 0.52 - ETA: 1s - loss: 0.6891 - accuracy: 0.52 - ETA: 1s - loss: 0.6889 - accuracy: 0.52 - ETA: 1s - loss: 0.6889 - accuracy: 0.52 - ETA: 1s - loss: 0.6888 - accuracy: 0.52 - ETA: 1s - loss: 0.6884 - accuracy: 0.52 - ETA: 1s - loss: 0.6884 - accuracy: 0.52 - ETA: 1s - loss: 0.6884 - accuracy: 0.52 - ETA: 1s - loss: 0.6884 - accuracy: 0.52 - ETA: 1s - loss: 0.6881 - accuracy: 0.52 - ETA: 1s - loss: 0.6877 - accuracy: 0.53 - ETA: 1s - loss: 0.6876 - accuracy: 0.53 - ETA: 1s - loss: 0.6873 - accuracy: 0.53 - ETA: 1s - loss: 0.6872 - accuracy: 0.53 - ETA: 0s - loss: 0.6869 - accuracy: 0.53 - ETA: 0s - loss: 0.6868 - accuracy: 0.53 - ETA: 0s - loss: 0.6865 - accuracy: 0.53 - ETA: 0s - loss: 0.6863 - accuracy: 0.53 - ETA: 0s - loss: 0.6861 - accuracy: 0.53 - ETA: 0s - loss: 0.6858 - accuracy: 0.53 - ETA: 0s - loss: 0.6854 - accuracy: 0.54 - ETA: 0s - loss: 0.6851 - accuracy: 0.54 - ETA: 0s - loss: 0.6848 - accuracy: 0.54 - ETA: 0s - loss: 0.6847 - accuracy: 0.54 - ETA: 0s - loss: 0.6844 - accuracy: 0.54 - ETA: 0s - loss: 0.6843 - accuracy: 0.54 - ETA: 0s - loss: 0.6843 - accuracy: 0.54 - ETA: 0s - loss: 0.6842 - accuracy: 0.54 - ETA: 0s - loss: 0.6842 - accuracy: 0.54 - ETA: 0s - loss: 0.6840 - accuracy: 0.54 - ETA: 0s - loss: 0.6839 - accuracy: 0.54 - ETA: 0s - loss: 0.6838 - accuracy: 0.54 - ETA: 0s - loss: 0.6838 - accuracy: 0.54 - 3s 250us/step - loss: 0.6836 - accuracy: 0.5463 - val_loss: 0.6712 - val_accuracy: 0.6974\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 100us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:07 - loss: 0.6929 - accuracy: 0.75 - ETA: 8s - loss: 0.6936 - accuracy: 0.5179 - ETA: 5s - loss: 0.6929 - accuracy: 0.55 - ETA: 4s - loss: 0.6924 - accuracy: 0.56 - ETA: 4s - loss: 0.6918 - accuracy: 0.58 - ETA: 3s - loss: 0.6913 - accuracy: 0.57 - ETA: 3s - loss: 0.6912 - accuracy: 0.56 - ETA: 3s - loss: 0.6912 - accuracy: 0.56 - ETA: 3s - loss: 0.6911 - accuracy: 0.56 - ETA: 3s - loss: 0.6909 - accuracy: 0.56 - ETA: 2s - loss: 0.6909 - accuracy: 0.56 - ETA: 2s - loss: 0.6908 - accuracy: 0.56 - ETA: 2s - loss: 0.6905 - accuracy: 0.56 - ETA: 2s - loss: 0.6900 - accuracy: 0.56 - ETA: 2s - loss: 0.6899 - accuracy: 0.56 - ETA: 2s - loss: 0.6898 - accuracy: 0.56 - ETA: 2s - loss: 0.6896 - accuracy: 0.56 - ETA: 2s - loss: 0.6895 - accuracy: 0.56 - ETA: 2s - loss: 0.6893 - accuracy: 0.57 - ETA: 2s - loss: 0.6891 - accuracy: 0.57 - ETA: 2s - loss: 0.6889 - accuracy: 0.57 - ETA: 2s - loss: 0.6888 - accuracy: 0.57 - ETA: 2s - loss: 0.6886 - accuracy: 0.57 - ETA: 1s - loss: 0.6886 - accuracy: 0.57 - ETA: 1s - loss: 0.6885 - accuracy: 0.57 - ETA: 1s - loss: 0.6881 - accuracy: 0.58 - ETA: 1s - loss: 0.6879 - accuracy: 0.58 - ETA: 1s - loss: 0.6875 - accuracy: 0.58 - ETA: 1s - loss: 0.6873 - accuracy: 0.58 - ETA: 1s - loss: 0.6871 - accuracy: 0.59 - ETA: 1s - loss: 0.6867 - accuracy: 0.59 - ETA: 1s - loss: 0.6864 - accuracy: 0.59 - ETA: 1s - loss: 0.6862 - accuracy: 0.59 - ETA: 1s - loss: 0.6860 - accuracy: 0.59 - ETA: 1s - loss: 0.6855 - accuracy: 0.60 - ETA: 1s - loss: 0.6851 - accuracy: 0.60 - ETA: 1s - loss: 0.6851 - accuracy: 0.60 - ETA: 1s - loss: 0.6850 - accuracy: 0.60 - ETA: 1s - loss: 0.6849 - accuracy: 0.60 - ETA: 1s - loss: 0.6846 - accuracy: 0.60 - ETA: 1s - loss: 0.6844 - accuracy: 0.60 - ETA: 0s - loss: 0.6842 - accuracy: 0.60 - ETA: 0s - loss: 0.6841 - accuracy: 0.60 - ETA: 0s - loss: 0.6840 - accuracy: 0.60 - ETA: 0s - loss: 0.6839 - accuracy: 0.60 - ETA: 0s - loss: 0.6835 - accuracy: 0.60 - ETA: 0s - loss: 0.6835 - accuracy: 0.60 - ETA: 0s - loss: 0.6833 - accuracy: 0.60 - ETA: 0s - loss: 0.6832 - accuracy: 0.60 - ETA: 0s - loss: 0.6830 - accuracy: 0.60 - ETA: 0s - loss: 0.6828 - accuracy: 0.60 - ETA: 0s - loss: 0.6826 - accuracy: 0.60 - ETA: 0s - loss: 0.6824 - accuracy: 0.60 - ETA: 0s - loss: 0.6823 - accuracy: 0.60 - ETA: 0s - loss: 0.6821 - accuracy: 0.60 - ETA: 0s - loss: 0.6819 - accuracy: 0.60 - ETA: 0s - loss: 0.6817 - accuracy: 0.60 - ETA: 0s - loss: 0.6815 - accuracy: 0.60 - ETA: 0s - loss: 0.6816 - accuracy: 0.60 - 3s 258us/step - loss: 0.6813 - accuracy: 0.6069 - val_loss: 0.6690 - val_accuracy: 0.6996\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:10 - loss: 0.6934 - accuracy: 0.75 - ETA: 8s - loss: 0.6950 - accuracy: 0.4815 - ETA: 5s - loss: 0.6941 - accuracy: 0.47 - ETA: 4s - loss: 0.6934 - accuracy: 0.48 - ETA: 4s - loss: 0.6928 - accuracy: 0.49 - ETA: 3s - loss: 0.6927 - accuracy: 0.50 - ETA: 3s - loss: 0.6925 - accuracy: 0.50 - ETA: 3s - loss: 0.6920 - accuracy: 0.51 - ETA: 3s - loss: 0.6913 - accuracy: 0.51 - ETA: 3s - loss: 0.6917 - accuracy: 0.51 - ETA: 2s - loss: 0.6916 - accuracy: 0.51 - ETA: 2s - loss: 0.6912 - accuracy: 0.51 - ETA: 2s - loss: 0.6910 - accuracy: 0.52 - ETA: 2s - loss: 0.6907 - accuracy: 0.52 - ETA: 2s - loss: 0.6902 - accuracy: 0.53 - ETA: 2s - loss: 0.6897 - accuracy: 0.53 - ETA: 2s - loss: 0.6892 - accuracy: 0.53 - ETA: 2s - loss: 0.6883 - accuracy: 0.53 - ETA: 2s - loss: 0.6887 - accuracy: 0.53 - ETA: 2s - loss: 0.6887 - accuracy: 0.53 - ETA: 2s - loss: 0.6886 - accuracy: 0.53 - ETA: 2s - loss: 0.6882 - accuracy: 0.53 - ETA: 1s - loss: 0.6878 - accuracy: 0.53 - ETA: 1s - loss: 0.6874 - accuracy: 0.53 - ETA: 1s - loss: 0.6869 - accuracy: 0.54 - ETA: 1s - loss: 0.6868 - accuracy: 0.54 - ETA: 1s - loss: 0.6864 - accuracy: 0.54 - ETA: 1s - loss: 0.6863 - accuracy: 0.54 - ETA: 1s - loss: 0.6863 - accuracy: 0.54 - ETA: 1s - loss: 0.6859 - accuracy: 0.54 - ETA: 1s - loss: 0.6857 - accuracy: 0.54 - ETA: 1s - loss: 0.6855 - accuracy: 0.54 - ETA: 1s - loss: 0.6854 - accuracy: 0.54 - ETA: 1s - loss: 0.6851 - accuracy: 0.54 - ETA: 1s - loss: 0.6847 - accuracy: 0.54 - ETA: 1s - loss: 0.6846 - accuracy: 0.54 - ETA: 1s - loss: 0.6843 - accuracy: 0.55 - ETA: 1s - loss: 0.6841 - accuracy: 0.55 - ETA: 1s - loss: 0.6837 - accuracy: 0.55 - ETA: 1s - loss: 0.6835 - accuracy: 0.55 - ETA: 1s - loss: 0.6830 - accuracy: 0.55 - ETA: 0s - loss: 0.6827 - accuracy: 0.55 - ETA: 0s - loss: 0.6826 - accuracy: 0.55 - ETA: 0s - loss: 0.6825 - accuracy: 0.55 - ETA: 0s - loss: 0.6822 - accuracy: 0.55 - ETA: 0s - loss: 0.6820 - accuracy: 0.55 - ETA: 0s - loss: 0.6816 - accuracy: 0.55 - ETA: 0s - loss: 0.6814 - accuracy: 0.55 - ETA: 0s - loss: 0.6812 - accuracy: 0.55 - ETA: 0s - loss: 0.6812 - accuracy: 0.55 - ETA: 0s - loss: 0.6810 - accuracy: 0.55 - ETA: 0s - loss: 0.6806 - accuracy: 0.56 - ETA: 0s - loss: 0.6805 - accuracy: 0.56 - ETA: 0s - loss: 0.6805 - accuracy: 0.56 - ETA: 0s - loss: 0.6803 - accuracy: 0.56 - ETA: 0s - loss: 0.6801 - accuracy: 0.56 - ETA: 0s - loss: 0.6800 - accuracy: 0.56 - ETA: 0s - loss: 0.6797 - accuracy: 0.56 - ETA: 0s - loss: 0.6792 - accuracy: 0.56 - 3s 257us/step - loss: 0.6790 - accuracy: 0.5627 - val_loss: 0.6619 - val_accuracy: 0.6982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 110us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:10 - loss: 0.6873 - accuracy: 0.50 - ETA: 7s - loss: 0.6951 - accuracy: 0.4483 - ETA: 5s - loss: 0.6939 - accuracy: 0.51 - ETA: 4s - loss: 0.6931 - accuracy: 0.52 - ETA: 3s - loss: 0.6923 - accuracy: 0.52 - ETA: 3s - loss: 0.6919 - accuracy: 0.53 - ETA: 3s - loss: 0.6911 - accuracy: 0.53 - ETA: 3s - loss: 0.6904 - accuracy: 0.53 - ETA: 3s - loss: 0.6899 - accuracy: 0.54 - ETA: 2s - loss: 0.6892 - accuracy: 0.55 - ETA: 2s - loss: 0.6890 - accuracy: 0.55 - ETA: 2s - loss: 0.6881 - accuracy: 0.56 - ETA: 2s - loss: 0.6882 - accuracy: 0.56 - ETA: 2s - loss: 0.6880 - accuracy: 0.56 - ETA: 2s - loss: 0.6870 - accuracy: 0.57 - ETA: 2s - loss: 0.6864 - accuracy: 0.57 - ETA: 2s - loss: 0.6858 - accuracy: 0.57 - ETA: 2s - loss: 0.6850 - accuracy: 0.58 - ETA: 2s - loss: 0.6843 - accuracy: 0.58 - ETA: 2s - loss: 0.6839 - accuracy: 0.58 - ETA: 2s - loss: 0.6839 - accuracy: 0.58 - ETA: 1s - loss: 0.6836 - accuracy: 0.58 - ETA: 1s - loss: 0.6834 - accuracy: 0.58 - ETA: 1s - loss: 0.6831 - accuracy: 0.58 - ETA: 1s - loss: 0.6825 - accuracy: 0.58 - ETA: 1s - loss: 0.6822 - accuracy: 0.58 - ETA: 1s - loss: 0.6814 - accuracy: 0.58 - ETA: 1s - loss: 0.6806 - accuracy: 0.59 - ETA: 1s - loss: 0.6799 - accuracy: 0.59 - ETA: 1s - loss: 0.6794 - accuracy: 0.59 - ETA: 1s - loss: 0.6788 - accuracy: 0.59 - ETA: 1s - loss: 0.6779 - accuracy: 0.59 - ETA: 1s - loss: 0.6769 - accuracy: 0.59 - ETA: 1s - loss: 0.6762 - accuracy: 0.60 - ETA: 1s - loss: 0.6757 - accuracy: 0.60 - ETA: 1s - loss: 0.6755 - accuracy: 0.60 - ETA: 1s - loss: 0.6749 - accuracy: 0.60 - ETA: 1s - loss: 0.6742 - accuracy: 0.60 - ETA: 0s - loss: 0.6737 - accuracy: 0.60 - ETA: 0s - loss: 0.6732 - accuracy: 0.60 - ETA: 0s - loss: 0.6727 - accuracy: 0.60 - ETA: 0s - loss: 0.6725 - accuracy: 0.60 - ETA: 0s - loss: 0.6720 - accuracy: 0.60 - ETA: 0s - loss: 0.6717 - accuracy: 0.60 - ETA: 0s - loss: 0.6715 - accuracy: 0.60 - ETA: 0s - loss: 0.6714 - accuracy: 0.60 - ETA: 0s - loss: 0.6707 - accuracy: 0.60 - ETA: 0s - loss: 0.6704 - accuracy: 0.60 - ETA: 0s - loss: 0.6702 - accuracy: 0.61 - ETA: 0s - loss: 0.6697 - accuracy: 0.61 - ETA: 0s - loss: 0.6690 - accuracy: 0.61 - ETA: 0s - loss: 0.6688 - accuracy: 0.61 - ETA: 0s - loss: 0.6684 - accuracy: 0.61 - ETA: 0s - loss: 0.6679 - accuracy: 0.61 - ETA: 0s - loss: 0.6678 - accuracy: 0.61 - ETA: 0s - loss: 0.6674 - accuracy: 0.61 - ETA: 0s - loss: 0.6669 - accuracy: 0.61 - 3s 250us/step - loss: 0.6665 - accuracy: 0.6157 - val_loss: 0.6351 - val_accuracy: 0.7180\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:07 - loss: 0.6931 - accuracy: 0.50 - ETA: 8s - loss: 0.6947 - accuracy: 0.4911 - ETA: 5s - loss: 0.6942 - accuracy: 0.49 - ETA: 4s - loss: 0.6934 - accuracy: 0.50 - ETA: 4s - loss: 0.6932 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 3s - loss: 0.6922 - accuracy: 0.52 - ETA: 3s - loss: 0.6921 - accuracy: 0.52 - ETA: 3s - loss: 0.6917 - accuracy: 0.52 - ETA: 2s - loss: 0.6915 - accuracy: 0.53 - ETA: 2s - loss: 0.6911 - accuracy: 0.53 - ETA: 2s - loss: 0.6910 - accuracy: 0.53 - ETA: 2s - loss: 0.6907 - accuracy: 0.53 - ETA: 2s - loss: 0.6902 - accuracy: 0.54 - ETA: 2s - loss: 0.6899 - accuracy: 0.54 - ETA: 2s - loss: 0.6894 - accuracy: 0.55 - ETA: 2s - loss: 0.6892 - accuracy: 0.55 - ETA: 2s - loss: 0.6886 - accuracy: 0.55 - ETA: 2s - loss: 0.6886 - accuracy: 0.55 - ETA: 2s - loss: 0.6883 - accuracy: 0.55 - ETA: 2s - loss: 0.6878 - accuracy: 0.55 - ETA: 1s - loss: 0.6875 - accuracy: 0.55 - ETA: 1s - loss: 0.6873 - accuracy: 0.55 - ETA: 1s - loss: 0.6868 - accuracy: 0.56 - ETA: 1s - loss: 0.6867 - accuracy: 0.56 - ETA: 1s - loss: 0.6864 - accuracy: 0.56 - ETA: 1s - loss: 0.6860 - accuracy: 0.56 - ETA: 1s - loss: 0.6856 - accuracy: 0.56 - ETA: 1s - loss: 0.6854 - accuracy: 0.56 - ETA: 1s - loss: 0.6853 - accuracy: 0.56 - ETA: 1s - loss: 0.6847 - accuracy: 0.57 - ETA: 1s - loss: 0.6843 - accuracy: 0.57 - ETA: 1s - loss: 0.6839 - accuracy: 0.57 - ETA: 1s - loss: 0.6837 - accuracy: 0.57 - ETA: 1s - loss: 0.6835 - accuracy: 0.57 - ETA: 1s - loss: 0.6832 - accuracy: 0.57 - ETA: 1s - loss: 0.6830 - accuracy: 0.57 - ETA: 1s - loss: 0.6826 - accuracy: 0.57 - ETA: 1s - loss: 0.6824 - accuracy: 0.57 - ETA: 0s - loss: 0.6821 - accuracy: 0.57 - ETA: 0s - loss: 0.6819 - accuracy: 0.58 - ETA: 0s - loss: 0.6815 - accuracy: 0.58 - ETA: 0s - loss: 0.6812 - accuracy: 0.58 - ETA: 0s - loss: 0.6809 - accuracy: 0.58 - ETA: 0s - loss: 0.6804 - accuracy: 0.58 - ETA: 0s - loss: 0.6802 - accuracy: 0.58 - ETA: 0s - loss: 0.6796 - accuracy: 0.58 - ETA: 0s - loss: 0.6793 - accuracy: 0.59 - ETA: 0s - loss: 0.6790 - accuracy: 0.59 - ETA: 0s - loss: 0.6786 - accuracy: 0.59 - ETA: 0s - loss: 0.6781 - accuracy: 0.59 - ETA: 0s - loss: 0.6775 - accuracy: 0.59 - ETA: 0s - loss: 0.6773 - accuracy: 0.59 - ETA: 0s - loss: 0.6769 - accuracy: 0.59 - ETA: 0s - loss: 0.6766 - accuracy: 0.59 - ETA: 0s - loss: 0.6763 - accuracy: 0.59 - ETA: 0s - loss: 0.6762 - accuracy: 0.59 - ETA: 0s - loss: 0.6759 - accuracy: 0.59 - 3s 252us/step - loss: 0.6759 - accuracy: 0.5989 - val_loss: 0.6513 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:07 - loss: 0.6932 - accuracy: 1.00 - ETA: 8s - loss: 0.6937 - accuracy: 0.5045 - ETA: 5s - loss: 0.6925 - accuracy: 0.52 - ETA: 4s - loss: 0.6916 - accuracy: 0.52 - ETA: 4s - loss: 0.6915 - accuracy: 0.52 - ETA: 3s - loss: 0.6916 - accuracy: 0.52 - ETA: 3s - loss: 0.6915 - accuracy: 0.52 - ETA: 3s - loss: 0.6906 - accuracy: 0.54 - ETA: 3s - loss: 0.6907 - accuracy: 0.53 - ETA: 3s - loss: 0.6902 - accuracy: 0.54 - ETA: 2s - loss: 0.6896 - accuracy: 0.55 - ETA: 2s - loss: 0.6894 - accuracy: 0.55 - ETA: 2s - loss: 0.6890 - accuracy: 0.55 - ETA: 2s - loss: 0.6887 - accuracy: 0.55 - ETA: 2s - loss: 0.6886 - accuracy: 0.54 - ETA: 2s - loss: 0.6883 - accuracy: 0.55 - ETA: 2s - loss: 0.6878 - accuracy: 0.55 - ETA: 2s - loss: 0.6874 - accuracy: 0.55 - ETA: 2s - loss: 0.6872 - accuracy: 0.55 - ETA: 2s - loss: 0.6870 - accuracy: 0.55 - ETA: 2s - loss: 0.6866 - accuracy: 0.56 - ETA: 2s - loss: 0.6857 - accuracy: 0.56 - ETA: 2s - loss: 0.6857 - accuracy: 0.56 - ETA: 1s - loss: 0.6853 - accuracy: 0.56 - ETA: 1s - loss: 0.6850 - accuracy: 0.57 - ETA: 1s - loss: 0.6851 - accuracy: 0.56 - ETA: 1s - loss: 0.6849 - accuracy: 0.56 - ETA: 1s - loss: 0.6846 - accuracy: 0.57 - ETA: 1s - loss: 0.6843 - accuracy: 0.57 - ETA: 1s - loss: 0.6839 - accuracy: 0.57 - ETA: 1s - loss: 0.6836 - accuracy: 0.57 - ETA: 1s - loss: 0.6832 - accuracy: 0.57 - ETA: 1s - loss: 0.6829 - accuracy: 0.57 - ETA: 1s - loss: 0.6825 - accuracy: 0.57 - ETA: 1s - loss: 0.6823 - accuracy: 0.57 - ETA: 1s - loss: 0.6819 - accuracy: 0.57 - ETA: 1s - loss: 0.6816 - accuracy: 0.58 - ETA: 1s - loss: 0.6814 - accuracy: 0.58 - ETA: 1s - loss: 0.6812 - accuracy: 0.58 - ETA: 1s - loss: 0.6808 - accuracy: 0.58 - ETA: 0s - loss: 0.6807 - accuracy: 0.58 - ETA: 0s - loss: 0.6803 - accuracy: 0.58 - ETA: 0s - loss: 0.6803 - accuracy: 0.58 - ETA: 0s - loss: 0.6801 - accuracy: 0.58 - ETA: 0s - loss: 0.6801 - accuracy: 0.58 - ETA: 0s - loss: 0.6800 - accuracy: 0.58 - ETA: 0s - loss: 0.6800 - accuracy: 0.58 - ETA: 0s - loss: 0.6799 - accuracy: 0.58 - ETA: 0s - loss: 0.6797 - accuracy: 0.58 - ETA: 0s - loss: 0.6796 - accuracy: 0.58 - ETA: 0s - loss: 0.6794 - accuracy: 0.58 - ETA: 0s - loss: 0.6793 - accuracy: 0.58 - ETA: 0s - loss: 0.6791 - accuracy: 0.58 - ETA: 0s - loss: 0.6788 - accuracy: 0.58 - ETA: 0s - loss: 0.6787 - accuracy: 0.58 - ETA: 0s - loss: 0.6787 - accuracy: 0.58 - ETA: 0s - loss: 0.6786 - accuracy: 0.58 - ETA: 0s - loss: 0.6786 - accuracy: 0.58 - ETA: 0s - loss: 0.6782 - accuracy: 0.58 - 3s 258us/step - loss: 0.6779 - accuracy: 0.5895 - val_loss: 0.6594 - val_accuracy: 0.7060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:07 - loss: 0.6901 - accuracy: 0.75 - ETA: 8s - loss: 0.6927 - accuracy: 0.5500 - ETA: 5s - loss: 0.6934 - accuracy: 0.53 - ETA: 4s - loss: 0.6922 - accuracy: 0.53 - ETA: 4s - loss: 0.6913 - accuracy: 0.53 - ETA: 3s - loss: 0.6910 - accuracy: 0.53 - ETA: 3s - loss: 0.6908 - accuracy: 0.53 - ETA: 3s - loss: 0.6899 - accuracy: 0.53 - ETA: 3s - loss: 0.6898 - accuracy: 0.53 - ETA: 3s - loss: 0.6894 - accuracy: 0.53 - ETA: 2s - loss: 0.6882 - accuracy: 0.54 - ETA: 2s - loss: 0.6875 - accuracy: 0.55 - ETA: 2s - loss: 0.6867 - accuracy: 0.55 - ETA: 2s - loss: 0.6857 - accuracy: 0.55 - ETA: 2s - loss: 0.6851 - accuracy: 0.56 - ETA: 2s - loss: 0.6849 - accuracy: 0.56 - ETA: 2s - loss: 0.6847 - accuracy: 0.56 - ETA: 2s - loss: 0.6843 - accuracy: 0.56 - ETA: 2s - loss: 0.6835 - accuracy: 0.57 - ETA: 2s - loss: 0.6830 - accuracy: 0.57 - ETA: 2s - loss: 0.6821 - accuracy: 0.57 - ETA: 2s - loss: 0.6819 - accuracy: 0.57 - ETA: 2s - loss: 0.6820 - accuracy: 0.57 - ETA: 1s - loss: 0.6814 - accuracy: 0.57 - ETA: 1s - loss: 0.6811 - accuracy: 0.57 - ETA: 1s - loss: 0.6807 - accuracy: 0.57 - ETA: 1s - loss: 0.6803 - accuracy: 0.57 - ETA: 1s - loss: 0.6802 - accuracy: 0.57 - ETA: 1s - loss: 0.6799 - accuracy: 0.58 - ETA: 1s - loss: 0.6795 - accuracy: 0.58 - ETA: 1s - loss: 0.6788 - accuracy: 0.58 - ETA: 1s - loss: 0.6787 - accuracy: 0.58 - ETA: 1s - loss: 0.6779 - accuracy: 0.58 - ETA: 1s - loss: 0.6778 - accuracy: 0.59 - ETA: 1s - loss: 0.6774 - accuracy: 0.59 - ETA: 1s - loss: 0.6768 - accuracy: 0.59 - ETA: 1s - loss: 0.6767 - accuracy: 0.59 - ETA: 1s - loss: 0.6765 - accuracy: 0.59 - ETA: 1s - loss: 0.6758 - accuracy: 0.59 - ETA: 1s - loss: 0.6752 - accuracy: 0.59 - ETA: 0s - loss: 0.6749 - accuracy: 0.59 - ETA: 0s - loss: 0.6746 - accuracy: 0.59 - ETA: 0s - loss: 0.6742 - accuracy: 0.59 - ETA: 0s - loss: 0.6740 - accuracy: 0.59 - ETA: 0s - loss: 0.6740 - accuracy: 0.59 - ETA: 0s - loss: 0.6737 - accuracy: 0.59 - ETA: 0s - loss: 0.6732 - accuracy: 0.59 - ETA: 0s - loss: 0.6730 - accuracy: 0.60 - ETA: 0s - loss: 0.6728 - accuracy: 0.60 - ETA: 0s - loss: 0.6725 - accuracy: 0.60 - ETA: 0s - loss: 0.6721 - accuracy: 0.60 - ETA: 0s - loss: 0.6716 - accuracy: 0.60 - ETA: 0s - loss: 0.6714 - accuracy: 0.60 - ETA: 0s - loss: 0.6712 - accuracy: 0.60 - ETA: 0s - loss: 0.6708 - accuracy: 0.60 - ETA: 0s - loss: 0.6706 - accuracy: 0.60 - ETA: 0s - loss: 0.6705 - accuracy: 0.60 - ETA: 0s - loss: 0.6701 - accuracy: 0.60 - ETA: 0s - loss: 0.6695 - accuracy: 0.61 - 3s 258us/step - loss: 0.6693 - accuracy: 0.6117 - val_loss: 0.6408 - val_accuracy: 0.7102\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:00 - loss: 0.6930 - accuracy: 0.25 - ETA: 8s - loss: 0.6923 - accuracy: 0.5236 - ETA: 5s - loss: 0.6914 - accuracy: 0.53 - ETA: 4s - loss: 0.6890 - accuracy: 0.55 - ETA: 3s - loss: 0.6872 - accuracy: 0.56 - ETA: 3s - loss: 0.6864 - accuracy: 0.56 - ETA: 3s - loss: 0.6862 - accuracy: 0.56 - ETA: 3s - loss: 0.6849 - accuracy: 0.56 - ETA: 3s - loss: 0.6845 - accuracy: 0.57 - ETA: 2s - loss: 0.6846 - accuracy: 0.56 - ETA: 2s - loss: 0.6835 - accuracy: 0.57 - ETA: 2s - loss: 0.6825 - accuracy: 0.57 - ETA: 2s - loss: 0.6827 - accuracy: 0.57 - ETA: 2s - loss: 0.6824 - accuracy: 0.57 - ETA: 2s - loss: 0.6810 - accuracy: 0.58 - ETA: 2s - loss: 0.6802 - accuracy: 0.58 - ETA: 2s - loss: 0.6793 - accuracy: 0.58 - ETA: 2s - loss: 0.6794 - accuracy: 0.58 - ETA: 2s - loss: 0.6790 - accuracy: 0.58 - ETA: 2s - loss: 0.6790 - accuracy: 0.59 - ETA: 2s - loss: 0.6785 - accuracy: 0.59 - ETA: 2s - loss: 0.6780 - accuracy: 0.59 - ETA: 1s - loss: 0.6772 - accuracy: 0.59 - ETA: 1s - loss: 0.6770 - accuracy: 0.59 - ETA: 1s - loss: 0.6761 - accuracy: 0.59 - ETA: 1s - loss: 0.6761 - accuracy: 0.59 - ETA: 1s - loss: 0.6753 - accuracy: 0.59 - ETA: 1s - loss: 0.6742 - accuracy: 0.60 - ETA: 1s - loss: 0.6730 - accuracy: 0.60 - ETA: 1s - loss: 0.6727 - accuracy: 0.60 - ETA: 1s - loss: 0.6718 - accuracy: 0.60 - ETA: 1s - loss: 0.6709 - accuracy: 0.61 - ETA: 1s - loss: 0.6707 - accuracy: 0.61 - ETA: 1s - loss: 0.6704 - accuracy: 0.61 - ETA: 1s - loss: 0.6698 - accuracy: 0.61 - ETA: 1s - loss: 0.6694 - accuracy: 0.61 - ETA: 1s - loss: 0.6691 - accuracy: 0.61 - ETA: 1s - loss: 0.6688 - accuracy: 0.61 - ETA: 1s - loss: 0.6683 - accuracy: 0.61 - ETA: 1s - loss: 0.6676 - accuracy: 0.61 - ETA: 0s - loss: 0.6670 - accuracy: 0.61 - ETA: 0s - loss: 0.6666 - accuracy: 0.61 - ETA: 0s - loss: 0.6658 - accuracy: 0.62 - ETA: 0s - loss: 0.6652 - accuracy: 0.62 - ETA: 0s - loss: 0.6646 - accuracy: 0.62 - ETA: 0s - loss: 0.6638 - accuracy: 0.62 - ETA: 0s - loss: 0.6638 - accuracy: 0.62 - ETA: 0s - loss: 0.6636 - accuracy: 0.62 - ETA: 0s - loss: 0.6628 - accuracy: 0.62 - ETA: 0s - loss: 0.6621 - accuracy: 0.62 - ETA: 0s - loss: 0.6612 - accuracy: 0.62 - ETA: 0s - loss: 0.6609 - accuracy: 0.62 - ETA: 0s - loss: 0.6608 - accuracy: 0.62 - ETA: 0s - loss: 0.6606 - accuracy: 0.62 - ETA: 0s - loss: 0.6602 - accuracy: 0.63 - ETA: 0s - loss: 0.6598 - accuracy: 0.63 - ETA: 0s - loss: 0.6596 - accuracy: 0.63 - ETA: 0s - loss: 0.6592 - accuracy: 0.63 - ETA: 0s - loss: 0.6589 - accuracy: 0.63 - ETA: 0s - loss: 0.6586 - accuracy: 0.63 - 3s 261us/step - loss: 0.6582 - accuracy: 0.6332 - val_loss: 0.6223 - val_accuracy: 0.7124\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:04 - loss: 0.6928 - accuracy: 0.25 - ETA: 8s - loss: 0.6939 - accuracy: 0.4911 - ETA: 5s - loss: 0.6939 - accuracy: 0.49 - ETA: 4s - loss: 0.6937 - accuracy: 0.49 - ETA: 4s - loss: 0.6937 - accuracy: 0.49 - ETA: 3s - loss: 0.6935 - accuracy: 0.50 - ETA: 3s - loss: 0.6928 - accuracy: 0.52 - ETA: 3s - loss: 0.6924 - accuracy: 0.53 - ETA: 3s - loss: 0.6920 - accuracy: 0.53 - ETA: 2s - loss: 0.6911 - accuracy: 0.54 - ETA: 2s - loss: 0.6907 - accuracy: 0.54 - ETA: 2s - loss: 0.6901 - accuracy: 0.55 - ETA: 2s - loss: 0.6896 - accuracy: 0.55 - ETA: 2s - loss: 0.6891 - accuracy: 0.55 - ETA: 2s - loss: 0.6881 - accuracy: 0.56 - ETA: 2s - loss: 0.6873 - accuracy: 0.56 - ETA: 2s - loss: 0.6874 - accuracy: 0.56 - ETA: 2s - loss: 0.6874 - accuracy: 0.56 - ETA: 2s - loss: 0.6869 - accuracy: 0.56 - ETA: 2s - loss: 0.6865 - accuracy: 0.56 - ETA: 2s - loss: 0.6862 - accuracy: 0.56 - ETA: 2s - loss: 0.6862 - accuracy: 0.56 - ETA: 2s - loss: 0.6858 - accuracy: 0.57 - ETA: 1s - loss: 0.6851 - accuracy: 0.57 - ETA: 1s - loss: 0.6845 - accuracy: 0.57 - ETA: 1s - loss: 0.6839 - accuracy: 0.58 - ETA: 1s - loss: 0.6833 - accuracy: 0.58 - ETA: 1s - loss: 0.6825 - accuracy: 0.58 - ETA: 1s - loss: 0.6822 - accuracy: 0.58 - ETA: 1s - loss: 0.6817 - accuracy: 0.58 - ETA: 1s - loss: 0.6814 - accuracy: 0.58 - ETA: 1s - loss: 0.6809 - accuracy: 0.58 - ETA: 1s - loss: 0.6801 - accuracy: 0.59 - ETA: 1s - loss: 0.6791 - accuracy: 0.59 - ETA: 1s - loss: 0.6785 - accuracy: 0.59 - ETA: 1s - loss: 0.6781 - accuracy: 0.59 - ETA: 1s - loss: 0.6776 - accuracy: 0.59 - ETA: 1s - loss: 0.6772 - accuracy: 0.60 - ETA: 1s - loss: 0.6772 - accuracy: 0.59 - ETA: 1s - loss: 0.6766 - accuracy: 0.60 - ETA: 0s - loss: 0.6763 - accuracy: 0.60 - ETA: 0s - loss: 0.6761 - accuracy: 0.60 - ETA: 0s - loss: 0.6755 - accuracy: 0.60 - ETA: 0s - loss: 0.6752 - accuracy: 0.60 - ETA: 0s - loss: 0.6746 - accuracy: 0.60 - ETA: 0s - loss: 0.6742 - accuracy: 0.60 - ETA: 0s - loss: 0.6738 - accuracy: 0.60 - ETA: 0s - loss: 0.6734 - accuracy: 0.60 - ETA: 0s - loss: 0.6730 - accuracy: 0.60 - ETA: 0s - loss: 0.6729 - accuracy: 0.60 - ETA: 0s - loss: 0.6723 - accuracy: 0.60 - ETA: 0s - loss: 0.6717 - accuracy: 0.61 - ETA: 0s - loss: 0.6711 - accuracy: 0.61 - ETA: 0s - loss: 0.6705 - accuracy: 0.61 - ETA: 0s - loss: 0.6706 - accuracy: 0.61 - ETA: 0s - loss: 0.6706 - accuracy: 0.61 - ETA: 0s - loss: 0.6705 - accuracy: 0.61 - ETA: 0s - loss: 0.6707 - accuracy: 0.61 - ETA: 0s - loss: 0.6703 - accuracy: 0.61 - 3s 256us/step - loss: 0.6703 - accuracy: 0.6134 - val_loss: 0.6468 - val_accuracy: 0.7081\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:04 - loss: 0.6974 - accuracy: 0.50 - ETA: 8s - loss: 0.6950 - accuracy: 0.5231 - ETA: 5s - loss: 0.6934 - accuracy: 0.52 - ETA: 4s - loss: 0.6924 - accuracy: 0.53 - ETA: 4s - loss: 0.6917 - accuracy: 0.53 - ETA: 3s - loss: 0.6916 - accuracy: 0.53 - ETA: 3s - loss: 0.6912 - accuracy: 0.53 - ETA: 3s - loss: 0.6907 - accuracy: 0.54 - ETA: 3s - loss: 0.6898 - accuracy: 0.55 - ETA: 3s - loss: 0.6889 - accuracy: 0.55 - ETA: 2s - loss: 0.6878 - accuracy: 0.55 - ETA: 2s - loss: 0.6877 - accuracy: 0.55 - ETA: 2s - loss: 0.6872 - accuracy: 0.55 - ETA: 2s - loss: 0.6860 - accuracy: 0.56 - ETA: 2s - loss: 0.6853 - accuracy: 0.56 - ETA: 2s - loss: 0.6842 - accuracy: 0.56 - ETA: 2s - loss: 0.6840 - accuracy: 0.56 - ETA: 2s - loss: 0.6836 - accuracy: 0.57 - ETA: 2s - loss: 0.6829 - accuracy: 0.57 - ETA: 2s - loss: 0.6826 - accuracy: 0.57 - ETA: 2s - loss: 0.6819 - accuracy: 0.57 - ETA: 2s - loss: 0.6810 - accuracy: 0.57 - ETA: 2s - loss: 0.6811 - accuracy: 0.57 - ETA: 1s - loss: 0.6805 - accuracy: 0.57 - ETA: 1s - loss: 0.6804 - accuracy: 0.57 - ETA: 1s - loss: 0.6798 - accuracy: 0.58 - ETA: 1s - loss: 0.6790 - accuracy: 0.58 - ETA: 1s - loss: 0.6784 - accuracy: 0.58 - ETA: 1s - loss: 0.6781 - accuracy: 0.58 - ETA: 1s - loss: 0.6778 - accuracy: 0.58 - ETA: 1s - loss: 0.6772 - accuracy: 0.58 - ETA: 1s - loss: 0.6765 - accuracy: 0.59 - ETA: 1s - loss: 0.6756 - accuracy: 0.59 - ETA: 1s - loss: 0.6749 - accuracy: 0.59 - ETA: 1s - loss: 0.6743 - accuracy: 0.59 - ETA: 1s - loss: 0.6739 - accuracy: 0.59 - ETA: 1s - loss: 0.6737 - accuracy: 0.59 - ETA: 1s - loss: 0.6736 - accuracy: 0.59 - ETA: 1s - loss: 0.6733 - accuracy: 0.59 - ETA: 1s - loss: 0.6728 - accuracy: 0.59 - ETA: 1s - loss: 0.6719 - accuracy: 0.60 - ETA: 0s - loss: 0.6717 - accuracy: 0.60 - ETA: 0s - loss: 0.6713 - accuracy: 0.60 - ETA: 0s - loss: 0.6708 - accuracy: 0.60 - ETA: 0s - loss: 0.6704 - accuracy: 0.60 - ETA: 0s - loss: 0.6698 - accuracy: 0.60 - ETA: 0s - loss: 0.6693 - accuracy: 0.60 - ETA: 0s - loss: 0.6691 - accuracy: 0.60 - ETA: 0s - loss: 0.6685 - accuracy: 0.60 - ETA: 0s - loss: 0.6682 - accuracy: 0.60 - ETA: 0s - loss: 0.6679 - accuracy: 0.61 - ETA: 0s - loss: 0.6677 - accuracy: 0.61 - ETA: 0s - loss: 0.6672 - accuracy: 0.61 - ETA: 0s - loss: 0.6668 - accuracy: 0.61 - ETA: 0s - loss: 0.6663 - accuracy: 0.61 - ETA: 0s - loss: 0.6658 - accuracy: 0.61 - ETA: 0s - loss: 0.6656 - accuracy: 0.61 - ETA: 0s - loss: 0.6654 - accuracy: 0.61 - ETA: 0s - loss: 0.6650 - accuracy: 0.61 - ETA: 0s - loss: 0.6645 - accuracy: 0.61 - 3s 262us/step - loss: 0.6640 - accuracy: 0.6175 - val_loss: 0.6300 - val_accuracy: 0.7095\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:00 - loss: 0.6911 - accuracy: 0.75 - ETA: 8s - loss: 0.6943 - accuracy: 0.5227 - ETA: 5s - loss: 0.6935 - accuracy: 0.52 - ETA: 4s - loss: 0.6911 - accuracy: 0.53 - ETA: 3s - loss: 0.6909 - accuracy: 0.53 - ETA: 3s - loss: 0.6909 - accuracy: 0.53 - ETA: 3s - loss: 0.6893 - accuracy: 0.54 - ETA: 3s - loss: 0.6898 - accuracy: 0.53 - ETA: 3s - loss: 0.6894 - accuracy: 0.54 - ETA: 2s - loss: 0.6890 - accuracy: 0.54 - ETA: 2s - loss: 0.6888 - accuracy: 0.53 - ETA: 2s - loss: 0.6879 - accuracy: 0.54 - ETA: 2s - loss: 0.6874 - accuracy: 0.54 - ETA: 2s - loss: 0.6870 - accuracy: 0.54 - ETA: 2s - loss: 0.6871 - accuracy: 0.54 - ETA: 2s - loss: 0.6870 - accuracy: 0.53 - ETA: 2s - loss: 0.6863 - accuracy: 0.54 - ETA: 2s - loss: 0.6859 - accuracy: 0.54 - ETA: 2s - loss: 0.6854 - accuracy: 0.55 - ETA: 2s - loss: 0.6852 - accuracy: 0.55 - ETA: 2s - loss: 0.6845 - accuracy: 0.55 - ETA: 2s - loss: 0.6840 - accuracy: 0.56 - ETA: 1s - loss: 0.6832 - accuracy: 0.56 - ETA: 1s - loss: 0.6828 - accuracy: 0.56 - ETA: 1s - loss: 0.6824 - accuracy: 0.56 - ETA: 1s - loss: 0.6820 - accuracy: 0.57 - ETA: 1s - loss: 0.6815 - accuracy: 0.57 - ETA: 1s - loss: 0.6806 - accuracy: 0.57 - ETA: 1s - loss: 0.6798 - accuracy: 0.58 - ETA: 1s - loss: 0.6793 - accuracy: 0.58 - ETA: 1s - loss: 0.6789 - accuracy: 0.58 - ETA: 1s - loss: 0.6782 - accuracy: 0.58 - ETA: 1s - loss: 0.6780 - accuracy: 0.58 - ETA: 1s - loss: 0.6775 - accuracy: 0.58 - ETA: 1s - loss: 0.6767 - accuracy: 0.58 - ETA: 1s - loss: 0.6761 - accuracy: 0.59 - ETA: 1s - loss: 0.6760 - accuracy: 0.59 - ETA: 1s - loss: 0.6756 - accuracy: 0.59 - ETA: 1s - loss: 0.6750 - accuracy: 0.59 - ETA: 1s - loss: 0.6746 - accuracy: 0.59 - ETA: 0s - loss: 0.6737 - accuracy: 0.59 - ETA: 0s - loss: 0.6729 - accuracy: 0.60 - ETA: 0s - loss: 0.6725 - accuracy: 0.60 - ETA: 0s - loss: 0.6719 - accuracy: 0.60 - ETA: 0s - loss: 0.6716 - accuracy: 0.60 - ETA: 0s - loss: 0.6708 - accuracy: 0.60 - ETA: 0s - loss: 0.6699 - accuracy: 0.60 - ETA: 0s - loss: 0.6696 - accuracy: 0.60 - ETA: 0s - loss: 0.6689 - accuracy: 0.60 - ETA: 0s - loss: 0.6683 - accuracy: 0.61 - ETA: 0s - loss: 0.6678 - accuracy: 0.61 - ETA: 0s - loss: 0.6675 - accuracy: 0.61 - ETA: 0s - loss: 0.6669 - accuracy: 0.61 - ETA: 0s - loss: 0.6665 - accuracy: 0.61 - ETA: 0s - loss: 0.6657 - accuracy: 0.61 - ETA: 0s - loss: 0.6649 - accuracy: 0.61 - ETA: 0s - loss: 0.6644 - accuracy: 0.61 - ETA: 0s - loss: 0.6642 - accuracy: 0.61 - ETA: 0s - loss: 0.6640 - accuracy: 0.61 - 3s 256us/step - loss: 0.6641 - accuracy: 0.6173 - val_loss: 0.6294 - val_accuracy: 0.7109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:10 - loss: 0.6970 - accuracy: 0.25 - ETA: 8s - loss: 0.6960 - accuracy: 0.4909 - ETA: 5s - loss: 0.6955 - accuracy: 0.50 - ETA: 4s - loss: 0.6934 - accuracy: 0.53 - ETA: 4s - loss: 0.6929 - accuracy: 0.54 - ETA: 3s - loss: 0.6914 - accuracy: 0.55 - ETA: 3s - loss: 0.6904 - accuracy: 0.56 - ETA: 3s - loss: 0.6898 - accuracy: 0.57 - ETA: 3s - loss: 0.6894 - accuracy: 0.56 - ETA: 3s - loss: 0.6891 - accuracy: 0.56 - ETA: 2s - loss: 0.6885 - accuracy: 0.57 - ETA: 2s - loss: 0.6882 - accuracy: 0.57 - ETA: 2s - loss: 0.6873 - accuracy: 0.58 - ETA: 2s - loss: 0.6868 - accuracy: 0.58 - ETA: 2s - loss: 0.6855 - accuracy: 0.58 - ETA: 2s - loss: 0.6849 - accuracy: 0.59 - ETA: 2s - loss: 0.6840 - accuracy: 0.59 - ETA: 2s - loss: 0.6831 - accuracy: 0.59 - ETA: 2s - loss: 0.6816 - accuracy: 0.60 - ETA: 2s - loss: 0.6812 - accuracy: 0.60 - ETA: 2s - loss: 0.6811 - accuracy: 0.59 - ETA: 2s - loss: 0.6807 - accuracy: 0.59 - ETA: 2s - loss: 0.6801 - accuracy: 0.59 - ETA: 1s - loss: 0.6797 - accuracy: 0.59 - ETA: 1s - loss: 0.6783 - accuracy: 0.60 - ETA: 1s - loss: 0.6782 - accuracy: 0.60 - ETA: 1s - loss: 0.6771 - accuracy: 0.60 - ETA: 1s - loss: 0.6767 - accuracy: 0.60 - ETA: 1s - loss: 0.6760 - accuracy: 0.60 - ETA: 1s - loss: 0.6754 - accuracy: 0.60 - ETA: 1s - loss: 0.6747 - accuracy: 0.61 - ETA: 1s - loss: 0.6735 - accuracy: 0.61 - ETA: 1s - loss: 0.6731 - accuracy: 0.61 - ETA: 1s - loss: 0.6724 - accuracy: 0.61 - ETA: 1s - loss: 0.6714 - accuracy: 0.61 - ETA: 1s - loss: 0.6705 - accuracy: 0.62 - ETA: 1s - loss: 0.6697 - accuracy: 0.62 - ETA: 1s - loss: 0.6691 - accuracy: 0.62 - ETA: 1s - loss: 0.6684 - accuracy: 0.62 - ETA: 1s - loss: 0.6674 - accuracy: 0.62 - ETA: 1s - loss: 0.6671 - accuracy: 0.62 - ETA: 0s - loss: 0.6663 - accuracy: 0.62 - ETA: 0s - loss: 0.6658 - accuracy: 0.62 - ETA: 0s - loss: 0.6650 - accuracy: 0.63 - ETA: 0s - loss: 0.6649 - accuracy: 0.62 - ETA: 0s - loss: 0.6643 - accuracy: 0.63 - ETA: 0s - loss: 0.6642 - accuracy: 0.63 - ETA: 0s - loss: 0.6634 - accuracy: 0.63 - ETA: 0s - loss: 0.6629 - accuracy: 0.63 - ETA: 0s - loss: 0.6628 - accuracy: 0.63 - ETA: 0s - loss: 0.6626 - accuracy: 0.63 - ETA: 0s - loss: 0.6623 - accuracy: 0.63 - ETA: 0s - loss: 0.6620 - accuracy: 0.63 - ETA: 0s - loss: 0.6616 - accuracy: 0.63 - ETA: 0s - loss: 0.6611 - accuracy: 0.63 - ETA: 0s - loss: 0.6607 - accuracy: 0.63 - ETA: 0s - loss: 0.6604 - accuracy: 0.63 - ETA: 0s - loss: 0.6600 - accuracy: 0.63 - ETA: 0s - loss: 0.6598 - accuracy: 0.63 - ETA: 0s - loss: 0.6590 - accuracy: 0.63 - 3s 261us/step - loss: 0.6588 - accuracy: 0.6362 - val_loss: 0.6206 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:26 - loss: 0.6934 - accuracy: 0.75 - ETA: 8s - loss: 0.6897 - accuracy: 0.5364 - ETA: 5s - loss: 0.6937 - accuracy: 0.51 - ETA: 4s - loss: 0.6930 - accuracy: 0.51 - ETA: 4s - loss: 0.6903 - accuracy: 0.52 - ETA: 3s - loss: 0.6876 - accuracy: 0.52 - ETA: 3s - loss: 0.6880 - accuracy: 0.52 - ETA: 3s - loss: 0.6876 - accuracy: 0.52 - ETA: 3s - loss: 0.6867 - accuracy: 0.53 - ETA: 3s - loss: 0.6863 - accuracy: 0.53 - ETA: 2s - loss: 0.6853 - accuracy: 0.54 - ETA: 2s - loss: 0.6854 - accuracy: 0.54 - ETA: 2s - loss: 0.6852 - accuracy: 0.55 - ETA: 2s - loss: 0.6833 - accuracy: 0.55 - ETA: 2s - loss: 0.6825 - accuracy: 0.56 - ETA: 2s - loss: 0.6812 - accuracy: 0.56 - ETA: 2s - loss: 0.6798 - accuracy: 0.56 - ETA: 2s - loss: 0.6800 - accuracy: 0.56 - ETA: 2s - loss: 0.6791 - accuracy: 0.57 - ETA: 2s - loss: 0.6782 - accuracy: 0.57 - ETA: 2s - loss: 0.6781 - accuracy: 0.57 - ETA: 2s - loss: 0.6770 - accuracy: 0.58 - ETA: 1s - loss: 0.6761 - accuracy: 0.58 - ETA: 1s - loss: 0.6756 - accuracy: 0.58 - ETA: 1s - loss: 0.6751 - accuracy: 0.58 - ETA: 1s - loss: 0.6740 - accuracy: 0.59 - ETA: 1s - loss: 0.6728 - accuracy: 0.59 - ETA: 1s - loss: 0.6722 - accuracy: 0.59 - ETA: 1s - loss: 0.6715 - accuracy: 0.60 - ETA: 1s - loss: 0.6709 - accuracy: 0.60 - ETA: 1s - loss: 0.6702 - accuracy: 0.60 - ETA: 1s - loss: 0.6695 - accuracy: 0.60 - ETA: 1s - loss: 0.6689 - accuracy: 0.60 - ETA: 1s - loss: 0.6688 - accuracy: 0.60 - ETA: 1s - loss: 0.6682 - accuracy: 0.60 - ETA: 1s - loss: 0.6680 - accuracy: 0.60 - ETA: 1s - loss: 0.6674 - accuracy: 0.61 - ETA: 1s - loss: 0.6661 - accuracy: 0.61 - ETA: 1s - loss: 0.6655 - accuracy: 0.61 - ETA: 1s - loss: 0.6650 - accuracy: 0.61 - ETA: 1s - loss: 0.6646 - accuracy: 0.61 - ETA: 0s - loss: 0.6641 - accuracy: 0.61 - ETA: 0s - loss: 0.6638 - accuracy: 0.61 - ETA: 0s - loss: 0.6634 - accuracy: 0.62 - ETA: 0s - loss: 0.6624 - accuracy: 0.62 - ETA: 0s - loss: 0.6616 - accuracy: 0.62 - ETA: 0s - loss: 0.6614 - accuracy: 0.62 - ETA: 0s - loss: 0.6612 - accuracy: 0.62 - ETA: 0s - loss: 0.6605 - accuracy: 0.62 - ETA: 0s - loss: 0.6601 - accuracy: 0.62 - ETA: 0s - loss: 0.6598 - accuracy: 0.62 - ETA: 0s - loss: 0.6600 - accuracy: 0.62 - ETA: 0s - loss: 0.6598 - accuracy: 0.62 - ETA: 0s - loss: 0.6593 - accuracy: 0.62 - ETA: 0s - loss: 0.6586 - accuracy: 0.62 - ETA: 0s - loss: 0.6583 - accuracy: 0.63 - ETA: 0s - loss: 0.6578 - accuracy: 0.63 - ETA: 0s - loss: 0.6570 - accuracy: 0.63 - ETA: 0s - loss: 0.6568 - accuracy: 0.63 - 3s 260us/step - loss: 0.6561 - accuracy: 0.6352 - val_loss: 0.6215 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:16 - loss: 0.6938 - accuracy: 0.50 - ETA: 8s - loss: 0.6947 - accuracy: 0.5046 - ETA: 5s - loss: 0.6918 - accuracy: 0.54 - ETA: 4s - loss: 0.6916 - accuracy: 0.54 - ETA: 4s - loss: 0.6895 - accuracy: 0.56 - ETA: 3s - loss: 0.6885 - accuracy: 0.56 - ETA: 3s - loss: 0.6881 - accuracy: 0.56 - ETA: 3s - loss: 0.6882 - accuracy: 0.56 - ETA: 3s - loss: 0.6877 - accuracy: 0.56 - ETA: 3s - loss: 0.6861 - accuracy: 0.57 - ETA: 2s - loss: 0.6847 - accuracy: 0.57 - ETA: 2s - loss: 0.6835 - accuracy: 0.58 - ETA: 2s - loss: 0.6835 - accuracy: 0.58 - ETA: 2s - loss: 0.6835 - accuracy: 0.58 - ETA: 2s - loss: 0.6823 - accuracy: 0.59 - ETA: 2s - loss: 0.6816 - accuracy: 0.59 - ETA: 2s - loss: 0.6807 - accuracy: 0.60 - ETA: 2s - loss: 0.6807 - accuracy: 0.59 - ETA: 2s - loss: 0.6797 - accuracy: 0.59 - ETA: 2s - loss: 0.6786 - accuracy: 0.60 - ETA: 2s - loss: 0.6774 - accuracy: 0.60 - ETA: 2s - loss: 0.6768 - accuracy: 0.60 - ETA: 2s - loss: 0.6757 - accuracy: 0.61 - ETA: 2s - loss: 0.6751 - accuracy: 0.61 - ETA: 1s - loss: 0.6745 - accuracy: 0.61 - ETA: 1s - loss: 0.6738 - accuracy: 0.61 - ETA: 1s - loss: 0.6730 - accuracy: 0.61 - ETA: 1s - loss: 0.6725 - accuracy: 0.61 - ETA: 1s - loss: 0.6717 - accuracy: 0.62 - ETA: 1s - loss: 0.6707 - accuracy: 0.62 - ETA: 1s - loss: 0.6695 - accuracy: 0.62 - ETA: 1s - loss: 0.6691 - accuracy: 0.62 - ETA: 1s - loss: 0.6686 - accuracy: 0.62 - ETA: 1s - loss: 0.6682 - accuracy: 0.62 - ETA: 1s - loss: 0.6677 - accuracy: 0.62 - ETA: 1s - loss: 0.6666 - accuracy: 0.62 - ETA: 1s - loss: 0.6663 - accuracy: 0.62 - ETA: 1s - loss: 0.6658 - accuracy: 0.62 - ETA: 1s - loss: 0.6656 - accuracy: 0.62 - ETA: 1s - loss: 0.6656 - accuracy: 0.62 - ETA: 1s - loss: 0.6652 - accuracy: 0.62 - ETA: 0s - loss: 0.6646 - accuracy: 0.62 - ETA: 0s - loss: 0.6642 - accuracy: 0.63 - ETA: 0s - loss: 0.6641 - accuracy: 0.63 - ETA: 0s - loss: 0.6636 - accuracy: 0.63 - ETA: 0s - loss: 0.6633 - accuracy: 0.63 - ETA: 0s - loss: 0.6633 - accuracy: 0.63 - ETA: 0s - loss: 0.6631 - accuracy: 0.63 - ETA: 0s - loss: 0.6625 - accuracy: 0.63 - ETA: 0s - loss: 0.6622 - accuracy: 0.63 - ETA: 0s - loss: 0.6615 - accuracy: 0.63 - ETA: 0s - loss: 0.6612 - accuracy: 0.63 - ETA: 0s - loss: 0.6609 - accuracy: 0.63 - ETA: 0s - loss: 0.6604 - accuracy: 0.63 - ETA: 0s - loss: 0.6596 - accuracy: 0.63 - ETA: 0s - loss: 0.6593 - accuracy: 0.64 - ETA: 0s - loss: 0.6586 - accuracy: 0.64 - ETA: 0s - loss: 0.6582 - accuracy: 0.64 - ETA: 0s - loss: 0.6576 - accuracy: 0.64 - ETA: 0s - loss: 0.6571 - accuracy: 0.64 - 3s 262us/step - loss: 0.6570 - accuracy: 0.6430 - val_loss: 0.6230 - val_accuracy: 0.7152\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:07 - loss: 0.7009 - accuracy: 0.0000e+ - ETA: 8s - loss: 0.6953 - accuracy: 0.4474     - ETA: 5s - loss: 0.6941 - accuracy: 0.48 - ETA: 4s - loss: 0.6929 - accuracy: 0.50 - ETA: 4s - loss: 0.6916 - accuracy: 0.51 - ETA: 3s - loss: 0.6919 - accuracy: 0.51 - ETA: 3s - loss: 0.6917 - accuracy: 0.51 - ETA: 3s - loss: 0.6911 - accuracy: 0.51 - ETA: 3s - loss: 0.6903 - accuracy: 0.52 - ETA: 3s - loss: 0.6897 - accuracy: 0.53 - ETA: 2s - loss: 0.6890 - accuracy: 0.53 - ETA: 2s - loss: 0.6887 - accuracy: 0.54 - ETA: 2s - loss: 0.6883 - accuracy: 0.54 - ETA: 2s - loss: 0.6876 - accuracy: 0.54 - ETA: 2s - loss: 0.6871 - accuracy: 0.55 - ETA: 2s - loss: 0.6867 - accuracy: 0.55 - ETA: 2s - loss: 0.6862 - accuracy: 0.55 - ETA: 2s - loss: 0.6856 - accuracy: 0.56 - ETA: 2s - loss: 0.6848 - accuracy: 0.56 - ETA: 2s - loss: 0.6841 - accuracy: 0.57 - ETA: 2s - loss: 0.6835 - accuracy: 0.57 - ETA: 2s - loss: 0.6829 - accuracy: 0.57 - ETA: 1s - loss: 0.6829 - accuracy: 0.57 - ETA: 1s - loss: 0.6821 - accuracy: 0.57 - ETA: 1s - loss: 0.6814 - accuracy: 0.58 - ETA: 1s - loss: 0.6807 - accuracy: 0.58 - ETA: 1s - loss: 0.6802 - accuracy: 0.58 - ETA: 1s - loss: 0.6798 - accuracy: 0.58 - ETA: 1s - loss: 0.6792 - accuracy: 0.58 - ETA: 1s - loss: 0.6785 - accuracy: 0.59 - ETA: 1s - loss: 0.6780 - accuracy: 0.59 - ETA: 1s - loss: 0.6772 - accuracy: 0.59 - ETA: 1s - loss: 0.6765 - accuracy: 0.59 - ETA: 1s - loss: 0.6756 - accuracy: 0.59 - ETA: 1s - loss: 0.6750 - accuracy: 0.60 - ETA: 1s - loss: 0.6745 - accuracy: 0.60 - ETA: 1s - loss: 0.6739 - accuracy: 0.60 - ETA: 1s - loss: 0.6734 - accuracy: 0.60 - ETA: 1s - loss: 0.6728 - accuracy: 0.60 - ETA: 1s - loss: 0.6722 - accuracy: 0.60 - ETA: 0s - loss: 0.6717 - accuracy: 0.60 - ETA: 0s - loss: 0.6715 - accuracy: 0.60 - ETA: 0s - loss: 0.6709 - accuracy: 0.61 - ETA: 0s - loss: 0.6700 - accuracy: 0.61 - ETA: 0s - loss: 0.6696 - accuracy: 0.61 - ETA: 0s - loss: 0.6690 - accuracy: 0.61 - ETA: 0s - loss: 0.6687 - accuracy: 0.61 - ETA: 0s - loss: 0.6685 - accuracy: 0.61 - ETA: 0s - loss: 0.6685 - accuracy: 0.61 - ETA: 0s - loss: 0.6674 - accuracy: 0.61 - ETA: 0s - loss: 0.6671 - accuracy: 0.61 - ETA: 0s - loss: 0.6661 - accuracy: 0.62 - ETA: 0s - loss: 0.6653 - accuracy: 0.62 - ETA: 0s - loss: 0.6649 - accuracy: 0.62 - ETA: 0s - loss: 0.6642 - accuracy: 0.62 - ETA: 0s - loss: 0.6637 - accuracy: 0.62 - ETA: 0s - loss: 0.6630 - accuracy: 0.62 - ETA: 0s - loss: 0.6628 - accuracy: 0.62 - ETA: 0s - loss: 0.6620 - accuracy: 0.62 - 3s 256us/step - loss: 0.6619 - accuracy: 0.6276 - val_loss: 0.6318 - val_accuracy: 0.7010\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 107us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 6:04 - loss: 0.6953 - accuracy: 0.50 - ETA: 9s - loss: 0.6937 - accuracy: 0.5093 - ETA: 6s - loss: 0.6937 - accuracy: 0.50 - ETA: 5s - loss: 0.6934 - accuracy: 0.52 - ETA: 4s - loss: 0.6933 - accuracy: 0.51 - ETA: 4s - loss: 0.6930 - accuracy: 0.51 - ETA: 3s - loss: 0.6932 - accuracy: 0.52 - ETA: 3s - loss: 0.6926 - accuracy: 0.53 - ETA: 3s - loss: 0.6926 - accuracy: 0.52 - ETA: 3s - loss: 0.6917 - accuracy: 0.53 - ETA: 3s - loss: 0.6906 - accuracy: 0.52 - ETA: 3s - loss: 0.6905 - accuracy: 0.53 - ETA: 2s - loss: 0.6901 - accuracy: 0.53 - ETA: 2s - loss: 0.6898 - accuracy: 0.53 - ETA: 2s - loss: 0.6888 - accuracy: 0.54 - ETA: 2s - loss: 0.6881 - accuracy: 0.54 - ETA: 2s - loss: 0.6874 - accuracy: 0.54 - ETA: 2s - loss: 0.6861 - accuracy: 0.54 - ETA: 2s - loss: 0.6844 - accuracy: 0.54 - ETA: 2s - loss: 0.6828 - accuracy: 0.54 - ETA: 2s - loss: 0.6828 - accuracy: 0.55 - ETA: 2s - loss: 0.6817 - accuracy: 0.55 - ETA: 2s - loss: 0.6804 - accuracy: 0.55 - ETA: 2s - loss: 0.6796 - accuracy: 0.55 - ETA: 2s - loss: 0.6790 - accuracy: 0.55 - ETA: 2s - loss: 0.6783 - accuracy: 0.55 - ETA: 1s - loss: 0.6777 - accuracy: 0.56 - ETA: 1s - loss: 0.6770 - accuracy: 0.56 - ETA: 1s - loss: 0.6767 - accuracy: 0.56 - ETA: 1s - loss: 0.6758 - accuracy: 0.56 - ETA: 1s - loss: 0.6745 - accuracy: 0.56 - ETA: 1s - loss: 0.6738 - accuracy: 0.57 - ETA: 1s - loss: 0.6729 - accuracy: 0.57 - ETA: 1s - loss: 0.6720 - accuracy: 0.57 - ETA: 1s - loss: 0.6709 - accuracy: 0.57 - ETA: 1s - loss: 0.6705 - accuracy: 0.57 - ETA: 1s - loss: 0.6699 - accuracy: 0.57 - ETA: 1s - loss: 0.6687 - accuracy: 0.58 - ETA: 1s - loss: 0.6676 - accuracy: 0.58 - ETA: 1s - loss: 0.6670 - accuracy: 0.58 - ETA: 1s - loss: 0.6665 - accuracy: 0.58 - ETA: 1s - loss: 0.6665 - accuracy: 0.58 - ETA: 1s - loss: 0.6657 - accuracy: 0.59 - ETA: 0s - loss: 0.6652 - accuracy: 0.59 - ETA: 0s - loss: 0.6651 - accuracy: 0.59 - ETA: 0s - loss: 0.6642 - accuracy: 0.59 - ETA: 0s - loss: 0.6643 - accuracy: 0.59 - ETA: 0s - loss: 0.6639 - accuracy: 0.59 - ETA: 0s - loss: 0.6637 - accuracy: 0.59 - ETA: 0s - loss: 0.6626 - accuracy: 0.59 - ETA: 0s - loss: 0.6622 - accuracy: 0.59 - ETA: 0s - loss: 0.6615 - accuracy: 0.59 - ETA: 0s - loss: 0.6612 - accuracy: 0.59 - ETA: 0s - loss: 0.6607 - accuracy: 0.59 - ETA: 0s - loss: 0.6602 - accuracy: 0.60 - ETA: 0s - loss: 0.6601 - accuracy: 0.60 - ETA: 0s - loss: 0.6597 - accuracy: 0.60 - ETA: 0s - loss: 0.6595 - accuracy: 0.60 - ETA: 0s - loss: 0.6590 - accuracy: 0.60 - ETA: 0s - loss: 0.6587 - accuracy: 0.60 - ETA: 0s - loss: 0.6585 - accuracy: 0.60 - ETA: 0s - loss: 0.6579 - accuracy: 0.60 - 3s 271us/step - loss: 0.6576 - accuracy: 0.6059 - val_loss: 0.6083 - val_accuracy: 0.7109\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 99us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:07 - loss: 0.6936 - accuracy: 1.00 - ETA: 9s - loss: 0.6926 - accuracy: 0.5943 - ETA: 6s - loss: 0.6932 - accuracy: 0.55 - ETA: 5s - loss: 0.6936 - accuracy: 0.52 - ETA: 4s - loss: 0.6933 - accuracy: 0.53 - ETA: 4s - loss: 0.6930 - accuracy: 0.52 - ETA: 3s - loss: 0.6908 - accuracy: 0.53 - ETA: 3s - loss: 0.6908 - accuracy: 0.52 - ETA: 3s - loss: 0.6902 - accuracy: 0.52 - ETA: 3s - loss: 0.6898 - accuracy: 0.52 - ETA: 3s - loss: 0.6893 - accuracy: 0.53 - ETA: 3s - loss: 0.6889 - accuracy: 0.52 - ETA: 3s - loss: 0.6872 - accuracy: 0.53 - ETA: 2s - loss: 0.6869 - accuracy: 0.52 - ETA: 2s - loss: 0.6868 - accuracy: 0.52 - ETA: 2s - loss: 0.6861 - accuracy: 0.53 - ETA: 2s - loss: 0.6851 - accuracy: 0.53 - ETA: 2s - loss: 0.6839 - accuracy: 0.53 - ETA: 2s - loss: 0.6848 - accuracy: 0.53 - ETA: 2s - loss: 0.6848 - accuracy: 0.54 - ETA: 2s - loss: 0.6837 - accuracy: 0.54 - ETA: 2s - loss: 0.6837 - accuracy: 0.54 - ETA: 2s - loss: 0.6828 - accuracy: 0.54 - ETA: 2s - loss: 0.6819 - accuracy: 0.54 - ETA: 2s - loss: 0.6816 - accuracy: 0.54 - ETA: 2s - loss: 0.6802 - accuracy: 0.54 - ETA: 1s - loss: 0.6789 - accuracy: 0.54 - ETA: 1s - loss: 0.6787 - accuracy: 0.54 - ETA: 1s - loss: 0.6782 - accuracy: 0.54 - ETA: 1s - loss: 0.6775 - accuracy: 0.54 - ETA: 1s - loss: 0.6763 - accuracy: 0.55 - ETA: 1s - loss: 0.6759 - accuracy: 0.55 - ETA: 1s - loss: 0.6758 - accuracy: 0.55 - ETA: 1s - loss: 0.6748 - accuracy: 0.55 - ETA: 1s - loss: 0.6742 - accuracy: 0.55 - ETA: 1s - loss: 0.6736 - accuracy: 0.55 - ETA: 1s - loss: 0.6736 - accuracy: 0.55 - ETA: 1s - loss: 0.6736 - accuracy: 0.55 - ETA: 1s - loss: 0.6735 - accuracy: 0.55 - ETA: 1s - loss: 0.6720 - accuracy: 0.55 - ETA: 1s - loss: 0.6716 - accuracy: 0.55 - ETA: 1s - loss: 0.6718 - accuracy: 0.55 - ETA: 1s - loss: 0.6719 - accuracy: 0.55 - ETA: 1s - loss: 0.6716 - accuracy: 0.55 - ETA: 0s - loss: 0.6711 - accuracy: 0.55 - ETA: 0s - loss: 0.6709 - accuracy: 0.56 - ETA: 0s - loss: 0.6703 - accuracy: 0.56 - ETA: 0s - loss: 0.6703 - accuracy: 0.56 - ETA: 0s - loss: 0.6701 - accuracy: 0.56 - ETA: 0s - loss: 0.6696 - accuracy: 0.56 - ETA: 0s - loss: 0.6691 - accuracy: 0.56 - ETA: 0s - loss: 0.6686 - accuracy: 0.56 - ETA: 0s - loss: 0.6682 - accuracy: 0.56 - ETA: 0s - loss: 0.6679 - accuracy: 0.56 - ETA: 0s - loss: 0.6677 - accuracy: 0.56 - ETA: 0s - loss: 0.6668 - accuracy: 0.56 - ETA: 0s - loss: 0.6663 - accuracy: 0.56 - ETA: 0s - loss: 0.6661 - accuracy: 0.56 - ETA: 0s - loss: 0.6655 - accuracy: 0.56 - ETA: 0s - loss: 0.6653 - accuracy: 0.56 - ETA: 0s - loss: 0.6653 - accuracy: 0.56 - ETA: 0s - loss: 0.6649 - accuracy: 0.56 - ETA: 0s - loss: 0.6642 - accuracy: 0.56 - ETA: 0s - loss: 0.6642 - accuracy: 0.56 - 4s 278us/step - loss: 0.6642 - accuracy: 0.5693 - val_loss: 0.6214 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 104us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:04 - loss: 0.6985 - accuracy: 0.25 - ETA: 9s - loss: 0.6941 - accuracy: 0.4676 - ETA: 6s - loss: 0.6938 - accuracy: 0.47 - ETA: 5s - loss: 0.6931 - accuracy: 0.51 - ETA: 4s - loss: 0.6935 - accuracy: 0.50 - ETA: 4s - loss: 0.6933 - accuracy: 0.50 - ETA: 3s - loss: 0.6933 - accuracy: 0.51 - ETA: 3s - loss: 0.6931 - accuracy: 0.51 - ETA: 3s - loss: 0.6931 - accuracy: 0.51 - ETA: 3s - loss: 0.6929 - accuracy: 0.52 - ETA: 3s - loss: 0.6926 - accuracy: 0.52 - ETA: 3s - loss: 0.6920 - accuracy: 0.53 - ETA: 2s - loss: 0.6916 - accuracy: 0.53 - ETA: 2s - loss: 0.6914 - accuracy: 0.53 - ETA: 2s - loss: 0.6911 - accuracy: 0.54 - ETA: 2s - loss: 0.6901 - accuracy: 0.54 - ETA: 2s - loss: 0.6897 - accuracy: 0.54 - ETA: 2s - loss: 0.6891 - accuracy: 0.55 - ETA: 2s - loss: 0.6881 - accuracy: 0.55 - ETA: 2s - loss: 0.6876 - accuracy: 0.55 - ETA: 2s - loss: 0.6872 - accuracy: 0.56 - ETA: 2s - loss: 0.6870 - accuracy: 0.56 - ETA: 2s - loss: 0.6866 - accuracy: 0.56 - ETA: 2s - loss: 0.6858 - accuracy: 0.56 - ETA: 2s - loss: 0.6848 - accuracy: 0.57 - ETA: 2s - loss: 0.6843 - accuracy: 0.57 - ETA: 1s - loss: 0.6835 - accuracy: 0.57 - ETA: 1s - loss: 0.6829 - accuracy: 0.58 - ETA: 1s - loss: 0.6823 - accuracy: 0.58 - ETA: 1s - loss: 0.6815 - accuracy: 0.58 - ETA: 1s - loss: 0.6805 - accuracy: 0.58 - ETA: 1s - loss: 0.6798 - accuracy: 0.59 - ETA: 1s - loss: 0.6793 - accuracy: 0.59 - ETA: 1s - loss: 0.6783 - accuracy: 0.59 - ETA: 1s - loss: 0.6774 - accuracy: 0.59 - ETA: 1s - loss: 0.6769 - accuracy: 0.59 - ETA: 1s - loss: 0.6763 - accuracy: 0.60 - ETA: 1s - loss: 0.6760 - accuracy: 0.60 - ETA: 1s - loss: 0.6754 - accuracy: 0.60 - ETA: 1s - loss: 0.6750 - accuracy: 0.60 - ETA: 1s - loss: 0.6741 - accuracy: 0.60 - ETA: 1s - loss: 0.6734 - accuracy: 0.60 - ETA: 1s - loss: 0.6730 - accuracy: 0.60 - ETA: 1s - loss: 0.6723 - accuracy: 0.60 - ETA: 0s - loss: 0.6711 - accuracy: 0.60 - ETA: 0s - loss: 0.6704 - accuracy: 0.61 - ETA: 0s - loss: 0.6698 - accuracy: 0.61 - ETA: 0s - loss: 0.6684 - accuracy: 0.61 - ETA: 0s - loss: 0.6680 - accuracy: 0.61 - ETA: 0s - loss: 0.6670 - accuracy: 0.61 - ETA: 0s - loss: 0.6664 - accuracy: 0.61 - ETA: 0s - loss: 0.6655 - accuracy: 0.61 - ETA: 0s - loss: 0.6649 - accuracy: 0.62 - ETA: 0s - loss: 0.6644 - accuracy: 0.62 - ETA: 0s - loss: 0.6636 - accuracy: 0.62 - ETA: 0s - loss: 0.6634 - accuracy: 0.62 - ETA: 0s - loss: 0.6630 - accuracy: 0.62 - ETA: 0s - loss: 0.6626 - accuracy: 0.62 - ETA: 0s - loss: 0.6619 - accuracy: 0.62 - ETA: 0s - loss: 0.6613 - accuracy: 0.62 - ETA: 0s - loss: 0.6608 - accuracy: 0.62 - ETA: 0s - loss: 0.6603 - accuracy: 0.62 - ETA: 0s - loss: 0.6597 - accuracy: 0.62 - 3s 275us/step - loss: 0.6596 - accuracy: 0.6299 - val_loss: 0.6119 - val_accuracy: 0.7145\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:01 - loss: 0.6805 - accuracy: 1.00 - ETA: 10s - loss: 0.6940 - accuracy: 0.4800 - ETA: 6s - loss: 0.6953 - accuracy: 0.477 - ETA: 5s - loss: 0.6948 - accuracy: 0.48 - ETA: 4s - loss: 0.6943 - accuracy: 0.50 - ETA: 4s - loss: 0.6937 - accuracy: 0.51 - ETA: 3s - loss: 0.6937 - accuracy: 0.51 - ETA: 3s - loss: 0.6935 - accuracy: 0.51 - ETA: 3s - loss: 0.6933 - accuracy: 0.52 - ETA: 3s - loss: 0.6931 - accuracy: 0.52 - ETA: 3s - loss: 0.6931 - accuracy: 0.52 - ETA: 3s - loss: 0.6930 - accuracy: 0.52 - ETA: 3s - loss: 0.6929 - accuracy: 0.52 - ETA: 2s - loss: 0.6927 - accuracy: 0.52 - ETA: 2s - loss: 0.6927 - accuracy: 0.52 - ETA: 2s - loss: 0.6926 - accuracy: 0.52 - ETA: 2s - loss: 0.6924 - accuracy: 0.52 - ETA: 2s - loss: 0.6922 - accuracy: 0.52 - ETA: 2s - loss: 0.6919 - accuracy: 0.53 - ETA: 2s - loss: 0.6918 - accuracy: 0.52 - ETA: 2s - loss: 0.6914 - accuracy: 0.53 - ETA: 2s - loss: 0.6912 - accuracy: 0.53 - ETA: 2s - loss: 0.6907 - accuracy: 0.54 - ETA: 2s - loss: 0.6901 - accuracy: 0.54 - ETA: 2s - loss: 0.6897 - accuracy: 0.54 - ETA: 2s - loss: 0.6891 - accuracy: 0.54 - ETA: 2s - loss: 0.6883 - accuracy: 0.54 - ETA: 1s - loss: 0.6882 - accuracy: 0.54 - ETA: 1s - loss: 0.6880 - accuracy: 0.55 - ETA: 1s - loss: 0.6876 - accuracy: 0.55 - ETA: 1s - loss: 0.6869 - accuracy: 0.55 - ETA: 1s - loss: 0.6863 - accuracy: 0.55 - ETA: 1s - loss: 0.6857 - accuracy: 0.55 - ETA: 1s - loss: 0.6850 - accuracy: 0.55 - ETA: 1s - loss: 0.6842 - accuracy: 0.55 - ETA: 1s - loss: 0.6830 - accuracy: 0.55 - ETA: 1s - loss: 0.6821 - accuracy: 0.56 - ETA: 1s - loss: 0.6819 - accuracy: 0.56 - ETA: 1s - loss: 0.6813 - accuracy: 0.56 - ETA: 1s - loss: 0.6803 - accuracy: 0.56 - ETA: 1s - loss: 0.6798 - accuracy: 0.56 - ETA: 1s - loss: 0.6791 - accuracy: 0.56 - ETA: 1s - loss: 0.6782 - accuracy: 0.56 - ETA: 1s - loss: 0.6778 - accuracy: 0.56 - ETA: 1s - loss: 0.6775 - accuracy: 0.56 - ETA: 0s - loss: 0.6772 - accuracy: 0.56 - ETA: 0s - loss: 0.6767 - accuracy: 0.56 - ETA: 0s - loss: 0.6756 - accuracy: 0.56 - ETA: 0s - loss: 0.6751 - accuracy: 0.56 - ETA: 0s - loss: 0.6750 - accuracy: 0.56 - ETA: 0s - loss: 0.6744 - accuracy: 0.57 - ETA: 0s - loss: 0.6736 - accuracy: 0.57 - ETA: 0s - loss: 0.6729 - accuracy: 0.57 - ETA: 0s - loss: 0.6724 - accuracy: 0.57 - ETA: 0s - loss: 0.6719 - accuracy: 0.57 - ETA: 0s - loss: 0.6717 - accuracy: 0.57 - ETA: 0s - loss: 0.6714 - accuracy: 0.57 - ETA: 0s - loss: 0.6707 - accuracy: 0.57 - ETA: 0s - loss: 0.6707 - accuracy: 0.57 - ETA: 0s - loss: 0.6703 - accuracy: 0.57 - ETA: 0s - loss: 0.6695 - accuracy: 0.57 - ETA: 0s - loss: 0.6688 - accuracy: 0.57 - ETA: 0s - loss: 0.6682 - accuracy: 0.57 - ETA: 0s - loss: 0.6672 - accuracy: 0.57 - 4s 279us/step - loss: 0.6671 - accuracy: 0.5773 - val_loss: 0.6176 - val_accuracy: 0.7166\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 100us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 6:01 - loss: 0.6958 - accuracy: 0.0000e+ - ETA: 9s - loss: 0.6937 - accuracy: 0.5096     - ETA: 6s - loss: 0.6938 - accuracy: 0.48 - ETA: 5s - loss: 0.6938 - accuracy: 0.48 - ETA: 4s - loss: 0.6938 - accuracy: 0.48 - ETA: 4s - loss: 0.6935 - accuracy: 0.49 - ETA: 3s - loss: 0.6931 - accuracy: 0.49 - ETA: 3s - loss: 0.6929 - accuracy: 0.50 - ETA: 3s - loss: 0.6927 - accuracy: 0.50 - ETA: 3s - loss: 0.6926 - accuracy: 0.50 - ETA: 3s - loss: 0.6922 - accuracy: 0.51 - ETA: 3s - loss: 0.6911 - accuracy: 0.51 - ETA: 2s - loss: 0.6906 - accuracy: 0.51 - ETA: 2s - loss: 0.6901 - accuracy: 0.52 - ETA: 2s - loss: 0.6895 - accuracy: 0.52 - ETA: 2s - loss: 0.6892 - accuracy: 0.52 - ETA: 2s - loss: 0.6890 - accuracy: 0.52 - ETA: 2s - loss: 0.6890 - accuracy: 0.52 - ETA: 2s - loss: 0.6887 - accuracy: 0.52 - ETA: 2s - loss: 0.6878 - accuracy: 0.52 - ETA: 2s - loss: 0.6868 - accuracy: 0.53 - ETA: 2s - loss: 0.6863 - accuracy: 0.52 - ETA: 2s - loss: 0.6857 - accuracy: 0.53 - ETA: 2s - loss: 0.6855 - accuracy: 0.53 - ETA: 2s - loss: 0.6854 - accuracy: 0.53 - ETA: 1s - loss: 0.6850 - accuracy: 0.53 - ETA: 1s - loss: 0.6839 - accuracy: 0.53 - ETA: 1s - loss: 0.6833 - accuracy: 0.54 - ETA: 1s - loss: 0.6823 - accuracy: 0.54 - ETA: 1s - loss: 0.6815 - accuracy: 0.54 - ETA: 1s - loss: 0.6807 - accuracy: 0.54 - ETA: 1s - loss: 0.6794 - accuracy: 0.54 - ETA: 1s - loss: 0.6791 - accuracy: 0.54 - ETA: 1s - loss: 0.6782 - accuracy: 0.55 - ETA: 1s - loss: 0.6784 - accuracy: 0.55 - ETA: 1s - loss: 0.6775 - accuracy: 0.55 - ETA: 1s - loss: 0.6768 - accuracy: 0.55 - ETA: 1s - loss: 0.6761 - accuracy: 0.55 - ETA: 1s - loss: 0.6748 - accuracy: 0.55 - ETA: 1s - loss: 0.6740 - accuracy: 0.56 - ETA: 1s - loss: 0.6729 - accuracy: 0.56 - ETA: 1s - loss: 0.6724 - accuracy: 0.56 - ETA: 0s - loss: 0.6720 - accuracy: 0.56 - ETA: 0s - loss: 0.6715 - accuracy: 0.56 - ETA: 0s - loss: 0.6711 - accuracy: 0.56 - ETA: 0s - loss: 0.6699 - accuracy: 0.57 - ETA: 0s - loss: 0.6688 - accuracy: 0.57 - ETA: 0s - loss: 0.6685 - accuracy: 0.57 - ETA: 0s - loss: 0.6676 - accuracy: 0.57 - ETA: 0s - loss: 0.6668 - accuracy: 0.57 - ETA: 0s - loss: 0.6665 - accuracy: 0.57 - ETA: 0s - loss: 0.6660 - accuracy: 0.57 - ETA: 0s - loss: 0.6661 - accuracy: 0.57 - ETA: 0s - loss: 0.6653 - accuracy: 0.58 - ETA: 0s - loss: 0.6650 - accuracy: 0.58 - ETA: 0s - loss: 0.6638 - accuracy: 0.58 - ETA: 0s - loss: 0.6630 - accuracy: 0.58 - ETA: 0s - loss: 0.6626 - accuracy: 0.58 - ETA: 0s - loss: 0.6619 - accuracy: 0.58 - ETA: 0s - loss: 0.6614 - accuracy: 0.58 - ETA: 0s - loss: 0.6609 - accuracy: 0.58 - 3s 265us/step - loss: 0.6609 - accuracy: 0.5891 - val_loss: 0.6070 - val_accuracy: 0.7145\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 99us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:13 - loss: 0.6919 - accuracy: 0.75 - ETA: 10s - loss: 0.6925 - accuracy: 0.5577 - ETA: 6s - loss: 0.6935 - accuracy: 0.533 - ETA: 5s - loss: 0.6936 - accuracy: 0.50 - ETA: 4s - loss: 0.6933 - accuracy: 0.51 - ETA: 4s - loss: 0.6934 - accuracy: 0.51 - ETA: 3s - loss: 0.6928 - accuracy: 0.52 - ETA: 3s - loss: 0.6928 - accuracy: 0.52 - ETA: 3s - loss: 0.6923 - accuracy: 0.53 - ETA: 3s - loss: 0.6918 - accuracy: 0.53 - ETA: 3s - loss: 0.6911 - accuracy: 0.53 - ETA: 3s - loss: 0.6911 - accuracy: 0.53 - ETA: 2s - loss: 0.6907 - accuracy: 0.53 - ETA: 2s - loss: 0.6903 - accuracy: 0.53 - ETA: 2s - loss: 0.6894 - accuracy: 0.54 - ETA: 2s - loss: 0.6890 - accuracy: 0.54 - ETA: 2s - loss: 0.6880 - accuracy: 0.54 - ETA: 2s - loss: 0.6870 - accuracy: 0.55 - ETA: 2s - loss: 0.6865 - accuracy: 0.55 - ETA: 2s - loss: 0.6855 - accuracy: 0.55 - ETA: 2s - loss: 0.6829 - accuracy: 0.56 - ETA: 2s - loss: 0.6812 - accuracy: 0.56 - ETA: 2s - loss: 0.6797 - accuracy: 0.56 - ETA: 2s - loss: 0.6787 - accuracy: 0.56 - ETA: 2s - loss: 0.6774 - accuracy: 0.57 - ETA: 1s - loss: 0.6765 - accuracy: 0.57 - ETA: 1s - loss: 0.6747 - accuracy: 0.57 - ETA: 1s - loss: 0.6742 - accuracy: 0.57 - ETA: 1s - loss: 0.6728 - accuracy: 0.58 - ETA: 1s - loss: 0.6720 - accuracy: 0.58 - ETA: 1s - loss: 0.6713 - accuracy: 0.58 - ETA: 1s - loss: 0.6704 - accuracy: 0.58 - ETA: 1s - loss: 0.6698 - accuracy: 0.58 - ETA: 1s - loss: 0.6697 - accuracy: 0.58 - ETA: 1s - loss: 0.6685 - accuracy: 0.58 - ETA: 1s - loss: 0.6682 - accuracy: 0.58 - ETA: 1s - loss: 0.6676 - accuracy: 0.58 - ETA: 1s - loss: 0.6673 - accuracy: 0.58 - ETA: 1s - loss: 0.6657 - accuracy: 0.59 - ETA: 1s - loss: 0.6647 - accuracy: 0.59 - ETA: 1s - loss: 0.6636 - accuracy: 0.59 - ETA: 1s - loss: 0.6630 - accuracy: 0.59 - ETA: 1s - loss: 0.6623 - accuracy: 0.59 - ETA: 0s - loss: 0.6614 - accuracy: 0.59 - ETA: 0s - loss: 0.6610 - accuracy: 0.59 - ETA: 0s - loss: 0.6597 - accuracy: 0.59 - ETA: 0s - loss: 0.6594 - accuracy: 0.59 - ETA: 0s - loss: 0.6591 - accuracy: 0.59 - ETA: 0s - loss: 0.6583 - accuracy: 0.60 - ETA: 0s - loss: 0.6579 - accuracy: 0.60 - ETA: 0s - loss: 0.6571 - accuracy: 0.60 - ETA: 0s - loss: 0.6567 - accuracy: 0.60 - ETA: 0s - loss: 0.6561 - accuracy: 0.60 - ETA: 0s - loss: 0.6556 - accuracy: 0.60 - ETA: 0s - loss: 0.6550 - accuracy: 0.60 - ETA: 0s - loss: 0.6544 - accuracy: 0.60 - ETA: 0s - loss: 0.6535 - accuracy: 0.60 - ETA: 0s - loss: 0.6529 - accuracy: 0.60 - ETA: 0s - loss: 0.6526 - accuracy: 0.60 - ETA: 0s - loss: 0.6528 - accuracy: 0.60 - ETA: 0s - loss: 0.6521 - accuracy: 0.60 - ETA: 0s - loss: 0.6518 - accuracy: 0.60 - 3s 269us/step - loss: 0.6519 - accuracy: 0.6086 - val_loss: 0.6078 - val_accuracy: 0.7152\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:16 - loss: 0.6977 - accuracy: 0.25 - ETA: 9s - loss: 0.6936 - accuracy: 0.5185 - ETA: 6s - loss: 0.6938 - accuracy: 0.52 - ETA: 5s - loss: 0.6936 - accuracy: 0.53 - ETA: 4s - loss: 0.6929 - accuracy: 0.54 - ETA: 4s - loss: 0.6917 - accuracy: 0.54 - ETA: 3s - loss: 0.6902 - accuracy: 0.56 - ETA: 3s - loss: 0.6903 - accuracy: 0.55 - ETA: 3s - loss: 0.6887 - accuracy: 0.56 - ETA: 3s - loss: 0.6865 - accuracy: 0.57 - ETA: 3s - loss: 0.6859 - accuracy: 0.57 - ETA: 3s - loss: 0.6849 - accuracy: 0.57 - ETA: 3s - loss: 0.6838 - accuracy: 0.57 - ETA: 2s - loss: 0.6819 - accuracy: 0.58 - ETA: 2s - loss: 0.6798 - accuracy: 0.58 - ETA: 2s - loss: 0.6780 - accuracy: 0.58 - ETA: 2s - loss: 0.6757 - accuracy: 0.59 - ETA: 2s - loss: 0.6730 - accuracy: 0.59 - ETA: 2s - loss: 0.6727 - accuracy: 0.59 - ETA: 2s - loss: 0.6713 - accuracy: 0.59 - ETA: 2s - loss: 0.6704 - accuracy: 0.60 - ETA: 2s - loss: 0.6686 - accuracy: 0.60 - ETA: 2s - loss: 0.6675 - accuracy: 0.60 - ETA: 2s - loss: 0.6667 - accuracy: 0.60 - ETA: 2s - loss: 0.6657 - accuracy: 0.60 - ETA: 2s - loss: 0.6643 - accuracy: 0.61 - ETA: 1s - loss: 0.6625 - accuracy: 0.61 - ETA: 1s - loss: 0.6620 - accuracy: 0.61 - ETA: 1s - loss: 0.6612 - accuracy: 0.61 - ETA: 1s - loss: 0.6592 - accuracy: 0.61 - ETA: 1s - loss: 0.6596 - accuracy: 0.61 - ETA: 1s - loss: 0.6588 - accuracy: 0.61 - ETA: 1s - loss: 0.6583 - accuracy: 0.61 - ETA: 1s - loss: 0.6580 - accuracy: 0.61 - ETA: 1s - loss: 0.6568 - accuracy: 0.62 - ETA: 1s - loss: 0.6567 - accuracy: 0.62 - ETA: 1s - loss: 0.6546 - accuracy: 0.62 - ETA: 1s - loss: 0.6535 - accuracy: 0.62 - ETA: 1s - loss: 0.6523 - accuracy: 0.62 - ETA: 1s - loss: 0.6524 - accuracy: 0.62 - ETA: 1s - loss: 0.6520 - accuracy: 0.62 - ETA: 1s - loss: 0.6512 - accuracy: 0.62 - ETA: 1s - loss: 0.6504 - accuracy: 0.62 - ETA: 1s - loss: 0.6497 - accuracy: 0.62 - ETA: 0s - loss: 0.6486 - accuracy: 0.63 - ETA: 0s - loss: 0.6481 - accuracy: 0.63 - ETA: 0s - loss: 0.6477 - accuracy: 0.63 - ETA: 0s - loss: 0.6472 - accuracy: 0.63 - ETA: 0s - loss: 0.6460 - accuracy: 0.63 - ETA: 0s - loss: 0.6443 - accuracy: 0.63 - ETA: 0s - loss: 0.6439 - accuracy: 0.63 - ETA: 0s - loss: 0.6433 - accuracy: 0.63 - ETA: 0s - loss: 0.6422 - accuracy: 0.63 - ETA: 0s - loss: 0.6414 - accuracy: 0.63 - ETA: 0s - loss: 0.6404 - accuracy: 0.64 - ETA: 0s - loss: 0.6400 - accuracy: 0.64 - ETA: 0s - loss: 0.6400 - accuracy: 0.64 - ETA: 0s - loss: 0.6394 - accuracy: 0.64 - ETA: 0s - loss: 0.6389 - accuracy: 0.64 - ETA: 0s - loss: 0.6391 - accuracy: 0.64 - ETA: 0s - loss: 0.6382 - accuracy: 0.64 - ETA: 0s - loss: 0.6380 - accuracy: 0.64 - ETA: 0s - loss: 0.6379 - accuracy: 0.64 - 3s 276us/step - loss: 0.6376 - accuracy: 0.6432 - val_loss: 0.5876 - val_accuracy: 0.7294\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 106us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:20 - loss: 0.6895 - accuracy: 0.75 - ETA: 9s - loss: 0.6923 - accuracy: 0.5283 - ETA: 6s - loss: 0.6926 - accuracy: 0.52 - ETA: 5s - loss: 0.6935 - accuracy: 0.50 - ETA: 4s - loss: 0.6934 - accuracy: 0.51 - ETA: 4s - loss: 0.6926 - accuracy: 0.51 - ETA: 4s - loss: 0.6912 - accuracy: 0.52 - ETA: 3s - loss: 0.6911 - accuracy: 0.51 - ETA: 3s - loss: 0.6898 - accuracy: 0.52 - ETA: 3s - loss: 0.6876 - accuracy: 0.53 - ETA: 3s - loss: 0.6871 - accuracy: 0.53 - ETA: 3s - loss: 0.6851 - accuracy: 0.53 - ETA: 3s - loss: 0.6851 - accuracy: 0.54 - ETA: 2s - loss: 0.6830 - accuracy: 0.55 - ETA: 2s - loss: 0.6811 - accuracy: 0.55 - ETA: 2s - loss: 0.6807 - accuracy: 0.56 - ETA: 2s - loss: 0.6791 - accuracy: 0.56 - ETA: 2s - loss: 0.6782 - accuracy: 0.56 - ETA: 2s - loss: 0.6760 - accuracy: 0.57 - ETA: 2s - loss: 0.6758 - accuracy: 0.57 - ETA: 2s - loss: 0.6732 - accuracy: 0.58 - ETA: 2s - loss: 0.6715 - accuracy: 0.58 - ETA: 2s - loss: 0.6706 - accuracy: 0.59 - ETA: 2s - loss: 0.6684 - accuracy: 0.59 - ETA: 2s - loss: 0.6668 - accuracy: 0.59 - ETA: 2s - loss: 0.6652 - accuracy: 0.60 - ETA: 2s - loss: 0.6640 - accuracy: 0.60 - ETA: 1s - loss: 0.6632 - accuracy: 0.60 - ETA: 1s - loss: 0.6617 - accuracy: 0.60 - ETA: 1s - loss: 0.6608 - accuracy: 0.61 - ETA: 1s - loss: 0.6599 - accuracy: 0.61 - ETA: 1s - loss: 0.6597 - accuracy: 0.61 - ETA: 1s - loss: 0.6594 - accuracy: 0.61 - ETA: 1s - loss: 0.6584 - accuracy: 0.61 - ETA: 1s - loss: 0.6581 - accuracy: 0.61 - ETA: 1s - loss: 0.6558 - accuracy: 0.62 - ETA: 1s - loss: 0.6552 - accuracy: 0.62 - ETA: 1s - loss: 0.6546 - accuracy: 0.62 - ETA: 1s - loss: 0.6544 - accuracy: 0.62 - ETA: 1s - loss: 0.6533 - accuracy: 0.62 - ETA: 1s - loss: 0.6523 - accuracy: 0.62 - ETA: 1s - loss: 0.6513 - accuracy: 0.63 - ETA: 1s - loss: 0.6499 - accuracy: 0.63 - ETA: 1s - loss: 0.6498 - accuracy: 0.63 - ETA: 1s - loss: 0.6495 - accuracy: 0.63 - ETA: 0s - loss: 0.6491 - accuracy: 0.63 - ETA: 0s - loss: 0.6498 - accuracy: 0.63 - ETA: 0s - loss: 0.6482 - accuracy: 0.63 - ETA: 0s - loss: 0.6481 - accuracy: 0.63 - ETA: 0s - loss: 0.6477 - accuracy: 0.63 - ETA: 0s - loss: 0.6469 - accuracy: 0.63 - ETA: 0s - loss: 0.6467 - accuracy: 0.63 - ETA: 0s - loss: 0.6466 - accuracy: 0.64 - ETA: 0s - loss: 0.6459 - accuracy: 0.64 - ETA: 0s - loss: 0.6449 - accuracy: 0.64 - ETA: 0s - loss: 0.6438 - accuracy: 0.64 - ETA: 0s - loss: 0.6441 - accuracy: 0.64 - ETA: 0s - loss: 0.6434 - accuracy: 0.64 - ETA: 0s - loss: 0.6425 - accuracy: 0.64 - ETA: 0s - loss: 0.6416 - accuracy: 0.64 - ETA: 0s - loss: 0.6407 - accuracy: 0.64 - ETA: 0s - loss: 0.6396 - accuracy: 0.64 - ETA: 0s - loss: 0.6389 - accuracy: 0.64 - ETA: 0s - loss: 0.6387 - accuracy: 0.65 - 4s 277us/step - loss: 0.6387 - accuracy: 0.6507 - val_loss: 0.5827 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 6:13 - loss: 0.6923 - accuracy: 0.75 - ETA: 9s - loss: 0.6947 - accuracy: 0.4769 - ETA: 6s - loss: 0.6942 - accuracy: 0.50 - ETA: 5s - loss: 0.6929 - accuracy: 0.51 - ETA: 4s - loss: 0.6930 - accuracy: 0.51 - ETA: 4s - loss: 0.6913 - accuracy: 0.52 - ETA: 4s - loss: 0.6906 - accuracy: 0.52 - ETA: 3s - loss: 0.6898 - accuracy: 0.52 - ETA: 3s - loss: 0.6899 - accuracy: 0.52 - ETA: 3s - loss: 0.6903 - accuracy: 0.52 - ETA: 3s - loss: 0.6894 - accuracy: 0.52 - ETA: 3s - loss: 0.6896 - accuracy: 0.52 - ETA: 3s - loss: 0.6883 - accuracy: 0.53 - ETA: 3s - loss: 0.6878 - accuracy: 0.53 - ETA: 2s - loss: 0.6867 - accuracy: 0.53 - ETA: 2s - loss: 0.6859 - accuracy: 0.53 - ETA: 2s - loss: 0.6859 - accuracy: 0.53 - ETA: 2s - loss: 0.6849 - accuracy: 0.54 - ETA: 2s - loss: 0.6834 - accuracy: 0.54 - ETA: 2s - loss: 0.6828 - accuracy: 0.54 - ETA: 2s - loss: 0.6822 - accuracy: 0.55 - ETA: 2s - loss: 0.6815 - accuracy: 0.55 - ETA: 2s - loss: 0.6807 - accuracy: 0.55 - ETA: 2s - loss: 0.6799 - accuracy: 0.55 - ETA: 2s - loss: 0.6789 - accuracy: 0.55 - ETA: 2s - loss: 0.6783 - accuracy: 0.55 - ETA: 2s - loss: 0.6771 - accuracy: 0.56 - ETA: 2s - loss: 0.6755 - accuracy: 0.56 - ETA: 2s - loss: 0.6733 - accuracy: 0.56 - ETA: 2s - loss: 0.6729 - accuracy: 0.56 - ETA: 1s - loss: 0.6718 - accuracy: 0.56 - ETA: 1s - loss: 0.6706 - accuracy: 0.56 - ETA: 1s - loss: 0.6700 - accuracy: 0.56 - ETA: 1s - loss: 0.6688 - accuracy: 0.57 - ETA: 1s - loss: 0.6681 - accuracy: 0.57 - ETA: 1s - loss: 0.6672 - accuracy: 0.57 - ETA: 1s - loss: 0.6666 - accuracy: 0.57 - ETA: 1s - loss: 0.6660 - accuracy: 0.57 - ETA: 1s - loss: 0.6656 - accuracy: 0.57 - ETA: 1s - loss: 0.6646 - accuracy: 0.57 - ETA: 1s - loss: 0.6627 - accuracy: 0.57 - ETA: 1s - loss: 0.6621 - accuracy: 0.58 - ETA: 1s - loss: 0.6616 - accuracy: 0.58 - ETA: 1s - loss: 0.6611 - accuracy: 0.58 - ETA: 1s - loss: 0.6601 - accuracy: 0.58 - ETA: 1s - loss: 0.6603 - accuracy: 0.58 - ETA: 1s - loss: 0.6596 - accuracy: 0.58 - ETA: 0s - loss: 0.6592 - accuracy: 0.58 - ETA: 0s - loss: 0.6586 - accuracy: 0.58 - ETA: 0s - loss: 0.6580 - accuracy: 0.59 - ETA: 0s - loss: 0.6576 - accuracy: 0.59 - ETA: 0s - loss: 0.6578 - accuracy: 0.59 - ETA: 0s - loss: 0.6577 - accuracy: 0.59 - ETA: 0s - loss: 0.6574 - accuracy: 0.59 - ETA: 0s - loss: 0.6567 - accuracy: 0.59 - ETA: 0s - loss: 0.6565 - accuracy: 0.59 - ETA: 0s - loss: 0.6557 - accuracy: 0.59 - ETA: 0s - loss: 0.6547 - accuracy: 0.60 - ETA: 0s - loss: 0.6542 - accuracy: 0.60 - ETA: 0s - loss: 0.6539 - accuracy: 0.60 - ETA: 0s - loss: 0.6528 - accuracy: 0.60 - ETA: 0s - loss: 0.6517 - accuracy: 0.60 - ETA: 0s - loss: 0.6514 - accuracy: 0.60 - ETA: 0s - loss: 0.6513 - accuracy: 0.60 - ETA: 0s - loss: 0.6506 - accuracy: 0.60 - ETA: 0s - loss: 0.6503 - accuracy: 0.60 - 4s 288us/step - loss: 0.6498 - accuracy: 0.6103 - val_loss: 0.5971 - val_accuracy: 0.7266\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:10 - loss: 0.6969 - accuracy: 0.25 - ETA: 9s - loss: 0.6946 - accuracy: 0.4864 - ETA: 6s - loss: 0.6942 - accuracy: 0.49 - ETA: 4s - loss: 0.6934 - accuracy: 0.52 - ETA: 4s - loss: 0.6917 - accuracy: 0.53 - ETA: 3s - loss: 0.6905 - accuracy: 0.53 - ETA: 3s - loss: 0.6906 - accuracy: 0.52 - ETA: 3s - loss: 0.6878 - accuracy: 0.53 - ETA: 3s - loss: 0.6877 - accuracy: 0.54 - ETA: 3s - loss: 0.6878 - accuracy: 0.53 - ETA: 3s - loss: 0.6865 - accuracy: 0.54 - ETA: 2s - loss: 0.6851 - accuracy: 0.55 - ETA: 2s - loss: 0.6836 - accuracy: 0.55 - ETA: 2s - loss: 0.6825 - accuracy: 0.56 - ETA: 2s - loss: 0.6810 - accuracy: 0.56 - ETA: 2s - loss: 0.6801 - accuracy: 0.56 - ETA: 2s - loss: 0.6791 - accuracy: 0.56 - ETA: 2s - loss: 0.6782 - accuracy: 0.57 - ETA: 2s - loss: 0.6768 - accuracy: 0.57 - ETA: 2s - loss: 0.6760 - accuracy: 0.57 - ETA: 2s - loss: 0.6752 - accuracy: 0.57 - ETA: 2s - loss: 0.6737 - accuracy: 0.58 - ETA: 2s - loss: 0.6729 - accuracy: 0.58 - ETA: 2s - loss: 0.6722 - accuracy: 0.58 - ETA: 1s - loss: 0.6716 - accuracy: 0.58 - ETA: 1s - loss: 0.6704 - accuracy: 0.59 - ETA: 1s - loss: 0.6692 - accuracy: 0.59 - ETA: 1s - loss: 0.6682 - accuracy: 0.59 - ETA: 1s - loss: 0.6669 - accuracy: 0.59 - ETA: 1s - loss: 0.6666 - accuracy: 0.59 - ETA: 1s - loss: 0.6649 - accuracy: 0.60 - ETA: 1s - loss: 0.6637 - accuracy: 0.60 - ETA: 1s - loss: 0.6626 - accuracy: 0.60 - ETA: 1s - loss: 0.6620 - accuracy: 0.60 - ETA: 1s - loss: 0.6617 - accuracy: 0.60 - ETA: 1s - loss: 0.6609 - accuracy: 0.60 - ETA: 1s - loss: 0.6601 - accuracy: 0.60 - ETA: 1s - loss: 0.6591 - accuracy: 0.61 - ETA: 1s - loss: 0.6578 - accuracy: 0.61 - ETA: 1s - loss: 0.6568 - accuracy: 0.61 - ETA: 1s - loss: 0.6560 - accuracy: 0.61 - ETA: 1s - loss: 0.6555 - accuracy: 0.61 - ETA: 0s - loss: 0.6554 - accuracy: 0.61 - ETA: 0s - loss: 0.6549 - accuracy: 0.61 - ETA: 0s - loss: 0.6549 - accuracy: 0.62 - ETA: 0s - loss: 0.6544 - accuracy: 0.62 - ETA: 0s - loss: 0.6542 - accuracy: 0.62 - ETA: 0s - loss: 0.6534 - accuracy: 0.62 - ETA: 0s - loss: 0.6527 - accuracy: 0.62 - ETA: 0s - loss: 0.6514 - accuracy: 0.62 - ETA: 0s - loss: 0.6507 - accuracy: 0.62 - ETA: 0s - loss: 0.6504 - accuracy: 0.62 - ETA: 0s - loss: 0.6499 - accuracy: 0.62 - ETA: 0s - loss: 0.6494 - accuracy: 0.62 - ETA: 0s - loss: 0.6488 - accuracy: 0.62 - ETA: 0s - loss: 0.6488 - accuracy: 0.62 - ETA: 0s - loss: 0.6476 - accuracy: 0.63 - ETA: 0s - loss: 0.6471 - accuracy: 0.63 - ETA: 0s - loss: 0.6469 - accuracy: 0.63 - ETA: 0s - loss: 0.6462 - accuracy: 0.63 - ETA: 0s - loss: 0.6455 - accuracy: 0.63 - 3s 266us/step - loss: 0.6450 - accuracy: 0.6345 - val_loss: 0.5872 - val_accuracy: 0.7251\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 104us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:10 - loss: 0.7022 - accuracy: 0.0000e+ - ETA: 9s - loss: 0.6944 - accuracy: 0.4545     - ETA: 6s - loss: 0.6942 - accuracy: 0.47 - ETA: 5s - loss: 0.6941 - accuracy: 0.49 - ETA: 4s - loss: 0.6939 - accuracy: 0.50 - ETA: 4s - loss: 0.6938 - accuracy: 0.51 - ETA: 3s - loss: 0.6916 - accuracy: 0.52 - ETA: 3s - loss: 0.6897 - accuracy: 0.53 - ETA: 3s - loss: 0.6870 - accuracy: 0.54 - ETA: 3s - loss: 0.6857 - accuracy: 0.54 - ETA: 3s - loss: 0.6840 - accuracy: 0.55 - ETA: 2s - loss: 0.6824 - accuracy: 0.55 - ETA: 2s - loss: 0.6815 - accuracy: 0.55 - ETA: 2s - loss: 0.6800 - accuracy: 0.56 - ETA: 2s - loss: 0.6779 - accuracy: 0.56 - ETA: 2s - loss: 0.6760 - accuracy: 0.57 - ETA: 2s - loss: 0.6756 - accuracy: 0.57 - ETA: 2s - loss: 0.6732 - accuracy: 0.57 - ETA: 2s - loss: 0.6728 - accuracy: 0.57 - ETA: 2s - loss: 0.6710 - accuracy: 0.57 - ETA: 2s - loss: 0.6711 - accuracy: 0.57 - ETA: 2s - loss: 0.6687 - accuracy: 0.58 - ETA: 2s - loss: 0.6687 - accuracy: 0.58 - ETA: 2s - loss: 0.6681 - accuracy: 0.58 - ETA: 1s - loss: 0.6669 - accuracy: 0.58 - ETA: 1s - loss: 0.6665 - accuracy: 0.58 - ETA: 1s - loss: 0.6649 - accuracy: 0.59 - ETA: 1s - loss: 0.6648 - accuracy: 0.59 - ETA: 1s - loss: 0.6632 - accuracy: 0.59 - ETA: 1s - loss: 0.6626 - accuracy: 0.59 - ETA: 1s - loss: 0.6623 - accuracy: 0.59 - ETA: 1s - loss: 0.6619 - accuracy: 0.59 - ETA: 1s - loss: 0.6600 - accuracy: 0.59 - ETA: 1s - loss: 0.6597 - accuracy: 0.59 - ETA: 1s - loss: 0.6588 - accuracy: 0.59 - ETA: 1s - loss: 0.6579 - accuracy: 0.59 - ETA: 1s - loss: 0.6567 - accuracy: 0.60 - ETA: 1s - loss: 0.6564 - accuracy: 0.60 - ETA: 1s - loss: 0.6564 - accuracy: 0.60 - ETA: 1s - loss: 0.6558 - accuracy: 0.60 - ETA: 1s - loss: 0.6551 - accuracy: 0.60 - ETA: 1s - loss: 0.6545 - accuracy: 0.60 - ETA: 0s - loss: 0.6536 - accuracy: 0.60 - ETA: 0s - loss: 0.6534 - accuracy: 0.60 - ETA: 0s - loss: 0.6531 - accuracy: 0.61 - ETA: 0s - loss: 0.6521 - accuracy: 0.61 - ETA: 0s - loss: 0.6520 - accuracy: 0.61 - ETA: 0s - loss: 0.6525 - accuracy: 0.61 - ETA: 0s - loss: 0.6515 - accuracy: 0.61 - ETA: 0s - loss: 0.6503 - accuracy: 0.61 - ETA: 0s - loss: 0.6495 - accuracy: 0.61 - ETA: 0s - loss: 0.6489 - accuracy: 0.61 - ETA: 0s - loss: 0.6479 - accuracy: 0.61 - ETA: 0s - loss: 0.6478 - accuracy: 0.61 - ETA: 0s - loss: 0.6468 - accuracy: 0.62 - ETA: 0s - loss: 0.6465 - accuracy: 0.62 - ETA: 0s - loss: 0.6461 - accuracy: 0.62 - ETA: 0s - loss: 0.6453 - accuracy: 0.62 - ETA: 0s - loss: 0.6447 - accuracy: 0.62 - ETA: 0s - loss: 0.6443 - accuracy: 0.62 - ETA: 0s - loss: 0.6441 - accuracy: 0.62 - 3s 265us/step - loss: 0.6439 - accuracy: 0.6272 - val_loss: 0.5875 - val_accuracy: 0.7209\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:10 - loss: 0.6934 - accuracy: 0.50 - ETA: 9s - loss: 0.6944 - accuracy: 0.4953 - ETA: 6s - loss: 0.6911 - accuracy: 0.53 - ETA: 5s - loss: 0.6906 - accuracy: 0.51 - ETA: 4s - loss: 0.6884 - accuracy: 0.52 - ETA: 4s - loss: 0.6881 - accuracy: 0.52 - ETA: 3s - loss: 0.6882 - accuracy: 0.52 - ETA: 3s - loss: 0.6882 - accuracy: 0.52 - ETA: 3s - loss: 0.6878 - accuracy: 0.53 - ETA: 3s - loss: 0.6875 - accuracy: 0.53 - ETA: 3s - loss: 0.6866 - accuracy: 0.54 - ETA: 3s - loss: 0.6856 - accuracy: 0.55 - ETA: 2s - loss: 0.6849 - accuracy: 0.55 - ETA: 2s - loss: 0.6847 - accuracy: 0.55 - ETA: 2s - loss: 0.6836 - accuracy: 0.56 - ETA: 2s - loss: 0.6832 - accuracy: 0.57 - ETA: 2s - loss: 0.6824 - accuracy: 0.58 - ETA: 2s - loss: 0.6813 - accuracy: 0.58 - ETA: 2s - loss: 0.6806 - accuracy: 0.58 - ETA: 2s - loss: 0.6804 - accuracy: 0.58 - ETA: 2s - loss: 0.6794 - accuracy: 0.59 - ETA: 2s - loss: 0.6772 - accuracy: 0.59 - ETA: 2s - loss: 0.6764 - accuracy: 0.59 - ETA: 2s - loss: 0.6758 - accuracy: 0.60 - ETA: 2s - loss: 0.6745 - accuracy: 0.60 - ETA: 1s - loss: 0.6739 - accuracy: 0.60 - ETA: 1s - loss: 0.6737 - accuracy: 0.60 - ETA: 1s - loss: 0.6725 - accuracy: 0.60 - ETA: 1s - loss: 0.6722 - accuracy: 0.60 - ETA: 1s - loss: 0.6715 - accuracy: 0.61 - ETA: 1s - loss: 0.6713 - accuracy: 0.61 - ETA: 1s - loss: 0.6702 - accuracy: 0.61 - ETA: 1s - loss: 0.6698 - accuracy: 0.61 - ETA: 1s - loss: 0.6696 - accuracy: 0.61 - ETA: 1s - loss: 0.6696 - accuracy: 0.61 - ETA: 1s - loss: 0.6693 - accuracy: 0.61 - ETA: 1s - loss: 0.6692 - accuracy: 0.61 - ETA: 1s - loss: 0.6681 - accuracy: 0.61 - ETA: 1s - loss: 0.6677 - accuracy: 0.62 - ETA: 1s - loss: 0.6670 - accuracy: 0.62 - ETA: 1s - loss: 0.6661 - accuracy: 0.62 - ETA: 1s - loss: 0.6653 - accuracy: 0.62 - ETA: 1s - loss: 0.6650 - accuracy: 0.62 - ETA: 0s - loss: 0.6653 - accuracy: 0.62 - ETA: 0s - loss: 0.6639 - accuracy: 0.62 - ETA: 0s - loss: 0.6630 - accuracy: 0.62 - ETA: 0s - loss: 0.6630 - accuracy: 0.62 - ETA: 0s - loss: 0.6625 - accuracy: 0.62 - ETA: 0s - loss: 0.6622 - accuracy: 0.62 - ETA: 0s - loss: 0.6622 - accuracy: 0.62 - ETA: 0s - loss: 0.6614 - accuracy: 0.63 - ETA: 0s - loss: 0.6616 - accuracy: 0.63 - ETA: 0s - loss: 0.6613 - accuracy: 0.63 - ETA: 0s - loss: 0.6609 - accuracy: 0.63 - ETA: 0s - loss: 0.6602 - accuracy: 0.63 - ETA: 0s - loss: 0.6603 - accuracy: 0.63 - ETA: 0s - loss: 0.6598 - accuracy: 0.63 - ETA: 0s - loss: 0.6598 - accuracy: 0.63 - ETA: 0s - loss: 0.6593 - accuracy: 0.63 - ETA: 0s - loss: 0.6586 - accuracy: 0.63 - ETA: 0s - loss: 0.6585 - accuracy: 0.63 - ETA: 0s - loss: 0.6585 - accuracy: 0.63 - 3s 270us/step - loss: 0.6585 - accuracy: 0.6368 - val_loss: 0.6329 - val_accuracy: 0.6911\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 6:10 - loss: 0.6790 - accuracy: 0.75 - ETA: 13s - loss: 0.6941 - accuracy: 0.5066 - ETA: 7s - loss: 0.6929 - accuracy: 0.525 - ETA: 6s - loss: 0.6918 - accuracy: 0.52 - ETA: 5s - loss: 0.6917 - accuracy: 0.53 - ETA: 4s - loss: 0.6897 - accuracy: 0.54 - ETA: 4s - loss: 0.6905 - accuracy: 0.54 - ETA: 3s - loss: 0.6904 - accuracy: 0.55 - ETA: 3s - loss: 0.6894 - accuracy: 0.55 - ETA: 3s - loss: 0.6878 - accuracy: 0.56 - ETA: 3s - loss: 0.6866 - accuracy: 0.56 - ETA: 3s - loss: 0.6855 - accuracy: 0.57 - ETA: 3s - loss: 0.6830 - accuracy: 0.57 - ETA: 2s - loss: 0.6813 - accuracy: 0.58 - ETA: 2s - loss: 0.6801 - accuracy: 0.58 - ETA: 2s - loss: 0.6786 - accuracy: 0.58 - ETA: 2s - loss: 0.6770 - accuracy: 0.59 - ETA: 2s - loss: 0.6748 - accuracy: 0.59 - ETA: 2s - loss: 0.6748 - accuracy: 0.59 - ETA: 2s - loss: 0.6736 - accuracy: 0.60 - ETA: 2s - loss: 0.6709 - accuracy: 0.60 - ETA: 2s - loss: 0.6702 - accuracy: 0.60 - ETA: 2s - loss: 0.6697 - accuracy: 0.60 - ETA: 2s - loss: 0.6680 - accuracy: 0.60 - ETA: 2s - loss: 0.6672 - accuracy: 0.61 - ETA: 2s - loss: 0.6670 - accuracy: 0.61 - ETA: 1s - loss: 0.6640 - accuracy: 0.61 - ETA: 1s - loss: 0.6619 - accuracy: 0.61 - ETA: 1s - loss: 0.6611 - accuracy: 0.62 - ETA: 1s - loss: 0.6592 - accuracy: 0.62 - ETA: 1s - loss: 0.6582 - accuracy: 0.62 - ETA: 1s - loss: 0.6569 - accuracy: 0.62 - ETA: 1s - loss: 0.6555 - accuracy: 0.62 - ETA: 1s - loss: 0.6545 - accuracy: 0.63 - ETA: 1s - loss: 0.6531 - accuracy: 0.63 - ETA: 1s - loss: 0.6513 - accuracy: 0.63 - ETA: 1s - loss: 0.6511 - accuracy: 0.63 - ETA: 1s - loss: 0.6501 - accuracy: 0.63 - ETA: 1s - loss: 0.6499 - accuracy: 0.63 - ETA: 1s - loss: 0.6482 - accuracy: 0.63 - ETA: 1s - loss: 0.6474 - accuracy: 0.64 - ETA: 1s - loss: 0.6462 - accuracy: 0.64 - ETA: 1s - loss: 0.6457 - accuracy: 0.64 - ETA: 1s - loss: 0.6443 - accuracy: 0.64 - ETA: 0s - loss: 0.6430 - accuracy: 0.64 - ETA: 0s - loss: 0.6416 - accuracy: 0.64 - ETA: 0s - loss: 0.6408 - accuracy: 0.64 - ETA: 0s - loss: 0.6403 - accuracy: 0.64 - ETA: 0s - loss: 0.6407 - accuracy: 0.64 - ETA: 0s - loss: 0.6402 - accuracy: 0.64 - ETA: 0s - loss: 0.6392 - accuracy: 0.65 - ETA: 0s - loss: 0.6385 - accuracy: 0.65 - ETA: 0s - loss: 0.6379 - accuracy: 0.65 - ETA: 0s - loss: 0.6372 - accuracy: 0.65 - ETA: 0s - loss: 0.6362 - accuracy: 0.65 - ETA: 0s - loss: 0.6354 - accuracy: 0.65 - ETA: 0s - loss: 0.6351 - accuracy: 0.65 - ETA: 0s - loss: 0.6348 - accuracy: 0.65 - ETA: 0s - loss: 0.6339 - accuracy: 0.65 - ETA: 0s - loss: 0.6329 - accuracy: 0.65 - ETA: 0s - loss: 0.6327 - accuracy: 0.65 - ETA: 0s - loss: 0.6323 - accuracy: 0.65 - ETA: 0s - loss: 0.6320 - accuracy: 0.65 - 3s 273us/step - loss: 0.6319 - accuracy: 0.6595 - val_loss: 0.5793 - val_accuracy: 0.7287\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 109us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:10 - loss: 0.6899 - accuracy: 0.75 - ETA: 10s - loss: 0.6956 - accuracy: 0.4615 - ETA: 6s - loss: 0.6948 - accuracy: 0.507 - ETA: 5s - loss: 0.6945 - accuracy: 0.50 - ETA: 4s - loss: 0.6938 - accuracy: 0.52 - ETA: 4s - loss: 0.6929 - accuracy: 0.54 - ETA: 3s - loss: 0.6926 - accuracy: 0.54 - ETA: 3s - loss: 0.6919 - accuracy: 0.55 - ETA: 3s - loss: 0.6909 - accuracy: 0.55 - ETA: 3s - loss: 0.6904 - accuracy: 0.55 - ETA: 3s - loss: 0.6898 - accuracy: 0.55 - ETA: 3s - loss: 0.6883 - accuracy: 0.56 - ETA: 3s - loss: 0.6877 - accuracy: 0.56 - ETA: 2s - loss: 0.6865 - accuracy: 0.57 - ETA: 2s - loss: 0.6850 - accuracy: 0.57 - ETA: 2s - loss: 0.6834 - accuracy: 0.58 - ETA: 2s - loss: 0.6825 - accuracy: 0.58 - ETA: 2s - loss: 0.6810 - accuracy: 0.58 - ETA: 2s - loss: 0.6805 - accuracy: 0.58 - ETA: 2s - loss: 0.6801 - accuracy: 0.58 - ETA: 2s - loss: 0.6790 - accuracy: 0.59 - ETA: 2s - loss: 0.6773 - accuracy: 0.59 - ETA: 2s - loss: 0.6761 - accuracy: 0.59 - ETA: 2s - loss: 0.6746 - accuracy: 0.60 - ETA: 2s - loss: 0.6715 - accuracy: 0.60 - ETA: 2s - loss: 0.6714 - accuracy: 0.60 - ETA: 2s - loss: 0.6698 - accuracy: 0.61 - ETA: 2s - loss: 0.6687 - accuracy: 0.61 - ETA: 1s - loss: 0.6672 - accuracy: 0.61 - ETA: 1s - loss: 0.6660 - accuracy: 0.61 - ETA: 1s - loss: 0.6649 - accuracy: 0.61 - ETA: 1s - loss: 0.6632 - accuracy: 0.62 - ETA: 1s - loss: 0.6614 - accuracy: 0.62 - ETA: 1s - loss: 0.6599 - accuracy: 0.62 - ETA: 1s - loss: 0.6597 - accuracy: 0.62 - ETA: 1s - loss: 0.6572 - accuracy: 0.63 - ETA: 1s - loss: 0.6561 - accuracy: 0.63 - ETA: 1s - loss: 0.6542 - accuracy: 0.63 - ETA: 1s - loss: 0.6533 - accuracy: 0.63 - ETA: 1s - loss: 0.6523 - accuracy: 0.63 - ETA: 1s - loss: 0.6508 - accuracy: 0.63 - ETA: 1s - loss: 0.6499 - accuracy: 0.63 - ETA: 1s - loss: 0.6483 - accuracy: 0.64 - ETA: 1s - loss: 0.6483 - accuracy: 0.64 - ETA: 1s - loss: 0.6468 - accuracy: 0.64 - ETA: 0s - loss: 0.6463 - accuracy: 0.64 - ETA: 0s - loss: 0.6454 - accuracy: 0.64 - ETA: 0s - loss: 0.6448 - accuracy: 0.64 - ETA: 0s - loss: 0.6436 - accuracy: 0.64 - ETA: 0s - loss: 0.6426 - accuracy: 0.64 - ETA: 0s - loss: 0.6420 - accuracy: 0.65 - ETA: 0s - loss: 0.6412 - accuracy: 0.65 - ETA: 0s - loss: 0.6413 - accuracy: 0.65 - ETA: 0s - loss: 0.6406 - accuracy: 0.65 - ETA: 0s - loss: 0.6395 - accuracy: 0.65 - ETA: 0s - loss: 0.6391 - accuracy: 0.65 - ETA: 0s - loss: 0.6383 - accuracy: 0.65 - ETA: 0s - loss: 0.6378 - accuracy: 0.65 - ETA: 0s - loss: 0.6372 - accuracy: 0.65 - ETA: 0s - loss: 0.6369 - accuracy: 0.65 - ETA: 0s - loss: 0.6363 - accuracy: 0.65 - ETA: 0s - loss: 0.6365 - accuracy: 0.65 - ETA: 0s - loss: 0.6366 - accuracy: 0.65 - ETA: 0s - loss: 0.6368 - accuracy: 0.65 - 4s 280us/step - loss: 0.6361 - accuracy: 0.6578 - val_loss: 0.5826 - val_accuracy: 0.7180\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 99us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:39 - loss: 0.6998 - accuracy: 0.50 - ETA: 10s - loss: 0.6949 - accuracy: 0.5144 - ETA: 6s - loss: 0.6932 - accuracy: 0.528 - ETA: 5s - loss: 0.6926 - accuracy: 0.51 - ETA: 4s - loss: 0.6921 - accuracy: 0.52 - ETA: 4s - loss: 0.6919 - accuracy: 0.52 - ETA: 3s - loss: 0.6908 - accuracy: 0.52 - ETA: 3s - loss: 0.6903 - accuracy: 0.52 - ETA: 3s - loss: 0.6893 - accuracy: 0.53 - ETA: 3s - loss: 0.6878 - accuracy: 0.54 - ETA: 3s - loss: 0.6865 - accuracy: 0.55 - ETA: 3s - loss: 0.6854 - accuracy: 0.56 - ETA: 2s - loss: 0.6826 - accuracy: 0.57 - ETA: 2s - loss: 0.6804 - accuracy: 0.57 - ETA: 2s - loss: 0.6795 - accuracy: 0.58 - ETA: 2s - loss: 0.6776 - accuracy: 0.58 - ETA: 2s - loss: 0.6756 - accuracy: 0.58 - ETA: 2s - loss: 0.6752 - accuracy: 0.58 - ETA: 2s - loss: 0.6751 - accuracy: 0.59 - ETA: 2s - loss: 0.6730 - accuracy: 0.59 - ETA: 2s - loss: 0.6723 - accuracy: 0.59 - ETA: 2s - loss: 0.6703 - accuracy: 0.60 - ETA: 2s - loss: 0.6688 - accuracy: 0.60 - ETA: 2s - loss: 0.6672 - accuracy: 0.60 - ETA: 2s - loss: 0.6649 - accuracy: 0.61 - ETA: 1s - loss: 0.6639 - accuracy: 0.60 - ETA: 1s - loss: 0.6626 - accuracy: 0.61 - ETA: 1s - loss: 0.6609 - accuracy: 0.61 - ETA: 1s - loss: 0.6590 - accuracy: 0.62 - ETA: 1s - loss: 0.6570 - accuracy: 0.62 - ETA: 1s - loss: 0.6546 - accuracy: 0.62 - ETA: 1s - loss: 0.6528 - accuracy: 0.62 - ETA: 1s - loss: 0.6520 - accuracy: 0.62 - ETA: 1s - loss: 0.6508 - accuracy: 0.62 - ETA: 1s - loss: 0.6502 - accuracy: 0.63 - ETA: 1s - loss: 0.6484 - accuracy: 0.63 - ETA: 1s - loss: 0.6480 - accuracy: 0.63 - ETA: 1s - loss: 0.6476 - accuracy: 0.63 - ETA: 1s - loss: 0.6461 - accuracy: 0.63 - ETA: 1s - loss: 0.6451 - accuracy: 0.63 - ETA: 1s - loss: 0.6446 - accuracy: 0.64 - ETA: 1s - loss: 0.6434 - accuracy: 0.64 - ETA: 0s - loss: 0.6426 - accuracy: 0.64 - ETA: 0s - loss: 0.6424 - accuracy: 0.64 - ETA: 0s - loss: 0.6422 - accuracy: 0.64 - ETA: 0s - loss: 0.6415 - accuracy: 0.64 - ETA: 0s - loss: 0.6413 - accuracy: 0.64 - ETA: 0s - loss: 0.6399 - accuracy: 0.64 - ETA: 0s - loss: 0.6396 - accuracy: 0.64 - ETA: 0s - loss: 0.6389 - accuracy: 0.64 - ETA: 0s - loss: 0.6383 - accuracy: 0.65 - ETA: 0s - loss: 0.6373 - accuracy: 0.65 - ETA: 0s - loss: 0.6364 - accuracy: 0.65 - ETA: 0s - loss: 0.6348 - accuracy: 0.65 - ETA: 0s - loss: 0.6339 - accuracy: 0.65 - ETA: 0s - loss: 0.6337 - accuracy: 0.65 - ETA: 0s - loss: 0.6333 - accuracy: 0.65 - ETA: 0s - loss: 0.6328 - accuracy: 0.65 - ETA: 0s - loss: 0.6320 - accuracy: 0.65 - ETA: 0s - loss: 0.6310 - accuracy: 0.66 - ETA: 0s - loss: 0.6306 - accuracy: 0.66 - 3s 267us/step - loss: 0.6300 - accuracy: 0.6611 - val_loss: 0.5771 - val_accuracy: 0.7251\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:10 - loss: 0.6903 - accuracy: 0.75 - ETA: 10s - loss: 0.6918 - accuracy: 0.5192 - ETA: 6s - loss: 0.6943 - accuracy: 0.512 - ETA: 5s - loss: 0.6925 - accuracy: 0.53 - ETA: 4s - loss: 0.6919 - accuracy: 0.53 - ETA: 4s - loss: 0.6912 - accuracy: 0.53 - ETA: 3s - loss: 0.6904 - accuracy: 0.53 - ETA: 3s - loss: 0.6891 - accuracy: 0.53 - ETA: 3s - loss: 0.6883 - accuracy: 0.54 - ETA: 3s - loss: 0.6870 - accuracy: 0.55 - ETA: 3s - loss: 0.6850 - accuracy: 0.56 - ETA: 3s - loss: 0.6832 - accuracy: 0.56 - ETA: 3s - loss: 0.6810 - accuracy: 0.57 - ETA: 2s - loss: 0.6793 - accuracy: 0.57 - ETA: 2s - loss: 0.6768 - accuracy: 0.58 - ETA: 2s - loss: 0.6754 - accuracy: 0.58 - ETA: 2s - loss: 0.6746 - accuracy: 0.58 - ETA: 2s - loss: 0.6725 - accuracy: 0.59 - ETA: 2s - loss: 0.6704 - accuracy: 0.59 - ETA: 2s - loss: 0.6692 - accuracy: 0.59 - ETA: 2s - loss: 0.6670 - accuracy: 0.60 - ETA: 2s - loss: 0.6658 - accuracy: 0.60 - ETA: 2s - loss: 0.6656 - accuracy: 0.60 - ETA: 2s - loss: 0.6653 - accuracy: 0.60 - ETA: 2s - loss: 0.6640 - accuracy: 0.60 - ETA: 2s - loss: 0.6626 - accuracy: 0.61 - ETA: 2s - loss: 0.6604 - accuracy: 0.61 - ETA: 1s - loss: 0.6590 - accuracy: 0.61 - ETA: 1s - loss: 0.6583 - accuracy: 0.61 - ETA: 1s - loss: 0.6582 - accuracy: 0.61 - ETA: 1s - loss: 0.6572 - accuracy: 0.61 - ETA: 1s - loss: 0.6571 - accuracy: 0.61 - ETA: 1s - loss: 0.6571 - accuracy: 0.61 - ETA: 1s - loss: 0.6558 - accuracy: 0.62 - ETA: 1s - loss: 0.6536 - accuracy: 0.62 - ETA: 1s - loss: 0.6516 - accuracy: 0.62 - ETA: 1s - loss: 0.6507 - accuracy: 0.62 - ETA: 1s - loss: 0.6492 - accuracy: 0.62 - ETA: 1s - loss: 0.6489 - accuracy: 0.63 - ETA: 1s - loss: 0.6480 - accuracy: 0.63 - ETA: 1s - loss: 0.6468 - accuracy: 0.63 - ETA: 1s - loss: 0.6457 - accuracy: 0.63 - ETA: 1s - loss: 0.6447 - accuracy: 0.63 - ETA: 1s - loss: 0.6437 - accuracy: 0.63 - ETA: 1s - loss: 0.6425 - accuracy: 0.63 - ETA: 0s - loss: 0.6409 - accuracy: 0.64 - ETA: 0s - loss: 0.6401 - accuracy: 0.64 - ETA: 0s - loss: 0.6395 - accuracy: 0.64 - ETA: 0s - loss: 0.6388 - accuracy: 0.64 - ETA: 0s - loss: 0.6387 - accuracy: 0.64 - ETA: 0s - loss: 0.6380 - accuracy: 0.64 - ETA: 0s - loss: 0.6376 - accuracy: 0.64 - ETA: 0s - loss: 0.6371 - accuracy: 0.64 - ETA: 0s - loss: 0.6372 - accuracy: 0.64 - ETA: 0s - loss: 0.6368 - accuracy: 0.64 - ETA: 0s - loss: 0.6358 - accuracy: 0.64 - ETA: 0s - loss: 0.6355 - accuracy: 0.64 - ETA: 0s - loss: 0.6350 - accuracy: 0.64 - ETA: 0s - loss: 0.6341 - accuracy: 0.65 - ETA: 0s - loss: 0.6330 - accuracy: 0.65 - ETA: 0s - loss: 0.6326 - accuracy: 0.65 - ETA: 0s - loss: 0.6325 - accuracy: 0.65 - ETA: 0s - loss: 0.6316 - accuracy: 0.65 - ETA: 0s - loss: 0.6305 - accuracy: 0.65 - 4s 278us/step - loss: 0.6302 - accuracy: 0.6539 - val_loss: 0.5740 - val_accuracy: 0.7230\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 6:07 - loss: 0.6934 - accuracy: 0.50 - ETA: 10s - loss: 0.6928 - accuracy: 0.5450 - ETA: 6s - loss: 0.6945 - accuracy: 0.507 - ETA: 5s - loss: 0.6946 - accuracy: 0.49 - ETA: 4s - loss: 0.6941 - accuracy: 0.51 - ETA: 4s - loss: 0.6933 - accuracy: 0.52 - ETA: 3s - loss: 0.6923 - accuracy: 0.52 - ETA: 3s - loss: 0.6917 - accuracy: 0.52 - ETA: 3s - loss: 0.6903 - accuracy: 0.53 - ETA: 3s - loss: 0.6883 - accuracy: 0.54 - ETA: 3s - loss: 0.6868 - accuracy: 0.55 - ETA: 3s - loss: 0.6854 - accuracy: 0.56 - ETA: 3s - loss: 0.6839 - accuracy: 0.56 - ETA: 2s - loss: 0.6842 - accuracy: 0.56 - ETA: 2s - loss: 0.6821 - accuracy: 0.56 - ETA: 2s - loss: 0.6804 - accuracy: 0.57 - ETA: 2s - loss: 0.6779 - accuracy: 0.58 - ETA: 2s - loss: 0.6767 - accuracy: 0.58 - ETA: 2s - loss: 0.6745 - accuracy: 0.58 - ETA: 2s - loss: 0.6737 - accuracy: 0.59 - ETA: 2s - loss: 0.6710 - accuracy: 0.59 - ETA: 2s - loss: 0.6695 - accuracy: 0.59 - ETA: 2s - loss: 0.6678 - accuracy: 0.60 - ETA: 2s - loss: 0.6661 - accuracy: 0.60 - ETA: 2s - loss: 0.6640 - accuracy: 0.60 - ETA: 2s - loss: 0.6629 - accuracy: 0.60 - ETA: 1s - loss: 0.6623 - accuracy: 0.61 - ETA: 1s - loss: 0.6608 - accuracy: 0.61 - ETA: 1s - loss: 0.6600 - accuracy: 0.61 - ETA: 1s - loss: 0.6593 - accuracy: 0.61 - ETA: 1s - loss: 0.6589 - accuracy: 0.61 - ETA: 1s - loss: 0.6570 - accuracy: 0.61 - ETA: 1s - loss: 0.6560 - accuracy: 0.61 - ETA: 1s - loss: 0.6551 - accuracy: 0.62 - ETA: 1s - loss: 0.6544 - accuracy: 0.62 - ETA: 1s - loss: 0.6534 - accuracy: 0.62 - ETA: 1s - loss: 0.6513 - accuracy: 0.62 - ETA: 1s - loss: 0.6515 - accuracy: 0.62 - ETA: 1s - loss: 0.6513 - accuracy: 0.62 - ETA: 1s - loss: 0.6494 - accuracy: 0.62 - ETA: 1s - loss: 0.6493 - accuracy: 0.63 - ETA: 1s - loss: 0.6486 - accuracy: 0.63 - ETA: 1s - loss: 0.6478 - accuracy: 0.63 - ETA: 1s - loss: 0.6481 - accuracy: 0.63 - ETA: 0s - loss: 0.6473 - accuracy: 0.63 - ETA: 0s - loss: 0.6459 - accuracy: 0.63 - ETA: 0s - loss: 0.6446 - accuracy: 0.63 - ETA: 0s - loss: 0.6430 - accuracy: 0.64 - ETA: 0s - loss: 0.6418 - accuracy: 0.64 - ETA: 0s - loss: 0.6414 - accuracy: 0.64 - ETA: 0s - loss: 0.6418 - accuracy: 0.64 - ETA: 0s - loss: 0.6411 - accuracy: 0.64 - ETA: 0s - loss: 0.6404 - accuracy: 0.64 - ETA: 0s - loss: 0.6394 - accuracy: 0.64 - ETA: 0s - loss: 0.6386 - accuracy: 0.64 - ETA: 0s - loss: 0.6388 - accuracy: 0.64 - ETA: 0s - loss: 0.6384 - accuracy: 0.64 - ETA: 0s - loss: 0.6376 - accuracy: 0.64 - ETA: 0s - loss: 0.6375 - accuracy: 0.64 - ETA: 0s - loss: 0.6373 - accuracy: 0.64 - ETA: 0s - loss: 0.6372 - accuracy: 0.64 - ETA: 0s - loss: 0.6358 - accuracy: 0.64 - ETA: 0s - loss: 0.6349 - accuracy: 0.64 - 3s 273us/step - loss: 0.6349 - accuracy: 0.6497 - val_loss: 0.5762 - val_accuracy: 0.7237\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:04 - loss: 0.6930 - accuracy: 0.50 - ETA: 9s - loss: 0.6925 - accuracy: 0.5529 - ETA: 6s - loss: 0.6936 - accuracy: 0.53 - ETA: 5s - loss: 0.6917 - accuracy: 0.54 - ETA: 4s - loss: 0.6905 - accuracy: 0.53 - ETA: 4s - loss: 0.6899 - accuracy: 0.53 - ETA: 3s - loss: 0.6891 - accuracy: 0.53 - ETA: 3s - loss: 0.6871 - accuracy: 0.55 - ETA: 3s - loss: 0.6856 - accuracy: 0.56 - ETA: 3s - loss: 0.6828 - accuracy: 0.57 - ETA: 3s - loss: 0.6801 - accuracy: 0.58 - ETA: 3s - loss: 0.6761 - accuracy: 0.58 - ETA: 3s - loss: 0.6732 - accuracy: 0.59 - ETA: 3s - loss: 0.6694 - accuracy: 0.59 - ETA: 2s - loss: 0.6692 - accuracy: 0.60 - ETA: 2s - loss: 0.6671 - accuracy: 0.60 - ETA: 2s - loss: 0.6653 - accuracy: 0.60 - ETA: 2s - loss: 0.6628 - accuracy: 0.61 - ETA: 2s - loss: 0.6604 - accuracy: 0.61 - ETA: 2s - loss: 0.6596 - accuracy: 0.61 - ETA: 2s - loss: 0.6591 - accuracy: 0.61 - ETA: 2s - loss: 0.6582 - accuracy: 0.61 - ETA: 2s - loss: 0.6561 - accuracy: 0.62 - ETA: 2s - loss: 0.6564 - accuracy: 0.62 - ETA: 2s - loss: 0.6564 - accuracy: 0.62 - ETA: 2s - loss: 0.6567 - accuracy: 0.62 - ETA: 2s - loss: 0.6566 - accuracy: 0.62 - ETA: 2s - loss: 0.6556 - accuracy: 0.62 - ETA: 1s - loss: 0.6542 - accuracy: 0.62 - ETA: 1s - loss: 0.6539 - accuracy: 0.62 - ETA: 1s - loss: 0.6532 - accuracy: 0.62 - ETA: 1s - loss: 0.6511 - accuracy: 0.63 - ETA: 1s - loss: 0.6500 - accuracy: 0.63 - ETA: 1s - loss: 0.6481 - accuracy: 0.63 - ETA: 1s - loss: 0.6467 - accuracy: 0.63 - ETA: 1s - loss: 0.6465 - accuracy: 0.63 - ETA: 1s - loss: 0.6456 - accuracy: 0.63 - ETA: 1s - loss: 0.6441 - accuracy: 0.63 - ETA: 1s - loss: 0.6438 - accuracy: 0.64 - ETA: 1s - loss: 0.6431 - accuracy: 0.64 - ETA: 1s - loss: 0.6438 - accuracy: 0.64 - ETA: 1s - loss: 0.6425 - accuracy: 0.64 - ETA: 1s - loss: 0.6420 - accuracy: 0.64 - ETA: 1s - loss: 0.6410 - accuracy: 0.64 - ETA: 1s - loss: 0.6409 - accuracy: 0.64 - ETA: 1s - loss: 0.6405 - accuracy: 0.64 - ETA: 0s - loss: 0.6393 - accuracy: 0.64 - ETA: 0s - loss: 0.6387 - accuracy: 0.64 - ETA: 0s - loss: 0.6383 - accuracy: 0.64 - ETA: 0s - loss: 0.6365 - accuracy: 0.65 - ETA: 0s - loss: 0.6350 - accuracy: 0.65 - ETA: 0s - loss: 0.6338 - accuracy: 0.65 - ETA: 0s - loss: 0.6338 - accuracy: 0.65 - ETA: 0s - loss: 0.6333 - accuracy: 0.65 - ETA: 0s - loss: 0.6322 - accuracy: 0.65 - ETA: 0s - loss: 0.6314 - accuracy: 0.65 - ETA: 0s - loss: 0.6303 - accuracy: 0.66 - ETA: 0s - loss: 0.6298 - accuracy: 0.66 - ETA: 0s - loss: 0.6298 - accuracy: 0.66 - ETA: 0s - loss: 0.6300 - accuracy: 0.66 - ETA: 0s - loss: 0.6297 - accuracy: 0.66 - ETA: 0s - loss: 0.6289 - accuracy: 0.66 - ETA: 0s - loss: 0.6291 - accuracy: 0.66 - ETA: 0s - loss: 0.6295 - accuracy: 0.66 - 4s 279us/step - loss: 0.6292 - accuracy: 0.6623 - val_loss: 0.5863 - val_accuracy: 0.7251\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 108us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:26 - loss: 0.6897 - accuracy: 0.50 - ETA: 8s - loss: 0.6917 - accuracy: 0.5472 - ETA: 5s - loss: 0.6910 - accuracy: 0.54 - ETA: 4s - loss: 0.6918 - accuracy: 0.53 - ETA: 4s - loss: 0.6928 - accuracy: 0.51 - ETA: 3s - loss: 0.6918 - accuracy: 0.52 - ETA: 3s - loss: 0.6923 - accuracy: 0.52 - ETA: 3s - loss: 0.6916 - accuracy: 0.52 - ETA: 3s - loss: 0.6908 - accuracy: 0.52 - ETA: 3s - loss: 0.6910 - accuracy: 0.52 - ETA: 2s - loss: 0.6901 - accuracy: 0.52 - ETA: 2s - loss: 0.6897 - accuracy: 0.52 - ETA: 2s - loss: 0.6895 - accuracy: 0.52 - ETA: 2s - loss: 0.6894 - accuracy: 0.52 - ETA: 2s - loss: 0.6890 - accuracy: 0.52 - ETA: 2s - loss: 0.6886 - accuracy: 0.53 - ETA: 2s - loss: 0.6886 - accuracy: 0.53 - ETA: 2s - loss: 0.6884 - accuracy: 0.53 - ETA: 2s - loss: 0.6876 - accuracy: 0.53 - ETA: 2s - loss: 0.6865 - accuracy: 0.53 - ETA: 2s - loss: 0.6855 - accuracy: 0.54 - ETA: 2s - loss: 0.6845 - accuracy: 0.54 - ETA: 2s - loss: 0.6831 - accuracy: 0.54 - ETA: 2s - loss: 0.6829 - accuracy: 0.54 - ETA: 1s - loss: 0.6825 - accuracy: 0.54 - ETA: 1s - loss: 0.6822 - accuracy: 0.54 - ETA: 1s - loss: 0.6818 - accuracy: 0.54 - ETA: 1s - loss: 0.6812 - accuracy: 0.55 - ETA: 1s - loss: 0.6806 - accuracy: 0.55 - ETA: 1s - loss: 0.6796 - accuracy: 0.55 - ETA: 1s - loss: 0.6792 - accuracy: 0.55 - ETA: 1s - loss: 0.6792 - accuracy: 0.55 - ETA: 1s - loss: 0.6790 - accuracy: 0.55 - ETA: 1s - loss: 0.6782 - accuracy: 0.55 - ETA: 1s - loss: 0.6777 - accuracy: 0.55 - ETA: 1s - loss: 0.6770 - accuracy: 0.55 - ETA: 1s - loss: 0.6760 - accuracy: 0.56 - ETA: 1s - loss: 0.6754 - accuracy: 0.56 - ETA: 1s - loss: 0.6749 - accuracy: 0.56 - ETA: 1s - loss: 0.6746 - accuracy: 0.56 - ETA: 1s - loss: 0.6736 - accuracy: 0.56 - ETA: 1s - loss: 0.6740 - accuracy: 0.56 - ETA: 0s - loss: 0.6731 - accuracy: 0.56 - ETA: 0s - loss: 0.6731 - accuracy: 0.56 - ETA: 0s - loss: 0.6725 - accuracy: 0.56 - ETA: 0s - loss: 0.6721 - accuracy: 0.57 - ETA: 0s - loss: 0.6719 - accuracy: 0.57 - ETA: 0s - loss: 0.6714 - accuracy: 0.57 - ETA: 0s - loss: 0.6704 - accuracy: 0.57 - ETA: 0s - loss: 0.6698 - accuracy: 0.57 - ETA: 0s - loss: 0.6698 - accuracy: 0.57 - ETA: 0s - loss: 0.6690 - accuracy: 0.57 - ETA: 0s - loss: 0.6688 - accuracy: 0.57 - ETA: 0s - loss: 0.6679 - accuracy: 0.57 - ETA: 0s - loss: 0.6674 - accuracy: 0.57 - ETA: 0s - loss: 0.6669 - accuracy: 0.58 - ETA: 0s - loss: 0.6669 - accuracy: 0.58 - ETA: 0s - loss: 0.6660 - accuracy: 0.58 - ETA: 0s - loss: 0.6653 - accuracy: 0.58 - ETA: 0s - loss: 0.6647 - accuracy: 0.58 - 3s 263us/step - loss: 0.6643 - accuracy: 0.5857 - val_loss: 0.6128 - val_accuracy: 0.7188\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:35 - loss: 0.6904 - accuracy: 0.50 - ETA: 9s - loss: 0.6933 - accuracy: 0.5189 - ETA: 6s - loss: 0.6938 - accuracy: 0.45 - ETA: 4s - loss: 0.6934 - accuracy: 0.48 - ETA: 4s - loss: 0.6934 - accuracy: 0.49 - ETA: 3s - loss: 0.6932 - accuracy: 0.49 - ETA: 3s - loss: 0.6922 - accuracy: 0.50 - ETA: 3s - loss: 0.6927 - accuracy: 0.50 - ETA: 3s - loss: 0.6929 - accuracy: 0.49 - ETA: 3s - loss: 0.6928 - accuracy: 0.49 - ETA: 3s - loss: 0.6926 - accuracy: 0.49 - ETA: 2s - loss: 0.6928 - accuracy: 0.49 - ETA: 2s - loss: 0.6925 - accuracy: 0.50 - ETA: 2s - loss: 0.6922 - accuracy: 0.50 - ETA: 2s - loss: 0.6917 - accuracy: 0.51 - ETA: 2s - loss: 0.6915 - accuracy: 0.51 - ETA: 2s - loss: 0.6913 - accuracy: 0.51 - ETA: 2s - loss: 0.6903 - accuracy: 0.52 - ETA: 2s - loss: 0.6900 - accuracy: 0.52 - ETA: 2s - loss: 0.6893 - accuracy: 0.52 - ETA: 2s - loss: 0.6886 - accuracy: 0.52 - ETA: 2s - loss: 0.6884 - accuracy: 0.52 - ETA: 2s - loss: 0.6878 - accuracy: 0.53 - ETA: 2s - loss: 0.6866 - accuracy: 0.53 - ETA: 2s - loss: 0.6865 - accuracy: 0.53 - ETA: 1s - loss: 0.6856 - accuracy: 0.53 - ETA: 1s - loss: 0.6855 - accuracy: 0.53 - ETA: 1s - loss: 0.6850 - accuracy: 0.53 - ETA: 1s - loss: 0.6848 - accuracy: 0.53 - ETA: 1s - loss: 0.6845 - accuracy: 0.53 - ETA: 1s - loss: 0.6840 - accuracy: 0.54 - ETA: 1s - loss: 0.6834 - accuracy: 0.54 - ETA: 1s - loss: 0.6832 - accuracy: 0.54 - ETA: 1s - loss: 0.6824 - accuracy: 0.54 - ETA: 1s - loss: 0.6816 - accuracy: 0.54 - ETA: 1s - loss: 0.6813 - accuracy: 0.54 - ETA: 1s - loss: 0.6814 - accuracy: 0.54 - ETA: 1s - loss: 0.6805 - accuracy: 0.54 - ETA: 1s - loss: 0.6803 - accuracy: 0.54 - ETA: 1s - loss: 0.6797 - accuracy: 0.54 - ETA: 1s - loss: 0.6793 - accuracy: 0.54 - ETA: 1s - loss: 0.6789 - accuracy: 0.54 - ETA: 1s - loss: 0.6782 - accuracy: 0.55 - ETA: 0s - loss: 0.6777 - accuracy: 0.55 - ETA: 0s - loss: 0.6773 - accuracy: 0.55 - ETA: 0s - loss: 0.6772 - accuracy: 0.55 - ETA: 0s - loss: 0.6764 - accuracy: 0.55 - ETA: 0s - loss: 0.6755 - accuracy: 0.55 - ETA: 0s - loss: 0.6750 - accuracy: 0.55 - ETA: 0s - loss: 0.6745 - accuracy: 0.55 - ETA: 0s - loss: 0.6743 - accuracy: 0.55 - ETA: 0s - loss: 0.6738 - accuracy: 0.55 - ETA: 0s - loss: 0.6739 - accuracy: 0.55 - ETA: 0s - loss: 0.6732 - accuracy: 0.55 - ETA: 0s - loss: 0.6725 - accuracy: 0.55 - ETA: 0s - loss: 0.6724 - accuracy: 0.56 - ETA: 0s - loss: 0.6723 - accuracy: 0.56 - ETA: 0s - loss: 0.6721 - accuracy: 0.56 - ETA: 0s - loss: 0.6716 - accuracy: 0.56 - ETA: 0s - loss: 0.6712 - accuracy: 0.56 - ETA: 0s - loss: 0.6710 - accuracy: 0.56 - ETA: 0s - loss: 0.6706 - accuracy: 0.56 - 3s 268us/step - loss: 0.6707 - accuracy: 0.5640 - val_loss: 0.6269 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 107us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 5:23 - loss: 0.6976 - accuracy: 0.50 - ETA: 8s - loss: 0.6932 - accuracy: 0.5139 - ETA: 5s - loss: 0.6935 - accuracy: 0.49 - ETA: 4s - loss: 0.6929 - accuracy: 0.50 - ETA: 4s - loss: 0.6926 - accuracy: 0.50 - ETA: 3s - loss: 0.6924 - accuracy: 0.51 - ETA: 3s - loss: 0.6924 - accuracy: 0.50 - ETA: 3s - loss: 0.6923 - accuracy: 0.51 - ETA: 3s - loss: 0.6924 - accuracy: 0.50 - ETA: 3s - loss: 0.6924 - accuracy: 0.50 - ETA: 3s - loss: 0.6915 - accuracy: 0.51 - ETA: 2s - loss: 0.6911 - accuracy: 0.51 - ETA: 2s - loss: 0.6909 - accuracy: 0.51 - ETA: 2s - loss: 0.6903 - accuracy: 0.51 - ETA: 2s - loss: 0.6906 - accuracy: 0.51 - ETA: 2s - loss: 0.6906 - accuracy: 0.50 - ETA: 2s - loss: 0.6905 - accuracy: 0.50 - ETA: 2s - loss: 0.6903 - accuracy: 0.50 - ETA: 2s - loss: 0.6899 - accuracy: 0.51 - ETA: 2s - loss: 0.6894 - accuracy: 0.51 - ETA: 2s - loss: 0.6893 - accuracy: 0.51 - ETA: 2s - loss: 0.6884 - accuracy: 0.51 - ETA: 2s - loss: 0.6881 - accuracy: 0.51 - ETA: 2s - loss: 0.6874 - accuracy: 0.51 - ETA: 1s - loss: 0.6865 - accuracy: 0.51 - ETA: 1s - loss: 0.6865 - accuracy: 0.51 - ETA: 1s - loss: 0.6863 - accuracy: 0.51 - ETA: 1s - loss: 0.6859 - accuracy: 0.52 - ETA: 1s - loss: 0.6857 - accuracy: 0.52 - ETA: 1s - loss: 0.6854 - accuracy: 0.52 - ETA: 1s - loss: 0.6850 - accuracy: 0.52 - ETA: 1s - loss: 0.6850 - accuracy: 0.52 - ETA: 1s - loss: 0.6845 - accuracy: 0.52 - ETA: 1s - loss: 0.6842 - accuracy: 0.53 - ETA: 1s - loss: 0.6840 - accuracy: 0.53 - ETA: 1s - loss: 0.6836 - accuracy: 0.53 - ETA: 1s - loss: 0.6832 - accuracy: 0.53 - ETA: 1s - loss: 0.6829 - accuracy: 0.53 - ETA: 1s - loss: 0.6825 - accuracy: 0.53 - ETA: 1s - loss: 0.6820 - accuracy: 0.54 - ETA: 1s - loss: 0.6815 - accuracy: 0.54 - ETA: 1s - loss: 0.6812 - accuracy: 0.54 - ETA: 0s - loss: 0.6809 - accuracy: 0.54 - ETA: 0s - loss: 0.6802 - accuracy: 0.54 - ETA: 0s - loss: 0.6797 - accuracy: 0.55 - ETA: 0s - loss: 0.6790 - accuracy: 0.55 - ETA: 0s - loss: 0.6788 - accuracy: 0.55 - ETA: 0s - loss: 0.6785 - accuracy: 0.55 - ETA: 0s - loss: 0.6782 - accuracy: 0.55 - ETA: 0s - loss: 0.6780 - accuracy: 0.55 - ETA: 0s - loss: 0.6776 - accuracy: 0.55 - ETA: 0s - loss: 0.6775 - accuracy: 0.55 - ETA: 0s - loss: 0.6771 - accuracy: 0.56 - ETA: 0s - loss: 0.6768 - accuracy: 0.56 - ETA: 0s - loss: 0.6761 - accuracy: 0.56 - ETA: 0s - loss: 0.6758 - accuracy: 0.56 - ETA: 0s - loss: 0.6757 - accuracy: 0.56 - ETA: 0s - loss: 0.6753 - accuracy: 0.56 - ETA: 0s - loss: 0.6750 - accuracy: 0.56 - ETA: 0s - loss: 0.6747 - accuracy: 0.56 - ETA: 0s - loss: 0.6746 - accuracy: 0.56 - 3s 264us/step - loss: 0.6746 - accuracy: 0.5696 - val_loss: 0.6376 - val_accuracy: 0.7060\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:26 - loss: 0.6938 - accuracy: 0.25 - ETA: 8s - loss: 0.6937 - accuracy: 0.5091 - ETA: 5s - loss: 0.6936 - accuracy: 0.50 - ETA: 4s - loss: 0.6929 - accuracy: 0.50 - ETA: 4s - loss: 0.6929 - accuracy: 0.49 - ETA: 3s - loss: 0.6928 - accuracy: 0.50 - ETA: 3s - loss: 0.6922 - accuracy: 0.51 - ETA: 3s - loss: 0.6921 - accuracy: 0.52 - ETA: 3s - loss: 0.6916 - accuracy: 0.53 - ETA: 3s - loss: 0.6913 - accuracy: 0.53 - ETA: 3s - loss: 0.6909 - accuracy: 0.53 - ETA: 2s - loss: 0.6910 - accuracy: 0.53 - ETA: 2s - loss: 0.6908 - accuracy: 0.54 - ETA: 2s - loss: 0.6906 - accuracy: 0.55 - ETA: 2s - loss: 0.6904 - accuracy: 0.55 - ETA: 2s - loss: 0.6902 - accuracy: 0.55 - ETA: 2s - loss: 0.6898 - accuracy: 0.56 - ETA: 2s - loss: 0.6894 - accuracy: 0.56 - ETA: 2s - loss: 0.6891 - accuracy: 0.56 - ETA: 2s - loss: 0.6885 - accuracy: 0.56 - ETA: 2s - loss: 0.6884 - accuracy: 0.56 - ETA: 2s - loss: 0.6877 - accuracy: 0.57 - ETA: 2s - loss: 0.6873 - accuracy: 0.57 - ETA: 2s - loss: 0.6867 - accuracy: 0.57 - ETA: 1s - loss: 0.6864 - accuracy: 0.57 - ETA: 1s - loss: 0.6858 - accuracy: 0.57 - ETA: 1s - loss: 0.6853 - accuracy: 0.58 - ETA: 1s - loss: 0.6845 - accuracy: 0.58 - ETA: 1s - loss: 0.6839 - accuracy: 0.58 - ETA: 1s - loss: 0.6837 - accuracy: 0.58 - ETA: 1s - loss: 0.6833 - accuracy: 0.58 - ETA: 1s - loss: 0.6829 - accuracy: 0.58 - ETA: 1s - loss: 0.6826 - accuracy: 0.58 - ETA: 1s - loss: 0.6823 - accuracy: 0.59 - ETA: 1s - loss: 0.6815 - accuracy: 0.59 - ETA: 1s - loss: 0.6808 - accuracy: 0.59 - ETA: 1s - loss: 0.6804 - accuracy: 0.59 - ETA: 1s - loss: 0.6799 - accuracy: 0.59 - ETA: 1s - loss: 0.6796 - accuracy: 0.59 - ETA: 1s - loss: 0.6792 - accuracy: 0.59 - ETA: 1s - loss: 0.6790 - accuracy: 0.59 - ETA: 1s - loss: 0.6784 - accuracy: 0.60 - ETA: 0s - loss: 0.6777 - accuracy: 0.60 - ETA: 0s - loss: 0.6770 - accuracy: 0.60 - ETA: 0s - loss: 0.6766 - accuracy: 0.60 - ETA: 0s - loss: 0.6762 - accuracy: 0.60 - ETA: 0s - loss: 0.6755 - accuracy: 0.60 - ETA: 0s - loss: 0.6754 - accuracy: 0.60 - ETA: 0s - loss: 0.6749 - accuracy: 0.60 - ETA: 0s - loss: 0.6743 - accuracy: 0.60 - ETA: 0s - loss: 0.6740 - accuracy: 0.60 - ETA: 0s - loss: 0.6737 - accuracy: 0.61 - ETA: 0s - loss: 0.6728 - accuracy: 0.61 - ETA: 0s - loss: 0.6725 - accuracy: 0.61 - ETA: 0s - loss: 0.6725 - accuracy: 0.61 - ETA: 0s - loss: 0.6719 - accuracy: 0.61 - ETA: 0s - loss: 0.6713 - accuracy: 0.61 - ETA: 0s - loss: 0.6707 - accuracy: 0.61 - ETA: 0s - loss: 0.6700 - accuracy: 0.61 - ETA: 0s - loss: 0.6697 - accuracy: 0.61 - ETA: 0s - loss: 0.6694 - accuracy: 0.61 - 3s 265us/step - loss: 0.6691 - accuracy: 0.6168 - val_loss: 0.6290 - val_accuracy: 0.7145\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:23 - loss: 0.6938 - accuracy: 0.50 - ETA: 8s - loss: 0.6933 - accuracy: 0.5409 - ETA: 5s - loss: 0.6932 - accuracy: 0.53 - ETA: 4s - loss: 0.6930 - accuracy: 0.54 - ETA: 4s - loss: 0.6931 - accuracy: 0.53 - ETA: 3s - loss: 0.6932 - accuracy: 0.52 - ETA: 3s - loss: 0.6929 - accuracy: 0.52 - ETA: 3s - loss: 0.6926 - accuracy: 0.53 - ETA: 3s - loss: 0.6926 - accuracy: 0.53 - ETA: 3s - loss: 0.6924 - accuracy: 0.53 - ETA: 3s - loss: 0.6920 - accuracy: 0.53 - ETA: 2s - loss: 0.6918 - accuracy: 0.54 - ETA: 2s - loss: 0.6913 - accuracy: 0.54 - ETA: 2s - loss: 0.6906 - accuracy: 0.54 - ETA: 2s - loss: 0.6900 - accuracy: 0.54 - ETA: 2s - loss: 0.6894 - accuracy: 0.54 - ETA: 2s - loss: 0.6885 - accuracy: 0.55 - ETA: 2s - loss: 0.6876 - accuracy: 0.55 - ETA: 2s - loss: 0.6875 - accuracy: 0.55 - ETA: 2s - loss: 0.6865 - accuracy: 0.56 - ETA: 2s - loss: 0.6860 - accuracy: 0.56 - ETA: 2s - loss: 0.6852 - accuracy: 0.56 - ETA: 2s - loss: 0.6843 - accuracy: 0.56 - ETA: 2s - loss: 0.6831 - accuracy: 0.57 - ETA: 1s - loss: 0.6825 - accuracy: 0.57 - ETA: 1s - loss: 0.6818 - accuracy: 0.57 - ETA: 1s - loss: 0.6814 - accuracy: 0.57 - ETA: 1s - loss: 0.6806 - accuracy: 0.57 - ETA: 1s - loss: 0.6802 - accuracy: 0.57 - ETA: 1s - loss: 0.6798 - accuracy: 0.57 - ETA: 1s - loss: 0.6787 - accuracy: 0.58 - ETA: 1s - loss: 0.6776 - accuracy: 0.58 - ETA: 1s - loss: 0.6772 - accuracy: 0.58 - ETA: 1s - loss: 0.6771 - accuracy: 0.58 - ETA: 1s - loss: 0.6763 - accuracy: 0.58 - ETA: 1s - loss: 0.6759 - accuracy: 0.58 - ETA: 1s - loss: 0.6751 - accuracy: 0.58 - ETA: 1s - loss: 0.6747 - accuracy: 0.58 - ETA: 1s - loss: 0.6741 - accuracy: 0.58 - ETA: 1s - loss: 0.6735 - accuracy: 0.59 - ETA: 1s - loss: 0.6728 - accuracy: 0.59 - ETA: 1s - loss: 0.6721 - accuracy: 0.59 - ETA: 0s - loss: 0.6711 - accuracy: 0.59 - ETA: 0s - loss: 0.6702 - accuracy: 0.59 - ETA: 0s - loss: 0.6694 - accuracy: 0.59 - ETA: 0s - loss: 0.6689 - accuracy: 0.59 - ETA: 0s - loss: 0.6685 - accuracy: 0.59 - ETA: 0s - loss: 0.6680 - accuracy: 0.59 - ETA: 0s - loss: 0.6671 - accuracy: 0.60 - ETA: 0s - loss: 0.6664 - accuracy: 0.60 - ETA: 0s - loss: 0.6654 - accuracy: 0.60 - ETA: 0s - loss: 0.6653 - accuracy: 0.60 - ETA: 0s - loss: 0.6651 - accuracy: 0.60 - ETA: 0s - loss: 0.6642 - accuracy: 0.60 - ETA: 0s - loss: 0.6634 - accuracy: 0.60 - ETA: 0s - loss: 0.6629 - accuracy: 0.60 - ETA: 0s - loss: 0.6626 - accuracy: 0.60 - ETA: 0s - loss: 0.6619 - accuracy: 0.60 - ETA: 0s - loss: 0.6614 - accuracy: 0.60 - ETA: 0s - loss: 0.6611 - accuracy: 0.61 - ETA: 0s - loss: 0.6608 - accuracy: 0.61 - 3s 266us/step - loss: 0.6606 - accuracy: 0.6108 - val_loss: 0.6113 - val_accuracy: 0.7109\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:45 - loss: 0.6892 - accuracy: 0.75 - ETA: 9s - loss: 0.6941 - accuracy: 0.4455 - ETA: 5s - loss: 0.6928 - accuracy: 0.50 - ETA: 4s - loss: 0.6926 - accuracy: 0.50 - ETA: 4s - loss: 0.6917 - accuracy: 0.51 - ETA: 3s - loss: 0.6918 - accuracy: 0.52 - ETA: 3s - loss: 0.6917 - accuracy: 0.51 - ETA: 3s - loss: 0.6910 - accuracy: 0.52 - ETA: 3s - loss: 0.6911 - accuracy: 0.52 - ETA: 3s - loss: 0.6906 - accuracy: 0.52 - ETA: 2s - loss: 0.6905 - accuracy: 0.52 - ETA: 2s - loss: 0.6900 - accuracy: 0.53 - ETA: 2s - loss: 0.6887 - accuracy: 0.53 - ETA: 2s - loss: 0.6884 - accuracy: 0.53 - ETA: 2s - loss: 0.6883 - accuracy: 0.53 - ETA: 2s - loss: 0.6876 - accuracy: 0.54 - ETA: 2s - loss: 0.6867 - accuracy: 0.54 - ETA: 2s - loss: 0.6856 - accuracy: 0.55 - ETA: 2s - loss: 0.6856 - accuracy: 0.55 - ETA: 2s - loss: 0.6852 - accuracy: 0.55 - ETA: 2s - loss: 0.6844 - accuracy: 0.55 - ETA: 2s - loss: 0.6839 - accuracy: 0.55 - ETA: 2s - loss: 0.6834 - accuracy: 0.55 - ETA: 1s - loss: 0.6826 - accuracy: 0.55 - ETA: 1s - loss: 0.6820 - accuracy: 0.56 - ETA: 1s - loss: 0.6812 - accuracy: 0.56 - ETA: 1s - loss: 0.6804 - accuracy: 0.56 - ETA: 1s - loss: 0.6801 - accuracy: 0.56 - ETA: 1s - loss: 0.6796 - accuracy: 0.56 - ETA: 1s - loss: 0.6787 - accuracy: 0.56 - ETA: 1s - loss: 0.6772 - accuracy: 0.56 - ETA: 1s - loss: 0.6768 - accuracy: 0.56 - ETA: 1s - loss: 0.6757 - accuracy: 0.56 - ETA: 1s - loss: 0.6752 - accuracy: 0.56 - ETA: 1s - loss: 0.6738 - accuracy: 0.57 - ETA: 1s - loss: 0.6732 - accuracy: 0.57 - ETA: 1s - loss: 0.6725 - accuracy: 0.57 - ETA: 1s - loss: 0.6718 - accuracy: 0.57 - ETA: 1s - loss: 0.6710 - accuracy: 0.57 - ETA: 1s - loss: 0.6712 - accuracy: 0.57 - ETA: 0s - loss: 0.6710 - accuracy: 0.57 - ETA: 0s - loss: 0.6706 - accuracy: 0.57 - ETA: 0s - loss: 0.6702 - accuracy: 0.57 - ETA: 0s - loss: 0.6696 - accuracy: 0.57 - ETA: 0s - loss: 0.6693 - accuracy: 0.57 - ETA: 0s - loss: 0.6683 - accuracy: 0.58 - ETA: 0s - loss: 0.6675 - accuracy: 0.58 - ETA: 0s - loss: 0.6671 - accuracy: 0.58 - ETA: 0s - loss: 0.6662 - accuracy: 0.58 - ETA: 0s - loss: 0.6653 - accuracy: 0.58 - ETA: 0s - loss: 0.6645 - accuracy: 0.58 - ETA: 0s - loss: 0.6640 - accuracy: 0.58 - ETA: 0s - loss: 0.6643 - accuracy: 0.58 - ETA: 0s - loss: 0.6639 - accuracy: 0.58 - ETA: 0s - loss: 0.6637 - accuracy: 0.58 - ETA: 0s - loss: 0.6630 - accuracy: 0.59 - ETA: 0s - loss: 0.6623 - accuracy: 0.59 - ETA: 0s - loss: 0.6614 - accuracy: 0.59 - ETA: 0s - loss: 0.6611 - accuracy: 0.59 - 3s 257us/step - loss: 0.6611 - accuracy: 0.5928 - val_loss: 0.6172 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 5:26 - loss: 0.6939 - accuracy: 0.25 - ETA: 8s - loss: 0.6938 - accuracy: 0.4815 - ETA: 5s - loss: 0.6937 - accuracy: 0.50 - ETA: 4s - loss: 0.6937 - accuracy: 0.49 - ETA: 4s - loss: 0.6936 - accuracy: 0.49 - ETA: 3s - loss: 0.6932 - accuracy: 0.50 - ETA: 3s - loss: 0.6925 - accuracy: 0.51 - ETA: 3s - loss: 0.6924 - accuracy: 0.50 - ETA: 3s - loss: 0.6917 - accuracy: 0.51 - ETA: 3s - loss: 0.6910 - accuracy: 0.52 - ETA: 2s - loss: 0.6907 - accuracy: 0.52 - ETA: 2s - loss: 0.6903 - accuracy: 0.52 - ETA: 2s - loss: 0.6895 - accuracy: 0.53 - ETA: 2s - loss: 0.6889 - accuracy: 0.53 - ETA: 2s - loss: 0.6886 - accuracy: 0.53 - ETA: 2s - loss: 0.6885 - accuracy: 0.53 - ETA: 2s - loss: 0.6879 - accuracy: 0.53 - ETA: 2s - loss: 0.6874 - accuracy: 0.54 - ETA: 2s - loss: 0.6860 - accuracy: 0.55 - ETA: 2s - loss: 0.6846 - accuracy: 0.55 - ETA: 2s - loss: 0.6840 - accuracy: 0.55 - ETA: 2s - loss: 0.6829 - accuracy: 0.55 - ETA: 2s - loss: 0.6821 - accuracy: 0.56 - ETA: 2s - loss: 0.6816 - accuracy: 0.56 - ETA: 1s - loss: 0.6802 - accuracy: 0.56 - ETA: 1s - loss: 0.6796 - accuracy: 0.56 - ETA: 1s - loss: 0.6787 - accuracy: 0.57 - ETA: 1s - loss: 0.6779 - accuracy: 0.57 - ETA: 1s - loss: 0.6772 - accuracy: 0.57 - ETA: 1s - loss: 0.6767 - accuracy: 0.58 - ETA: 1s - loss: 0.6759 - accuracy: 0.58 - ETA: 1s - loss: 0.6747 - accuracy: 0.58 - ETA: 1s - loss: 0.6733 - accuracy: 0.58 - ETA: 1s - loss: 0.6726 - accuracy: 0.58 - ETA: 1s - loss: 0.6719 - accuracy: 0.59 - ETA: 1s - loss: 0.6712 - accuracy: 0.59 - ETA: 1s - loss: 0.6707 - accuracy: 0.59 - ETA: 1s - loss: 0.6693 - accuracy: 0.59 - ETA: 1s - loss: 0.6683 - accuracy: 0.59 - ETA: 1s - loss: 0.6677 - accuracy: 0.59 - ETA: 1s - loss: 0.6668 - accuracy: 0.60 - ETA: 0s - loss: 0.6653 - accuracy: 0.60 - ETA: 0s - loss: 0.6641 - accuracy: 0.60 - ETA: 0s - loss: 0.6631 - accuracy: 0.60 - ETA: 0s - loss: 0.6626 - accuracy: 0.60 - ETA: 0s - loss: 0.6619 - accuracy: 0.61 - ETA: 0s - loss: 0.6613 - accuracy: 0.61 - ETA: 0s - loss: 0.6610 - accuracy: 0.61 - ETA: 0s - loss: 0.6599 - accuracy: 0.61 - ETA: 0s - loss: 0.6596 - accuracy: 0.61 - ETA: 0s - loss: 0.6586 - accuracy: 0.61 - ETA: 0s - loss: 0.6574 - accuracy: 0.61 - ETA: 0s - loss: 0.6564 - accuracy: 0.61 - ETA: 0s - loss: 0.6561 - accuracy: 0.62 - ETA: 0s - loss: 0.6556 - accuracy: 0.62 - ETA: 0s - loss: 0.6548 - accuracy: 0.62 - ETA: 0s - loss: 0.6543 - accuracy: 0.62 - ETA: 0s - loss: 0.6540 - accuracy: 0.62 - ETA: 0s - loss: 0.6539 - accuracy: 0.62 - ETA: 0s - loss: 0.6522 - accuracy: 0.62 - 3s 262us/step - loss: 0.6514 - accuracy: 0.6269 - val_loss: 0.5875 - val_accuracy: 0.7095\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:16 - loss: 0.6949 - accuracy: 0.25 - ETA: 8s - loss: 0.6948 - accuracy: 0.4537 - ETA: 5s - loss: 0.6943 - accuracy: 0.48 - ETA: 4s - loss: 0.6944 - accuracy: 0.48 - ETA: 4s - loss: 0.6942 - accuracy: 0.48 - ETA: 3s - loss: 0.6934 - accuracy: 0.49 - ETA: 3s - loss: 0.6934 - accuracy: 0.49 - ETA: 3s - loss: 0.6922 - accuracy: 0.50 - ETA: 3s - loss: 0.6919 - accuracy: 0.50 - ETA: 3s - loss: 0.6909 - accuracy: 0.51 - ETA: 3s - loss: 0.6901 - accuracy: 0.51 - ETA: 2s - loss: 0.6894 - accuracy: 0.51 - ETA: 2s - loss: 0.6891 - accuracy: 0.52 - ETA: 2s - loss: 0.6885 - accuracy: 0.52 - ETA: 2s - loss: 0.6874 - accuracy: 0.52 - ETA: 2s - loss: 0.6863 - accuracy: 0.52 - ETA: 2s - loss: 0.6858 - accuracy: 0.52 - ETA: 2s - loss: 0.6850 - accuracy: 0.53 - ETA: 2s - loss: 0.6843 - accuracy: 0.53 - ETA: 2s - loss: 0.6835 - accuracy: 0.54 - ETA: 2s - loss: 0.6824 - accuracy: 0.54 - ETA: 2s - loss: 0.6819 - accuracy: 0.54 - ETA: 2s - loss: 0.6818 - accuracy: 0.54 - ETA: 2s - loss: 0.6812 - accuracy: 0.55 - ETA: 2s - loss: 0.6802 - accuracy: 0.55 - ETA: 1s - loss: 0.6793 - accuracy: 0.55 - ETA: 1s - loss: 0.6787 - accuracy: 0.56 - ETA: 1s - loss: 0.6785 - accuracy: 0.56 - ETA: 1s - loss: 0.6775 - accuracy: 0.56 - ETA: 1s - loss: 0.6766 - accuracy: 0.56 - ETA: 1s - loss: 0.6761 - accuracy: 0.57 - ETA: 1s - loss: 0.6753 - accuracy: 0.57 - ETA: 1s - loss: 0.6742 - accuracy: 0.57 - ETA: 1s - loss: 0.6732 - accuracy: 0.57 - ETA: 1s - loss: 0.6724 - accuracy: 0.58 - ETA: 1s - loss: 0.6716 - accuracy: 0.58 - ETA: 1s - loss: 0.6708 - accuracy: 0.58 - ETA: 1s - loss: 0.6696 - accuracy: 0.58 - ETA: 1s - loss: 0.6685 - accuracy: 0.59 - ETA: 1s - loss: 0.6673 - accuracy: 0.59 - ETA: 1s - loss: 0.6662 - accuracy: 0.59 - ETA: 1s - loss: 0.6654 - accuracy: 0.59 - ETA: 1s - loss: 0.6644 - accuracy: 0.60 - ETA: 0s - loss: 0.6633 - accuracy: 0.60 - ETA: 0s - loss: 0.6622 - accuracy: 0.60 - ETA: 0s - loss: 0.6625 - accuracy: 0.60 - ETA: 0s - loss: 0.6621 - accuracy: 0.60 - ETA: 0s - loss: 0.6610 - accuracy: 0.60 - ETA: 0s - loss: 0.6595 - accuracy: 0.61 - ETA: 0s - loss: 0.6586 - accuracy: 0.61 - ETA: 0s - loss: 0.6580 - accuracy: 0.61 - ETA: 0s - loss: 0.6574 - accuracy: 0.61 - ETA: 0s - loss: 0.6572 - accuracy: 0.61 - ETA: 0s - loss: 0.6569 - accuracy: 0.61 - ETA: 0s - loss: 0.6559 - accuracy: 0.61 - ETA: 0s - loss: 0.6558 - accuracy: 0.61 - ETA: 0s - loss: 0.6551 - accuracy: 0.61 - ETA: 0s - loss: 0.6547 - accuracy: 0.61 - ETA: 0s - loss: 0.6542 - accuracy: 0.62 - ETA: 0s - loss: 0.6540 - accuracy: 0.62 - ETA: 0s - loss: 0.6536 - accuracy: 0.62 - ETA: 0s - loss: 0.6533 - accuracy: 0.62 - 3s 270us/step - loss: 0.6535 - accuracy: 0.6217 - val_loss: 0.6059 - val_accuracy: 0.7230\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:26 - loss: 0.6998 - accuracy: 0.50 - ETA: 8s - loss: 0.6886 - accuracy: 0.5773 - ETA: 5s - loss: 0.6870 - accuracy: 0.56 - ETA: 4s - loss: 0.6896 - accuracy: 0.55 - ETA: 4s - loss: 0.6891 - accuracy: 0.54 - ETA: 3s - loss: 0.6888 - accuracy: 0.54 - ETA: 3s - loss: 0.6884 - accuracy: 0.53 - ETA: 3s - loss: 0.6883 - accuracy: 0.53 - ETA: 3s - loss: 0.6885 - accuracy: 0.53 - ETA: 3s - loss: 0.6883 - accuracy: 0.52 - ETA: 3s - loss: 0.6879 - accuracy: 0.52 - ETA: 3s - loss: 0.6883 - accuracy: 0.52 - ETA: 2s - loss: 0.6870 - accuracy: 0.52 - ETA: 2s - loss: 0.6865 - accuracy: 0.52 - ETA: 2s - loss: 0.6851 - accuracy: 0.53 - ETA: 2s - loss: 0.6849 - accuracy: 0.53 - ETA: 2s - loss: 0.6849 - accuracy: 0.53 - ETA: 2s - loss: 0.6833 - accuracy: 0.53 - ETA: 2s - loss: 0.6826 - accuracy: 0.54 - ETA: 2s - loss: 0.6821 - accuracy: 0.54 - ETA: 2s - loss: 0.6817 - accuracy: 0.54 - ETA: 2s - loss: 0.6812 - accuracy: 0.55 - ETA: 2s - loss: 0.6809 - accuracy: 0.55 - ETA: 2s - loss: 0.6795 - accuracy: 0.56 - ETA: 2s - loss: 0.6784 - accuracy: 0.56 - ETA: 1s - loss: 0.6775 - accuracy: 0.57 - ETA: 1s - loss: 0.6774 - accuracy: 0.57 - ETA: 1s - loss: 0.6764 - accuracy: 0.57 - ETA: 1s - loss: 0.6755 - accuracy: 0.58 - ETA: 1s - loss: 0.6749 - accuracy: 0.58 - ETA: 1s - loss: 0.6747 - accuracy: 0.58 - ETA: 1s - loss: 0.6738 - accuracy: 0.58 - ETA: 1s - loss: 0.6728 - accuracy: 0.59 - ETA: 1s - loss: 0.6718 - accuracy: 0.59 - ETA: 1s - loss: 0.6709 - accuracy: 0.59 - ETA: 1s - loss: 0.6698 - accuracy: 0.59 - ETA: 1s - loss: 0.6690 - accuracy: 0.60 - ETA: 1s - loss: 0.6676 - accuracy: 0.60 - ETA: 1s - loss: 0.6673 - accuracy: 0.60 - ETA: 1s - loss: 0.6661 - accuracy: 0.60 - ETA: 1s - loss: 0.6656 - accuracy: 0.61 - ETA: 1s - loss: 0.6648 - accuracy: 0.61 - ETA: 0s - loss: 0.6636 - accuracy: 0.61 - ETA: 0s - loss: 0.6628 - accuracy: 0.61 - ETA: 0s - loss: 0.6617 - accuracy: 0.61 - ETA: 0s - loss: 0.6611 - accuracy: 0.62 - ETA: 0s - loss: 0.6612 - accuracy: 0.62 - ETA: 0s - loss: 0.6607 - accuracy: 0.62 - ETA: 0s - loss: 0.6599 - accuracy: 0.62 - ETA: 0s - loss: 0.6585 - accuracy: 0.62 - ETA: 0s - loss: 0.6582 - accuracy: 0.62 - ETA: 0s - loss: 0.6572 - accuracy: 0.63 - ETA: 0s - loss: 0.6571 - accuracy: 0.63 - ETA: 0s - loss: 0.6567 - accuracy: 0.63 - ETA: 0s - loss: 0.6564 - accuracy: 0.63 - ETA: 0s - loss: 0.6562 - accuracy: 0.63 - ETA: 0s - loss: 0.6560 - accuracy: 0.63 - ETA: 0s - loss: 0.6557 - accuracy: 0.63 - ETA: 0s - loss: 0.6552 - accuracy: 0.63 - ETA: 0s - loss: 0.6542 - accuracy: 0.63 - ETA: 0s - loss: 0.6536 - accuracy: 0.63 - 3s 266us/step - loss: 0.6531 - accuracy: 0.6381 - val_loss: 0.6033 - val_accuracy: 0.7173\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:19 - loss: 0.6949 - accuracy: 0.75 - ETA: 8s - loss: 0.6939 - accuracy: 0.5236 - ETA: 5s - loss: 0.6938 - accuracy: 0.53 - ETA: 4s - loss: 0.6927 - accuracy: 0.55 - ETA: 4s - loss: 0.6919 - accuracy: 0.55 - ETA: 3s - loss: 0.6921 - accuracy: 0.55 - ETA: 3s - loss: 0.6918 - accuracy: 0.55 - ETA: 3s - loss: 0.6919 - accuracy: 0.55 - ETA: 3s - loss: 0.6917 - accuracy: 0.55 - ETA: 3s - loss: 0.6907 - accuracy: 0.56 - ETA: 3s - loss: 0.6903 - accuracy: 0.56 - ETA: 2s - loss: 0.6898 - accuracy: 0.56 - ETA: 2s - loss: 0.6896 - accuracy: 0.56 - ETA: 2s - loss: 0.6892 - accuracy: 0.56 - ETA: 2s - loss: 0.6883 - accuracy: 0.57 - ETA: 2s - loss: 0.6874 - accuracy: 0.57 - ETA: 2s - loss: 0.6870 - accuracy: 0.57 - ETA: 2s - loss: 0.6858 - accuracy: 0.57 - ETA: 2s - loss: 0.6850 - accuracy: 0.57 - ETA: 2s - loss: 0.6841 - accuracy: 0.58 - ETA: 2s - loss: 0.6830 - accuracy: 0.58 - ETA: 2s - loss: 0.6828 - accuracy: 0.58 - ETA: 2s - loss: 0.6815 - accuracy: 0.59 - ETA: 2s - loss: 0.6810 - accuracy: 0.59 - ETA: 2s - loss: 0.6797 - accuracy: 0.59 - ETA: 1s - loss: 0.6785 - accuracy: 0.59 - ETA: 1s - loss: 0.6774 - accuracy: 0.59 - ETA: 1s - loss: 0.6762 - accuracy: 0.59 - ETA: 1s - loss: 0.6754 - accuracy: 0.60 - ETA: 1s - loss: 0.6740 - accuracy: 0.60 - ETA: 1s - loss: 0.6726 - accuracy: 0.60 - ETA: 1s - loss: 0.6713 - accuracy: 0.60 - ETA: 1s - loss: 0.6700 - accuracy: 0.61 - ETA: 1s - loss: 0.6692 - accuracy: 0.61 - ETA: 1s - loss: 0.6690 - accuracy: 0.61 - ETA: 1s - loss: 0.6672 - accuracy: 0.61 - ETA: 1s - loss: 0.6667 - accuracy: 0.61 - ETA: 1s - loss: 0.6658 - accuracy: 0.61 - ETA: 1s - loss: 0.6654 - accuracy: 0.61 - ETA: 1s - loss: 0.6640 - accuracy: 0.61 - ETA: 1s - loss: 0.6629 - accuracy: 0.61 - ETA: 1s - loss: 0.6617 - accuracy: 0.62 - ETA: 0s - loss: 0.6604 - accuracy: 0.62 - ETA: 0s - loss: 0.6597 - accuracy: 0.62 - ETA: 0s - loss: 0.6592 - accuracy: 0.62 - ETA: 0s - loss: 0.6589 - accuracy: 0.62 - ETA: 0s - loss: 0.6583 - accuracy: 0.62 - ETA: 0s - loss: 0.6575 - accuracy: 0.62 - ETA: 0s - loss: 0.6570 - accuracy: 0.62 - ETA: 0s - loss: 0.6555 - accuracy: 0.62 - ETA: 0s - loss: 0.6549 - accuracy: 0.62 - ETA: 0s - loss: 0.6533 - accuracy: 0.63 - ETA: 0s - loss: 0.6541 - accuracy: 0.62 - ETA: 0s - loss: 0.6532 - accuracy: 0.63 - ETA: 0s - loss: 0.6529 - accuracy: 0.63 - ETA: 0s - loss: 0.6516 - accuracy: 0.63 - ETA: 0s - loss: 0.6513 - accuracy: 0.63 - ETA: 0s - loss: 0.6511 - accuracy: 0.63 - ETA: 0s - loss: 0.6505 - accuracy: 0.63 - ETA: 0s - loss: 0.6501 - accuracy: 0.63 - ETA: 0s - loss: 0.6494 - accuracy: 0.63 - ETA: 0s - loss: 0.6486 - accuracy: 0.63 - 3s 268us/step - loss: 0.6485 - accuracy: 0.6334 - val_loss: 0.5854 - val_accuracy: 0.7138\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 100us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 5:23 - loss: 0.6887 - accuracy: 0.75 - ETA: 8s - loss: 0.6935 - accuracy: 0.4769 - ETA: 5s - loss: 0.6935 - accuracy: 0.48 - ETA: 4s - loss: 0.6928 - accuracy: 0.50 - ETA: 4s - loss: 0.6928 - accuracy: 0.50 - ETA: 3s - loss: 0.6922 - accuracy: 0.52 - ETA: 3s - loss: 0.6918 - accuracy: 0.52 - ETA: 3s - loss: 0.6910 - accuracy: 0.53 - ETA: 3s - loss: 0.6909 - accuracy: 0.54 - ETA: 3s - loss: 0.6899 - accuracy: 0.55 - ETA: 2s - loss: 0.6888 - accuracy: 0.55 - ETA: 2s - loss: 0.6879 - accuracy: 0.55 - ETA: 2s - loss: 0.6857 - accuracy: 0.56 - ETA: 2s - loss: 0.6852 - accuracy: 0.57 - ETA: 2s - loss: 0.6841 - accuracy: 0.57 - ETA: 2s - loss: 0.6825 - accuracy: 0.57 - ETA: 2s - loss: 0.6808 - accuracy: 0.58 - ETA: 2s - loss: 0.6781 - accuracy: 0.58 - ETA: 2s - loss: 0.6769 - accuracy: 0.59 - ETA: 2s - loss: 0.6748 - accuracy: 0.59 - ETA: 2s - loss: 0.6744 - accuracy: 0.59 - ETA: 2s - loss: 0.6732 - accuracy: 0.59 - ETA: 1s - loss: 0.6725 - accuracy: 0.60 - ETA: 1s - loss: 0.6719 - accuracy: 0.60 - ETA: 1s - loss: 0.6707 - accuracy: 0.60 - ETA: 1s - loss: 0.6691 - accuracy: 0.60 - ETA: 1s - loss: 0.6684 - accuracy: 0.60 - ETA: 1s - loss: 0.6673 - accuracy: 0.61 - ETA: 1s - loss: 0.6664 - accuracy: 0.61 - ETA: 1s - loss: 0.6660 - accuracy: 0.61 - ETA: 1s - loss: 0.6652 - accuracy: 0.61 - ETA: 1s - loss: 0.6642 - accuracy: 0.61 - ETA: 1s - loss: 0.6633 - accuracy: 0.61 - ETA: 1s - loss: 0.6629 - accuracy: 0.61 - ETA: 1s - loss: 0.6622 - accuracy: 0.61 - ETA: 1s - loss: 0.6619 - accuracy: 0.61 - ETA: 1s - loss: 0.6621 - accuracy: 0.61 - ETA: 1s - loss: 0.6616 - accuracy: 0.61 - ETA: 1s - loss: 0.6607 - accuracy: 0.61 - ETA: 1s - loss: 0.6599 - accuracy: 0.61 - ETA: 1s - loss: 0.6586 - accuracy: 0.62 - ETA: 0s - loss: 0.6577 - accuracy: 0.62 - ETA: 0s - loss: 0.6564 - accuracy: 0.62 - ETA: 0s - loss: 0.6553 - accuracy: 0.62 - ETA: 0s - loss: 0.6548 - accuracy: 0.62 - ETA: 0s - loss: 0.6547 - accuracy: 0.62 - ETA: 0s - loss: 0.6543 - accuracy: 0.62 - ETA: 0s - loss: 0.6535 - accuracy: 0.62 - ETA: 0s - loss: 0.6522 - accuracy: 0.62 - ETA: 0s - loss: 0.6521 - accuracy: 0.62 - ETA: 0s - loss: 0.6518 - accuracy: 0.62 - ETA: 0s - loss: 0.6507 - accuracy: 0.62 - ETA: 0s - loss: 0.6501 - accuracy: 0.62 - ETA: 0s - loss: 0.6493 - accuracy: 0.63 - ETA: 0s - loss: 0.6484 - accuracy: 0.63 - ETA: 0s - loss: 0.6484 - accuracy: 0.63 - ETA: 0s - loss: 0.6480 - accuracy: 0.63 - ETA: 0s - loss: 0.6473 - accuracy: 0.63 - ETA: 0s - loss: 0.6462 - accuracy: 0.63 - ETA: 0s - loss: 0.6463 - accuracy: 0.63 - 3s 261us/step - loss: 0.6464 - accuracy: 0.6353 - val_loss: 0.5893 - val_accuracy: 0.7166\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:26 - loss: 0.6948 - accuracy: 0.50 - ETA: 8s - loss: 0.6899 - accuracy: 0.6136 - ETA: 5s - loss: 0.6912 - accuracy: 0.55 - ETA: 4s - loss: 0.6916 - accuracy: 0.55 - ETA: 4s - loss: 0.6910 - accuracy: 0.54 - ETA: 3s - loss: 0.6903 - accuracy: 0.55 - ETA: 3s - loss: 0.6905 - accuracy: 0.54 - ETA: 3s - loss: 0.6902 - accuracy: 0.55 - ETA: 3s - loss: 0.6896 - accuracy: 0.55 - ETA: 3s - loss: 0.6894 - accuracy: 0.55 - ETA: 2s - loss: 0.6887 - accuracy: 0.56 - ETA: 2s - loss: 0.6877 - accuracy: 0.56 - ETA: 2s - loss: 0.6867 - accuracy: 0.57 - ETA: 2s - loss: 0.6862 - accuracy: 0.57 - ETA: 2s - loss: 0.6846 - accuracy: 0.58 - ETA: 2s - loss: 0.6826 - accuracy: 0.58 - ETA: 2s - loss: 0.6813 - accuracy: 0.58 - ETA: 2s - loss: 0.6800 - accuracy: 0.59 - ETA: 2s - loss: 0.6799 - accuracy: 0.59 - ETA: 2s - loss: 0.6791 - accuracy: 0.59 - ETA: 2s - loss: 0.6777 - accuracy: 0.59 - ETA: 2s - loss: 0.6757 - accuracy: 0.60 - ETA: 1s - loss: 0.6741 - accuracy: 0.60 - ETA: 1s - loss: 0.6728 - accuracy: 0.60 - ETA: 1s - loss: 0.6720 - accuracy: 0.60 - ETA: 1s - loss: 0.6725 - accuracy: 0.60 - ETA: 1s - loss: 0.6711 - accuracy: 0.60 - ETA: 1s - loss: 0.6694 - accuracy: 0.61 - ETA: 1s - loss: 0.6682 - accuracy: 0.61 - ETA: 1s - loss: 0.6673 - accuracy: 0.61 - ETA: 1s - loss: 0.6659 - accuracy: 0.61 - ETA: 1s - loss: 0.6650 - accuracy: 0.61 - ETA: 1s - loss: 0.6637 - accuracy: 0.61 - ETA: 1s - loss: 0.6633 - accuracy: 0.61 - ETA: 1s - loss: 0.6633 - accuracy: 0.61 - ETA: 1s - loss: 0.6623 - accuracy: 0.61 - ETA: 1s - loss: 0.6612 - accuracy: 0.61 - ETA: 1s - loss: 0.6605 - accuracy: 0.62 - ETA: 1s - loss: 0.6599 - accuracy: 0.62 - ETA: 1s - loss: 0.6588 - accuracy: 0.62 - ETA: 0s - loss: 0.6582 - accuracy: 0.62 - ETA: 0s - loss: 0.6578 - accuracy: 0.62 - ETA: 0s - loss: 0.6563 - accuracy: 0.62 - ETA: 0s - loss: 0.6557 - accuracy: 0.62 - ETA: 0s - loss: 0.6544 - accuracy: 0.62 - ETA: 0s - loss: 0.6539 - accuracy: 0.62 - ETA: 0s - loss: 0.6539 - accuracy: 0.62 - ETA: 0s - loss: 0.6538 - accuracy: 0.62 - ETA: 0s - loss: 0.6526 - accuracy: 0.62 - ETA: 0s - loss: 0.6518 - accuracy: 0.63 - ETA: 0s - loss: 0.6510 - accuracy: 0.63 - ETA: 0s - loss: 0.6507 - accuracy: 0.63 - ETA: 0s - loss: 0.6500 - accuracy: 0.63 - ETA: 0s - loss: 0.6493 - accuracy: 0.63 - ETA: 0s - loss: 0.6491 - accuracy: 0.63 - ETA: 0s - loss: 0.6481 - accuracy: 0.63 - ETA: 0s - loss: 0.6475 - accuracy: 0.63 - ETA: 0s - loss: 0.6467 - accuracy: 0.63 - ETA: 0s - loss: 0.6456 - accuracy: 0.63 - 3s 258us/step - loss: 0.6458 - accuracy: 0.6360 - val_loss: 0.5930 - val_accuracy: 0.7173\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:29 - loss: 0.7062 - accuracy: 0.0000e+ - ETA: 8s - loss: 0.6942 - accuracy: 0.4864     - ETA: 5s - loss: 0.6943 - accuracy: 0.47 - ETA: 4s - loss: 0.6933 - accuracy: 0.50 - ETA: 4s - loss: 0.6926 - accuracy: 0.50 - ETA: 3s - loss: 0.6912 - accuracy: 0.51 - ETA: 3s - loss: 0.6904 - accuracy: 0.51 - ETA: 3s - loss: 0.6897 - accuracy: 0.51 - ETA: 3s - loss: 0.6896 - accuracy: 0.52 - ETA: 3s - loss: 0.6890 - accuracy: 0.52 - ETA: 3s - loss: 0.6885 - accuracy: 0.52 - ETA: 3s - loss: 0.6878 - accuracy: 0.53 - ETA: 2s - loss: 0.6869 - accuracy: 0.54 - ETA: 2s - loss: 0.6858 - accuracy: 0.55 - ETA: 2s - loss: 0.6844 - accuracy: 0.55 - ETA: 2s - loss: 0.6826 - accuracy: 0.56 - ETA: 2s - loss: 0.6806 - accuracy: 0.56 - ETA: 2s - loss: 0.6794 - accuracy: 0.57 - ETA: 2s - loss: 0.6776 - accuracy: 0.57 - ETA: 2s - loss: 0.6758 - accuracy: 0.58 - ETA: 2s - loss: 0.6740 - accuracy: 0.58 - ETA: 2s - loss: 0.6729 - accuracy: 0.59 - ETA: 2s - loss: 0.6709 - accuracy: 0.59 - ETA: 2s - loss: 0.6698 - accuracy: 0.59 - ETA: 2s - loss: 0.6687 - accuracy: 0.60 - ETA: 1s - loss: 0.6686 - accuracy: 0.60 - ETA: 1s - loss: 0.6681 - accuracy: 0.60 - ETA: 1s - loss: 0.6661 - accuracy: 0.60 - ETA: 1s - loss: 0.6653 - accuracy: 0.60 - ETA: 1s - loss: 0.6650 - accuracy: 0.60 - ETA: 1s - loss: 0.6646 - accuracy: 0.60 - ETA: 1s - loss: 0.6634 - accuracy: 0.61 - ETA: 1s - loss: 0.6623 - accuracy: 0.61 - ETA: 1s - loss: 0.6605 - accuracy: 0.61 - ETA: 1s - loss: 0.6601 - accuracy: 0.61 - ETA: 1s - loss: 0.6587 - accuracy: 0.61 - ETA: 1s - loss: 0.6577 - accuracy: 0.62 - ETA: 1s - loss: 0.6567 - accuracy: 0.62 - ETA: 1s - loss: 0.6560 - accuracy: 0.62 - ETA: 1s - loss: 0.6543 - accuracy: 0.62 - ETA: 1s - loss: 0.6546 - accuracy: 0.62 - ETA: 1s - loss: 0.6531 - accuracy: 0.62 - ETA: 0s - loss: 0.6517 - accuracy: 0.63 - ETA: 0s - loss: 0.6512 - accuracy: 0.63 - ETA: 0s - loss: 0.6502 - accuracy: 0.63 - ETA: 0s - loss: 0.6494 - accuracy: 0.63 - ETA: 0s - loss: 0.6480 - accuracy: 0.63 - ETA: 0s - loss: 0.6471 - accuracy: 0.63 - ETA: 0s - loss: 0.6464 - accuracy: 0.63 - ETA: 0s - loss: 0.6461 - accuracy: 0.63 - ETA: 0s - loss: 0.6450 - accuracy: 0.63 - ETA: 0s - loss: 0.6437 - accuracy: 0.63 - ETA: 0s - loss: 0.6424 - accuracy: 0.64 - ETA: 0s - loss: 0.6416 - accuracy: 0.64 - ETA: 0s - loss: 0.6409 - accuracy: 0.64 - ETA: 0s - loss: 0.6413 - accuracy: 0.64 - ETA: 0s - loss: 0.6409 - accuracy: 0.64 - ETA: 0s - loss: 0.6395 - accuracy: 0.64 - ETA: 0s - loss: 0.6390 - accuracy: 0.64 - ETA: 0s - loss: 0.6382 - accuracy: 0.64 - 3s 263us/step - loss: 0.6380 - accuracy: 0.6473 - val_loss: 0.5765 - val_accuracy: 0.7266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:26 - loss: 0.6911 - accuracy: 0.50 - ETA: 8s - loss: 0.6933 - accuracy: 0.5402 - ETA: 5s - loss: 0.6923 - accuracy: 0.52 - ETA: 4s - loss: 0.6923 - accuracy: 0.50 - ETA: 4s - loss: 0.6918 - accuracy: 0.53 - ETA: 3s - loss: 0.6903 - accuracy: 0.54 - ETA: 3s - loss: 0.6897 - accuracy: 0.54 - ETA: 3s - loss: 0.6889 - accuracy: 0.55 - ETA: 3s - loss: 0.6882 - accuracy: 0.55 - ETA: 3s - loss: 0.6869 - accuracy: 0.56 - ETA: 2s - loss: 0.6859 - accuracy: 0.56 - ETA: 2s - loss: 0.6862 - accuracy: 0.56 - ETA: 2s - loss: 0.6849 - accuracy: 0.57 - ETA: 2s - loss: 0.6832 - accuracy: 0.57 - ETA: 2s - loss: 0.6806 - accuracy: 0.58 - ETA: 2s - loss: 0.6798 - accuracy: 0.58 - ETA: 2s - loss: 0.6792 - accuracy: 0.58 - ETA: 2s - loss: 0.6775 - accuracy: 0.59 - ETA: 2s - loss: 0.6752 - accuracy: 0.59 - ETA: 2s - loss: 0.6737 - accuracy: 0.60 - ETA: 2s - loss: 0.6725 - accuracy: 0.60 - ETA: 2s - loss: 0.6711 - accuracy: 0.60 - ETA: 1s - loss: 0.6687 - accuracy: 0.60 - ETA: 1s - loss: 0.6678 - accuracy: 0.61 - ETA: 1s - loss: 0.6663 - accuracy: 0.61 - ETA: 1s - loss: 0.6639 - accuracy: 0.61 - ETA: 1s - loss: 0.6623 - accuracy: 0.61 - ETA: 1s - loss: 0.6610 - accuracy: 0.62 - ETA: 1s - loss: 0.6598 - accuracy: 0.62 - ETA: 1s - loss: 0.6583 - accuracy: 0.62 - ETA: 1s - loss: 0.6574 - accuracy: 0.62 - ETA: 1s - loss: 0.6562 - accuracy: 0.62 - ETA: 1s - loss: 0.6558 - accuracy: 0.62 - ETA: 1s - loss: 0.6536 - accuracy: 0.62 - ETA: 1s - loss: 0.6533 - accuracy: 0.62 - ETA: 1s - loss: 0.6521 - accuracy: 0.63 - ETA: 1s - loss: 0.6510 - accuracy: 0.63 - ETA: 1s - loss: 0.6490 - accuracy: 0.63 - ETA: 1s - loss: 0.6486 - accuracy: 0.63 - ETA: 1s - loss: 0.6475 - accuracy: 0.63 - ETA: 0s - loss: 0.6470 - accuracy: 0.63 - ETA: 0s - loss: 0.6462 - accuracy: 0.63 - ETA: 0s - loss: 0.6447 - accuracy: 0.63 - ETA: 0s - loss: 0.6439 - accuracy: 0.64 - ETA: 0s - loss: 0.6437 - accuracy: 0.64 - ETA: 0s - loss: 0.6436 - accuracy: 0.64 - ETA: 0s - loss: 0.6425 - accuracy: 0.64 - ETA: 0s - loss: 0.6411 - accuracy: 0.64 - ETA: 0s - loss: 0.6398 - accuracy: 0.64 - ETA: 0s - loss: 0.6397 - accuracy: 0.64 - ETA: 0s - loss: 0.6387 - accuracy: 0.64 - ETA: 0s - loss: 0.6385 - accuracy: 0.64 - ETA: 0s - loss: 0.6385 - accuracy: 0.64 - ETA: 0s - loss: 0.6371 - accuracy: 0.64 - ETA: 0s - loss: 0.6363 - accuracy: 0.65 - ETA: 0s - loss: 0.6351 - accuracy: 0.65 - ETA: 0s - loss: 0.6347 - accuracy: 0.65 - ETA: 0s - loss: 0.6346 - accuracy: 0.65 - ETA: 0s - loss: 0.6343 - accuracy: 0.65 - ETA: 0s - loss: 0.6343 - accuracy: 0.65 - 3s 260us/step - loss: 0.6344 - accuracy: 0.6537 - val_loss: 0.5755 - val_accuracy: 0.7237\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 108us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:39 - loss: 0.7051 - accuracy: 0.25 - ETA: 12s - loss: 0.6951 - accuracy: 0.5233 - ETA: 8s - loss: 0.6942 - accuracy: 0.521 - ETA: 6s - loss: 0.6934 - accuracy: 0.52 - ETA: 5s - loss: 0.6927 - accuracy: 0.53 - ETA: 4s - loss: 0.6914 - accuracy: 0.54 - ETA: 4s - loss: 0.6911 - accuracy: 0.54 - ETA: 3s - loss: 0.6901 - accuracy: 0.54 - ETA: 3s - loss: 0.6888 - accuracy: 0.55 - ETA: 3s - loss: 0.6881 - accuracy: 0.55 - ETA: 3s - loss: 0.6867 - accuracy: 0.56 - ETA: 3s - loss: 0.6861 - accuracy: 0.56 - ETA: 2s - loss: 0.6851 - accuracy: 0.57 - ETA: 2s - loss: 0.6836 - accuracy: 0.57 - ETA: 2s - loss: 0.6824 - accuracy: 0.57 - ETA: 2s - loss: 0.6811 - accuracy: 0.58 - ETA: 2s - loss: 0.6792 - accuracy: 0.59 - ETA: 2s - loss: 0.6775 - accuracy: 0.59 - ETA: 2s - loss: 0.6767 - accuracy: 0.59 - ETA: 2s - loss: 0.6757 - accuracy: 0.59 - ETA: 2s - loss: 0.6753 - accuracy: 0.59 - ETA: 2s - loss: 0.6741 - accuracy: 0.60 - ETA: 2s - loss: 0.6726 - accuracy: 0.60 - ETA: 2s - loss: 0.6708 - accuracy: 0.60 - ETA: 2s - loss: 0.6691 - accuracy: 0.61 - ETA: 1s - loss: 0.6668 - accuracy: 0.61 - ETA: 1s - loss: 0.6657 - accuracy: 0.61 - ETA: 1s - loss: 0.6640 - accuracy: 0.61 - ETA: 1s - loss: 0.6626 - accuracy: 0.62 - ETA: 1s - loss: 0.6611 - accuracy: 0.62 - ETA: 1s - loss: 0.6602 - accuracy: 0.62 - ETA: 1s - loss: 0.6577 - accuracy: 0.62 - ETA: 1s - loss: 0.6563 - accuracy: 0.62 - ETA: 1s - loss: 0.6555 - accuracy: 0.62 - ETA: 1s - loss: 0.6550 - accuracy: 0.62 - ETA: 1s - loss: 0.6538 - accuracy: 0.63 - ETA: 1s - loss: 0.6518 - accuracy: 0.63 - ETA: 1s - loss: 0.6508 - accuracy: 0.63 - ETA: 1s - loss: 0.6499 - accuracy: 0.63 - ETA: 1s - loss: 0.6489 - accuracy: 0.63 - ETA: 1s - loss: 0.6476 - accuracy: 0.63 - ETA: 1s - loss: 0.6461 - accuracy: 0.64 - ETA: 1s - loss: 0.6457 - accuracy: 0.64 - ETA: 0s - loss: 0.6450 - accuracy: 0.64 - ETA: 0s - loss: 0.6437 - accuracy: 0.64 - ETA: 0s - loss: 0.6422 - accuracy: 0.64 - ETA: 0s - loss: 0.6417 - accuracy: 0.64 - ETA: 0s - loss: 0.6413 - accuracy: 0.64 - ETA: 0s - loss: 0.6407 - accuracy: 0.64 - ETA: 0s - loss: 0.6394 - accuracy: 0.65 - ETA: 0s - loss: 0.6388 - accuracy: 0.65 - ETA: 0s - loss: 0.6384 - accuracy: 0.65 - ETA: 0s - loss: 0.6375 - accuracy: 0.65 - ETA: 0s - loss: 0.6372 - accuracy: 0.65 - ETA: 0s - loss: 0.6368 - accuracy: 0.65 - ETA: 0s - loss: 0.6367 - accuracy: 0.65 - ETA: 0s - loss: 0.6359 - accuracy: 0.65 - ETA: 0s - loss: 0.6359 - accuracy: 0.65 - ETA: 0s - loss: 0.6351 - accuracy: 0.65 - ETA: 0s - loss: 0.6350 - accuracy: 0.65 - ETA: 0s - loss: 0.6345 - accuracy: 0.65 - ETA: 0s - loss: 0.6342 - accuracy: 0.65 - 3s 270us/step - loss: 0.6340 - accuracy: 0.6560 - val_loss: 0.5759 - val_accuracy: 0.7251\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 106us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:32 - loss: 0.6895 - accuracy: 1.00 - ETA: 9s - loss: 0.6952 - accuracy: 0.5240 - ETA: 5s - loss: 0.6947 - accuracy: 0.50 - ETA: 4s - loss: 0.6940 - accuracy: 0.51 - ETA: 4s - loss: 0.6925 - accuracy: 0.53 - ETA: 3s - loss: 0.6924 - accuracy: 0.52 - ETA: 3s - loss: 0.6918 - accuracy: 0.53 - ETA: 3s - loss: 0.6908 - accuracy: 0.53 - ETA: 3s - loss: 0.6901 - accuracy: 0.54 - ETA: 3s - loss: 0.6891 - accuracy: 0.55 - ETA: 2s - loss: 0.6882 - accuracy: 0.55 - ETA: 2s - loss: 0.6866 - accuracy: 0.56 - ETA: 2s - loss: 0.6858 - accuracy: 0.56 - ETA: 2s - loss: 0.6851 - accuracy: 0.56 - ETA: 2s - loss: 0.6843 - accuracy: 0.56 - ETA: 2s - loss: 0.6825 - accuracy: 0.57 - ETA: 2s - loss: 0.6806 - accuracy: 0.58 - ETA: 2s - loss: 0.6786 - accuracy: 0.58 - ETA: 2s - loss: 0.6770 - accuracy: 0.58 - ETA: 2s - loss: 0.6756 - accuracy: 0.59 - ETA: 2s - loss: 0.6740 - accuracy: 0.59 - ETA: 2s - loss: 0.6724 - accuracy: 0.59 - ETA: 1s - loss: 0.6702 - accuracy: 0.60 - ETA: 1s - loss: 0.6687 - accuracy: 0.60 - ETA: 1s - loss: 0.6672 - accuracy: 0.60 - ETA: 1s - loss: 0.6652 - accuracy: 0.60 - ETA: 1s - loss: 0.6629 - accuracy: 0.61 - ETA: 1s - loss: 0.6625 - accuracy: 0.61 - ETA: 1s - loss: 0.6606 - accuracy: 0.61 - ETA: 1s - loss: 0.6594 - accuracy: 0.61 - ETA: 1s - loss: 0.6581 - accuracy: 0.62 - ETA: 1s - loss: 0.6563 - accuracy: 0.62 - ETA: 1s - loss: 0.6549 - accuracy: 0.62 - ETA: 1s - loss: 0.6534 - accuracy: 0.62 - ETA: 1s - loss: 0.6524 - accuracy: 0.62 - ETA: 1s - loss: 0.6529 - accuracy: 0.62 - ETA: 1s - loss: 0.6521 - accuracy: 0.63 - ETA: 1s - loss: 0.6511 - accuracy: 0.63 - ETA: 1s - loss: 0.6501 - accuracy: 0.63 - ETA: 1s - loss: 0.6497 - accuracy: 0.63 - ETA: 0s - loss: 0.6490 - accuracy: 0.63 - ETA: 0s - loss: 0.6484 - accuracy: 0.63 - ETA: 0s - loss: 0.6475 - accuracy: 0.63 - ETA: 0s - loss: 0.6462 - accuracy: 0.63 - ETA: 0s - loss: 0.6452 - accuracy: 0.63 - ETA: 0s - loss: 0.6448 - accuracy: 0.64 - ETA: 0s - loss: 0.6440 - accuracy: 0.64 - ETA: 0s - loss: 0.6429 - accuracy: 0.64 - ETA: 0s - loss: 0.6418 - accuracy: 0.64 - ETA: 0s - loss: 0.6407 - accuracy: 0.64 - ETA: 0s - loss: 0.6400 - accuracy: 0.64 - ETA: 0s - loss: 0.6389 - accuracy: 0.64 - ETA: 0s - loss: 0.6385 - accuracy: 0.64 - ETA: 0s - loss: 0.6385 - accuracy: 0.64 - ETA: 0s - loss: 0.6373 - accuracy: 0.65 - ETA: 0s - loss: 0.6373 - accuracy: 0.65 - ETA: 0s - loss: 0.6365 - accuracy: 0.65 - ETA: 0s - loss: 0.6353 - accuracy: 0.65 - ETA: 0s - loss: 0.6336 - accuracy: 0.65 - ETA: 0s - loss: 0.6331 - accuracy: 0.65 - 3s 261us/step - loss: 0.6326 - accuracy: 0.6565 - val_loss: 0.5848 - val_accuracy: 0.7180\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 100us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:26 - loss: 0.6957 - accuracy: 0.25 - ETA: 9s - loss: 0.6934 - accuracy: 0.4950 - ETA: 6s - loss: 0.6914 - accuracy: 0.54 - ETA: 4s - loss: 0.6919 - accuracy: 0.53 - ETA: 4s - loss: 0.6917 - accuracy: 0.54 - ETA: 3s - loss: 0.6910 - accuracy: 0.54 - ETA: 3s - loss: 0.6906 - accuracy: 0.55 - ETA: 3s - loss: 0.6897 - accuracy: 0.55 - ETA: 3s - loss: 0.6885 - accuracy: 0.55 - ETA: 3s - loss: 0.6886 - accuracy: 0.55 - ETA: 3s - loss: 0.6873 - accuracy: 0.56 - ETA: 2s - loss: 0.6864 - accuracy: 0.56 - ETA: 2s - loss: 0.6842 - accuracy: 0.56 - ETA: 2s - loss: 0.6818 - accuracy: 0.57 - ETA: 2s - loss: 0.6808 - accuracy: 0.57 - ETA: 2s - loss: 0.6799 - accuracy: 0.57 - ETA: 2s - loss: 0.6786 - accuracy: 0.58 - ETA: 2s - loss: 0.6768 - accuracy: 0.58 - ETA: 2s - loss: 0.6746 - accuracy: 0.59 - ETA: 2s - loss: 0.6739 - accuracy: 0.59 - ETA: 2s - loss: 0.6728 - accuracy: 0.59 - ETA: 2s - loss: 0.6715 - accuracy: 0.59 - ETA: 2s - loss: 0.6714 - accuracy: 0.59 - ETA: 2s - loss: 0.6711 - accuracy: 0.59 - ETA: 1s - loss: 0.6699 - accuracy: 0.60 - ETA: 1s - loss: 0.6685 - accuracy: 0.60 - ETA: 1s - loss: 0.6668 - accuracy: 0.60 - ETA: 1s - loss: 0.6652 - accuracy: 0.60 - ETA: 1s - loss: 0.6641 - accuracy: 0.61 - ETA: 1s - loss: 0.6622 - accuracy: 0.61 - ETA: 1s - loss: 0.6617 - accuracy: 0.61 - ETA: 1s - loss: 0.6616 - accuracy: 0.61 - ETA: 1s - loss: 0.6593 - accuracy: 0.61 - ETA: 1s - loss: 0.6575 - accuracy: 0.61 - ETA: 1s - loss: 0.6561 - accuracy: 0.62 - ETA: 1s - loss: 0.6548 - accuracy: 0.62 - ETA: 1s - loss: 0.6541 - accuracy: 0.62 - ETA: 1s - loss: 0.6527 - accuracy: 0.62 - ETA: 1s - loss: 0.6510 - accuracy: 0.62 - ETA: 1s - loss: 0.6509 - accuracy: 0.62 - ETA: 1s - loss: 0.6499 - accuracy: 0.63 - ETA: 0s - loss: 0.6491 - accuracy: 0.63 - ETA: 0s - loss: 0.6481 - accuracy: 0.63 - ETA: 0s - loss: 0.6481 - accuracy: 0.63 - ETA: 0s - loss: 0.6471 - accuracy: 0.63 - ETA: 0s - loss: 0.6470 - accuracy: 0.63 - ETA: 0s - loss: 0.6471 - accuracy: 0.63 - ETA: 0s - loss: 0.6465 - accuracy: 0.63 - ETA: 0s - loss: 0.6464 - accuracy: 0.63 - ETA: 0s - loss: 0.6458 - accuracy: 0.63 - ETA: 0s - loss: 0.6448 - accuracy: 0.63 - ETA: 0s - loss: 0.6437 - accuracy: 0.63 - ETA: 0s - loss: 0.6428 - accuracy: 0.63 - ETA: 0s - loss: 0.6428 - accuracy: 0.64 - ETA: 0s - loss: 0.6417 - accuracy: 0.64 - ETA: 0s - loss: 0.6414 - accuracy: 0.64 - ETA: 0s - loss: 0.6411 - accuracy: 0.64 - ETA: 0s - loss: 0.6408 - accuracy: 0.64 - ETA: 0s - loss: 0.6403 - accuracy: 0.64 - ETA: 0s - loss: 0.6395 - accuracy: 0.64 - 3s 262us/step - loss: 0.6390 - accuracy: 0.6447 - val_loss: 0.5813 - val_accuracy: 0.7173\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 94us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:19 - loss: 0.6904 - accuracy: 0.75 - ETA: 8s - loss: 0.6895 - accuracy: 0.6065 - ETA: 5s - loss: 0.6910 - accuracy: 0.55 - ETA: 4s - loss: 0.6892 - accuracy: 0.57 - ETA: 4s - loss: 0.6909 - accuracy: 0.55 - ETA: 3s - loss: 0.6911 - accuracy: 0.55 - ETA: 3s - loss: 0.6903 - accuracy: 0.56 - ETA: 3s - loss: 0.6895 - accuracy: 0.56 - ETA: 3s - loss: 0.6883 - accuracy: 0.57 - ETA: 2s - loss: 0.6871 - accuracy: 0.57 - ETA: 2s - loss: 0.6859 - accuracy: 0.58 - ETA: 2s - loss: 0.6858 - accuracy: 0.57 - ETA: 2s - loss: 0.6849 - accuracy: 0.58 - ETA: 2s - loss: 0.6837 - accuracy: 0.58 - ETA: 2s - loss: 0.6823 - accuracy: 0.59 - ETA: 2s - loss: 0.6807 - accuracy: 0.59 - ETA: 2s - loss: 0.6790 - accuracy: 0.60 - ETA: 2s - loss: 0.6767 - accuracy: 0.60 - ETA: 2s - loss: 0.6749 - accuracy: 0.61 - ETA: 2s - loss: 0.6736 - accuracy: 0.61 - ETA: 2s - loss: 0.6713 - accuracy: 0.61 - ETA: 2s - loss: 0.6710 - accuracy: 0.61 - ETA: 1s - loss: 0.6694 - accuracy: 0.61 - ETA: 1s - loss: 0.6687 - accuracy: 0.61 - ETA: 1s - loss: 0.6681 - accuracy: 0.61 - ETA: 1s - loss: 0.6672 - accuracy: 0.62 - ETA: 1s - loss: 0.6661 - accuracy: 0.62 - ETA: 1s - loss: 0.6645 - accuracy: 0.62 - ETA: 1s - loss: 0.6632 - accuracy: 0.62 - ETA: 1s - loss: 0.6629 - accuracy: 0.62 - ETA: 1s - loss: 0.6622 - accuracy: 0.62 - ETA: 1s - loss: 0.6603 - accuracy: 0.62 - ETA: 1s - loss: 0.6589 - accuracy: 0.63 - ETA: 1s - loss: 0.6579 - accuracy: 0.63 - ETA: 1s - loss: 0.6566 - accuracy: 0.63 - ETA: 1s - loss: 0.6554 - accuracy: 0.63 - ETA: 1s - loss: 0.6542 - accuracy: 0.63 - ETA: 1s - loss: 0.6528 - accuracy: 0.63 - ETA: 1s - loss: 0.6516 - accuracy: 0.63 - ETA: 0s - loss: 0.6512 - accuracy: 0.64 - ETA: 0s - loss: 0.6500 - accuracy: 0.64 - ETA: 0s - loss: 0.6490 - accuracy: 0.64 - ETA: 0s - loss: 0.6485 - accuracy: 0.64 - ETA: 0s - loss: 0.6481 - accuracy: 0.64 - ETA: 0s - loss: 0.6485 - accuracy: 0.64 - ETA: 0s - loss: 0.6475 - accuracy: 0.64 - ETA: 0s - loss: 0.6468 - accuracy: 0.64 - ETA: 0s - loss: 0.6462 - accuracy: 0.64 - ETA: 0s - loss: 0.6454 - accuracy: 0.64 - ETA: 0s - loss: 0.6448 - accuracy: 0.64 - ETA: 0s - loss: 0.6435 - accuracy: 0.64 - ETA: 0s - loss: 0.6428 - accuracy: 0.65 - ETA: 0s - loss: 0.6413 - accuracy: 0.65 - ETA: 0s - loss: 0.6407 - accuracy: 0.65 - ETA: 0s - loss: 0.6399 - accuracy: 0.65 - ETA: 0s - loss: 0.6405 - accuracy: 0.65 - ETA: 0s - loss: 0.6394 - accuracy: 0.65 - ETA: 0s - loss: 0.6385 - accuracy: 0.65 - 3s 254us/step - loss: 0.6383 - accuracy: 0.6570 - val_loss: 0.5887 - val_accuracy: 0.7124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 95us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 4:35 - loss: 0.6968 - accuracy: 0.50 - ETA: 7s - loss: 0.6932 - accuracy: 0.5127 - ETA: 4s - loss: 0.6935 - accuracy: 0.50 - ETA: 4s - loss: 0.6938 - accuracy: 0.50 - ETA: 3s - loss: 0.6941 - accuracy: 0.49 - ETA: 3s - loss: 0.6941 - accuracy: 0.49 - ETA: 3s - loss: 0.6941 - accuracy: 0.50 - ETA: 3s - loss: 0.6939 - accuracy: 0.50 - ETA: 3s - loss: 0.6939 - accuracy: 0.50 - ETA: 2s - loss: 0.6939 - accuracy: 0.50 - ETA: 2s - loss: 0.6939 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6939 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 2s - loss: 0.6939 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6939 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - 3s 237us/step - loss: 0.6927 - accuracy: 0.5203 - val_loss: 0.6876 - val_accuracy: 0.5291\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 92us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 4:32 - loss: 0.6814 - accuracy: 1.00 - ETA: 7s - loss: 0.6949 - accuracy: 0.4784 - ETA: 4s - loss: 0.6947 - accuracy: 0.49 - ETA: 4s - loss: 0.6944 - accuracy: 0.48 - ETA: 3s - loss: 0.6945 - accuracy: 0.48 - ETA: 3s - loss: 0.6946 - accuracy: 0.48 - ETA: 3s - loss: 0.6945 - accuracy: 0.48 - ETA: 2s - loss: 0.6944 - accuracy: 0.49 - ETA: 2s - loss: 0.6944 - accuracy: 0.48 - ETA: 2s - loss: 0.6942 - accuracy: 0.49 - ETA: 2s - loss: 0.6940 - accuracy: 0.49 - ETA: 2s - loss: 0.6941 - accuracy: 0.49 - ETA: 2s - loss: 0.6940 - accuracy: 0.49 - ETA: 2s - loss: 0.6939 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 2s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.50 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.50 - ETA: 1s - loss: 0.6933 - accuracy: 0.50 - ETA: 1s - loss: 0.6933 - accuracy: 0.50 - ETA: 1s - loss: 0.6933 - accuracy: 0.50 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - ETA: 0s - loss: 0.6918 - accuracy: 0.53 - ETA: 0s - loss: 0.6917 - accuracy: 0.53 - ETA: 0s - loss: 0.6916 - accuracy: 0.53 - ETA: 0s - loss: 0.6913 - accuracy: 0.53 - ETA: 0s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6909 - accuracy: 0.53 - ETA: 0s - loss: 0.6908 - accuracy: 0.54 - ETA: 0s - loss: 0.6907 - accuracy: 0.54 - 3s 235us/step - loss: 0.6907 - accuracy: 0.5404 - val_loss: 0.6782 - val_accuracy: 0.6442\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 97us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 4:54 - loss: 0.6918 - accuracy: 0.25 - ETA: 7s - loss: 0.6934 - accuracy: 0.4661 - ETA: 4s - loss: 0.6930 - accuracy: 0.48 - ETA: 4s - loss: 0.6916 - accuracy: 0.51 - ETA: 3s - loss: 0.6924 - accuracy: 0.50 - ETA: 3s - loss: 0.6921 - accuracy: 0.50 - ETA: 3s - loss: 0.6916 - accuracy: 0.50 - ETA: 2s - loss: 0.6913 - accuracy: 0.51 - ETA: 2s - loss: 0.6916 - accuracy: 0.51 - ETA: 2s - loss: 0.6917 - accuracy: 0.50 - ETA: 2s - loss: 0.6918 - accuracy: 0.50 - ETA: 2s - loss: 0.6919 - accuracy: 0.50 - ETA: 2s - loss: 0.6921 - accuracy: 0.50 - ETA: 2s - loss: 0.6920 - accuracy: 0.51 - ETA: 2s - loss: 0.6918 - accuracy: 0.51 - ETA: 2s - loss: 0.6919 - accuracy: 0.51 - ETA: 2s - loss: 0.6919 - accuracy: 0.51 - ETA: 2s - loss: 0.6919 - accuracy: 0.51 - ETA: 1s - loss: 0.6917 - accuracy: 0.51 - ETA: 1s - loss: 0.6916 - accuracy: 0.51 - ETA: 1s - loss: 0.6914 - accuracy: 0.51 - ETA: 1s - loss: 0.6912 - accuracy: 0.51 - ETA: 1s - loss: 0.6911 - accuracy: 0.51 - ETA: 1s - loss: 0.6907 - accuracy: 0.51 - ETA: 1s - loss: 0.6905 - accuracy: 0.51 - ETA: 1s - loss: 0.6900 - accuracy: 0.51 - ETA: 1s - loss: 0.6898 - accuracy: 0.51 - ETA: 1s - loss: 0.6899 - accuracy: 0.51 - ETA: 1s - loss: 0.6898 - accuracy: 0.51 - ETA: 1s - loss: 0.6896 - accuracy: 0.51 - ETA: 1s - loss: 0.6894 - accuracy: 0.51 - ETA: 1s - loss: 0.6890 - accuracy: 0.51 - ETA: 1s - loss: 0.6890 - accuracy: 0.52 - ETA: 1s - loss: 0.6887 - accuracy: 0.52 - ETA: 1s - loss: 0.6884 - accuracy: 0.52 - ETA: 0s - loss: 0.6881 - accuracy: 0.52 - ETA: 0s - loss: 0.6878 - accuracy: 0.53 - ETA: 0s - loss: 0.6875 - accuracy: 0.53 - ETA: 0s - loss: 0.6871 - accuracy: 0.53 - ETA: 0s - loss: 0.6870 - accuracy: 0.53 - ETA: 0s - loss: 0.6868 - accuracy: 0.53 - ETA: 0s - loss: 0.6866 - accuracy: 0.53 - ETA: 0s - loss: 0.6865 - accuracy: 0.53 - ETA: 0s - loss: 0.6863 - accuracy: 0.53 - ETA: 0s - loss: 0.6862 - accuracy: 0.53 - ETA: 0s - loss: 0.6857 - accuracy: 0.54 - ETA: 0s - loss: 0.6853 - accuracy: 0.54 - ETA: 0s - loss: 0.6847 - accuracy: 0.54 - ETA: 0s - loss: 0.6846 - accuracy: 0.54 - ETA: 0s - loss: 0.6843 - accuracy: 0.54 - ETA: 0s - loss: 0.6841 - accuracy: 0.54 - ETA: 0s - loss: 0.6839 - accuracy: 0.54 - ETA: 0s - loss: 0.6835 - accuracy: 0.54 - ETA: 0s - loss: 0.6832 - accuracy: 0.55 - 3s 237us/step - loss: 0.6831 - accuracy: 0.5513 - val_loss: 0.6597 - val_accuracy: 0.7131\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 95us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:38 - loss: 0.6949 - accuracy: 0.50 - ETA: 7s - loss: 0.6942 - accuracy: 0.4875 - ETA: 4s - loss: 0.6943 - accuracy: 0.48 - ETA: 3s - loss: 0.6941 - accuracy: 0.49 - ETA: 3s - loss: 0.6939 - accuracy: 0.50 - ETA: 3s - loss: 0.6938 - accuracy: 0.51 - ETA: 3s - loss: 0.6941 - accuracy: 0.50 - ETA: 2s - loss: 0.6940 - accuracy: 0.50 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - 3s 238us/step - loss: 0.6924 - accuracy: 0.5218 - val_loss: 0.6868 - val_accuracy: 0.5561\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 94us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:35 - loss: 0.7001 - accuracy: 0.50 - ETA: 7s - loss: 0.6938 - accuracy: 0.5042 - ETA: 4s - loss: 0.6926 - accuracy: 0.53 - ETA: 4s - loss: 0.6934 - accuracy: 0.51 - ETA: 3s - loss: 0.6929 - accuracy: 0.52 - ETA: 3s - loss: 0.6931 - accuracy: 0.52 - ETA: 3s - loss: 0.6921 - accuracy: 0.53 - ETA: 2s - loss: 0.6926 - accuracy: 0.52 - ETA: 2s - loss: 0.6928 - accuracy: 0.52 - ETA: 2s - loss: 0.6929 - accuracy: 0.52 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6928 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - 3s 233us/step - loss: 0.6921 - accuracy: 0.5126 - val_loss: 0.6852 - val_accuracy: 0.5661\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 99us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:32 - loss: 0.6938 - accuracy: 0.50 - ETA: 7s - loss: 0.6937 - accuracy: 0.5169 - ETA: 4s - loss: 0.6938 - accuracy: 0.51 - ETA: 4s - loss: 0.6934 - accuracy: 0.52 - ETA: 3s - loss: 0.6935 - accuracy: 0.52 - ETA: 3s - loss: 0.6936 - accuracy: 0.51 - ETA: 3s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.49 - ETA: 2s - loss: 0.6938 - accuracy: 0.49 - ETA: 2s - loss: 0.6938 - accuracy: 0.49 - ETA: 2s - loss: 0.6938 - accuracy: 0.49 - ETA: 2s - loss: 0.6938 - accuracy: 0.49 - ETA: 2s - loss: 0.6936 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.49 - ETA: 1s - loss: 0.6938 - accuracy: 0.49 - ETA: 1s - loss: 0.6938 - accuracy: 0.49 - ETA: 1s - loss: 0.6938 - accuracy: 0.49 - ETA: 1s - loss: 0.6938 - accuracy: 0.49 - ETA: 1s - loss: 0.6938 - accuracy: 0.49 - ETA: 1s - loss: 0.6938 - accuracy: 0.49 - ETA: 1s - loss: 0.6938 - accuracy: 0.49 - ETA: 1s - loss: 0.6938 - accuracy: 0.49 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - 3s 233us/step - loss: 0.6932 - accuracy: 0.5086 - val_loss: 0.6915 - val_accuracy: 0.5099\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 98us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 4:35 - loss: 0.6940 - accuracy: 0.50 - ETA: 7s - loss: 0.6932 - accuracy: 0.5169 - ETA: 5s - loss: 0.6943 - accuracy: 0.51 - ETA: 4s - loss: 0.6944 - accuracy: 0.50 - ETA: 3s - loss: 0.6942 - accuracy: 0.51 - ETA: 3s - loss: 0.6940 - accuracy: 0.51 - ETA: 3s - loss: 0.6938 - accuracy: 0.52 - ETA: 2s - loss: 0.6940 - accuracy: 0.51 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6941 - accuracy: 0.51 - ETA: 2s - loss: 0.6940 - accuracy: 0.51 - ETA: 2s - loss: 0.6942 - accuracy: 0.51 - ETA: 2s - loss: 0.6940 - accuracy: 0.51 - ETA: 2s - loss: 0.6941 - accuracy: 0.51 - ETA: 2s - loss: 0.6940 - accuracy: 0.51 - ETA: 2s - loss: 0.6940 - accuracy: 0.51 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6928 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - ETA: 0s - loss: 0.6919 - accuracy: 0.51 - ETA: 0s - loss: 0.6917 - accuracy: 0.51 - ETA: 0s - loss: 0.6916 - accuracy: 0.51 - ETA: 0s - loss: 0.6915 - accuracy: 0.51 - ETA: 0s - loss: 0.6915 - accuracy: 0.51 - ETA: 0s - loss: 0.6914 - accuracy: 0.52 - ETA: 0s - loss: 0.6913 - accuracy: 0.52 - ETA: 0s - loss: 0.6911 - accuracy: 0.52 - ETA: 0s - loss: 0.6908 - accuracy: 0.52 - ETA: 0s - loss: 0.6907 - accuracy: 0.52 - ETA: 0s - loss: 0.6906 - accuracy: 0.52 - ETA: 0s - loss: 0.6903 - accuracy: 0.52 - ETA: 0s - loss: 0.6899 - accuracy: 0.52 - ETA: 0s - loss: 0.6897 - accuracy: 0.52 - ETA: 0s - loss: 0.6897 - accuracy: 0.52 - 3s 236us/step - loss: 0.6896 - accuracy: 0.5292 - val_loss: 0.6727 - val_accuracy: 0.6861\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 98us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 4:29 - loss: 0.6897 - accuracy: 1.00 - ETA: 7s - loss: 0.6942 - accuracy: 0.5302 - ETA: 4s - loss: 0.6940 - accuracy: 0.51 - ETA: 4s - loss: 0.6940 - accuracy: 0.52 - ETA: 3s - loss: 0.6941 - accuracy: 0.51 - ETA: 3s - loss: 0.6944 - accuracy: 0.51 - ETA: 3s - loss: 0.6942 - accuracy: 0.51 - ETA: 2s - loss: 0.6940 - accuracy: 0.51 - ETA: 2s - loss: 0.6940 - accuracy: 0.51 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.52 - ETA: 2s - loss: 0.6934 - accuracy: 0.52 - ETA: 2s - loss: 0.6933 - accuracy: 0.52 - ETA: 2s - loss: 0.6933 - accuracy: 0.52 - ETA: 2s - loss: 0.6933 - accuracy: 0.52 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6923 - accuracy: 0.51 - ETA: 1s - loss: 0.6922 - accuracy: 0.52 - ETA: 1s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6920 - accuracy: 0.51 - ETA: 1s - loss: 0.6919 - accuracy: 0.51 - ETA: 0s - loss: 0.6919 - accuracy: 0.51 - ETA: 0s - loss: 0.6918 - accuracy: 0.51 - ETA: 0s - loss: 0.6918 - accuracy: 0.51 - ETA: 0s - loss: 0.6916 - accuracy: 0.52 - ETA: 0s - loss: 0.6915 - accuracy: 0.52 - ETA: 0s - loss: 0.6913 - accuracy: 0.52 - ETA: 0s - loss: 0.6911 - accuracy: 0.52 - ETA: 0s - loss: 0.6909 - accuracy: 0.52 - ETA: 0s - loss: 0.6908 - accuracy: 0.52 - ETA: 0s - loss: 0.6905 - accuracy: 0.52 - ETA: 0s - loss: 0.6902 - accuracy: 0.53 - ETA: 0s - loss: 0.6899 - accuracy: 0.53 - ETA: 0s - loss: 0.6898 - accuracy: 0.53 - ETA: 0s - loss: 0.6895 - accuracy: 0.53 - ETA: 0s - loss: 0.6892 - accuracy: 0.53 - ETA: 0s - loss: 0.6888 - accuracy: 0.53 - ETA: 0s - loss: 0.6887 - accuracy: 0.53 - ETA: 0s - loss: 0.6883 - accuracy: 0.53 - ETA: 0s - loss: 0.6882 - accuracy: 0.53 - 3s 237us/step - loss: 0.6880 - accuracy: 0.5376 - val_loss: 0.6671 - val_accuracy: 0.6989\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 95us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 4:38 - loss: 0.6956 - accuracy: 0.25 - ETA: 7s - loss: 0.6942 - accuracy: 0.5085 - ETA: 4s - loss: 0.6927 - accuracy: 0.54 - ETA: 4s - loss: 0.6922 - accuracy: 0.54 - ETA: 3s - loss: 0.6921 - accuracy: 0.54 - ETA: 3s - loss: 0.6924 - accuracy: 0.53 - ETA: 3s - loss: 0.6917 - accuracy: 0.53 - ETA: 2s - loss: 0.6917 - accuracy: 0.53 - ETA: 2s - loss: 0.6916 - accuracy: 0.53 - ETA: 2s - loss: 0.6917 - accuracy: 0.53 - ETA: 2s - loss: 0.6915 - accuracy: 0.53 - ETA: 2s - loss: 0.6916 - accuracy: 0.53 - ETA: 2s - loss: 0.6913 - accuracy: 0.53 - ETA: 2s - loss: 0.6916 - accuracy: 0.53 - ETA: 2s - loss: 0.6916 - accuracy: 0.52 - ETA: 2s - loss: 0.6917 - accuracy: 0.52 - ETA: 2s - loss: 0.6916 - accuracy: 0.52 - ETA: 2s - loss: 0.6917 - accuracy: 0.52 - ETA: 2s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6919 - accuracy: 0.51 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6916 - accuracy: 0.53 - ETA: 1s - loss: 0.6916 - accuracy: 0.53 - ETA: 1s - loss: 0.6915 - accuracy: 0.53 - ETA: 1s - loss: 0.6915 - accuracy: 0.53 - ETA: 1s - loss: 0.6914 - accuracy: 0.53 - ETA: 1s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6911 - accuracy: 0.53 - ETA: 1s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6906 - accuracy: 0.53 - ETA: 1s - loss: 0.6903 - accuracy: 0.54 - ETA: 1s - loss: 0.6901 - accuracy: 0.53 - ETA: 1s - loss: 0.6899 - accuracy: 0.53 - ETA: 0s - loss: 0.6896 - accuracy: 0.53 - ETA: 0s - loss: 0.6893 - accuracy: 0.53 - ETA: 0s - loss: 0.6893 - accuracy: 0.53 - ETA: 0s - loss: 0.6890 - accuracy: 0.53 - ETA: 0s - loss: 0.6888 - accuracy: 0.53 - ETA: 0s - loss: 0.6883 - accuracy: 0.54 - ETA: 0s - loss: 0.6882 - accuracy: 0.54 - ETA: 0s - loss: 0.6881 - accuracy: 0.54 - ETA: 0s - loss: 0.6875 - accuracy: 0.54 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6869 - accuracy: 0.54 - ETA: 0s - loss: 0.6867 - accuracy: 0.54 - ETA: 0s - loss: 0.6863 - accuracy: 0.54 - ETA: 0s - loss: 0.6859 - accuracy: 0.55 - ETA: 0s - loss: 0.6854 - accuracy: 0.55 - ETA: 0s - loss: 0.6849 - accuracy: 0.55 - ETA: 0s - loss: 0.6844 - accuracy: 0.55 - ETA: 0s - loss: 0.6843 - accuracy: 0.55 - ETA: 0s - loss: 0.6841 - accuracy: 0.55 - 3s 240us/step - loss: 0.6840 - accuracy: 0.5554 - val_loss: 0.6607 - val_accuracy: 0.7244\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 96us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:35 - loss: 0.6935 - accuracy: 0.75 - ETA: 7s - loss: 0.6933 - accuracy: 0.5129 - ETA: 4s - loss: 0.6946 - accuracy: 0.48 - ETA: 4s - loss: 0.6942 - accuracy: 0.49 - ETA: 3s - loss: 0.6940 - accuracy: 0.49 - ETA: 3s - loss: 0.6940 - accuracy: 0.49 - ETA: 3s - loss: 0.6936 - accuracy: 0.50 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6931 - accuracy: 0.51 - ETA: 2s - loss: 0.6928 - accuracy: 0.51 - ETA: 2s - loss: 0.6925 - accuracy: 0.52 - ETA: 2s - loss: 0.6923 - accuracy: 0.52 - ETA: 2s - loss: 0.6919 - accuracy: 0.52 - ETA: 2s - loss: 0.6917 - accuracy: 0.52 - ETA: 2s - loss: 0.6919 - accuracy: 0.52 - ETA: 2s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6922 - accuracy: 0.51 - ETA: 1s - loss: 0.6922 - accuracy: 0.52 - ETA: 1s - loss: 0.6922 - accuracy: 0.52 - ETA: 1s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6920 - accuracy: 0.51 - ETA: 1s - loss: 0.6920 - accuracy: 0.51 - ETA: 1s - loss: 0.6919 - accuracy: 0.51 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6917 - accuracy: 0.52 - ETA: 1s - loss: 0.6917 - accuracy: 0.52 - ETA: 1s - loss: 0.6915 - accuracy: 0.52 - ETA: 1s - loss: 0.6913 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.52 - ETA: 1s - loss: 0.6909 - accuracy: 0.53 - ETA: 0s - loss: 0.6906 - accuracy: 0.53 - ETA: 0s - loss: 0.6903 - accuracy: 0.53 - ETA: 0s - loss: 0.6900 - accuracy: 0.53 - ETA: 0s - loss: 0.6899 - accuracy: 0.53 - ETA: 0s - loss: 0.6893 - accuracy: 0.54 - ETA: 0s - loss: 0.6889 - accuracy: 0.54 - ETA: 0s - loss: 0.6887 - accuracy: 0.54 - ETA: 0s - loss: 0.6885 - accuracy: 0.54 - ETA: 0s - loss: 0.6883 - accuracy: 0.54 - ETA: 0s - loss: 0.6880 - accuracy: 0.54 - ETA: 0s - loss: 0.6875 - accuracy: 0.55 - ETA: 0s - loss: 0.6873 - accuracy: 0.55 - ETA: 0s - loss: 0.6870 - accuracy: 0.55 - ETA: 0s - loss: 0.6864 - accuracy: 0.55 - ETA: 0s - loss: 0.6861 - accuracy: 0.55 - ETA: 0s - loss: 0.6856 - accuracy: 0.55 - ETA: 0s - loss: 0.6853 - accuracy: 0.55 - ETA: 0s - loss: 0.6851 - accuracy: 0.56 - ETA: 0s - loss: 0.6849 - accuracy: 0.56 - 3s 238us/step - loss: 0.6845 - accuracy: 0.5627 - val_loss: 0.6563 - val_accuracy: 0.7045\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 99us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:51 - loss: 0.7036 - accuracy: 0.0000e+ - ETA: 8s - loss: 0.6949 - accuracy: 0.4591     - ETA: 5s - loss: 0.6943 - accuracy: 0.48 - ETA: 4s - loss: 0.6942 - accuracy: 0.50 - ETA: 4s - loss: 0.6942 - accuracy: 0.50 - ETA: 3s - loss: 0.6942 - accuracy: 0.50 - ETA: 3s - loss: 0.6940 - accuracy: 0.50 - ETA: 3s - loss: 0.6940 - accuracy: 0.50 - ETA: 3s - loss: 0.6940 - accuracy: 0.50 - ETA: 3s - loss: 0.6940 - accuracy: 0.50 - ETA: 2s - loss: 0.6939 - accuracy: 0.50 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.52 - ETA: 2s - loss: 0.6934 - accuracy: 0.52 - ETA: 2s - loss: 0.6933 - accuracy: 0.52 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 2s - loss: 0.6930 - accuracy: 0.52 - ETA: 2s - loss: 0.6928 - accuracy: 0.52 - ETA: 2s - loss: 0.6925 - accuracy: 0.52 - ETA: 2s - loss: 0.6921 - accuracy: 0.52 - ETA: 1s - loss: 0.6922 - accuracy: 0.52 - ETA: 1s - loss: 0.6921 - accuracy: 0.52 - ETA: 1s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6917 - accuracy: 0.52 - ETA: 1s - loss: 0.6915 - accuracy: 0.53 - ETA: 1s - loss: 0.6913 - accuracy: 0.53 - ETA: 1s - loss: 0.6911 - accuracy: 0.53 - ETA: 1s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6911 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 1s - loss: 0.6907 - accuracy: 0.54 - ETA: 1s - loss: 0.6904 - accuracy: 0.54 - ETA: 1s - loss: 0.6903 - accuracy: 0.54 - ETA: 1s - loss: 0.6903 - accuracy: 0.54 - ETA: 1s - loss: 0.6900 - accuracy: 0.54 - ETA: 0s - loss: 0.6898 - accuracy: 0.54 - ETA: 0s - loss: 0.6895 - accuracy: 0.54 - ETA: 0s - loss: 0.6893 - accuracy: 0.54 - ETA: 0s - loss: 0.6893 - accuracy: 0.54 - ETA: 0s - loss: 0.6890 - accuracy: 0.54 - ETA: 0s - loss: 0.6889 - accuracy: 0.54 - ETA: 0s - loss: 0.6887 - accuracy: 0.54 - ETA: 0s - loss: 0.6884 - accuracy: 0.54 - ETA: 0s - loss: 0.6882 - accuracy: 0.55 - ETA: 0s - loss: 0.6878 - accuracy: 0.55 - ETA: 0s - loss: 0.6875 - accuracy: 0.55 - ETA: 0s - loss: 0.6871 - accuracy: 0.55 - ETA: 0s - loss: 0.6870 - accuracy: 0.55 - ETA: 0s - loss: 0.6870 - accuracy: 0.55 - ETA: 0s - loss: 0.6869 - accuracy: 0.55 - ETA: 0s - loss: 0.6864 - accuracy: 0.55 - ETA: 0s - loss: 0.6862 - accuracy: 0.55 - ETA: 0s - loss: 0.6856 - accuracy: 0.55 - ETA: 0s - loss: 0.6852 - accuracy: 0.55 - ETA: 0s - loss: 0.6850 - accuracy: 0.55 - 3s 254us/step - loss: 0.6850 - accuracy: 0.5562 - val_loss: 0.6573 - val_accuracy: 0.7166\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:00 - loss: 0.6914 - accuracy: 0.75 - ETA: 8s - loss: 0.6931 - accuracy: 0.5536 - ETA: 5s - loss: 0.6936 - accuracy: 0.53 - ETA: 4s - loss: 0.6934 - accuracy: 0.53 - ETA: 4s - loss: 0.6932 - accuracy: 0.53 - ETA: 3s - loss: 0.6925 - accuracy: 0.54 - ETA: 3s - loss: 0.6933 - accuracy: 0.52 - ETA: 3s - loss: 0.6933 - accuracy: 0.52 - ETA: 3s - loss: 0.6929 - accuracy: 0.53 - ETA: 2s - loss: 0.6930 - accuracy: 0.52 - ETA: 2s - loss: 0.6926 - accuracy: 0.53 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.52 - ETA: 2s - loss: 0.6934 - accuracy: 0.52 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6932 - accuracy: 0.52 - ETA: 2s - loss: 0.6933 - accuracy: 0.52 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 2s - loss: 0.6930 - accuracy: 0.52 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.52 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.52 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6929 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6927 - accuracy: 0.52 - ETA: 1s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.53 - ETA: 0s - loss: 0.6924 - accuracy: 0.53 - ETA: 0s - loss: 0.6923 - accuracy: 0.53 - ETA: 0s - loss: 0.6921 - accuracy: 0.53 - ETA: 0s - loss: 0.6920 - accuracy: 0.53 - ETA: 0s - loss: 0.6919 - accuracy: 0.53 - ETA: 0s - loss: 0.6918 - accuracy: 0.53 - ETA: 0s - loss: 0.6915 - accuracy: 0.54 - ETA: 0s - loss: 0.6913 - accuracy: 0.54 - ETA: 0s - loss: 0.6910 - accuracy: 0.54 - ETA: 0s - loss: 0.6907 - accuracy: 0.54 - ETA: 0s - loss: 0.6906 - accuracy: 0.54 - ETA: 0s - loss: 0.6902 - accuracy: 0.54 - ETA: 0s - loss: 0.6901 - accuracy: 0.54 - ETA: 0s - loss: 0.6900 - accuracy: 0.54 - ETA: 0s - loss: 0.6899 - accuracy: 0.54 - ETA: 0s - loss: 0.6894 - accuracy: 0.54 - ETA: 0s - loss: 0.6892 - accuracy: 0.54 - ETA: 0s - loss: 0.6891 - accuracy: 0.55 - 3s 256us/step - loss: 0.6891 - accuracy: 0.5498 - val_loss: 0.6740 - val_accuracy: 0.6314\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 4:51 - loss: 0.6926 - accuracy: 0.75 - ETA: 7s - loss: 0.6942 - accuracy: 0.5223 - ETA: 5s - loss: 0.6947 - accuracy: 0.49 - ETA: 4s - loss: 0.6937 - accuracy: 0.52 - ETA: 3s - loss: 0.6930 - accuracy: 0.53 - ETA: 3s - loss: 0.6934 - accuracy: 0.52 - ETA: 3s - loss: 0.6940 - accuracy: 0.51 - ETA: 3s - loss: 0.6942 - accuracy: 0.51 - ETA: 3s - loss: 0.6935 - accuracy: 0.52 - ETA: 2s - loss: 0.6935 - accuracy: 0.52 - ETA: 2s - loss: 0.6936 - accuracy: 0.52 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6940 - accuracy: 0.51 - ETA: 2s - loss: 0.6940 - accuracy: 0.51 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 2s - loss: 0.6937 - accuracy: 0.52 - ETA: 2s - loss: 0.6937 - accuracy: 0.52 - ETA: 2s - loss: 0.6938 - accuracy: 0.52 - ETA: 2s - loss: 0.6937 - accuracy: 0.52 - ETA: 2s - loss: 0.6938 - accuracy: 0.52 - ETA: 1s - loss: 0.6936 - accuracy: 0.52 - ETA: 1s - loss: 0.6935 - accuracy: 0.52 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6927 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6927 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6917 - accuracy: 0.53 - ETA: 1s - loss: 0.6916 - accuracy: 0.53 - ETA: 1s - loss: 0.6914 - accuracy: 0.53 - ETA: 1s - loss: 0.6914 - accuracy: 0.53 - ETA: 0s - loss: 0.6912 - accuracy: 0.53 - ETA: 0s - loss: 0.6908 - accuracy: 0.53 - ETA: 0s - loss: 0.6908 - accuracy: 0.53 - ETA: 0s - loss: 0.6905 - accuracy: 0.53 - ETA: 0s - loss: 0.6902 - accuracy: 0.53 - ETA: 0s - loss: 0.6899 - accuracy: 0.53 - ETA: 0s - loss: 0.6895 - accuracy: 0.54 - ETA: 0s - loss: 0.6891 - accuracy: 0.54 - ETA: 0s - loss: 0.6888 - accuracy: 0.54 - ETA: 0s - loss: 0.6884 - accuracy: 0.54 - ETA: 0s - loss: 0.6879 - accuracy: 0.54 - ETA: 0s - loss: 0.6874 - accuracy: 0.55 - ETA: 0s - loss: 0.6869 - accuracy: 0.55 - ETA: 0s - loss: 0.6866 - accuracy: 0.55 - ETA: 0s - loss: 0.6862 - accuracy: 0.55 - ETA: 0s - loss: 0.6856 - accuracy: 0.55 - ETA: 0s - loss: 0.6853 - accuracy: 0.55 - ETA: 0s - loss: 0.6851 - accuracy: 0.55 - ETA: 0s - loss: 0.6848 - accuracy: 0.56 - 3s 256us/step - loss: 0.6848 - accuracy: 0.5606 - val_loss: 0.6560 - val_accuracy: 0.7017\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 104us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 4:45 - loss: 0.6936 - accuracy: 0.75 - ETA: 7s - loss: 0.6952 - accuracy: 0.4909 - ETA: 5s - loss: 0.6950 - accuracy: 0.50 - ETA: 4s - loss: 0.6947 - accuracy: 0.51 - ETA: 3s - loss: 0.6949 - accuracy: 0.50 - ETA: 3s - loss: 0.6949 - accuracy: 0.50 - ETA: 3s - loss: 0.6950 - accuracy: 0.50 - ETA: 3s - loss: 0.6949 - accuracy: 0.50 - ETA: 3s - loss: 0.6948 - accuracy: 0.50 - ETA: 2s - loss: 0.6948 - accuracy: 0.49 - ETA: 2s - loss: 0.6948 - accuracy: 0.50 - ETA: 2s - loss: 0.6947 - accuracy: 0.50 - ETA: 2s - loss: 0.6946 - accuracy: 0.50 - ETA: 2s - loss: 0.6945 - accuracy: 0.50 - ETA: 2s - loss: 0.6945 - accuracy: 0.50 - ETA: 2s - loss: 0.6945 - accuracy: 0.50 - ETA: 2s - loss: 0.6945 - accuracy: 0.51 - ETA: 2s - loss: 0.6945 - accuracy: 0.50 - ETA: 2s - loss: 0.6945 - accuracy: 0.51 - ETA: 2s - loss: 0.6943 - accuracy: 0.51 - ETA: 2s - loss: 0.6941 - accuracy: 0.51 - ETA: 1s - loss: 0.6940 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.52 - ETA: 1s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.52 - ETA: 1s - loss: 0.6936 - accuracy: 0.52 - ETA: 1s - loss: 0.6932 - accuracy: 0.52 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6927 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6922 - accuracy: 0.52 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6914 - accuracy: 0.52 - ETA: 1s - loss: 0.6915 - accuracy: 0.52 - ETA: 1s - loss: 0.6914 - accuracy: 0.52 - ETA: 0s - loss: 0.6912 - accuracy: 0.52 - ETA: 0s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6910 - accuracy: 0.53 - ETA: 0s - loss: 0.6908 - accuracy: 0.53 - ETA: 0s - loss: 0.6906 - accuracy: 0.53 - ETA: 0s - loss: 0.6903 - accuracy: 0.53 - ETA: 0s - loss: 0.6903 - accuracy: 0.53 - ETA: 0s - loss: 0.6898 - accuracy: 0.53 - ETA: 0s - loss: 0.6894 - accuracy: 0.53 - ETA: 0s - loss: 0.6893 - accuracy: 0.53 - ETA: 0s - loss: 0.6892 - accuracy: 0.53 - ETA: 0s - loss: 0.6889 - accuracy: 0.54 - ETA: 0s - loss: 0.6884 - accuracy: 0.54 - ETA: 0s - loss: 0.6880 - accuracy: 0.54 - ETA: 0s - loss: 0.6878 - accuracy: 0.54 - ETA: 0s - loss: 0.6875 - accuracy: 0.54 - ETA: 0s - loss: 0.6869 - accuracy: 0.54 - ETA: 0s - loss: 0.6864 - accuracy: 0.54 - ETA: 0s - loss: 0.6862 - accuracy: 0.55 - 3s 254us/step - loss: 0.6860 - accuracy: 0.5508 - val_loss: 0.6562 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 113us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 4:57 - loss: 0.7062 - accuracy: 0.0000e+ - ETA: 8s - loss: 0.6934 - accuracy: 0.5602     - ETA: 5s - loss: 0.6946 - accuracy: 0.53 - ETA: 4s - loss: 0.6947 - accuracy: 0.52 - ETA: 4s - loss: 0.6953 - accuracy: 0.51 - ETA: 3s - loss: 0.6953 - accuracy: 0.50 - ETA: 3s - loss: 0.6954 - accuracy: 0.49 - ETA: 3s - loss: 0.6953 - accuracy: 0.48 - ETA: 3s - loss: 0.6952 - accuracy: 0.49 - ETA: 2s - loss: 0.6952 - accuracy: 0.49 - ETA: 2s - loss: 0.6952 - accuracy: 0.49 - ETA: 2s - loss: 0.6949 - accuracy: 0.50 - ETA: 2s - loss: 0.6950 - accuracy: 0.50 - ETA: 2s - loss: 0.6950 - accuracy: 0.50 - ETA: 2s - loss: 0.6951 - accuracy: 0.49 - ETA: 2s - loss: 0.6950 - accuracy: 0.50 - ETA: 2s - loss: 0.6950 - accuracy: 0.50 - ETA: 2s - loss: 0.6949 - accuracy: 0.50 - ETA: 2s - loss: 0.6949 - accuracy: 0.50 - ETA: 2s - loss: 0.6949 - accuracy: 0.50 - ETA: 2s - loss: 0.6949 - accuracy: 0.50 - ETA: 2s - loss: 0.6948 - accuracy: 0.50 - ETA: 2s - loss: 0.6948 - accuracy: 0.50 - ETA: 2s - loss: 0.6947 - accuracy: 0.50 - ETA: 1s - loss: 0.6946 - accuracy: 0.50 - ETA: 1s - loss: 0.6944 - accuracy: 0.51 - ETA: 1s - loss: 0.6944 - accuracy: 0.51 - ETA: 1s - loss: 0.6944 - accuracy: 0.51 - ETA: 1s - loss: 0.6944 - accuracy: 0.51 - ETA: 1s - loss: 0.6944 - accuracy: 0.51 - ETA: 1s - loss: 0.6943 - accuracy: 0.51 - ETA: 1s - loss: 0.6944 - accuracy: 0.51 - ETA: 1s - loss: 0.6943 - accuracy: 0.51 - ETA: 1s - loss: 0.6943 - accuracy: 0.51 - ETA: 1s - loss: 0.6942 - accuracy: 0.51 - ETA: 1s - loss: 0.6941 - accuracy: 0.51 - ETA: 1s - loss: 0.6941 - accuracy: 0.51 - ETA: 1s - loss: 0.6940 - accuracy: 0.51 - ETA: 1s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.53 - 3s 265us/step - loss: 0.6926 - accuracy: 0.5311 - val_loss: 0.6837 - val_accuracy: 0.6925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 100us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:38 - loss: 0.6927 - accuracy: 0.75 - ETA: 7s - loss: 0.6944 - accuracy: 0.5182 - ETA: 5s - loss: 0.6940 - accuracy: 0.54 - ETA: 4s - loss: 0.6936 - accuracy: 0.53 - ETA: 3s - loss: 0.6922 - accuracy: 0.55 - ETA: 3s - loss: 0.6925 - accuracy: 0.55 - ETA: 3s - loss: 0.6925 - accuracy: 0.54 - ETA: 3s - loss: 0.6933 - accuracy: 0.53 - ETA: 3s - loss: 0.6937 - accuracy: 0.52 - ETA: 2s - loss: 0.6936 - accuracy: 0.52 - ETA: 2s - loss: 0.6937 - accuracy: 0.52 - ETA: 2s - loss: 0.6937 - accuracy: 0.52 - ETA: 2s - loss: 0.6937 - accuracy: 0.52 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.52 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.52 - ETA: 2s - loss: 0.6934 - accuracy: 0.52 - ETA: 2s - loss: 0.6933 - accuracy: 0.52 - ETA: 2s - loss: 0.6932 - accuracy: 0.52 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 2s - loss: 0.6929 - accuracy: 0.52 - ETA: 1s - loss: 0.6929 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6917 - accuracy: 0.52 - ETA: 1s - loss: 0.6914 - accuracy: 0.53 - ETA: 1s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 1s - loss: 0.6905 - accuracy: 0.53 - ETA: 1s - loss: 0.6900 - accuracy: 0.53 - ETA: 1s - loss: 0.6899 - accuracy: 0.53 - ETA: 1s - loss: 0.6893 - accuracy: 0.53 - ETA: 1s - loss: 0.6892 - accuracy: 0.53 - ETA: 1s - loss: 0.6889 - accuracy: 0.53 - ETA: 1s - loss: 0.6888 - accuracy: 0.53 - ETA: 0s - loss: 0.6885 - accuracy: 0.54 - ETA: 0s - loss: 0.6880 - accuracy: 0.54 - ETA: 0s - loss: 0.6877 - accuracy: 0.54 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6871 - accuracy: 0.54 - ETA: 0s - loss: 0.6869 - accuracy: 0.54 - ETA: 0s - loss: 0.6867 - accuracy: 0.54 - ETA: 0s - loss: 0.6864 - accuracy: 0.55 - ETA: 0s - loss: 0.6858 - accuracy: 0.55 - ETA: 0s - loss: 0.6854 - accuracy: 0.55 - ETA: 0s - loss: 0.6847 - accuracy: 0.55 - ETA: 0s - loss: 0.6841 - accuracy: 0.56 - ETA: 0s - loss: 0.6838 - accuracy: 0.56 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6828 - accuracy: 0.56 - ETA: 0s - loss: 0.6823 - accuracy: 0.56 - ETA: 0s - loss: 0.6819 - accuracy: 0.57 - ETA: 0s - loss: 0.6811 - accuracy: 0.57 - ETA: 0s - loss: 0.6809 - accuracy: 0.57 - ETA: 0s - loss: 0.6804 - accuracy: 0.57 - 3s 261us/step - loss: 0.6800 - accuracy: 0.5758 - val_loss: 0.6385 - val_accuracy: 0.6818\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 110us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:45 - loss: 0.6873 - accuracy: 1.00 - ETA: 8s - loss: 0.6954 - accuracy: 0.4808 - ETA: 5s - loss: 0.6951 - accuracy: 0.49 - ETA: 4s - loss: 0.6947 - accuracy: 0.51 - ETA: 3s - loss: 0.6942 - accuracy: 0.51 - ETA: 3s - loss: 0.6935 - accuracy: 0.52 - ETA: 3s - loss: 0.6925 - accuracy: 0.53 - ETA: 3s - loss: 0.6926 - accuracy: 0.53 - ETA: 3s - loss: 0.6930 - accuracy: 0.52 - ETA: 2s - loss: 0.6934 - accuracy: 0.52 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.52 - ETA: 2s - loss: 0.6932 - accuracy: 0.52 - ETA: 2s - loss: 0.6929 - accuracy: 0.52 - ETA: 2s - loss: 0.6927 - accuracy: 0.52 - ETA: 2s - loss: 0.6927 - accuracy: 0.52 - ETA: 2s - loss: 0.6924 - accuracy: 0.52 - ETA: 2s - loss: 0.6921 - accuracy: 0.52 - ETA: 2s - loss: 0.6921 - accuracy: 0.52 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6917 - accuracy: 0.52 - ETA: 1s - loss: 0.6915 - accuracy: 0.52 - ETA: 1s - loss: 0.6913 - accuracy: 0.52 - ETA: 1s - loss: 0.6913 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.52 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 1s - loss: 0.6905 - accuracy: 0.53 - ETA: 1s - loss: 0.6903 - accuracy: 0.53 - ETA: 1s - loss: 0.6901 - accuracy: 0.53 - ETA: 1s - loss: 0.6898 - accuracy: 0.54 - ETA: 1s - loss: 0.6897 - accuracy: 0.54 - ETA: 1s - loss: 0.6890 - accuracy: 0.54 - ETA: 1s - loss: 0.6886 - accuracy: 0.54 - ETA: 1s - loss: 0.6884 - accuracy: 0.54 - ETA: 1s - loss: 0.6879 - accuracy: 0.54 - ETA: 1s - loss: 0.6877 - accuracy: 0.54 - ETA: 1s - loss: 0.6873 - accuracy: 0.54 - ETA: 1s - loss: 0.6872 - accuracy: 0.54 - ETA: 0s - loss: 0.6870 - accuracy: 0.55 - ETA: 0s - loss: 0.6868 - accuracy: 0.55 - ETA: 0s - loss: 0.6865 - accuracy: 0.55 - ETA: 0s - loss: 0.6866 - accuracy: 0.55 - ETA: 0s - loss: 0.6862 - accuracy: 0.55 - ETA: 0s - loss: 0.6859 - accuracy: 0.55 - ETA: 0s - loss: 0.6855 - accuracy: 0.55 - ETA: 0s - loss: 0.6850 - accuracy: 0.55 - ETA: 0s - loss: 0.6848 - accuracy: 0.56 - ETA: 0s - loss: 0.6843 - accuracy: 0.56 - ETA: 0s - loss: 0.6838 - accuracy: 0.56 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6831 - accuracy: 0.56 - ETA: 0s - loss: 0.6828 - accuracy: 0.56 - ETA: 0s - loss: 0.6822 - accuracy: 0.57 - ETA: 0s - loss: 0.6815 - accuracy: 0.57 - ETA: 0s - loss: 0.6811 - accuracy: 0.57 - ETA: 0s - loss: 0.6803 - accuracy: 0.57 - ETA: 0s - loss: 0.6796 - accuracy: 0.57 - 3s 265us/step - loss: 0.6795 - accuracy: 0.5758 - val_loss: 0.6412 - val_accuracy: 0.7216\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 96us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:41 - loss: 0.6936 - accuracy: 1.00 - ETA: 7s - loss: 0.6958 - accuracy: 0.3750 - ETA: 4s - loss: 0.6948 - accuracy: 0.46 - ETA: 4s - loss: 0.6944 - accuracy: 0.48 - ETA: 3s - loss: 0.6945 - accuracy: 0.49 - ETA: 3s - loss: 0.6944 - accuracy: 0.49 - ETA: 3s - loss: 0.6944 - accuracy: 0.49 - ETA: 3s - loss: 0.6944 - accuracy: 0.49 - ETA: 2s - loss: 0.6942 - accuracy: 0.50 - ETA: 2s - loss: 0.6942 - accuracy: 0.50 - ETA: 2s - loss: 0.6940 - accuracy: 0.51 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6929 - accuracy: 0.52 - ETA: 2s - loss: 0.6929 - accuracy: 0.52 - ETA: 2s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6921 - accuracy: 0.52 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6915 - accuracy: 0.53 - ETA: 1s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6907 - accuracy: 0.53 - ETA: 1s - loss: 0.6906 - accuracy: 0.53 - ETA: 1s - loss: 0.6901 - accuracy: 0.54 - ETA: 1s - loss: 0.6895 - accuracy: 0.54 - ETA: 1s - loss: 0.6898 - accuracy: 0.54 - ETA: 1s - loss: 0.6893 - accuracy: 0.54 - ETA: 1s - loss: 0.6891 - accuracy: 0.54 - ETA: 1s - loss: 0.6889 - accuracy: 0.54 - ETA: 1s - loss: 0.6886 - accuracy: 0.54 - ETA: 1s - loss: 0.6883 - accuracy: 0.55 - ETA: 1s - loss: 0.6881 - accuracy: 0.55 - ETA: 1s - loss: 0.6879 - accuracy: 0.55 - ETA: 0s - loss: 0.6876 - accuracy: 0.55 - ETA: 0s - loss: 0.6869 - accuracy: 0.55 - ETA: 0s - loss: 0.6863 - accuracy: 0.55 - ETA: 0s - loss: 0.6860 - accuracy: 0.56 - ETA: 0s - loss: 0.6857 - accuracy: 0.56 - ETA: 0s - loss: 0.6852 - accuracy: 0.56 - ETA: 0s - loss: 0.6846 - accuracy: 0.56 - ETA: 0s - loss: 0.6840 - accuracy: 0.56 - ETA: 0s - loss: 0.6833 - accuracy: 0.56 - ETA: 0s - loss: 0.6830 - accuracy: 0.56 - ETA: 0s - loss: 0.6824 - accuracy: 0.57 - ETA: 0s - loss: 0.6820 - accuracy: 0.57 - ETA: 0s - loss: 0.6815 - accuracy: 0.57 - ETA: 0s - loss: 0.6811 - accuracy: 0.57 - ETA: 0s - loss: 0.6803 - accuracy: 0.57 - ETA: 0s - loss: 0.6798 - accuracy: 0.57 - ETA: 0s - loss: 0.6794 - accuracy: 0.57 - ETA: 0s - loss: 0.6787 - accuracy: 0.58 - ETA: 0s - loss: 0.6783 - accuracy: 0.58 - ETA: 0s - loss: 0.6779 - accuracy: 0.58 - 3s 244us/step - loss: 0.6778 - accuracy: 0.5840 - val_loss: 0.6408 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 98us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:26 - loss: 0.7021 - accuracy: 0.25 - ETA: 9s - loss: 0.6937 - accuracy: 0.4591 - ETA: 6s - loss: 0.6934 - accuracy: 0.50 - ETA: 5s - loss: 0.6928 - accuracy: 0.50 - ETA: 4s - loss: 0.6929 - accuracy: 0.51 - ETA: 4s - loss: 0.6924 - accuracy: 0.52 - ETA: 3s - loss: 0.6928 - accuracy: 0.51 - ETA: 3s - loss: 0.6922 - accuracy: 0.52 - ETA: 3s - loss: 0.6905 - accuracy: 0.53 - ETA: 3s - loss: 0.6906 - accuracy: 0.53 - ETA: 3s - loss: 0.6907 - accuracy: 0.53 - ETA: 3s - loss: 0.6902 - accuracy: 0.53 - ETA: 2s - loss: 0.6891 - accuracy: 0.54 - ETA: 2s - loss: 0.6880 - accuracy: 0.54 - ETA: 2s - loss: 0.6880 - accuracy: 0.54 - ETA: 2s - loss: 0.6865 - accuracy: 0.54 - ETA: 2s - loss: 0.6851 - accuracy: 0.55 - ETA: 2s - loss: 0.6850 - accuracy: 0.55 - ETA: 2s - loss: 0.6849 - accuracy: 0.55 - ETA: 2s - loss: 0.6844 - accuracy: 0.55 - ETA: 2s - loss: 0.6838 - accuracy: 0.55 - ETA: 2s - loss: 0.6824 - accuracy: 0.55 - ETA: 2s - loss: 0.6816 - accuracy: 0.55 - ETA: 2s - loss: 0.6812 - accuracy: 0.55 - ETA: 2s - loss: 0.6798 - accuracy: 0.56 - ETA: 2s - loss: 0.6779 - accuracy: 0.56 - ETA: 1s - loss: 0.6770 - accuracy: 0.56 - ETA: 1s - loss: 0.6760 - accuracy: 0.56 - ETA: 1s - loss: 0.6753 - accuracy: 0.56 - ETA: 1s - loss: 0.6740 - accuracy: 0.57 - ETA: 1s - loss: 0.6734 - accuracy: 0.57 - ETA: 1s - loss: 0.6729 - accuracy: 0.57 - ETA: 1s - loss: 0.6715 - accuracy: 0.57 - ETA: 1s - loss: 0.6707 - accuracy: 0.57 - ETA: 1s - loss: 0.6702 - accuracy: 0.58 - ETA: 1s - loss: 0.6692 - accuracy: 0.58 - ETA: 1s - loss: 0.6686 - accuracy: 0.58 - ETA: 1s - loss: 0.6682 - accuracy: 0.58 - ETA: 1s - loss: 0.6676 - accuracy: 0.58 - ETA: 1s - loss: 0.6667 - accuracy: 0.58 - ETA: 1s - loss: 0.6665 - accuracy: 0.58 - ETA: 1s - loss: 0.6655 - accuracy: 0.58 - ETA: 1s - loss: 0.6652 - accuracy: 0.58 - ETA: 0s - loss: 0.6644 - accuracy: 0.58 - ETA: 0s - loss: 0.6635 - accuracy: 0.59 - ETA: 0s - loss: 0.6631 - accuracy: 0.59 - ETA: 0s - loss: 0.6631 - accuracy: 0.59 - ETA: 0s - loss: 0.6632 - accuracy: 0.59 - ETA: 0s - loss: 0.6630 - accuracy: 0.59 - ETA: 0s - loss: 0.6625 - accuracy: 0.59 - ETA: 0s - loss: 0.6617 - accuracy: 0.59 - ETA: 0s - loss: 0.6611 - accuracy: 0.59 - ETA: 0s - loss: 0.6607 - accuracy: 0.59 - ETA: 0s - loss: 0.6606 - accuracy: 0.59 - ETA: 0s - loss: 0.6601 - accuracy: 0.59 - ETA: 0s - loss: 0.6600 - accuracy: 0.59 - ETA: 0s - loss: 0.6594 - accuracy: 0.59 - ETA: 0s - loss: 0.6591 - accuracy: 0.59 - ETA: 0s - loss: 0.6592 - accuracy: 0.59 - ETA: 0s - loss: 0.6590 - accuracy: 0.59 - ETA: 0s - loss: 0.6592 - accuracy: 0.59 - ETA: 0s - loss: 0.6586 - accuracy: 0.59 - 3s 270us/step - loss: 0.6585 - accuracy: 0.5982 - val_loss: 0.6138 - val_accuracy: 0.7124\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 98us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:51 - loss: 0.6961 - accuracy: 0.50 - ETA: 9s - loss: 0.6936 - accuracy: 0.4491 - ETA: 6s - loss: 0.6937 - accuracy: 0.44 - ETA: 4s - loss: 0.6930 - accuracy: 0.49 - ETA: 4s - loss: 0.6923 - accuracy: 0.50 - ETA: 3s - loss: 0.6914 - accuracy: 0.51 - ETA: 3s - loss: 0.6914 - accuracy: 0.51 - ETA: 3s - loss: 0.6911 - accuracy: 0.51 - ETA: 3s - loss: 0.6910 - accuracy: 0.51 - ETA: 3s - loss: 0.6909 - accuracy: 0.51 - ETA: 3s - loss: 0.6901 - accuracy: 0.52 - ETA: 2s - loss: 0.6891 - accuracy: 0.53 - ETA: 2s - loss: 0.6883 - accuracy: 0.53 - ETA: 2s - loss: 0.6878 - accuracy: 0.53 - ETA: 2s - loss: 0.6867 - accuracy: 0.53 - ETA: 2s - loss: 0.6859 - accuracy: 0.54 - ETA: 2s - loss: 0.6848 - accuracy: 0.54 - ETA: 2s - loss: 0.6836 - accuracy: 0.55 - ETA: 2s - loss: 0.6831 - accuracy: 0.54 - ETA: 2s - loss: 0.6820 - accuracy: 0.55 - ETA: 2s - loss: 0.6809 - accuracy: 0.55 - ETA: 2s - loss: 0.6806 - accuracy: 0.55 - ETA: 2s - loss: 0.6793 - accuracy: 0.56 - ETA: 2s - loss: 0.6780 - accuracy: 0.56 - ETA: 1s - loss: 0.6777 - accuracy: 0.56 - ETA: 1s - loss: 0.6776 - accuracy: 0.56 - ETA: 1s - loss: 0.6776 - accuracy: 0.56 - ETA: 1s - loss: 0.6770 - accuracy: 0.57 - ETA: 1s - loss: 0.6764 - accuracy: 0.57 - ETA: 1s - loss: 0.6758 - accuracy: 0.57 - ETA: 1s - loss: 0.6757 - accuracy: 0.57 - ETA: 1s - loss: 0.6752 - accuracy: 0.57 - ETA: 1s - loss: 0.6748 - accuracy: 0.57 - ETA: 1s - loss: 0.6743 - accuracy: 0.57 - ETA: 1s - loss: 0.6740 - accuracy: 0.57 - ETA: 1s - loss: 0.6723 - accuracy: 0.58 - ETA: 1s - loss: 0.6721 - accuracy: 0.58 - ETA: 1s - loss: 0.6714 - accuracy: 0.58 - ETA: 1s - loss: 0.6702 - accuracy: 0.58 - ETA: 1s - loss: 0.6702 - accuracy: 0.58 - ETA: 1s - loss: 0.6691 - accuracy: 0.58 - ETA: 0s - loss: 0.6682 - accuracy: 0.58 - ETA: 0s - loss: 0.6678 - accuracy: 0.59 - ETA: 0s - loss: 0.6672 - accuracy: 0.59 - ETA: 0s - loss: 0.6664 - accuracy: 0.59 - ETA: 0s - loss: 0.6661 - accuracy: 0.59 - ETA: 0s - loss: 0.6655 - accuracy: 0.59 - ETA: 0s - loss: 0.6646 - accuracy: 0.59 - ETA: 0s - loss: 0.6639 - accuracy: 0.59 - ETA: 0s - loss: 0.6635 - accuracy: 0.59 - ETA: 0s - loss: 0.6626 - accuracy: 0.59 - ETA: 0s - loss: 0.6619 - accuracy: 0.59 - ETA: 0s - loss: 0.6623 - accuracy: 0.59 - ETA: 0s - loss: 0.6614 - accuracy: 0.60 - ETA: 0s - loss: 0.6610 - accuracy: 0.60 - ETA: 0s - loss: 0.6606 - accuracy: 0.60 - ETA: 0s - loss: 0.6597 - accuracy: 0.60 - ETA: 0s - loss: 0.6594 - accuracy: 0.60 - ETA: 0s - loss: 0.6589 - accuracy: 0.60 - ETA: 0s - loss: 0.6582 - accuracy: 0.60 - 3s 263us/step - loss: 0.6579 - accuracy: 0.6078 - val_loss: 0.6075 - val_accuracy: 0.7081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 97us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:54 - loss: 0.6903 - accuracy: 0.75 - ETA: 9s - loss: 0.6925 - accuracy: 0.4722 - ETA: 6s - loss: 0.6919 - accuracy: 0.50 - ETA: 4s - loss: 0.6915 - accuracy: 0.50 - ETA: 4s - loss: 0.6904 - accuracy: 0.50 - ETA: 3s - loss: 0.6892 - accuracy: 0.51 - ETA: 3s - loss: 0.6882 - accuracy: 0.51 - ETA: 3s - loss: 0.6884 - accuracy: 0.51 - ETA: 3s - loss: 0.6888 - accuracy: 0.51 - ETA: 3s - loss: 0.6879 - accuracy: 0.51 - ETA: 3s - loss: 0.6870 - accuracy: 0.51 - ETA: 2s - loss: 0.6872 - accuracy: 0.51 - ETA: 2s - loss: 0.6871 - accuracy: 0.51 - ETA: 2s - loss: 0.6867 - accuracy: 0.52 - ETA: 2s - loss: 0.6863 - accuracy: 0.52 - ETA: 2s - loss: 0.6858 - accuracy: 0.53 - ETA: 2s - loss: 0.6851 - accuracy: 0.53 - ETA: 2s - loss: 0.6845 - accuracy: 0.53 - ETA: 2s - loss: 0.6839 - accuracy: 0.53 - ETA: 2s - loss: 0.6824 - accuracy: 0.53 - ETA: 2s - loss: 0.6826 - accuracy: 0.53 - ETA: 2s - loss: 0.6823 - accuracy: 0.53 - ETA: 2s - loss: 0.6823 - accuracy: 0.53 - ETA: 2s - loss: 0.6817 - accuracy: 0.54 - ETA: 2s - loss: 0.6810 - accuracy: 0.54 - ETA: 1s - loss: 0.6805 - accuracy: 0.55 - ETA: 1s - loss: 0.6804 - accuracy: 0.55 - ETA: 1s - loss: 0.6796 - accuracy: 0.55 - ETA: 1s - loss: 0.6798 - accuracy: 0.55 - ETA: 1s - loss: 0.6798 - accuracy: 0.55 - ETA: 1s - loss: 0.6792 - accuracy: 0.56 - ETA: 1s - loss: 0.6787 - accuracy: 0.56 - ETA: 1s - loss: 0.6785 - accuracy: 0.56 - ETA: 1s - loss: 0.6784 - accuracy: 0.56 - ETA: 1s - loss: 0.6781 - accuracy: 0.56 - ETA: 1s - loss: 0.6774 - accuracy: 0.57 - ETA: 1s - loss: 0.6768 - accuracy: 0.57 - ETA: 1s - loss: 0.6766 - accuracy: 0.57 - ETA: 1s - loss: 0.6765 - accuracy: 0.57 - ETA: 1s - loss: 0.6759 - accuracy: 0.57 - ETA: 1s - loss: 0.6756 - accuracy: 0.57 - ETA: 1s - loss: 0.6757 - accuracy: 0.57 - ETA: 0s - loss: 0.6758 - accuracy: 0.57 - ETA: 0s - loss: 0.6747 - accuracy: 0.58 - ETA: 0s - loss: 0.6745 - accuracy: 0.58 - ETA: 0s - loss: 0.6742 - accuracy: 0.58 - ETA: 0s - loss: 0.6742 - accuracy: 0.58 - ETA: 0s - loss: 0.6739 - accuracy: 0.58 - ETA: 0s - loss: 0.6734 - accuracy: 0.58 - ETA: 0s - loss: 0.6733 - accuracy: 0.58 - ETA: 0s - loss: 0.6729 - accuracy: 0.58 - ETA: 0s - loss: 0.6726 - accuracy: 0.58 - ETA: 0s - loss: 0.6720 - accuracy: 0.58 - ETA: 0s - loss: 0.6717 - accuracy: 0.58 - ETA: 0s - loss: 0.6718 - accuracy: 0.58 - ETA: 0s - loss: 0.6713 - accuracy: 0.59 - ETA: 0s - loss: 0.6713 - accuracy: 0.59 - ETA: 0s - loss: 0.6710 - accuracy: 0.59 - ETA: 0s - loss: 0.6707 - accuracy: 0.59 - ETA: 0s - loss: 0.6705 - accuracy: 0.59 - ETA: 0s - loss: 0.6700 - accuracy: 0.59 - 3s 264us/step - loss: 0.6700 - accuracy: 0.5958 - val_loss: 0.6324 - val_accuracy: 0.7195\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 119us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:07 - loss: 0.6941 - accuracy: 0.75 - ETA: 9s - loss: 0.6940 - accuracy: 0.4856 - ETA: 6s - loss: 0.6937 - accuracy: 0.49 - ETA: 5s - loss: 0.6934 - accuracy: 0.50 - ETA: 4s - loss: 0.6929 - accuracy: 0.52 - ETA: 4s - loss: 0.6925 - accuracy: 0.53 - ETA: 4s - loss: 0.6927 - accuracy: 0.52 - ETA: 3s - loss: 0.6928 - accuracy: 0.52 - ETA: 3s - loss: 0.6928 - accuracy: 0.52 - ETA: 3s - loss: 0.6926 - accuracy: 0.52 - ETA: 3s - loss: 0.6925 - accuracy: 0.53 - ETA: 3s - loss: 0.6923 - accuracy: 0.53 - ETA: 3s - loss: 0.6920 - accuracy: 0.54 - ETA: 3s - loss: 0.6917 - accuracy: 0.53 - ETA: 2s - loss: 0.6913 - accuracy: 0.54 - ETA: 2s - loss: 0.6906 - accuracy: 0.54 - ETA: 2s - loss: 0.6903 - accuracy: 0.54 - ETA: 2s - loss: 0.6898 - accuracy: 0.54 - ETA: 2s - loss: 0.6893 - accuracy: 0.54 - ETA: 2s - loss: 0.6889 - accuracy: 0.55 - ETA: 2s - loss: 0.6882 - accuracy: 0.55 - ETA: 2s - loss: 0.6876 - accuracy: 0.55 - ETA: 2s - loss: 0.6868 - accuracy: 0.56 - ETA: 2s - loss: 0.6863 - accuracy: 0.56 - ETA: 2s - loss: 0.6857 - accuracy: 0.56 - ETA: 2s - loss: 0.6854 - accuracy: 0.56 - ETA: 1s - loss: 0.6847 - accuracy: 0.56 - ETA: 1s - loss: 0.6842 - accuracy: 0.56 - ETA: 1s - loss: 0.6837 - accuracy: 0.57 - ETA: 1s - loss: 0.6832 - accuracy: 0.57 - ETA: 1s - loss: 0.6822 - accuracy: 0.57 - ETA: 1s - loss: 0.6815 - accuracy: 0.57 - ETA: 1s - loss: 0.6807 - accuracy: 0.57 - ETA: 1s - loss: 0.6803 - accuracy: 0.57 - ETA: 1s - loss: 0.6798 - accuracy: 0.58 - ETA: 1s - loss: 0.6785 - accuracy: 0.58 - ETA: 1s - loss: 0.6783 - accuracy: 0.58 - ETA: 1s - loss: 0.6779 - accuracy: 0.58 - ETA: 1s - loss: 0.6771 - accuracy: 0.58 - ETA: 1s - loss: 0.6764 - accuracy: 0.58 - ETA: 1s - loss: 0.6760 - accuracy: 0.58 - ETA: 1s - loss: 0.6755 - accuracy: 0.59 - ETA: 1s - loss: 0.6745 - accuracy: 0.59 - ETA: 0s - loss: 0.6738 - accuracy: 0.59 - ETA: 0s - loss: 0.6736 - accuracy: 0.59 - ETA: 0s - loss: 0.6728 - accuracy: 0.59 - ETA: 0s - loss: 0.6721 - accuracy: 0.60 - ETA: 0s - loss: 0.6724 - accuracy: 0.59 - ETA: 0s - loss: 0.6716 - accuracy: 0.60 - ETA: 0s - loss: 0.6711 - accuracy: 0.60 - ETA: 0s - loss: 0.6705 - accuracy: 0.60 - ETA: 0s - loss: 0.6706 - accuracy: 0.60 - ETA: 0s - loss: 0.6702 - accuracy: 0.60 - ETA: 0s - loss: 0.6698 - accuracy: 0.60 - ETA: 0s - loss: 0.6690 - accuracy: 0.60 - ETA: 0s - loss: 0.6686 - accuracy: 0.60 - ETA: 0s - loss: 0.6681 - accuracy: 0.60 - ETA: 0s - loss: 0.6679 - accuracy: 0.60 - ETA: 0s - loss: 0.6676 - accuracy: 0.60 - ETA: 0s - loss: 0.6671 - accuracy: 0.61 - ETA: 0s - loss: 0.6669 - accuracy: 0.61 - ETA: 0s - loss: 0.6665 - accuracy: 0.61 - ETA: 0s - loss: 0.6660 - accuracy: 0.61 - 3s 276us/step - loss: 0.6658 - accuracy: 0.6125 - val_loss: 0.6214 - val_accuracy: 0.7195\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:04 - loss: 0.6922 - accuracy: 0.75 - ETA: 9s - loss: 0.6938 - accuracy: 0.4953 - ETA: 6s - loss: 0.6932 - accuracy: 0.51 - ETA: 5s - loss: 0.6920 - accuracy: 0.52 - ETA: 4s - loss: 0.6925 - accuracy: 0.52 - ETA: 4s - loss: 0.6919 - accuracy: 0.52 - ETA: 3s - loss: 0.6912 - accuracy: 0.52 - ETA: 3s - loss: 0.6910 - accuracy: 0.52 - ETA: 3s - loss: 0.6902 - accuracy: 0.53 - ETA: 3s - loss: 0.6898 - accuracy: 0.53 - ETA: 3s - loss: 0.6886 - accuracy: 0.53 - ETA: 3s - loss: 0.6875 - accuracy: 0.54 - ETA: 2s - loss: 0.6867 - accuracy: 0.54 - ETA: 2s - loss: 0.6850 - accuracy: 0.54 - ETA: 2s - loss: 0.6843 - accuracy: 0.54 - ETA: 2s - loss: 0.6830 - accuracy: 0.55 - ETA: 2s - loss: 0.6816 - accuracy: 0.55 - ETA: 2s - loss: 0.6814 - accuracy: 0.55 - ETA: 2s - loss: 0.6803 - accuracy: 0.55 - ETA: 2s - loss: 0.6794 - accuracy: 0.55 - ETA: 2s - loss: 0.6782 - accuracy: 0.56 - ETA: 2s - loss: 0.6774 - accuracy: 0.56 - ETA: 2s - loss: 0.6761 - accuracy: 0.56 - ETA: 2s - loss: 0.6745 - accuracy: 0.57 - ETA: 2s - loss: 0.6735 - accuracy: 0.57 - ETA: 1s - loss: 0.6730 - accuracy: 0.57 - ETA: 1s - loss: 0.6714 - accuracy: 0.57 - ETA: 1s - loss: 0.6710 - accuracy: 0.57 - ETA: 1s - loss: 0.6702 - accuracy: 0.58 - ETA: 1s - loss: 0.6687 - accuracy: 0.58 - ETA: 1s - loss: 0.6676 - accuracy: 0.58 - ETA: 1s - loss: 0.6667 - accuracy: 0.58 - ETA: 1s - loss: 0.6662 - accuracy: 0.59 - ETA: 1s - loss: 0.6657 - accuracy: 0.59 - ETA: 1s - loss: 0.6653 - accuracy: 0.59 - ETA: 1s - loss: 0.6645 - accuracy: 0.59 - ETA: 1s - loss: 0.6637 - accuracy: 0.59 - ETA: 1s - loss: 0.6629 - accuracy: 0.59 - ETA: 1s - loss: 0.6622 - accuracy: 0.60 - ETA: 1s - loss: 0.6617 - accuracy: 0.60 - ETA: 1s - loss: 0.6612 - accuracy: 0.60 - ETA: 1s - loss: 0.6605 - accuracy: 0.60 - ETA: 0s - loss: 0.6603 - accuracy: 0.60 - ETA: 0s - loss: 0.6595 - accuracy: 0.60 - ETA: 0s - loss: 0.6588 - accuracy: 0.60 - ETA: 0s - loss: 0.6579 - accuracy: 0.60 - ETA: 0s - loss: 0.6570 - accuracy: 0.61 - ETA: 0s - loss: 0.6561 - accuracy: 0.61 - ETA: 0s - loss: 0.6555 - accuracy: 0.61 - ETA: 0s - loss: 0.6557 - accuracy: 0.61 - ETA: 0s - loss: 0.6558 - accuracy: 0.61 - ETA: 0s - loss: 0.6552 - accuracy: 0.61 - ETA: 0s - loss: 0.6549 - accuracy: 0.61 - ETA: 0s - loss: 0.6542 - accuracy: 0.61 - ETA: 0s - loss: 0.6537 - accuracy: 0.61 - ETA: 0s - loss: 0.6528 - accuracy: 0.62 - ETA: 0s - loss: 0.6522 - accuracy: 0.62 - ETA: 0s - loss: 0.6512 - accuracy: 0.62 - ETA: 0s - loss: 0.6510 - accuracy: 0.62 - ETA: 0s - loss: 0.6511 - accuracy: 0.62 - ETA: 0s - loss: 0.6510 - accuracy: 0.62 - 3s 268us/step - loss: 0.6505 - accuracy: 0.6246 - val_loss: 0.6058 - val_accuracy: 0.7216\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 96us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:23 - loss: 0.6925 - accuracy: 1.00 - ETA: 10s - loss: 0.6938 - accuracy: 0.5519 - ETA: 6s - loss: 0.6936 - accuracy: 0.518 - ETA: 5s - loss: 0.6934 - accuracy: 0.50 - ETA: 4s - loss: 0.6933 - accuracy: 0.50 - ETA: 4s - loss: 0.6930 - accuracy: 0.51 - ETA: 4s - loss: 0.6931 - accuracy: 0.52 - ETA: 3s - loss: 0.6929 - accuracy: 0.52 - ETA: 3s - loss: 0.6927 - accuracy: 0.53 - ETA: 3s - loss: 0.6920 - accuracy: 0.54 - ETA: 3s - loss: 0.6914 - accuracy: 0.54 - ETA: 3s - loss: 0.6907 - accuracy: 0.54 - ETA: 3s - loss: 0.6908 - accuracy: 0.54 - ETA: 2s - loss: 0.6899 - accuracy: 0.55 - ETA: 2s - loss: 0.6890 - accuracy: 0.55 - ETA: 2s - loss: 0.6876 - accuracy: 0.55 - ETA: 2s - loss: 0.6862 - accuracy: 0.55 - ETA: 2s - loss: 0.6854 - accuracy: 0.56 - ETA: 2s - loss: 0.6844 - accuracy: 0.56 - ETA: 2s - loss: 0.6831 - accuracy: 0.56 - ETA: 2s - loss: 0.6819 - accuracy: 0.56 - ETA: 2s - loss: 0.6807 - accuracy: 0.57 - ETA: 2s - loss: 0.6793 - accuracy: 0.57 - ETA: 2s - loss: 0.6793 - accuracy: 0.57 - ETA: 2s - loss: 0.6787 - accuracy: 0.57 - ETA: 2s - loss: 0.6782 - accuracy: 0.57 - ETA: 1s - loss: 0.6779 - accuracy: 0.57 - ETA: 1s - loss: 0.6776 - accuracy: 0.57 - ETA: 1s - loss: 0.6767 - accuracy: 0.57 - ETA: 1s - loss: 0.6760 - accuracy: 0.58 - ETA: 1s - loss: 0.6742 - accuracy: 0.58 - ETA: 1s - loss: 0.6731 - accuracy: 0.58 - ETA: 1s - loss: 0.6718 - accuracy: 0.58 - ETA: 1s - loss: 0.6696 - accuracy: 0.59 - ETA: 1s - loss: 0.6686 - accuracy: 0.59 - ETA: 1s - loss: 0.6667 - accuracy: 0.59 - ETA: 1s - loss: 0.6653 - accuracy: 0.59 - ETA: 1s - loss: 0.6636 - accuracy: 0.59 - ETA: 1s - loss: 0.6620 - accuracy: 0.60 - ETA: 1s - loss: 0.6603 - accuracy: 0.60 - ETA: 1s - loss: 0.6595 - accuracy: 0.60 - ETA: 1s - loss: 0.6586 - accuracy: 0.60 - ETA: 1s - loss: 0.6573 - accuracy: 0.60 - ETA: 0s - loss: 0.6564 - accuracy: 0.60 - ETA: 0s - loss: 0.6560 - accuracy: 0.60 - ETA: 0s - loss: 0.6544 - accuracy: 0.60 - ETA: 0s - loss: 0.6546 - accuracy: 0.60 - ETA: 0s - loss: 0.6547 - accuracy: 0.60 - ETA: 0s - loss: 0.6541 - accuracy: 0.60 - ETA: 0s - loss: 0.6531 - accuracy: 0.61 - ETA: 0s - loss: 0.6526 - accuracy: 0.61 - ETA: 0s - loss: 0.6522 - accuracy: 0.61 - ETA: 0s - loss: 0.6517 - accuracy: 0.61 - ETA: 0s - loss: 0.6509 - accuracy: 0.61 - ETA: 0s - loss: 0.6508 - accuracy: 0.61 - ETA: 0s - loss: 0.6507 - accuracy: 0.61 - ETA: 0s - loss: 0.6510 - accuracy: 0.61 - ETA: 0s - loss: 0.6503 - accuracy: 0.61 - ETA: 0s - loss: 0.6501 - accuracy: 0.61 - ETA: 0s - loss: 0.6492 - accuracy: 0.61 - ETA: 0s - loss: 0.6487 - accuracy: 0.61 - ETA: 0s - loss: 0.6483 - accuracy: 0.61 - ETA: 0s - loss: 0.6477 - accuracy: 0.61 - 3s 273us/step - loss: 0.6477 - accuracy: 0.6180 - val_loss: 0.6026 - val_accuracy: 0.6882\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 6:13 - loss: 0.6902 - accuracy: 0.50 - ETA: 9s - loss: 0.6946 - accuracy: 0.4864 - ETA: 6s - loss: 0.6939 - accuracy: 0.50 - ETA: 5s - loss: 0.6926 - accuracy: 0.52 - ETA: 4s - loss: 0.6924 - accuracy: 0.53 - ETA: 4s - loss: 0.6924 - accuracy: 0.52 - ETA: 4s - loss: 0.6919 - accuracy: 0.52 - ETA: 4s - loss: 0.6910 - accuracy: 0.53 - ETA: 3s - loss: 0.6900 - accuracy: 0.54 - ETA: 3s - loss: 0.6901 - accuracy: 0.54 - ETA: 3s - loss: 0.6886 - accuracy: 0.55 - ETA: 3s - loss: 0.6870 - accuracy: 0.55 - ETA: 3s - loss: 0.6856 - accuracy: 0.56 - ETA: 3s - loss: 0.6850 - accuracy: 0.57 - ETA: 3s - loss: 0.6848 - accuracy: 0.57 - ETA: 2s - loss: 0.6833 - accuracy: 0.57 - ETA: 2s - loss: 0.6826 - accuracy: 0.57 - ETA: 2s - loss: 0.6809 - accuracy: 0.58 - ETA: 2s - loss: 0.6794 - accuracy: 0.58 - ETA: 2s - loss: 0.6787 - accuracy: 0.59 - ETA: 2s - loss: 0.6766 - accuracy: 0.59 - ETA: 2s - loss: 0.6749 - accuracy: 0.59 - ETA: 2s - loss: 0.6744 - accuracy: 0.59 - ETA: 2s - loss: 0.6733 - accuracy: 0.60 - ETA: 2s - loss: 0.6719 - accuracy: 0.60 - ETA: 2s - loss: 0.6702 - accuracy: 0.60 - ETA: 2s - loss: 0.6682 - accuracy: 0.60 - ETA: 2s - loss: 0.6669 - accuracy: 0.60 - ETA: 1s - loss: 0.6661 - accuracy: 0.61 - ETA: 1s - loss: 0.6661 - accuracy: 0.60 - ETA: 1s - loss: 0.6647 - accuracy: 0.61 - ETA: 1s - loss: 0.6639 - accuracy: 0.61 - ETA: 1s - loss: 0.6628 - accuracy: 0.61 - ETA: 1s - loss: 0.6630 - accuracy: 0.61 - ETA: 1s - loss: 0.6616 - accuracy: 0.61 - ETA: 1s - loss: 0.6603 - accuracy: 0.61 - ETA: 1s - loss: 0.6597 - accuracy: 0.62 - ETA: 1s - loss: 0.6594 - accuracy: 0.62 - ETA: 1s - loss: 0.6593 - accuracy: 0.62 - ETA: 1s - loss: 0.6586 - accuracy: 0.62 - ETA: 1s - loss: 0.6589 - accuracy: 0.62 - ETA: 1s - loss: 0.6580 - accuracy: 0.62 - ETA: 1s - loss: 0.6572 - accuracy: 0.62 - ETA: 1s - loss: 0.6558 - accuracy: 0.62 - ETA: 0s - loss: 0.6550 - accuracy: 0.62 - ETA: 0s - loss: 0.6538 - accuracy: 0.62 - ETA: 0s - loss: 0.6528 - accuracy: 0.63 - ETA: 0s - loss: 0.6522 - accuracy: 0.63 - ETA: 0s - loss: 0.6508 - accuracy: 0.63 - ETA: 0s - loss: 0.6503 - accuracy: 0.63 - ETA: 0s - loss: 0.6494 - accuracy: 0.63 - ETA: 0s - loss: 0.6491 - accuracy: 0.63 - ETA: 0s - loss: 0.6490 - accuracy: 0.63 - ETA: 0s - loss: 0.6484 - accuracy: 0.63 - ETA: 0s - loss: 0.6477 - accuracy: 0.63 - ETA: 0s - loss: 0.6470 - accuracy: 0.63 - ETA: 0s - loss: 0.6464 - accuracy: 0.63 - ETA: 0s - loss: 0.6453 - accuracy: 0.64 - ETA: 0s - loss: 0.6449 - accuracy: 0.64 - ETA: 0s - loss: 0.6440 - accuracy: 0.64 - ETA: 0s - loss: 0.6427 - accuracy: 0.64 - ETA: 0s - loss: 0.6423 - accuracy: 0.64 - ETA: 0s - loss: 0.6418 - accuracy: 0.64 - 3s 273us/step - loss: 0.6417 - accuracy: 0.6447 - val_loss: 0.5812 - val_accuracy: 0.7180\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 97us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:13 - loss: 0.6931 - accuracy: 0.75 - ETA: 9s - loss: 0.6936 - accuracy: 0.5093 - ETA: 6s - loss: 0.6924 - accuracy: 0.51 - ETA: 5s - loss: 0.6920 - accuracy: 0.51 - ETA: 4s - loss: 0.6914 - accuracy: 0.51 - ETA: 4s - loss: 0.6912 - accuracy: 0.52 - ETA: 3s - loss: 0.6911 - accuracy: 0.53 - ETA: 3s - loss: 0.6907 - accuracy: 0.54 - ETA: 3s - loss: 0.6897 - accuracy: 0.55 - ETA: 3s - loss: 0.6890 - accuracy: 0.55 - ETA: 3s - loss: 0.6876 - accuracy: 0.56 - ETA: 3s - loss: 0.6867 - accuracy: 0.56 - ETA: 3s - loss: 0.6859 - accuracy: 0.56 - ETA: 3s - loss: 0.6841 - accuracy: 0.56 - ETA: 2s - loss: 0.6831 - accuracy: 0.57 - ETA: 2s - loss: 0.6816 - accuracy: 0.57 - ETA: 2s - loss: 0.6796 - accuracy: 0.58 - ETA: 2s - loss: 0.6790 - accuracy: 0.58 - ETA: 2s - loss: 0.6782 - accuracy: 0.58 - ETA: 2s - loss: 0.6775 - accuracy: 0.58 - ETA: 2s - loss: 0.6759 - accuracy: 0.58 - ETA: 2s - loss: 0.6744 - accuracy: 0.58 - ETA: 2s - loss: 0.6739 - accuracy: 0.58 - ETA: 2s - loss: 0.6734 - accuracy: 0.58 - ETA: 2s - loss: 0.6722 - accuracy: 0.59 - ETA: 2s - loss: 0.6716 - accuracy: 0.59 - ETA: 1s - loss: 0.6692 - accuracy: 0.59 - ETA: 1s - loss: 0.6685 - accuracy: 0.59 - ETA: 1s - loss: 0.6675 - accuracy: 0.60 - ETA: 1s - loss: 0.6670 - accuracy: 0.60 - ETA: 1s - loss: 0.6660 - accuracy: 0.60 - ETA: 1s - loss: 0.6645 - accuracy: 0.60 - ETA: 1s - loss: 0.6641 - accuracy: 0.60 - ETA: 1s - loss: 0.6631 - accuracy: 0.60 - ETA: 1s - loss: 0.6630 - accuracy: 0.60 - ETA: 1s - loss: 0.6617 - accuracy: 0.61 - ETA: 1s - loss: 0.6606 - accuracy: 0.61 - ETA: 1s - loss: 0.6601 - accuracy: 0.61 - ETA: 1s - loss: 0.6593 - accuracy: 0.61 - ETA: 1s - loss: 0.6583 - accuracy: 0.61 - ETA: 1s - loss: 0.6575 - accuracy: 0.61 - ETA: 1s - loss: 0.6571 - accuracy: 0.61 - ETA: 1s - loss: 0.6563 - accuracy: 0.61 - ETA: 0s - loss: 0.6555 - accuracy: 0.62 - ETA: 0s - loss: 0.6543 - accuracy: 0.62 - ETA: 0s - loss: 0.6541 - accuracy: 0.62 - ETA: 0s - loss: 0.6535 - accuracy: 0.62 - ETA: 0s - loss: 0.6521 - accuracy: 0.62 - ETA: 0s - loss: 0.6519 - accuracy: 0.62 - ETA: 0s - loss: 0.6518 - accuracy: 0.62 - ETA: 0s - loss: 0.6511 - accuracy: 0.62 - ETA: 0s - loss: 0.6505 - accuracy: 0.62 - ETA: 0s - loss: 0.6498 - accuracy: 0.62 - ETA: 0s - loss: 0.6497 - accuracy: 0.63 - ETA: 0s - loss: 0.6488 - accuracy: 0.63 - ETA: 0s - loss: 0.6482 - accuracy: 0.63 - ETA: 0s - loss: 0.6473 - accuracy: 0.63 - ETA: 0s - loss: 0.6467 - accuracy: 0.63 - ETA: 0s - loss: 0.6463 - accuracy: 0.63 - ETA: 0s - loss: 0.6454 - accuracy: 0.63 - ETA: 0s - loss: 0.6442 - accuracy: 0.63 - ETA: 0s - loss: 0.6444 - accuracy: 0.63 - 3s 270us/step - loss: 0.6446 - accuracy: 0.6368 - val_loss: 0.5940 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 96us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:26 - loss: 0.6919 - accuracy: 0.75 - ETA: 9s - loss: 0.6930 - accuracy: 0.5602 - ETA: 6s - loss: 0.6923 - accuracy: 0.56 - ETA: 5s - loss: 0.6903 - accuracy: 0.56 - ETA: 4s - loss: 0.6901 - accuracy: 0.56 - ETA: 4s - loss: 0.6895 - accuracy: 0.56 - ETA: 3s - loss: 0.6881 - accuracy: 0.58 - ETA: 3s - loss: 0.6864 - accuracy: 0.58 - ETA: 3s - loss: 0.6854 - accuracy: 0.58 - ETA: 3s - loss: 0.6850 - accuracy: 0.58 - ETA: 3s - loss: 0.6840 - accuracy: 0.58 - ETA: 2s - loss: 0.6830 - accuracy: 0.58 - ETA: 2s - loss: 0.6811 - accuracy: 0.59 - ETA: 2s - loss: 0.6798 - accuracy: 0.59 - ETA: 2s - loss: 0.6785 - accuracy: 0.59 - ETA: 2s - loss: 0.6765 - accuracy: 0.60 - ETA: 2s - loss: 0.6753 - accuracy: 0.60 - ETA: 2s - loss: 0.6743 - accuracy: 0.60 - ETA: 2s - loss: 0.6733 - accuracy: 0.60 - ETA: 2s - loss: 0.6710 - accuracy: 0.61 - ETA: 2s - loss: 0.6692 - accuracy: 0.61 - ETA: 2s - loss: 0.6676 - accuracy: 0.61 - ETA: 2s - loss: 0.6664 - accuracy: 0.61 - ETA: 1s - loss: 0.6642 - accuracy: 0.62 - ETA: 1s - loss: 0.6639 - accuracy: 0.62 - ETA: 1s - loss: 0.6636 - accuracy: 0.62 - ETA: 1s - loss: 0.6622 - accuracy: 0.62 - ETA: 1s - loss: 0.6618 - accuracy: 0.62 - ETA: 1s - loss: 0.6602 - accuracy: 0.62 - ETA: 1s - loss: 0.6586 - accuracy: 0.62 - ETA: 1s - loss: 0.6575 - accuracy: 0.63 - ETA: 1s - loss: 0.6561 - accuracy: 0.63 - ETA: 1s - loss: 0.6558 - accuracy: 0.63 - ETA: 1s - loss: 0.6543 - accuracy: 0.63 - ETA: 1s - loss: 0.6537 - accuracy: 0.63 - ETA: 1s - loss: 0.6532 - accuracy: 0.63 - ETA: 1s - loss: 0.6518 - accuracy: 0.63 - ETA: 1s - loss: 0.6502 - accuracy: 0.64 - ETA: 1s - loss: 0.6496 - accuracy: 0.64 - ETA: 1s - loss: 0.6487 - accuracy: 0.64 - ETA: 0s - loss: 0.6479 - accuracy: 0.64 - ETA: 0s - loss: 0.6486 - accuracy: 0.64 - ETA: 0s - loss: 0.6475 - accuracy: 0.64 - ETA: 0s - loss: 0.6472 - accuracy: 0.64 - ETA: 0s - loss: 0.6468 - accuracy: 0.64 - ETA: 0s - loss: 0.6458 - accuracy: 0.64 - ETA: 0s - loss: 0.6462 - accuracy: 0.64 - ETA: 0s - loss: 0.6458 - accuracy: 0.64 - ETA: 0s - loss: 0.6455 - accuracy: 0.64 - ETA: 0s - loss: 0.6449 - accuracy: 0.64 - ETA: 0s - loss: 0.6442 - accuracy: 0.65 - ETA: 0s - loss: 0.6435 - accuracy: 0.65 - ETA: 0s - loss: 0.6432 - accuracy: 0.65 - ETA: 0s - loss: 0.6438 - accuracy: 0.65 - ETA: 0s - loss: 0.6436 - accuracy: 0.65 - ETA: 0s - loss: 0.6432 - accuracy: 0.65 - ETA: 0s - loss: 0.6429 - accuracy: 0.65 - ETA: 0s - loss: 0.6431 - accuracy: 0.65 - ETA: 0s - loss: 0.6427 - accuracy: 0.65 - 3s 260us/step - loss: 0.6423 - accuracy: 0.6544 - val_loss: 0.5911 - val_accuracy: 0.7230\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 100us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:10 - loss: 0.6902 - accuracy: 0.25 - ETA: 10s - loss: 0.6934 - accuracy: 0.5096 - ETA: 6s - loss: 0.6934 - accuracy: 0.509 - ETA: 5s - loss: 0.6936 - accuracy: 0.51 - ETA: 4s - loss: 0.6933 - accuracy: 0.52 - ETA: 4s - loss: 0.6926 - accuracy: 0.52 - ETA: 3s - loss: 0.6920 - accuracy: 0.53 - ETA: 3s - loss: 0.6905 - accuracy: 0.53 - ETA: 3s - loss: 0.6899 - accuracy: 0.53 - ETA: 3s - loss: 0.6887 - accuracy: 0.54 - ETA: 3s - loss: 0.6874 - accuracy: 0.54 - ETA: 3s - loss: 0.6867 - accuracy: 0.55 - ETA: 3s - loss: 0.6859 - accuracy: 0.55 - ETA: 2s - loss: 0.6842 - accuracy: 0.55 - ETA: 2s - loss: 0.6833 - accuracy: 0.56 - ETA: 2s - loss: 0.6824 - accuracy: 0.56 - ETA: 2s - loss: 0.6812 - accuracy: 0.56 - ETA: 2s - loss: 0.6787 - accuracy: 0.57 - ETA: 2s - loss: 0.6777 - accuracy: 0.57 - ETA: 2s - loss: 0.6768 - accuracy: 0.57 - ETA: 2s - loss: 0.6756 - accuracy: 0.57 - ETA: 2s - loss: 0.6746 - accuracy: 0.57 - ETA: 2s - loss: 0.6737 - accuracy: 0.57 - ETA: 2s - loss: 0.6726 - accuracy: 0.58 - ETA: 2s - loss: 0.6705 - accuracy: 0.58 - ETA: 2s - loss: 0.6683 - accuracy: 0.59 - ETA: 2s - loss: 0.6672 - accuracy: 0.59 - ETA: 1s - loss: 0.6664 - accuracy: 0.59 - ETA: 1s - loss: 0.6660 - accuracy: 0.59 - ETA: 1s - loss: 0.6641 - accuracy: 0.59 - ETA: 1s - loss: 0.6635 - accuracy: 0.60 - ETA: 1s - loss: 0.6623 - accuracy: 0.60 - ETA: 1s - loss: 0.6614 - accuracy: 0.60 - ETA: 1s - loss: 0.6603 - accuracy: 0.60 - ETA: 1s - loss: 0.6598 - accuracy: 0.60 - ETA: 1s - loss: 0.6588 - accuracy: 0.60 - ETA: 1s - loss: 0.6579 - accuracy: 0.61 - ETA: 1s - loss: 0.6569 - accuracy: 0.61 - ETA: 1s - loss: 0.6565 - accuracy: 0.61 - ETA: 1s - loss: 0.6561 - accuracy: 0.61 - ETA: 1s - loss: 0.6551 - accuracy: 0.61 - ETA: 1s - loss: 0.6546 - accuracy: 0.61 - ETA: 1s - loss: 0.6536 - accuracy: 0.61 - ETA: 0s - loss: 0.6529 - accuracy: 0.62 - ETA: 0s - loss: 0.6521 - accuracy: 0.62 - ETA: 0s - loss: 0.6515 - accuracy: 0.62 - ETA: 0s - loss: 0.6509 - accuracy: 0.62 - ETA: 0s - loss: 0.6503 - accuracy: 0.62 - ETA: 0s - loss: 0.6499 - accuracy: 0.62 - ETA: 0s - loss: 0.6495 - accuracy: 0.62 - ETA: 0s - loss: 0.6491 - accuracy: 0.62 - ETA: 0s - loss: 0.6484 - accuracy: 0.63 - ETA: 0s - loss: 0.6480 - accuracy: 0.63 - ETA: 0s - loss: 0.6467 - accuracy: 0.63 - ETA: 0s - loss: 0.6460 - accuracy: 0.63 - ETA: 0s - loss: 0.6449 - accuracy: 0.63 - ETA: 0s - loss: 0.6447 - accuracy: 0.63 - ETA: 0s - loss: 0.6445 - accuracy: 0.63 - ETA: 0s - loss: 0.6430 - accuracy: 0.63 - ETA: 0s - loss: 0.6428 - accuracy: 0.63 - ETA: 0s - loss: 0.6434 - accuracy: 0.63 - ETA: 0s - loss: 0.6424 - accuracy: 0.63 - 3s 269us/step - loss: 0.6425 - accuracy: 0.6391 - val_loss: 0.5910 - val_accuracy: 0.7173\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 99us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 5:57 - loss: 0.6937 - accuracy: 0.50 - ETA: 9s - loss: 0.6940 - accuracy: 0.4777 - ETA: 5s - loss: 0.6934 - accuracy: 0.49 - ETA: 4s - loss: 0.6933 - accuracy: 0.48 - ETA: 4s - loss: 0.6927 - accuracy: 0.50 - ETA: 3s - loss: 0.6922 - accuracy: 0.50 - ETA: 3s - loss: 0.6919 - accuracy: 0.51 - ETA: 3s - loss: 0.6910 - accuracy: 0.52 - ETA: 3s - loss: 0.6898 - accuracy: 0.52 - ETA: 3s - loss: 0.6884 - accuracy: 0.53 - ETA: 2s - loss: 0.6867 - accuracy: 0.53 - ETA: 2s - loss: 0.6866 - accuracy: 0.53 - ETA: 2s - loss: 0.6852 - accuracy: 0.54 - ETA: 2s - loss: 0.6840 - accuracy: 0.55 - ETA: 2s - loss: 0.6821 - accuracy: 0.56 - ETA: 2s - loss: 0.6799 - accuracy: 0.56 - ETA: 2s - loss: 0.6777 - accuracy: 0.57 - ETA: 2s - loss: 0.6764 - accuracy: 0.57 - ETA: 2s - loss: 0.6759 - accuracy: 0.57 - ETA: 2s - loss: 0.6745 - accuracy: 0.57 - ETA: 2s - loss: 0.6723 - accuracy: 0.58 - ETA: 2s - loss: 0.6716 - accuracy: 0.58 - ETA: 2s - loss: 0.6709 - accuracy: 0.58 - ETA: 1s - loss: 0.6702 - accuracy: 0.58 - ETA: 1s - loss: 0.6690 - accuracy: 0.58 - ETA: 1s - loss: 0.6685 - accuracy: 0.58 - ETA: 1s - loss: 0.6665 - accuracy: 0.59 - ETA: 1s - loss: 0.6656 - accuracy: 0.59 - ETA: 1s - loss: 0.6646 - accuracy: 0.59 - ETA: 1s - loss: 0.6631 - accuracy: 0.60 - ETA: 1s - loss: 0.6628 - accuracy: 0.60 - ETA: 1s - loss: 0.6611 - accuracy: 0.60 - ETA: 1s - loss: 0.6595 - accuracy: 0.60 - ETA: 1s - loss: 0.6584 - accuracy: 0.60 - ETA: 1s - loss: 0.6578 - accuracy: 0.60 - ETA: 1s - loss: 0.6572 - accuracy: 0.61 - ETA: 1s - loss: 0.6562 - accuracy: 0.61 - ETA: 1s - loss: 0.6553 - accuracy: 0.61 - ETA: 1s - loss: 0.6545 - accuracy: 0.61 - ETA: 1s - loss: 0.6539 - accuracy: 0.61 - ETA: 1s - loss: 0.6537 - accuracy: 0.61 - ETA: 0s - loss: 0.6525 - accuracy: 0.61 - ETA: 0s - loss: 0.6519 - accuracy: 0.61 - ETA: 0s - loss: 0.6511 - accuracy: 0.61 - ETA: 0s - loss: 0.6499 - accuracy: 0.61 - ETA: 0s - loss: 0.6490 - accuracy: 0.62 - ETA: 0s - loss: 0.6480 - accuracy: 0.62 - ETA: 0s - loss: 0.6475 - accuracy: 0.62 - ETA: 0s - loss: 0.6462 - accuracy: 0.62 - ETA: 0s - loss: 0.6456 - accuracy: 0.62 - ETA: 0s - loss: 0.6451 - accuracy: 0.62 - ETA: 0s - loss: 0.6444 - accuracy: 0.62 - ETA: 0s - loss: 0.6442 - accuracy: 0.62 - ETA: 0s - loss: 0.6438 - accuracy: 0.62 - ETA: 0s - loss: 0.6440 - accuracy: 0.62 - ETA: 0s - loss: 0.6431 - accuracy: 0.63 - ETA: 0s - loss: 0.6424 - accuracy: 0.63 - ETA: 0s - loss: 0.6411 - accuracy: 0.63 - ETA: 0s - loss: 0.6401 - accuracy: 0.63 - ETA: 0s - loss: 0.6401 - accuracy: 0.63 - 3s 261us/step - loss: 0.6400 - accuracy: 0.6356 - val_loss: 0.5910 - val_accuracy: 0.7124\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:16 - loss: 0.6941 - accuracy: 0.50 - ETA: 9s - loss: 0.6937 - accuracy: 0.5278 - ETA: 6s - loss: 0.6938 - accuracy: 0.48 - ETA: 5s - loss: 0.6936 - accuracy: 0.49 - ETA: 4s - loss: 0.6935 - accuracy: 0.50 - ETA: 4s - loss: 0.6935 - accuracy: 0.49 - ETA: 3s - loss: 0.6934 - accuracy: 0.50 - ETA: 3s - loss: 0.6934 - accuracy: 0.50 - ETA: 3s - loss: 0.6932 - accuracy: 0.50 - ETA: 3s - loss: 0.6929 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 3s - loss: 0.6921 - accuracy: 0.51 - ETA: 2s - loss: 0.6918 - accuracy: 0.51 - ETA: 2s - loss: 0.6916 - accuracy: 0.52 - ETA: 2s - loss: 0.6912 - accuracy: 0.52 - ETA: 2s - loss: 0.6905 - accuracy: 0.53 - ETA: 2s - loss: 0.6902 - accuracy: 0.53 - ETA: 2s - loss: 0.6897 - accuracy: 0.53 - ETA: 2s - loss: 0.6886 - accuracy: 0.53 - ETA: 2s - loss: 0.6883 - accuracy: 0.53 - ETA: 2s - loss: 0.6878 - accuracy: 0.53 - ETA: 2s - loss: 0.6872 - accuracy: 0.53 - ETA: 2s - loss: 0.6867 - accuracy: 0.53 - ETA: 2s - loss: 0.6862 - accuracy: 0.54 - ETA: 2s - loss: 0.6857 - accuracy: 0.54 - ETA: 2s - loss: 0.6841 - accuracy: 0.54 - ETA: 1s - loss: 0.6838 - accuracy: 0.54 - ETA: 1s - loss: 0.6829 - accuracy: 0.54 - ETA: 1s - loss: 0.6820 - accuracy: 0.55 - ETA: 1s - loss: 0.6815 - accuracy: 0.55 - ETA: 1s - loss: 0.6810 - accuracy: 0.55 - ETA: 1s - loss: 0.6802 - accuracy: 0.55 - ETA: 1s - loss: 0.6790 - accuracy: 0.55 - ETA: 1s - loss: 0.6783 - accuracy: 0.55 - ETA: 1s - loss: 0.6770 - accuracy: 0.55 - ETA: 1s - loss: 0.6769 - accuracy: 0.55 - ETA: 1s - loss: 0.6763 - accuracy: 0.55 - ETA: 1s - loss: 0.6756 - accuracy: 0.56 - ETA: 1s - loss: 0.6751 - accuracy: 0.56 - ETA: 1s - loss: 0.6745 - accuracy: 0.56 - ETA: 1s - loss: 0.6739 - accuracy: 0.56 - ETA: 1s - loss: 0.6738 - accuracy: 0.56 - ETA: 1s - loss: 0.6737 - accuracy: 0.56 - ETA: 1s - loss: 0.6734 - accuracy: 0.56 - ETA: 1s - loss: 0.6729 - accuracy: 0.56 - ETA: 0s - loss: 0.6722 - accuracy: 0.56 - ETA: 0s - loss: 0.6727 - accuracy: 0.56 - ETA: 0s - loss: 0.6719 - accuracy: 0.56 - ETA: 0s - loss: 0.6711 - accuracy: 0.56 - ETA: 0s - loss: 0.6706 - accuracy: 0.56 - ETA: 0s - loss: 0.6705 - accuracy: 0.56 - ETA: 0s - loss: 0.6698 - accuracy: 0.56 - ETA: 0s - loss: 0.6698 - accuracy: 0.56 - ETA: 0s - loss: 0.6689 - accuracy: 0.56 - ETA: 0s - loss: 0.6682 - accuracy: 0.56 - ETA: 0s - loss: 0.6677 - accuracy: 0.56 - ETA: 0s - loss: 0.6675 - accuracy: 0.56 - ETA: 0s - loss: 0.6672 - accuracy: 0.56 - ETA: 0s - loss: 0.6667 - accuracy: 0.56 - ETA: 0s - loss: 0.6665 - accuracy: 0.56 - ETA: 0s - loss: 0.6658 - accuracy: 0.56 - ETA: 0s - loss: 0.6661 - accuracy: 0.56 - 3s 271us/step - loss: 0.6657 - accuracy: 0.5692 - val_loss: 0.6284 - val_accuracy: 0.7188\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 99us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:13 - loss: 0.7073 - accuracy: 0.25 - ETA: 10s - loss: 0.6927 - accuracy: 0.5588 - ETA: 6s - loss: 0.6931 - accuracy: 0.531 - ETA: 5s - loss: 0.6901 - accuracy: 0.55 - ETA: 4s - loss: 0.6902 - accuracy: 0.53 - ETA: 4s - loss: 0.6910 - accuracy: 0.53 - ETA: 3s - loss: 0.6910 - accuracy: 0.53 - ETA: 3s - loss: 0.6903 - accuracy: 0.54 - ETA: 3s - loss: 0.6885 - accuracy: 0.54 - ETA: 3s - loss: 0.6861 - accuracy: 0.55 - ETA: 3s - loss: 0.6842 - accuracy: 0.55 - ETA: 3s - loss: 0.6818 - accuracy: 0.56 - ETA: 2s - loss: 0.6807 - accuracy: 0.57 - ETA: 2s - loss: 0.6801 - accuracy: 0.57 - ETA: 2s - loss: 0.6784 - accuracy: 0.57 - ETA: 2s - loss: 0.6776 - accuracy: 0.58 - ETA: 2s - loss: 0.6754 - accuracy: 0.58 - ETA: 2s - loss: 0.6741 - accuracy: 0.59 - ETA: 2s - loss: 0.6730 - accuracy: 0.59 - ETA: 2s - loss: 0.6716 - accuracy: 0.59 - ETA: 2s - loss: 0.6684 - accuracy: 0.60 - ETA: 2s - loss: 0.6673 - accuracy: 0.60 - ETA: 2s - loss: 0.6671 - accuracy: 0.60 - ETA: 2s - loss: 0.6660 - accuracy: 0.61 - ETA: 2s - loss: 0.6645 - accuracy: 0.61 - ETA: 1s - loss: 0.6630 - accuracy: 0.61 - ETA: 1s - loss: 0.6606 - accuracy: 0.62 - ETA: 1s - loss: 0.6588 - accuracy: 0.62 - ETA: 1s - loss: 0.6592 - accuracy: 0.62 - ETA: 1s - loss: 0.6587 - accuracy: 0.62 - ETA: 1s - loss: 0.6567 - accuracy: 0.62 - ETA: 1s - loss: 0.6559 - accuracy: 0.62 - ETA: 1s - loss: 0.6553 - accuracy: 0.63 - ETA: 1s - loss: 0.6550 - accuracy: 0.63 - ETA: 1s - loss: 0.6543 - accuracy: 0.63 - ETA: 1s - loss: 0.6531 - accuracy: 0.63 - ETA: 1s - loss: 0.6524 - accuracy: 0.63 - ETA: 1s - loss: 0.6523 - accuracy: 0.63 - ETA: 1s - loss: 0.6520 - accuracy: 0.63 - ETA: 1s - loss: 0.6512 - accuracy: 0.63 - ETA: 1s - loss: 0.6510 - accuracy: 0.63 - ETA: 1s - loss: 0.6501 - accuracy: 0.63 - ETA: 1s - loss: 0.6497 - accuracy: 0.63 - ETA: 0s - loss: 0.6493 - accuracy: 0.64 - ETA: 0s - loss: 0.6492 - accuracy: 0.64 - ETA: 0s - loss: 0.6477 - accuracy: 0.64 - ETA: 0s - loss: 0.6462 - accuracy: 0.64 - ETA: 0s - loss: 0.6459 - accuracy: 0.64 - ETA: 0s - loss: 0.6458 - accuracy: 0.64 - ETA: 0s - loss: 0.6447 - accuracy: 0.64 - ETA: 0s - loss: 0.6437 - accuracy: 0.64 - ETA: 0s - loss: 0.6435 - accuracy: 0.64 - ETA: 0s - loss: 0.6430 - accuracy: 0.64 - ETA: 0s - loss: 0.6419 - accuracy: 0.64 - ETA: 0s - loss: 0.6411 - accuracy: 0.65 - ETA: 0s - loss: 0.6403 - accuracy: 0.65 - ETA: 0s - loss: 0.6397 - accuracy: 0.65 - ETA: 0s - loss: 0.6399 - accuracy: 0.65 - ETA: 0s - loss: 0.6392 - accuracy: 0.65 - ETA: 0s - loss: 0.6388 - accuracy: 0.65 - ETA: 0s - loss: 0.6383 - accuracy: 0.65 - ETA: 0s - loss: 0.6381 - accuracy: 0.65 - 3s 272us/step - loss: 0.6375 - accuracy: 0.6564 - val_loss: 0.5815 - val_accuracy: 0.7330\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 96us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:07 - loss: 0.6971 - accuracy: 0.25 - ETA: 9s - loss: 0.6926 - accuracy: 0.5385 - ETA: 6s - loss: 0.6919 - accuracy: 0.53 - ETA: 5s - loss: 0.6903 - accuracy: 0.54 - ETA: 4s - loss: 0.6904 - accuracy: 0.53 - ETA: 4s - loss: 0.6889 - accuracy: 0.53 - ETA: 4s - loss: 0.6880 - accuracy: 0.54 - ETA: 3s - loss: 0.6869 - accuracy: 0.54 - ETA: 3s - loss: 0.6859 - accuracy: 0.54 - ETA: 3s - loss: 0.6840 - accuracy: 0.56 - ETA: 3s - loss: 0.6829 - accuracy: 0.56 - ETA: 3s - loss: 0.6813 - accuracy: 0.57 - ETA: 3s - loss: 0.6789 - accuracy: 0.58 - ETA: 3s - loss: 0.6765 - accuracy: 0.58 - ETA: 2s - loss: 0.6750 - accuracy: 0.59 - ETA: 2s - loss: 0.6728 - accuracy: 0.59 - ETA: 2s - loss: 0.6719 - accuracy: 0.59 - ETA: 2s - loss: 0.6711 - accuracy: 0.59 - ETA: 2s - loss: 0.6699 - accuracy: 0.59 - ETA: 2s - loss: 0.6677 - accuracy: 0.60 - ETA: 2s - loss: 0.6671 - accuracy: 0.60 - ETA: 2s - loss: 0.6645 - accuracy: 0.61 - ETA: 2s - loss: 0.6632 - accuracy: 0.61 - ETA: 2s - loss: 0.6618 - accuracy: 0.61 - ETA: 2s - loss: 0.6610 - accuracy: 0.61 - ETA: 2s - loss: 0.6602 - accuracy: 0.62 - ETA: 2s - loss: 0.6587 - accuracy: 0.62 - ETA: 1s - loss: 0.6584 - accuracy: 0.62 - ETA: 1s - loss: 0.6573 - accuracy: 0.62 - ETA: 1s - loss: 0.6558 - accuracy: 0.62 - ETA: 1s - loss: 0.6537 - accuracy: 0.63 - ETA: 1s - loss: 0.6527 - accuracy: 0.63 - ETA: 1s - loss: 0.6518 - accuracy: 0.63 - ETA: 1s - loss: 0.6498 - accuracy: 0.63 - ETA: 1s - loss: 0.6489 - accuracy: 0.63 - ETA: 1s - loss: 0.6483 - accuracy: 0.63 - ETA: 1s - loss: 0.6480 - accuracy: 0.63 - ETA: 1s - loss: 0.6472 - accuracy: 0.63 - ETA: 1s - loss: 0.6463 - accuracy: 0.64 - ETA: 1s - loss: 0.6459 - accuracy: 0.64 - ETA: 1s - loss: 0.6453 - accuracy: 0.64 - ETA: 1s - loss: 0.6445 - accuracy: 0.64 - ETA: 1s - loss: 0.6442 - accuracy: 0.64 - ETA: 1s - loss: 0.6436 - accuracy: 0.64 - ETA: 0s - loss: 0.6432 - accuracy: 0.64 - ETA: 0s - loss: 0.6431 - accuracy: 0.64 - ETA: 0s - loss: 0.6427 - accuracy: 0.64 - ETA: 0s - loss: 0.6421 - accuracy: 0.64 - ETA: 0s - loss: 0.6417 - accuracy: 0.65 - ETA: 0s - loss: 0.6411 - accuracy: 0.65 - ETA: 0s - loss: 0.6406 - accuracy: 0.65 - ETA: 0s - loss: 0.6394 - accuracy: 0.65 - ETA: 0s - loss: 0.6388 - accuracy: 0.65 - ETA: 0s - loss: 0.6382 - accuracy: 0.65 - ETA: 0s - loss: 0.6379 - accuracy: 0.65 - ETA: 0s - loss: 0.6380 - accuracy: 0.65 - ETA: 0s - loss: 0.6378 - accuracy: 0.65 - ETA: 0s - loss: 0.6374 - accuracy: 0.65 - ETA: 0s - loss: 0.6364 - accuracy: 0.65 - ETA: 0s - loss: 0.6362 - accuracy: 0.65 - ETA: 0s - loss: 0.6358 - accuracy: 0.65 - ETA: 0s - loss: 0.6356 - accuracy: 0.65 - ETA: 0s - loss: 0.6351 - accuracy: 0.65 - ETA: 0s - loss: 0.6352 - accuracy: 0.66 - 4s 280us/step - loss: 0.6346 - accuracy: 0.6608 - val_loss: 0.6020 - val_accuracy: 0.6982\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 104us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 6:45 - loss: 0.6985 - accuracy: 0.25 - ETA: 10s - loss: 0.6947 - accuracy: 0.5240 - ETA: 6s - loss: 0.6944 - accuracy: 0.507 - ETA: 5s - loss: 0.6939 - accuracy: 0.50 - ETA: 4s - loss: 0.6938 - accuracy: 0.50 - ETA: 4s - loss: 0.6931 - accuracy: 0.52 - ETA: 4s - loss: 0.6930 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.52 - ETA: 3s - loss: 0.6912 - accuracy: 0.52 - ETA: 3s - loss: 0.6906 - accuracy: 0.52 - ETA: 3s - loss: 0.6903 - accuracy: 0.52 - ETA: 3s - loss: 0.6894 - accuracy: 0.52 - ETA: 3s - loss: 0.6884 - accuracy: 0.53 - ETA: 2s - loss: 0.6868 - accuracy: 0.54 - ETA: 2s - loss: 0.6854 - accuracy: 0.55 - ETA: 2s - loss: 0.6834 - accuracy: 0.56 - ETA: 2s - loss: 0.6826 - accuracy: 0.56 - ETA: 2s - loss: 0.6829 - accuracy: 0.56 - ETA: 2s - loss: 0.6825 - accuracy: 0.56 - ETA: 2s - loss: 0.6811 - accuracy: 0.56 - ETA: 2s - loss: 0.6802 - accuracy: 0.56 - ETA: 2s - loss: 0.6792 - accuracy: 0.57 - ETA: 2s - loss: 0.6777 - accuracy: 0.57 - ETA: 2s - loss: 0.6770 - accuracy: 0.58 - ETA: 2s - loss: 0.6751 - accuracy: 0.58 - ETA: 2s - loss: 0.6740 - accuracy: 0.58 - ETA: 2s - loss: 0.6726 - accuracy: 0.59 - ETA: 1s - loss: 0.6718 - accuracy: 0.59 - ETA: 1s - loss: 0.6711 - accuracy: 0.59 - ETA: 1s - loss: 0.6685 - accuracy: 0.59 - ETA: 1s - loss: 0.6676 - accuracy: 0.60 - ETA: 1s - loss: 0.6657 - accuracy: 0.60 - ETA: 1s - loss: 0.6649 - accuracy: 0.60 - ETA: 1s - loss: 0.6634 - accuracy: 0.61 - ETA: 1s - loss: 0.6618 - accuracy: 0.61 - ETA: 1s - loss: 0.6608 - accuracy: 0.61 - ETA: 1s - loss: 0.6602 - accuracy: 0.61 - ETA: 1s - loss: 0.6579 - accuracy: 0.62 - ETA: 1s - loss: 0.6567 - accuracy: 0.62 - ETA: 1s - loss: 0.6563 - accuracy: 0.62 - ETA: 1s - loss: 0.6541 - accuracy: 0.62 - ETA: 1s - loss: 0.6534 - accuracy: 0.62 - ETA: 1s - loss: 0.6525 - accuracy: 0.62 - ETA: 1s - loss: 0.6514 - accuracy: 0.63 - ETA: 0s - loss: 0.6503 - accuracy: 0.63 - ETA: 0s - loss: 0.6491 - accuracy: 0.63 - ETA: 0s - loss: 0.6480 - accuracy: 0.63 - ETA: 0s - loss: 0.6480 - accuracy: 0.63 - ETA: 0s - loss: 0.6471 - accuracy: 0.63 - ETA: 0s - loss: 0.6460 - accuracy: 0.63 - ETA: 0s - loss: 0.6451 - accuracy: 0.64 - ETA: 0s - loss: 0.6442 - accuracy: 0.64 - ETA: 0s - loss: 0.6437 - accuracy: 0.64 - ETA: 0s - loss: 0.6432 - accuracy: 0.64 - ETA: 0s - loss: 0.6431 - accuracy: 0.64 - ETA: 0s - loss: 0.6427 - accuracy: 0.64 - ETA: 0s - loss: 0.6413 - accuracy: 0.64 - ETA: 0s - loss: 0.6406 - accuracy: 0.65 - ETA: 0s - loss: 0.6400 - accuracy: 0.65 - ETA: 0s - loss: 0.6390 - accuracy: 0.65 - ETA: 0s - loss: 0.6385 - accuracy: 0.65 - ETA: 0s - loss: 0.6382 - accuracy: 0.65 - 3s 270us/step - loss: 0.6382 - accuracy: 0.6546 - val_loss: 0.5858 - val_accuracy: 0.7230\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 107us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:26 - loss: 0.6983 - accuracy: 0.25 - ETA: 9s - loss: 0.6944 - accuracy: 0.5185 - ETA: 6s - loss: 0.6901 - accuracy: 0.54 - ETA: 5s - loss: 0.6859 - accuracy: 0.54 - ETA: 4s - loss: 0.6872 - accuracy: 0.54 - ETA: 4s - loss: 0.6865 - accuracy: 0.54 - ETA: 4s - loss: 0.6874 - accuracy: 0.53 - ETA: 3s - loss: 0.6864 - accuracy: 0.54 - ETA: 3s - loss: 0.6846 - accuracy: 0.54 - ETA: 3s - loss: 0.6834 - accuracy: 0.55 - ETA: 3s - loss: 0.6822 - accuracy: 0.55 - ETA: 3s - loss: 0.6818 - accuracy: 0.55 - ETA: 3s - loss: 0.6793 - accuracy: 0.56 - ETA: 3s - loss: 0.6781 - accuracy: 0.56 - ETA: 2s - loss: 0.6761 - accuracy: 0.57 - ETA: 2s - loss: 0.6744 - accuracy: 0.58 - ETA: 2s - loss: 0.6726 - accuracy: 0.58 - ETA: 2s - loss: 0.6711 - accuracy: 0.58 - ETA: 2s - loss: 0.6694 - accuracy: 0.59 - ETA: 2s - loss: 0.6687 - accuracy: 0.59 - ETA: 2s - loss: 0.6666 - accuracy: 0.59 - ETA: 2s - loss: 0.6662 - accuracy: 0.60 - ETA: 2s - loss: 0.6645 - accuracy: 0.60 - ETA: 2s - loss: 0.6644 - accuracy: 0.60 - ETA: 2s - loss: 0.6638 - accuracy: 0.60 - ETA: 2s - loss: 0.6629 - accuracy: 0.60 - ETA: 2s - loss: 0.6619 - accuracy: 0.60 - ETA: 1s - loss: 0.6608 - accuracy: 0.61 - ETA: 1s - loss: 0.6597 - accuracy: 0.61 - ETA: 1s - loss: 0.6586 - accuracy: 0.61 - ETA: 1s - loss: 0.6579 - accuracy: 0.61 - ETA: 1s - loss: 0.6553 - accuracy: 0.61 - ETA: 1s - loss: 0.6551 - accuracy: 0.62 - ETA: 1s - loss: 0.6533 - accuracy: 0.62 - ETA: 1s - loss: 0.6516 - accuracy: 0.62 - ETA: 1s - loss: 0.6507 - accuracy: 0.62 - ETA: 1s - loss: 0.6510 - accuracy: 0.62 - ETA: 1s - loss: 0.6507 - accuracy: 0.62 - ETA: 1s - loss: 0.6506 - accuracy: 0.62 - ETA: 1s - loss: 0.6500 - accuracy: 0.63 - ETA: 1s - loss: 0.6496 - accuracy: 0.63 - ETA: 1s - loss: 0.6488 - accuracy: 0.63 - ETA: 1s - loss: 0.6486 - accuracy: 0.63 - ETA: 1s - loss: 0.6479 - accuracy: 0.63 - ETA: 0s - loss: 0.6465 - accuracy: 0.63 - ETA: 0s - loss: 0.6452 - accuracy: 0.63 - ETA: 0s - loss: 0.6443 - accuracy: 0.63 - ETA: 0s - loss: 0.6442 - accuracy: 0.63 - ETA: 0s - loss: 0.6434 - accuracy: 0.63 - ETA: 0s - loss: 0.6421 - accuracy: 0.64 - ETA: 0s - loss: 0.6418 - accuracy: 0.64 - ETA: 0s - loss: 0.6407 - accuracy: 0.64 - ETA: 0s - loss: 0.6401 - accuracy: 0.64 - ETA: 0s - loss: 0.6392 - accuracy: 0.64 - ETA: 0s - loss: 0.6387 - accuracy: 0.64 - ETA: 0s - loss: 0.6380 - accuracy: 0.64 - ETA: 0s - loss: 0.6376 - accuracy: 0.64 - ETA: 0s - loss: 0.6368 - accuracy: 0.64 - ETA: 0s - loss: 0.6362 - accuracy: 0.65 - ETA: 0s - loss: 0.6359 - accuracy: 0.65 - ETA: 0s - loss: 0.6353 - accuracy: 0.65 - ETA: 0s - loss: 0.6354 - accuracy: 0.65 - ETA: 0s - loss: 0.6346 - accuracy: 0.65 - 3s 274us/step - loss: 0.6343 - accuracy: 0.6514 - val_loss: 0.5858 - val_accuracy: 0.7237\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 8:07 - loss: 0.6921 - accuracy: 0.75 - ETA: 12s - loss: 0.6943 - accuracy: 0.5385 - ETA: 7s - loss: 0.6931 - accuracy: 0.543 - ETA: 5s - loss: 0.6918 - accuracy: 0.55 - ETA: 5s - loss: 0.6919 - accuracy: 0.54 - ETA: 4s - loss: 0.6914 - accuracy: 0.54 - ETA: 4s - loss: 0.6898 - accuracy: 0.55 - ETA: 4s - loss: 0.6883 - accuracy: 0.54 - ETA: 3s - loss: 0.6870 - accuracy: 0.55 - ETA: 3s - loss: 0.6843 - accuracy: 0.56 - ETA: 3s - loss: 0.6834 - accuracy: 0.56 - ETA: 3s - loss: 0.6816 - accuracy: 0.57 - ETA: 3s - loss: 0.6799 - accuracy: 0.57 - ETA: 3s - loss: 0.6793 - accuracy: 0.57 - ETA: 2s - loss: 0.6783 - accuracy: 0.57 - ETA: 2s - loss: 0.6759 - accuracy: 0.58 - ETA: 2s - loss: 0.6744 - accuracy: 0.58 - ETA: 2s - loss: 0.6731 - accuracy: 0.59 - ETA: 2s - loss: 0.6714 - accuracy: 0.59 - ETA: 2s - loss: 0.6682 - accuracy: 0.60 - ETA: 2s - loss: 0.6673 - accuracy: 0.60 - ETA: 2s - loss: 0.6665 - accuracy: 0.60 - ETA: 2s - loss: 0.6639 - accuracy: 0.61 - ETA: 2s - loss: 0.6636 - accuracy: 0.60 - ETA: 2s - loss: 0.6613 - accuracy: 0.61 - ETA: 2s - loss: 0.6590 - accuracy: 0.61 - ETA: 1s - loss: 0.6571 - accuracy: 0.62 - ETA: 1s - loss: 0.6561 - accuracy: 0.62 - ETA: 1s - loss: 0.6550 - accuracy: 0.62 - ETA: 1s - loss: 0.6529 - accuracy: 0.62 - ETA: 1s - loss: 0.6516 - accuracy: 0.63 - ETA: 1s - loss: 0.6511 - accuracy: 0.63 - ETA: 1s - loss: 0.6488 - accuracy: 0.63 - ETA: 1s - loss: 0.6471 - accuracy: 0.63 - ETA: 1s - loss: 0.6473 - accuracy: 0.63 - ETA: 1s - loss: 0.6460 - accuracy: 0.63 - ETA: 1s - loss: 0.6439 - accuracy: 0.64 - ETA: 1s - loss: 0.6428 - accuracy: 0.64 - ETA: 1s - loss: 0.6430 - accuracy: 0.64 - ETA: 1s - loss: 0.6420 - accuracy: 0.64 - ETA: 1s - loss: 0.6410 - accuracy: 0.64 - ETA: 1s - loss: 0.6402 - accuracy: 0.64 - ETA: 1s - loss: 0.6392 - accuracy: 0.64 - ETA: 0s - loss: 0.6386 - accuracy: 0.64 - ETA: 0s - loss: 0.6377 - accuracy: 0.64 - ETA: 0s - loss: 0.6371 - accuracy: 0.64 - ETA: 0s - loss: 0.6355 - accuracy: 0.65 - ETA: 0s - loss: 0.6348 - accuracy: 0.65 - ETA: 0s - loss: 0.6342 - accuracy: 0.65 - ETA: 0s - loss: 0.6340 - accuracy: 0.65 - ETA: 0s - loss: 0.6336 - accuracy: 0.65 - ETA: 0s - loss: 0.6331 - accuracy: 0.65 - ETA: 0s - loss: 0.6323 - accuracy: 0.65 - ETA: 0s - loss: 0.6319 - accuracy: 0.65 - ETA: 0s - loss: 0.6321 - accuracy: 0.65 - ETA: 0s - loss: 0.6324 - accuracy: 0.65 - ETA: 0s - loss: 0.6325 - accuracy: 0.65 - ETA: 0s - loss: 0.6320 - accuracy: 0.65 - ETA: 0s - loss: 0.6318 - accuracy: 0.65 - ETA: 0s - loss: 0.6315 - accuracy: 0.65 - ETA: 0s - loss: 0.6312 - accuracy: 0.65 - 3s 270us/step - loss: 0.6312 - accuracy: 0.6584 - val_loss: 0.5819 - val_accuracy: 0.7280\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:20 - loss: 0.6946 - accuracy: 0.25 - ETA: 9s - loss: 0.6946 - accuracy: 0.5047 - ETA: 6s - loss: 0.6934 - accuracy: 0.54 - ETA: 5s - loss: 0.6932 - accuracy: 0.54 - ETA: 4s - loss: 0.6920 - accuracy: 0.55 - ETA: 4s - loss: 0.6903 - accuracy: 0.55 - ETA: 3s - loss: 0.6899 - accuracy: 0.55 - ETA: 3s - loss: 0.6896 - accuracy: 0.55 - ETA: 3s - loss: 0.6881 - accuracy: 0.55 - ETA: 3s - loss: 0.6863 - accuracy: 0.55 - ETA: 3s - loss: 0.6847 - accuracy: 0.56 - ETA: 3s - loss: 0.6832 - accuracy: 0.57 - ETA: 3s - loss: 0.6813 - accuracy: 0.57 - ETA: 2s - loss: 0.6801 - accuracy: 0.58 - ETA: 2s - loss: 0.6786 - accuracy: 0.58 - ETA: 2s - loss: 0.6784 - accuracy: 0.58 - ETA: 2s - loss: 0.6766 - accuracy: 0.59 - ETA: 2s - loss: 0.6740 - accuracy: 0.59 - ETA: 2s - loss: 0.6713 - accuracy: 0.60 - ETA: 2s - loss: 0.6679 - accuracy: 0.61 - ETA: 2s - loss: 0.6677 - accuracy: 0.61 - ETA: 2s - loss: 0.6666 - accuracy: 0.61 - ETA: 2s - loss: 0.6665 - accuracy: 0.61 - ETA: 2s - loss: 0.6647 - accuracy: 0.61 - ETA: 2s - loss: 0.6628 - accuracy: 0.61 - ETA: 2s - loss: 0.6614 - accuracy: 0.61 - ETA: 1s - loss: 0.6613 - accuracy: 0.62 - ETA: 1s - loss: 0.6595 - accuracy: 0.62 - ETA: 1s - loss: 0.6575 - accuracy: 0.62 - ETA: 1s - loss: 0.6567 - accuracy: 0.62 - ETA: 1s - loss: 0.6559 - accuracy: 0.62 - ETA: 1s - loss: 0.6540 - accuracy: 0.63 - ETA: 1s - loss: 0.6522 - accuracy: 0.63 - ETA: 1s - loss: 0.6517 - accuracy: 0.63 - ETA: 1s - loss: 0.6511 - accuracy: 0.63 - ETA: 1s - loss: 0.6503 - accuracy: 0.63 - ETA: 1s - loss: 0.6497 - accuracy: 0.63 - ETA: 1s - loss: 0.6496 - accuracy: 0.64 - ETA: 1s - loss: 0.6484 - accuracy: 0.64 - ETA: 1s - loss: 0.6465 - accuracy: 0.64 - ETA: 1s - loss: 0.6454 - accuracy: 0.64 - ETA: 1s - loss: 0.6445 - accuracy: 0.64 - ETA: 1s - loss: 0.6435 - accuracy: 0.64 - ETA: 1s - loss: 0.6423 - accuracy: 0.64 - ETA: 0s - loss: 0.6416 - accuracy: 0.64 - ETA: 0s - loss: 0.6411 - accuracy: 0.64 - ETA: 0s - loss: 0.6403 - accuracy: 0.65 - ETA: 0s - loss: 0.6401 - accuracy: 0.65 - ETA: 0s - loss: 0.6390 - accuracy: 0.65 - ETA: 0s - loss: 0.6394 - accuracy: 0.65 - ETA: 0s - loss: 0.6386 - accuracy: 0.65 - ETA: 0s - loss: 0.6376 - accuracy: 0.65 - ETA: 0s - loss: 0.6365 - accuracy: 0.65 - ETA: 0s - loss: 0.6359 - accuracy: 0.65 - ETA: 0s - loss: 0.6352 - accuracy: 0.65 - ETA: 0s - loss: 0.6347 - accuracy: 0.65 - ETA: 0s - loss: 0.6334 - accuracy: 0.65 - ETA: 0s - loss: 0.6322 - accuracy: 0.65 - ETA: 0s - loss: 0.6311 - accuracy: 0.65 - ETA: 0s - loss: 0.6307 - accuracy: 0.66 - ETA: 0s - loss: 0.6306 - accuracy: 0.66 - ETA: 0s - loss: 0.6306 - accuracy: 0.66 - ETA: 0s - loss: 0.6304 - accuracy: 0.66 - 3s 273us/step - loss: 0.6302 - accuracy: 0.6617 - val_loss: 0.5887 - val_accuracy: 0.7223\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 108us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 5:16 - loss: 0.6927 - accuracy: 0.75 - ETA: 8s - loss: 0.6939 - accuracy: 0.4676 - ETA: 5s - loss: 0.6935 - accuracy: 0.47 - ETA: 4s - loss: 0.6923 - accuracy: 0.49 - ETA: 4s - loss: 0.6924 - accuracy: 0.49 - ETA: 3s - loss: 0.6922 - accuracy: 0.49 - ETA: 3s - loss: 0.6916 - accuracy: 0.50 - ETA: 3s - loss: 0.6911 - accuracy: 0.52 - ETA: 3s - loss: 0.6912 - accuracy: 0.52 - ETA: 3s - loss: 0.6907 - accuracy: 0.52 - ETA: 3s - loss: 0.6899 - accuracy: 0.53 - ETA: 2s - loss: 0.6901 - accuracy: 0.52 - ETA: 2s - loss: 0.6903 - accuracy: 0.52 - ETA: 2s - loss: 0.6902 - accuracy: 0.52 - ETA: 2s - loss: 0.6900 - accuracy: 0.52 - ETA: 2s - loss: 0.6899 - accuracy: 0.52 - ETA: 2s - loss: 0.6898 - accuracy: 0.52 - ETA: 2s - loss: 0.6893 - accuracy: 0.52 - ETA: 2s - loss: 0.6891 - accuracy: 0.53 - ETA: 2s - loss: 0.6887 - accuracy: 0.53 - ETA: 2s - loss: 0.6890 - accuracy: 0.53 - ETA: 2s - loss: 0.6890 - accuracy: 0.53 - ETA: 2s - loss: 0.6888 - accuracy: 0.53 - ETA: 2s - loss: 0.6887 - accuracy: 0.53 - ETA: 1s - loss: 0.6883 - accuracy: 0.53 - ETA: 1s - loss: 0.6881 - accuracy: 0.53 - ETA: 1s - loss: 0.6879 - accuracy: 0.53 - ETA: 1s - loss: 0.6877 - accuracy: 0.53 - ETA: 1s - loss: 0.6874 - accuracy: 0.54 - ETA: 1s - loss: 0.6871 - accuracy: 0.54 - ETA: 1s - loss: 0.6866 - accuracy: 0.54 - ETA: 1s - loss: 0.6861 - accuracy: 0.54 - ETA: 1s - loss: 0.6859 - accuracy: 0.54 - ETA: 1s - loss: 0.6854 - accuracy: 0.55 - ETA: 1s - loss: 0.6851 - accuracy: 0.55 - ETA: 1s - loss: 0.6848 - accuracy: 0.55 - ETA: 1s - loss: 0.6847 - accuracy: 0.55 - ETA: 1s - loss: 0.6846 - accuracy: 0.55 - ETA: 1s - loss: 0.6842 - accuracy: 0.55 - ETA: 1s - loss: 0.6842 - accuracy: 0.55 - ETA: 1s - loss: 0.6838 - accuracy: 0.55 - ETA: 0s - loss: 0.6839 - accuracy: 0.55 - ETA: 0s - loss: 0.6836 - accuracy: 0.55 - ETA: 0s - loss: 0.6834 - accuracy: 0.55 - ETA: 0s - loss: 0.6830 - accuracy: 0.55 - ETA: 0s - loss: 0.6826 - accuracy: 0.55 - ETA: 0s - loss: 0.6822 - accuracy: 0.55 - ETA: 0s - loss: 0.6821 - accuracy: 0.55 - ETA: 0s - loss: 0.6819 - accuracy: 0.55 - ETA: 0s - loss: 0.6817 - accuracy: 0.55 - ETA: 0s - loss: 0.6813 - accuracy: 0.55 - ETA: 0s - loss: 0.6811 - accuracy: 0.55 - ETA: 0s - loss: 0.6810 - accuracy: 0.56 - ETA: 0s - loss: 0.6806 - accuracy: 0.56 - ETA: 0s - loss: 0.6806 - accuracy: 0.56 - ETA: 0s - loss: 0.6803 - accuracy: 0.56 - ETA: 0s - loss: 0.6800 - accuracy: 0.56 - ETA: 0s - loss: 0.6798 - accuracy: 0.56 - ETA: 0s - loss: 0.6797 - accuracy: 0.56 - ETA: 0s - loss: 0.6795 - accuracy: 0.56 - ETA: 0s - loss: 0.6795 - accuracy: 0.56 - 3s 265us/step - loss: 0.6794 - accuracy: 0.5633 - val_loss: 0.6600 - val_accuracy: 0.7081\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 110us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:54 - loss: 0.6940 - accuracy: 0.50 - ETA: 9s - loss: 0.6940 - accuracy: 0.4815 - ETA: 6s - loss: 0.6937 - accuracy: 0.49 - ETA: 4s - loss: 0.6935 - accuracy: 0.49 - ETA: 4s - loss: 0.6932 - accuracy: 0.51 - ETA: 3s - loss: 0.6927 - accuracy: 0.52 - ETA: 3s - loss: 0.6927 - accuracy: 0.52 - ETA: 3s - loss: 0.6925 - accuracy: 0.52 - ETA: 3s - loss: 0.6919 - accuracy: 0.53 - ETA: 3s - loss: 0.6916 - accuracy: 0.53 - ETA: 3s - loss: 0.6916 - accuracy: 0.53 - ETA: 2s - loss: 0.6915 - accuracy: 0.53 - ETA: 2s - loss: 0.6912 - accuracy: 0.53 - ETA: 2s - loss: 0.6910 - accuracy: 0.53 - ETA: 2s - loss: 0.6907 - accuracy: 0.54 - ETA: 2s - loss: 0.6905 - accuracy: 0.54 - ETA: 2s - loss: 0.6900 - accuracy: 0.54 - ETA: 2s - loss: 0.6899 - accuracy: 0.54 - ETA: 2s - loss: 0.6899 - accuracy: 0.54 - ETA: 2s - loss: 0.6897 - accuracy: 0.54 - ETA: 2s - loss: 0.6895 - accuracy: 0.54 - ETA: 2s - loss: 0.6894 - accuracy: 0.54 - ETA: 2s - loss: 0.6894 - accuracy: 0.54 - ETA: 1s - loss: 0.6894 - accuracy: 0.54 - ETA: 1s - loss: 0.6893 - accuracy: 0.54 - ETA: 1s - loss: 0.6892 - accuracy: 0.54 - ETA: 1s - loss: 0.6891 - accuracy: 0.54 - ETA: 1s - loss: 0.6889 - accuracy: 0.54 - ETA: 1s - loss: 0.6886 - accuracy: 0.54 - ETA: 1s - loss: 0.6884 - accuracy: 0.55 - ETA: 1s - loss: 0.6881 - accuracy: 0.55 - ETA: 1s - loss: 0.6880 - accuracy: 0.55 - ETA: 1s - loss: 0.6876 - accuracy: 0.55 - ETA: 1s - loss: 0.6874 - accuracy: 0.55 - ETA: 1s - loss: 0.6872 - accuracy: 0.55 - ETA: 1s - loss: 0.6867 - accuracy: 0.56 - ETA: 1s - loss: 0.6863 - accuracy: 0.56 - ETA: 1s - loss: 0.6857 - accuracy: 0.56 - ETA: 1s - loss: 0.6855 - accuracy: 0.56 - ETA: 1s - loss: 0.6854 - accuracy: 0.56 - ETA: 1s - loss: 0.6853 - accuracy: 0.56 - ETA: 0s - loss: 0.6852 - accuracy: 0.56 - ETA: 0s - loss: 0.6849 - accuracy: 0.56 - ETA: 0s - loss: 0.6848 - accuracy: 0.56 - ETA: 0s - loss: 0.6847 - accuracy: 0.56 - ETA: 0s - loss: 0.6845 - accuracy: 0.56 - ETA: 0s - loss: 0.6845 - accuracy: 0.56 - ETA: 0s - loss: 0.6843 - accuracy: 0.56 - ETA: 0s - loss: 0.6840 - accuracy: 0.56 - ETA: 0s - loss: 0.6838 - accuracy: 0.56 - ETA: 0s - loss: 0.6836 - accuracy: 0.56 - ETA: 0s - loss: 0.6835 - accuracy: 0.56 - ETA: 0s - loss: 0.6832 - accuracy: 0.56 - ETA: 0s - loss: 0.6830 - accuracy: 0.56 - ETA: 0s - loss: 0.6828 - accuracy: 0.56 - ETA: 0s - loss: 0.6827 - accuracy: 0.56 - ETA: 0s - loss: 0.6826 - accuracy: 0.56 - ETA: 0s - loss: 0.6823 - accuracy: 0.56 - ETA: 0s - loss: 0.6821 - accuracy: 0.56 - 3s 260us/step - loss: 0.6819 - accuracy: 0.5664 - val_loss: 0.6674 - val_accuracy: 0.7017\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:13 - loss: 0.6998 - accuracy: 0.25 - ETA: 8s - loss: 0.6942 - accuracy: 0.4643 - ETA: 5s - loss: 0.6938 - accuracy: 0.49 - ETA: 4s - loss: 0.6937 - accuracy: 0.49 - ETA: 4s - loss: 0.6937 - accuracy: 0.49 - ETA: 3s - loss: 0.6935 - accuracy: 0.50 - ETA: 3s - loss: 0.6932 - accuracy: 0.52 - ETA: 3s - loss: 0.6931 - accuracy: 0.51 - ETA: 3s - loss: 0.6930 - accuracy: 0.51 - ETA: 3s - loss: 0.6929 - accuracy: 0.52 - ETA: 2s - loss: 0.6929 - accuracy: 0.52 - ETA: 2s - loss: 0.6927 - accuracy: 0.53 - ETA: 2s - loss: 0.6925 - accuracy: 0.53 - ETA: 2s - loss: 0.6925 - accuracy: 0.53 - ETA: 2s - loss: 0.6923 - accuracy: 0.54 - ETA: 2s - loss: 0.6922 - accuracy: 0.54 - ETA: 2s - loss: 0.6920 - accuracy: 0.55 - ETA: 2s - loss: 0.6920 - accuracy: 0.54 - ETA: 2s - loss: 0.6919 - accuracy: 0.54 - ETA: 2s - loss: 0.6919 - accuracy: 0.54 - ETA: 2s - loss: 0.6918 - accuracy: 0.55 - ETA: 2s - loss: 0.6917 - accuracy: 0.55 - ETA: 2s - loss: 0.6916 - accuracy: 0.55 - ETA: 2s - loss: 0.6915 - accuracy: 0.55 - ETA: 1s - loss: 0.6913 - accuracy: 0.55 - ETA: 1s - loss: 0.6911 - accuracy: 0.55 - ETA: 1s - loss: 0.6909 - accuracy: 0.55 - ETA: 1s - loss: 0.6907 - accuracy: 0.55 - ETA: 1s - loss: 0.6906 - accuracy: 0.55 - ETA: 1s - loss: 0.6907 - accuracy: 0.55 - ETA: 1s - loss: 0.6906 - accuracy: 0.55 - ETA: 1s - loss: 0.6906 - accuracy: 0.55 - ETA: 1s - loss: 0.6905 - accuracy: 0.55 - ETA: 1s - loss: 0.6904 - accuracy: 0.56 - ETA: 1s - loss: 0.6902 - accuracy: 0.56 - ETA: 1s - loss: 0.6899 - accuracy: 0.56 - ETA: 1s - loss: 0.6899 - accuracy: 0.56 - ETA: 1s - loss: 0.6897 - accuracy: 0.56 - ETA: 1s - loss: 0.6896 - accuracy: 0.56 - ETA: 1s - loss: 0.6894 - accuracy: 0.56 - ETA: 1s - loss: 0.6893 - accuracy: 0.56 - ETA: 0s - loss: 0.6892 - accuracy: 0.56 - ETA: 0s - loss: 0.6890 - accuracy: 0.56 - ETA: 0s - loss: 0.6890 - accuracy: 0.57 - ETA: 0s - loss: 0.6888 - accuracy: 0.57 - ETA: 0s - loss: 0.6887 - accuracy: 0.57 - ETA: 0s - loss: 0.6885 - accuracy: 0.57 - ETA: 0s - loss: 0.6884 - accuracy: 0.57 - ETA: 0s - loss: 0.6881 - accuracy: 0.57 - ETA: 0s - loss: 0.6881 - accuracy: 0.57 - ETA: 0s - loss: 0.6879 - accuracy: 0.57 - ETA: 0s - loss: 0.6877 - accuracy: 0.58 - ETA: 0s - loss: 0.6876 - accuracy: 0.58 - ETA: 0s - loss: 0.6873 - accuracy: 0.58 - ETA: 0s - loss: 0.6871 - accuracy: 0.58 - ETA: 0s - loss: 0.6869 - accuracy: 0.58 - ETA: 0s - loss: 0.6869 - accuracy: 0.58 - ETA: 0s - loss: 0.6866 - accuracy: 0.58 - ETA: 0s - loss: 0.6865 - accuracy: 0.58 - 3s 258us/step - loss: 0.6865 - accuracy: 0.5852 - val_loss: 0.6750 - val_accuracy: 0.7003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:10 - loss: 0.6930 - accuracy: 0.75 - ETA: 8s - loss: 0.6892 - accuracy: 0.5727 - ETA: 5s - loss: 0.6901 - accuracy: 0.54 - ETA: 4s - loss: 0.6900 - accuracy: 0.54 - ETA: 4s - loss: 0.6893 - accuracy: 0.53 - ETA: 3s - loss: 0.6891 - accuracy: 0.53 - ETA: 3s - loss: 0.6900 - accuracy: 0.52 - ETA: 3s - loss: 0.6894 - accuracy: 0.52 - ETA: 3s - loss: 0.6890 - accuracy: 0.52 - ETA: 3s - loss: 0.6885 - accuracy: 0.53 - ETA: 3s - loss: 0.6881 - accuracy: 0.53 - ETA: 2s - loss: 0.6879 - accuracy: 0.52 - ETA: 2s - loss: 0.6881 - accuracy: 0.52 - ETA: 2s - loss: 0.6886 - accuracy: 0.51 - ETA: 2s - loss: 0.6886 - accuracy: 0.51 - ETA: 2s - loss: 0.6886 - accuracy: 0.51 - ETA: 2s - loss: 0.6880 - accuracy: 0.51 - ETA: 2s - loss: 0.6878 - accuracy: 0.51 - ETA: 2s - loss: 0.6875 - accuracy: 0.51 - ETA: 2s - loss: 0.6874 - accuracy: 0.51 - ETA: 2s - loss: 0.6874 - accuracy: 0.51 - ETA: 2s - loss: 0.6865 - accuracy: 0.51 - ETA: 2s - loss: 0.6863 - accuracy: 0.51 - ETA: 2s - loss: 0.6855 - accuracy: 0.51 - ETA: 1s - loss: 0.6853 - accuracy: 0.51 - ETA: 1s - loss: 0.6855 - accuracy: 0.51 - ETA: 1s - loss: 0.6853 - accuracy: 0.51 - ETA: 1s - loss: 0.6853 - accuracy: 0.51 - ETA: 1s - loss: 0.6852 - accuracy: 0.51 - ETA: 1s - loss: 0.6848 - accuracy: 0.51 - ETA: 1s - loss: 0.6847 - accuracy: 0.51 - ETA: 1s - loss: 0.6846 - accuracy: 0.51 - ETA: 1s - loss: 0.6843 - accuracy: 0.51 - ETA: 1s - loss: 0.6839 - accuracy: 0.51 - ETA: 1s - loss: 0.6833 - accuracy: 0.52 - ETA: 1s - loss: 0.6831 - accuracy: 0.52 - ETA: 1s - loss: 0.6832 - accuracy: 0.52 - ETA: 1s - loss: 0.6828 - accuracy: 0.52 - ETA: 1s - loss: 0.6826 - accuracy: 0.52 - ETA: 1s - loss: 0.6825 - accuracy: 0.52 - ETA: 1s - loss: 0.6823 - accuracy: 0.52 - ETA: 0s - loss: 0.6822 - accuracy: 0.52 - ETA: 0s - loss: 0.6822 - accuracy: 0.52 - ETA: 0s - loss: 0.6819 - accuracy: 0.52 - ETA: 0s - loss: 0.6818 - accuracy: 0.53 - ETA: 0s - loss: 0.6815 - accuracy: 0.53 - ETA: 0s - loss: 0.6813 - accuracy: 0.53 - ETA: 0s - loss: 0.6812 - accuracy: 0.53 - ETA: 0s - loss: 0.6810 - accuracy: 0.53 - ETA: 0s - loss: 0.6809 - accuracy: 0.53 - ETA: 0s - loss: 0.6807 - accuracy: 0.53 - ETA: 0s - loss: 0.6802 - accuracy: 0.53 - ETA: 0s - loss: 0.6800 - accuracy: 0.54 - ETA: 0s - loss: 0.6798 - accuracy: 0.54 - ETA: 0s - loss: 0.6793 - accuracy: 0.54 - ETA: 0s - loss: 0.6795 - accuracy: 0.54 - ETA: 0s - loss: 0.6793 - accuracy: 0.54 - ETA: 0s - loss: 0.6789 - accuracy: 0.54 - ETA: 0s - loss: 0.6788 - accuracy: 0.54 - ETA: 0s - loss: 0.6785 - accuracy: 0.54 - ETA: 0s - loss: 0.6782 - accuracy: 0.54 - 3s 265us/step - loss: 0.6781 - accuracy: 0.5492 - val_loss: 0.6612 - val_accuracy: 0.6080\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 107us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:26 - loss: 0.6945 - accuracy: 0.25 - ETA: 8s - loss: 0.6933 - accuracy: 0.5134 - ETA: 5s - loss: 0.6938 - accuracy: 0.47 - ETA: 4s - loss: 0.6934 - accuracy: 0.49 - ETA: 4s - loss: 0.6931 - accuracy: 0.50 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 3s - loss: 0.6925 - accuracy: 0.51 - ETA: 3s - loss: 0.6921 - accuracy: 0.52 - ETA: 3s - loss: 0.6920 - accuracy: 0.52 - ETA: 3s - loss: 0.6917 - accuracy: 0.52 - ETA: 2s - loss: 0.6915 - accuracy: 0.52 - ETA: 2s - loss: 0.6909 - accuracy: 0.53 - ETA: 2s - loss: 0.6906 - accuracy: 0.53 - ETA: 2s - loss: 0.6900 - accuracy: 0.53 - ETA: 2s - loss: 0.6895 - accuracy: 0.54 - ETA: 2s - loss: 0.6891 - accuracy: 0.54 - ETA: 2s - loss: 0.6890 - accuracy: 0.54 - ETA: 2s - loss: 0.6889 - accuracy: 0.54 - ETA: 2s - loss: 0.6886 - accuracy: 0.54 - ETA: 2s - loss: 0.6883 - accuracy: 0.54 - ETA: 2s - loss: 0.6878 - accuracy: 0.54 - ETA: 1s - loss: 0.6874 - accuracy: 0.55 - ETA: 1s - loss: 0.6870 - accuracy: 0.55 - ETA: 1s - loss: 0.6869 - accuracy: 0.55 - ETA: 1s - loss: 0.6865 - accuracy: 0.55 - ETA: 1s - loss: 0.6866 - accuracy: 0.55 - ETA: 1s - loss: 0.6861 - accuracy: 0.55 - ETA: 1s - loss: 0.6860 - accuracy: 0.55 - ETA: 1s - loss: 0.6858 - accuracy: 0.55 - ETA: 1s - loss: 0.6855 - accuracy: 0.55 - ETA: 1s - loss: 0.6853 - accuracy: 0.55 - ETA: 1s - loss: 0.6849 - accuracy: 0.55 - ETA: 1s - loss: 0.6848 - accuracy: 0.55 - ETA: 1s - loss: 0.6848 - accuracy: 0.55 - ETA: 1s - loss: 0.6847 - accuracy: 0.56 - ETA: 1s - loss: 0.6844 - accuracy: 0.56 - ETA: 1s - loss: 0.6841 - accuracy: 0.56 - ETA: 1s - loss: 0.6839 - accuracy: 0.56 - ETA: 1s - loss: 0.6837 - accuracy: 0.56 - ETA: 1s - loss: 0.6832 - accuracy: 0.56 - ETA: 0s - loss: 0.6828 - accuracy: 0.56 - ETA: 0s - loss: 0.6828 - accuracy: 0.56 - ETA: 0s - loss: 0.6826 - accuracy: 0.56 - ETA: 0s - loss: 0.6825 - accuracy: 0.56 - ETA: 0s - loss: 0.6822 - accuracy: 0.56 - ETA: 0s - loss: 0.6819 - accuracy: 0.56 - ETA: 0s - loss: 0.6814 - accuracy: 0.57 - ETA: 0s - loss: 0.6809 - accuracy: 0.57 - ETA: 0s - loss: 0.6807 - accuracy: 0.57 - ETA: 0s - loss: 0.6805 - accuracy: 0.57 - ETA: 0s - loss: 0.6803 - accuracy: 0.57 - ETA: 0s - loss: 0.6800 - accuracy: 0.57 - ETA: 0s - loss: 0.6797 - accuracy: 0.57 - ETA: 0s - loss: 0.6795 - accuracy: 0.57 - ETA: 0s - loss: 0.6793 - accuracy: 0.57 - ETA: 0s - loss: 0.6792 - accuracy: 0.57 - ETA: 0s - loss: 0.6790 - accuracy: 0.57 - ETA: 0s - loss: 0.6787 - accuracy: 0.57 - ETA: 0s - loss: 0.6785 - accuracy: 0.57 - 3s 257us/step - loss: 0.6784 - accuracy: 0.5768 - val_loss: 0.6574 - val_accuracy: 0.7024\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 118us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:10 - loss: 0.6958 - accuracy: 0.50 - ETA: 8s - loss: 0.6943 - accuracy: 0.4818 - ETA: 5s - loss: 0.6938 - accuracy: 0.49 - ETA: 4s - loss: 0.6934 - accuracy: 0.50 - ETA: 4s - loss: 0.6933 - accuracy: 0.51 - ETA: 3s - loss: 0.6930 - accuracy: 0.52 - ETA: 3s - loss: 0.6931 - accuracy: 0.51 - ETA: 3s - loss: 0.6928 - accuracy: 0.52 - ETA: 3s - loss: 0.6928 - accuracy: 0.52 - ETA: 3s - loss: 0.6925 - accuracy: 0.53 - ETA: 2s - loss: 0.6922 - accuracy: 0.53 - ETA: 2s - loss: 0.6918 - accuracy: 0.53 - ETA: 2s - loss: 0.6920 - accuracy: 0.53 - ETA: 2s - loss: 0.6919 - accuracy: 0.53 - ETA: 2s - loss: 0.6918 - accuracy: 0.53 - ETA: 2s - loss: 0.6915 - accuracy: 0.53 - ETA: 2s - loss: 0.6912 - accuracy: 0.53 - ETA: 2s - loss: 0.6913 - accuracy: 0.54 - ETA: 2s - loss: 0.6910 - accuracy: 0.54 - ETA: 2s - loss: 0.6909 - accuracy: 0.54 - ETA: 2s - loss: 0.6907 - accuracy: 0.54 - ETA: 2s - loss: 0.6904 - accuracy: 0.54 - ETA: 2s - loss: 0.6903 - accuracy: 0.54 - ETA: 1s - loss: 0.6902 - accuracy: 0.54 - ETA: 1s - loss: 0.6900 - accuracy: 0.54 - ETA: 1s - loss: 0.6898 - accuracy: 0.54 - ETA: 1s - loss: 0.6896 - accuracy: 0.54 - ETA: 1s - loss: 0.6893 - accuracy: 0.54 - ETA: 1s - loss: 0.6891 - accuracy: 0.54 - ETA: 1s - loss: 0.6888 - accuracy: 0.55 - ETA: 1s - loss: 0.6885 - accuracy: 0.55 - ETA: 1s - loss: 0.6884 - accuracy: 0.55 - ETA: 1s - loss: 0.6881 - accuracy: 0.55 - ETA: 1s - loss: 0.6877 - accuracy: 0.55 - ETA: 1s - loss: 0.6875 - accuracy: 0.55 - ETA: 1s - loss: 0.6872 - accuracy: 0.55 - ETA: 1s - loss: 0.6870 - accuracy: 0.55 - ETA: 1s - loss: 0.6869 - accuracy: 0.55 - ETA: 1s - loss: 0.6867 - accuracy: 0.55 - ETA: 1s - loss: 0.6864 - accuracy: 0.55 - ETA: 0s - loss: 0.6863 - accuracy: 0.55 - ETA: 0s - loss: 0.6863 - accuracy: 0.55 - ETA: 0s - loss: 0.6860 - accuracy: 0.55 - ETA: 0s - loss: 0.6858 - accuracy: 0.55 - ETA: 0s - loss: 0.6856 - accuracy: 0.55 - ETA: 0s - loss: 0.6853 - accuracy: 0.55 - ETA: 0s - loss: 0.6852 - accuracy: 0.55 - ETA: 0s - loss: 0.6850 - accuracy: 0.55 - ETA: 0s - loss: 0.6850 - accuracy: 0.55 - ETA: 0s - loss: 0.6849 - accuracy: 0.55 - ETA: 0s - loss: 0.6847 - accuracy: 0.55 - ETA: 0s - loss: 0.6845 - accuracy: 0.56 - ETA: 0s - loss: 0.6844 - accuracy: 0.56 - ETA: 0s - loss: 0.6843 - accuracy: 0.56 - ETA: 0s - loss: 0.6841 - accuracy: 0.56 - ETA: 0s - loss: 0.6839 - accuracy: 0.56 - ETA: 0s - loss: 0.6840 - accuracy: 0.55 - ETA: 0s - loss: 0.6838 - accuracy: 0.56 - ETA: 0s - loss: 0.6833 - accuracy: 0.56 - 3s 257us/step - loss: 0.6833 - accuracy: 0.5612 - val_loss: 0.6697 - val_accuracy: 0.7045\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 100us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:10 - loss: 0.7036 - accuracy: 0.25 - ETA: 8s - loss: 0.6941 - accuracy: 0.5318 - ETA: 5s - loss: 0.6937 - accuracy: 0.52 - ETA: 4s - loss: 0.6932 - accuracy: 0.52 - ETA: 4s - loss: 0.6934 - accuracy: 0.52 - ETA: 3s - loss: 0.6929 - accuracy: 0.52 - ETA: 3s - loss: 0.6923 - accuracy: 0.53 - ETA: 3s - loss: 0.6922 - accuracy: 0.52 - ETA: 3s - loss: 0.6922 - accuracy: 0.52 - ETA: 2s - loss: 0.6920 - accuracy: 0.52 - ETA: 2s - loss: 0.6923 - accuracy: 0.51 - ETA: 2s - loss: 0.6919 - accuracy: 0.52 - ETA: 2s - loss: 0.6918 - accuracy: 0.52 - ETA: 2s - loss: 0.6914 - accuracy: 0.53 - ETA: 2s - loss: 0.6913 - accuracy: 0.53 - ETA: 2s - loss: 0.6909 - accuracy: 0.53 - ETA: 2s - loss: 0.6904 - accuracy: 0.53 - ETA: 2s - loss: 0.6900 - accuracy: 0.54 - ETA: 2s - loss: 0.6897 - accuracy: 0.54 - ETA: 2s - loss: 0.6894 - accuracy: 0.54 - ETA: 2s - loss: 0.6894 - accuracy: 0.54 - ETA: 2s - loss: 0.6890 - accuracy: 0.54 - ETA: 1s - loss: 0.6886 - accuracy: 0.54 - ETA: 1s - loss: 0.6884 - accuracy: 0.54 - ETA: 1s - loss: 0.6880 - accuracy: 0.55 - ETA: 1s - loss: 0.6880 - accuracy: 0.55 - ETA: 1s - loss: 0.6876 - accuracy: 0.55 - ETA: 1s - loss: 0.6873 - accuracy: 0.55 - ETA: 1s - loss: 0.6869 - accuracy: 0.56 - ETA: 1s - loss: 0.6867 - accuracy: 0.56 - ETA: 1s - loss: 0.6860 - accuracy: 0.56 - ETA: 1s - loss: 0.6856 - accuracy: 0.56 - ETA: 1s - loss: 0.6858 - accuracy: 0.56 - ETA: 1s - loss: 0.6856 - accuracy: 0.56 - ETA: 1s - loss: 0.6851 - accuracy: 0.56 - ETA: 1s - loss: 0.6846 - accuracy: 0.57 - ETA: 1s - loss: 0.6843 - accuracy: 0.57 - ETA: 1s - loss: 0.6837 - accuracy: 0.57 - ETA: 1s - loss: 0.6837 - accuracy: 0.57 - ETA: 0s - loss: 0.6836 - accuracy: 0.57 - ETA: 0s - loss: 0.6832 - accuracy: 0.57 - ETA: 0s - loss: 0.6830 - accuracy: 0.57 - ETA: 0s - loss: 0.6828 - accuracy: 0.57 - ETA: 0s - loss: 0.6824 - accuracy: 0.57 - ETA: 0s - loss: 0.6821 - accuracy: 0.57 - ETA: 0s - loss: 0.6816 - accuracy: 0.57 - ETA: 0s - loss: 0.6815 - accuracy: 0.58 - ETA: 0s - loss: 0.6812 - accuracy: 0.58 - ETA: 0s - loss: 0.6806 - accuracy: 0.58 - ETA: 0s - loss: 0.6804 - accuracy: 0.58 - ETA: 0s - loss: 0.6800 - accuracy: 0.58 - ETA: 0s - loss: 0.6799 - accuracy: 0.58 - ETA: 0s - loss: 0.6797 - accuracy: 0.58 - ETA: 0s - loss: 0.6795 - accuracy: 0.58 - ETA: 0s - loss: 0.6792 - accuracy: 0.58 - ETA: 0s - loss: 0.6790 - accuracy: 0.58 - ETA: 0s - loss: 0.6787 - accuracy: 0.58 - ETA: 0s - loss: 0.6783 - accuracy: 0.58 - ETA: 0s - loss: 0.6780 - accuracy: 0.58 - 3s 258us/step - loss: 0.6779 - accuracy: 0.5896 - val_loss: 0.6543 - val_accuracy: 0.7109\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 109us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:13 - loss: 0.6927 - accuracy: 0.50 - ETA: 8s - loss: 0.6939 - accuracy: 0.4907 - ETA: 5s - loss: 0.6934 - accuracy: 0.50 - ETA: 4s - loss: 0.6930 - accuracy: 0.51 - ETA: 4s - loss: 0.6919 - accuracy: 0.53 - ETA: 3s - loss: 0.6906 - accuracy: 0.55 - ETA: 3s - loss: 0.6891 - accuracy: 0.55 - ETA: 3s - loss: 0.6875 - accuracy: 0.56 - ETA: 3s - loss: 0.6870 - accuracy: 0.56 - ETA: 2s - loss: 0.6867 - accuracy: 0.56 - ETA: 2s - loss: 0.6866 - accuracy: 0.55 - ETA: 2s - loss: 0.6858 - accuracy: 0.56 - ETA: 2s - loss: 0.6852 - accuracy: 0.56 - ETA: 2s - loss: 0.6848 - accuracy: 0.56 - ETA: 2s - loss: 0.6843 - accuracy: 0.57 - ETA: 2s - loss: 0.6844 - accuracy: 0.57 - ETA: 2s - loss: 0.6836 - accuracy: 0.57 - ETA: 2s - loss: 0.6831 - accuracy: 0.57 - ETA: 2s - loss: 0.6823 - accuracy: 0.58 - ETA: 2s - loss: 0.6825 - accuracy: 0.57 - ETA: 2s - loss: 0.6819 - accuracy: 0.58 - ETA: 1s - loss: 0.6816 - accuracy: 0.58 - ETA: 1s - loss: 0.6805 - accuracy: 0.58 - ETA: 1s - loss: 0.6797 - accuracy: 0.58 - ETA: 1s - loss: 0.6793 - accuracy: 0.58 - ETA: 1s - loss: 0.6786 - accuracy: 0.58 - ETA: 1s - loss: 0.6777 - accuracy: 0.59 - ETA: 1s - loss: 0.6771 - accuracy: 0.59 - ETA: 1s - loss: 0.6764 - accuracy: 0.59 - ETA: 1s - loss: 0.6762 - accuracy: 0.59 - ETA: 1s - loss: 0.6762 - accuracy: 0.59 - ETA: 1s - loss: 0.6760 - accuracy: 0.59 - ETA: 1s - loss: 0.6751 - accuracy: 0.59 - ETA: 1s - loss: 0.6747 - accuracy: 0.60 - ETA: 1s - loss: 0.6739 - accuracy: 0.60 - ETA: 1s - loss: 0.6740 - accuracy: 0.60 - ETA: 1s - loss: 0.6732 - accuracy: 0.60 - ETA: 1s - loss: 0.6728 - accuracy: 0.60 - ETA: 1s - loss: 0.6718 - accuracy: 0.60 - ETA: 0s - loss: 0.6711 - accuracy: 0.61 - ETA: 0s - loss: 0.6705 - accuracy: 0.61 - ETA: 0s - loss: 0.6698 - accuracy: 0.61 - ETA: 0s - loss: 0.6695 - accuracy: 0.61 - ETA: 0s - loss: 0.6694 - accuracy: 0.61 - ETA: 0s - loss: 0.6688 - accuracy: 0.61 - ETA: 0s - loss: 0.6684 - accuracy: 0.61 - ETA: 0s - loss: 0.6684 - accuracy: 0.61 - ETA: 0s - loss: 0.6676 - accuracy: 0.61 - ETA: 0s - loss: 0.6675 - accuracy: 0.61 - ETA: 0s - loss: 0.6674 - accuracy: 0.61 - ETA: 0s - loss: 0.6669 - accuracy: 0.61 - ETA: 0s - loss: 0.6665 - accuracy: 0.61 - ETA: 0s - loss: 0.6664 - accuracy: 0.61 - ETA: 0s - loss: 0.6658 - accuracy: 0.62 - ETA: 0s - loss: 0.6655 - accuracy: 0.62 - ETA: 0s - loss: 0.6653 - accuracy: 0.62 - ETA: 0s - loss: 0.6651 - accuracy: 0.62 - ETA: 0s - loss: 0.6645 - accuracy: 0.62 - ETA: 0s - loss: 0.6641 - accuracy: 0.62 - 3s 257us/step - loss: 0.6640 - accuracy: 0.6223 - val_loss: 0.6329 - val_accuracy: 0.7088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 98us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:10 - loss: 0.6945 - accuracy: 0.50 - ETA: 8s - loss: 0.6930 - accuracy: 0.5182 - ETA: 5s - loss: 0.6939 - accuracy: 0.49 - ETA: 4s - loss: 0.6937 - accuracy: 0.51 - ETA: 4s - loss: 0.6930 - accuracy: 0.52 - ETA: 3s - loss: 0.6925 - accuracy: 0.52 - ETA: 3s - loss: 0.6919 - accuracy: 0.53 - ETA: 3s - loss: 0.6915 - accuracy: 0.53 - ETA: 3s - loss: 0.6905 - accuracy: 0.55 - ETA: 3s - loss: 0.6899 - accuracy: 0.56 - ETA: 2s - loss: 0.6890 - accuracy: 0.56 - ETA: 2s - loss: 0.6884 - accuracy: 0.56 - ETA: 2s - loss: 0.6883 - accuracy: 0.56 - ETA: 2s - loss: 0.6881 - accuracy: 0.56 - ETA: 2s - loss: 0.6877 - accuracy: 0.56 - ETA: 2s - loss: 0.6869 - accuracy: 0.57 - ETA: 2s - loss: 0.6863 - accuracy: 0.57 - ETA: 2s - loss: 0.6855 - accuracy: 0.57 - ETA: 2s - loss: 0.6849 - accuracy: 0.58 - ETA: 2s - loss: 0.6846 - accuracy: 0.58 - ETA: 2s - loss: 0.6840 - accuracy: 0.58 - ETA: 2s - loss: 0.6832 - accuracy: 0.58 - ETA: 2s - loss: 0.6824 - accuracy: 0.58 - ETA: 1s - loss: 0.6816 - accuracy: 0.59 - ETA: 1s - loss: 0.6813 - accuracy: 0.59 - ETA: 1s - loss: 0.6801 - accuracy: 0.59 - ETA: 1s - loss: 0.6799 - accuracy: 0.59 - ETA: 1s - loss: 0.6794 - accuracy: 0.60 - ETA: 1s - loss: 0.6786 - accuracy: 0.60 - ETA: 1s - loss: 0.6779 - accuracy: 0.60 - ETA: 1s - loss: 0.6780 - accuracy: 0.60 - ETA: 1s - loss: 0.6775 - accuracy: 0.60 - ETA: 1s - loss: 0.6769 - accuracy: 0.60 - ETA: 1s - loss: 0.6764 - accuracy: 0.60 - ETA: 1s - loss: 0.6759 - accuracy: 0.60 - ETA: 1s - loss: 0.6750 - accuracy: 0.61 - ETA: 1s - loss: 0.6747 - accuracy: 0.61 - ETA: 1s - loss: 0.6745 - accuracy: 0.61 - ETA: 1s - loss: 0.6739 - accuracy: 0.61 - ETA: 1s - loss: 0.6736 - accuracy: 0.61 - ETA: 1s - loss: 0.6730 - accuracy: 0.61 - ETA: 0s - loss: 0.6726 - accuracy: 0.61 - ETA: 0s - loss: 0.6721 - accuracy: 0.61 - ETA: 0s - loss: 0.6713 - accuracy: 0.61 - ETA: 0s - loss: 0.6707 - accuracy: 0.61 - ETA: 0s - loss: 0.6701 - accuracy: 0.61 - ETA: 0s - loss: 0.6703 - accuracy: 0.61 - ETA: 0s - loss: 0.6701 - accuracy: 0.61 - ETA: 0s - loss: 0.6698 - accuracy: 0.61 - ETA: 0s - loss: 0.6691 - accuracy: 0.61 - ETA: 0s - loss: 0.6686 - accuracy: 0.62 - ETA: 0s - loss: 0.6681 - accuracy: 0.62 - ETA: 0s - loss: 0.6673 - accuracy: 0.62 - ETA: 0s - loss: 0.6666 - accuracy: 0.62 - ETA: 0s - loss: 0.6659 - accuracy: 0.62 - ETA: 0s - loss: 0.6653 - accuracy: 0.62 - ETA: 0s - loss: 0.6650 - accuracy: 0.62 - ETA: 0s - loss: 0.6647 - accuracy: 0.62 - ETA: 0s - loss: 0.6643 - accuracy: 0.62 - ETA: 0s - loss: 0.6643 - accuracy: 0.62 - 3s 261us/step - loss: 0.6643 - accuracy: 0.6277 - val_loss: 0.6310 - val_accuracy: 0.7145\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 106us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:00 - loss: 0.6908 - accuracy: 0.75 - ETA: 8s - loss: 0.6927 - accuracy: 0.5312 - ETA: 5s - loss: 0.6904 - accuracy: 0.55 - ETA: 4s - loss: 0.6873 - accuracy: 0.58 - ETA: 3s - loss: 0.6871 - accuracy: 0.58 - ETA: 3s - loss: 0.6860 - accuracy: 0.58 - ETA: 3s - loss: 0.6851 - accuracy: 0.58 - ETA: 3s - loss: 0.6848 - accuracy: 0.58 - ETA: 3s - loss: 0.6837 - accuracy: 0.58 - ETA: 2s - loss: 0.6827 - accuracy: 0.58 - ETA: 2s - loss: 0.6818 - accuracy: 0.59 - ETA: 2s - loss: 0.6807 - accuracy: 0.59 - ETA: 2s - loss: 0.6802 - accuracy: 0.59 - ETA: 2s - loss: 0.6794 - accuracy: 0.59 - ETA: 2s - loss: 0.6783 - accuracy: 0.59 - ETA: 2s - loss: 0.6777 - accuracy: 0.60 - ETA: 2s - loss: 0.6777 - accuracy: 0.60 - ETA: 2s - loss: 0.6775 - accuracy: 0.60 - ETA: 2s - loss: 0.6764 - accuracy: 0.60 - ETA: 2s - loss: 0.6757 - accuracy: 0.61 - ETA: 2s - loss: 0.6750 - accuracy: 0.61 - ETA: 2s - loss: 0.6744 - accuracy: 0.61 - ETA: 1s - loss: 0.6739 - accuracy: 0.61 - ETA: 1s - loss: 0.6737 - accuracy: 0.61 - ETA: 1s - loss: 0.6733 - accuracy: 0.61 - ETA: 1s - loss: 0.6732 - accuracy: 0.61 - ETA: 1s - loss: 0.6730 - accuracy: 0.61 - ETA: 1s - loss: 0.6725 - accuracy: 0.62 - ETA: 1s - loss: 0.6714 - accuracy: 0.62 - ETA: 1s - loss: 0.6710 - accuracy: 0.62 - ETA: 1s - loss: 0.6706 - accuracy: 0.62 - ETA: 1s - loss: 0.6699 - accuracy: 0.62 - ETA: 1s - loss: 0.6688 - accuracy: 0.62 - ETA: 1s - loss: 0.6682 - accuracy: 0.63 - ETA: 1s - loss: 0.6680 - accuracy: 0.63 - ETA: 1s - loss: 0.6672 - accuracy: 0.63 - ETA: 1s - loss: 0.6669 - accuracy: 0.63 - ETA: 1s - loss: 0.6671 - accuracy: 0.63 - ETA: 1s - loss: 0.6662 - accuracy: 0.63 - ETA: 1s - loss: 0.6662 - accuracy: 0.63 - ETA: 1s - loss: 0.6659 - accuracy: 0.63 - ETA: 0s - loss: 0.6656 - accuracy: 0.63 - ETA: 0s - loss: 0.6652 - accuracy: 0.63 - ETA: 0s - loss: 0.6646 - accuracy: 0.63 - ETA: 0s - loss: 0.6643 - accuracy: 0.63 - ETA: 0s - loss: 0.6644 - accuracy: 0.63 - ETA: 0s - loss: 0.6643 - accuracy: 0.63 - ETA: 0s - loss: 0.6638 - accuracy: 0.63 - ETA: 0s - loss: 0.6631 - accuracy: 0.63 - ETA: 0s - loss: 0.6625 - accuracy: 0.64 - ETA: 0s - loss: 0.6620 - accuracy: 0.64 - ETA: 0s - loss: 0.6619 - accuracy: 0.64 - ETA: 0s - loss: 0.6614 - accuracy: 0.64 - ETA: 0s - loss: 0.6614 - accuracy: 0.64 - ETA: 0s - loss: 0.6610 - accuracy: 0.64 - ETA: 0s - loss: 0.6607 - accuracy: 0.64 - ETA: 0s - loss: 0.6604 - accuracy: 0.64 - ETA: 0s - loss: 0.6602 - accuracy: 0.64 - ETA: 0s - loss: 0.6597 - accuracy: 0.64 - 3s 257us/step - loss: 0.6597 - accuracy: 0.6445 - val_loss: 0.6287 - val_accuracy: 0.7109\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:07 - loss: 0.7000 - accuracy: 0.25 - ETA: 8s - loss: 0.6892 - accuracy: 0.5364 - ETA: 5s - loss: 0.6916 - accuracy: 0.50 - ETA: 4s - loss: 0.6920 - accuracy: 0.50 - ETA: 4s - loss: 0.6908 - accuracy: 0.53 - ETA: 3s - loss: 0.6901 - accuracy: 0.53 - ETA: 3s - loss: 0.6892 - accuracy: 0.55 - ETA: 3s - loss: 0.6888 - accuracy: 0.55 - ETA: 3s - loss: 0.6872 - accuracy: 0.56 - ETA: 3s - loss: 0.6871 - accuracy: 0.55 - ETA: 2s - loss: 0.6867 - accuracy: 0.55 - ETA: 2s - loss: 0.6865 - accuracy: 0.55 - ETA: 2s - loss: 0.6861 - accuracy: 0.55 - ETA: 2s - loss: 0.6852 - accuracy: 0.56 - ETA: 2s - loss: 0.6846 - accuracy: 0.56 - ETA: 2s - loss: 0.6843 - accuracy: 0.56 - ETA: 2s - loss: 0.6839 - accuracy: 0.57 - ETA: 2s - loss: 0.6840 - accuracy: 0.57 - ETA: 2s - loss: 0.6830 - accuracy: 0.57 - ETA: 2s - loss: 0.6825 - accuracy: 0.57 - ETA: 2s - loss: 0.6820 - accuracy: 0.57 - ETA: 2s - loss: 0.6815 - accuracy: 0.57 - ETA: 1s - loss: 0.6807 - accuracy: 0.58 - ETA: 1s - loss: 0.6804 - accuracy: 0.58 - ETA: 1s - loss: 0.6801 - accuracy: 0.58 - ETA: 1s - loss: 0.6796 - accuracy: 0.58 - ETA: 1s - loss: 0.6794 - accuracy: 0.58 - ETA: 1s - loss: 0.6789 - accuracy: 0.58 - ETA: 1s - loss: 0.6786 - accuracy: 0.58 - ETA: 1s - loss: 0.6783 - accuracy: 0.58 - ETA: 1s - loss: 0.6781 - accuracy: 0.58 - ETA: 1s - loss: 0.6776 - accuracy: 0.58 - ETA: 1s - loss: 0.6775 - accuracy: 0.58 - ETA: 1s - loss: 0.6770 - accuracy: 0.59 - ETA: 1s - loss: 0.6768 - accuracy: 0.59 - ETA: 1s - loss: 0.6765 - accuracy: 0.59 - ETA: 1s - loss: 0.6761 - accuracy: 0.59 - ETA: 1s - loss: 0.6757 - accuracy: 0.59 - ETA: 1s - loss: 0.6751 - accuracy: 0.59 - ETA: 1s - loss: 0.6746 - accuracy: 0.60 - ETA: 0s - loss: 0.6742 - accuracy: 0.60 - ETA: 0s - loss: 0.6737 - accuracy: 0.60 - ETA: 0s - loss: 0.6732 - accuracy: 0.60 - ETA: 0s - loss: 0.6728 - accuracy: 0.60 - ETA: 0s - loss: 0.6724 - accuracy: 0.60 - ETA: 0s - loss: 0.6720 - accuracy: 0.60 - ETA: 0s - loss: 0.6719 - accuracy: 0.60 - ETA: 0s - loss: 0.6716 - accuracy: 0.60 - ETA: 0s - loss: 0.6713 - accuracy: 0.60 - ETA: 0s - loss: 0.6711 - accuracy: 0.60 - ETA: 0s - loss: 0.6705 - accuracy: 0.60 - ETA: 0s - loss: 0.6701 - accuracy: 0.60 - ETA: 0s - loss: 0.6699 - accuracy: 0.60 - ETA: 0s - loss: 0.6696 - accuracy: 0.60 - ETA: 0s - loss: 0.6689 - accuracy: 0.61 - ETA: 0s - loss: 0.6687 - accuracy: 0.61 - ETA: 0s - loss: 0.6680 - accuracy: 0.61 - ETA: 0s - loss: 0.6679 - accuracy: 0.61 - ETA: 0s - loss: 0.6676 - accuracy: 0.61 - ETA: 0s - loss: 0.6671 - accuracy: 0.61 - 3s 260us/step - loss: 0.6671 - accuracy: 0.6157 - val_loss: 0.6397 - val_accuracy: 0.7095\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:13 - loss: 0.6973 - accuracy: 0.75 - ETA: 8s - loss: 0.6939 - accuracy: 0.5409 - ETA: 5s - loss: 0.6906 - accuracy: 0.55 - ETA: 4s - loss: 0.6933 - accuracy: 0.51 - ETA: 3s - loss: 0.6916 - accuracy: 0.53 - ETA: 3s - loss: 0.6924 - accuracy: 0.51 - ETA: 3s - loss: 0.6931 - accuracy: 0.50 - ETA: 3s - loss: 0.6926 - accuracy: 0.50 - ETA: 3s - loss: 0.6923 - accuracy: 0.50 - ETA: 2s - loss: 0.6915 - accuracy: 0.51 - ETA: 2s - loss: 0.6913 - accuracy: 0.51 - ETA: 2s - loss: 0.6915 - accuracy: 0.50 - ETA: 2s - loss: 0.6912 - accuracy: 0.50 - ETA: 2s - loss: 0.6909 - accuracy: 0.51 - ETA: 2s - loss: 0.6904 - accuracy: 0.51 - ETA: 2s - loss: 0.6905 - accuracy: 0.51 - ETA: 2s - loss: 0.6904 - accuracy: 0.51 - ETA: 2s - loss: 0.6903 - accuracy: 0.51 - ETA: 2s - loss: 0.6898 - accuracy: 0.51 - ETA: 2s - loss: 0.6895 - accuracy: 0.51 - ETA: 2s - loss: 0.6890 - accuracy: 0.52 - ETA: 1s - loss: 0.6888 - accuracy: 0.52 - ETA: 1s - loss: 0.6884 - accuracy: 0.52 - ETA: 1s - loss: 0.6880 - accuracy: 0.52 - ETA: 1s - loss: 0.6872 - accuracy: 0.53 - ETA: 1s - loss: 0.6871 - accuracy: 0.53 - ETA: 1s - loss: 0.6868 - accuracy: 0.53 - ETA: 1s - loss: 0.6864 - accuracy: 0.53 - ETA: 1s - loss: 0.6858 - accuracy: 0.53 - ETA: 1s - loss: 0.6848 - accuracy: 0.54 - ETA: 1s - loss: 0.6844 - accuracy: 0.54 - ETA: 1s - loss: 0.6840 - accuracy: 0.54 - ETA: 1s - loss: 0.6836 - accuracy: 0.54 - ETA: 1s - loss: 0.6829 - accuracy: 0.54 - ETA: 1s - loss: 0.6829 - accuracy: 0.54 - ETA: 1s - loss: 0.6827 - accuracy: 0.54 - ETA: 1s - loss: 0.6823 - accuracy: 0.54 - ETA: 1s - loss: 0.6817 - accuracy: 0.54 - ETA: 1s - loss: 0.6815 - accuracy: 0.54 - ETA: 0s - loss: 0.6813 - accuracy: 0.54 - ETA: 0s - loss: 0.6808 - accuracy: 0.55 - ETA: 0s - loss: 0.6804 - accuracy: 0.55 - ETA: 0s - loss: 0.6804 - accuracy: 0.55 - ETA: 0s - loss: 0.6804 - accuracy: 0.55 - ETA: 0s - loss: 0.6799 - accuracy: 0.55 - ETA: 0s - loss: 0.6798 - accuracy: 0.55 - ETA: 0s - loss: 0.6794 - accuracy: 0.55 - ETA: 0s - loss: 0.6792 - accuracy: 0.56 - ETA: 0s - loss: 0.6788 - accuracy: 0.56 - ETA: 0s - loss: 0.6786 - accuracy: 0.56 - ETA: 0s - loss: 0.6785 - accuracy: 0.56 - ETA: 0s - loss: 0.6781 - accuracy: 0.56 - ETA: 0s - loss: 0.6775 - accuracy: 0.56 - ETA: 0s - loss: 0.6773 - accuracy: 0.56 - ETA: 0s - loss: 0.6769 - accuracy: 0.56 - ETA: 0s - loss: 0.6768 - accuracy: 0.56 - ETA: 0s - loss: 0.6762 - accuracy: 0.57 - ETA: 0s - loss: 0.6758 - accuracy: 0.57 - 3s 253us/step - loss: 0.6757 - accuracy: 0.5730 - val_loss: 0.6546 - val_accuracy: 0.6946\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:19 - loss: 0.6957 - accuracy: 0.75 - ETA: 8s - loss: 0.6899 - accuracy: 0.5602 - ETA: 5s - loss: 0.6925 - accuracy: 0.53 - ETA: 4s - loss: 0.6918 - accuracy: 0.53 - ETA: 4s - loss: 0.6901 - accuracy: 0.54 - ETA: 3s - loss: 0.6890 - accuracy: 0.54 - ETA: 3s - loss: 0.6893 - accuracy: 0.53 - ETA: 3s - loss: 0.6879 - accuracy: 0.54 - ETA: 3s - loss: 0.6878 - accuracy: 0.53 - ETA: 3s - loss: 0.6874 - accuracy: 0.53 - ETA: 2s - loss: 0.6871 - accuracy: 0.54 - ETA: 2s - loss: 0.6861 - accuracy: 0.55 - ETA: 2s - loss: 0.6857 - accuracy: 0.55 - ETA: 2s - loss: 0.6856 - accuracy: 0.55 - ETA: 2s - loss: 0.6855 - accuracy: 0.55 - ETA: 2s - loss: 0.6853 - accuracy: 0.55 - ETA: 2s - loss: 0.6849 - accuracy: 0.55 - ETA: 2s - loss: 0.6846 - accuracy: 0.55 - ETA: 2s - loss: 0.6840 - accuracy: 0.55 - ETA: 2s - loss: 0.6836 - accuracy: 0.56 - ETA: 2s - loss: 0.6829 - accuracy: 0.56 - ETA: 2s - loss: 0.6828 - accuracy: 0.56 - ETA: 1s - loss: 0.6819 - accuracy: 0.57 - ETA: 1s - loss: 0.6809 - accuracy: 0.57 - ETA: 1s - loss: 0.6802 - accuracy: 0.57 - ETA: 1s - loss: 0.6796 - accuracy: 0.58 - ETA: 1s - loss: 0.6793 - accuracy: 0.58 - ETA: 1s - loss: 0.6787 - accuracy: 0.58 - ETA: 1s - loss: 0.6779 - accuracy: 0.58 - ETA: 1s - loss: 0.6774 - accuracy: 0.58 - ETA: 1s - loss: 0.6772 - accuracy: 0.58 - ETA: 1s - loss: 0.6768 - accuracy: 0.59 - ETA: 1s - loss: 0.6766 - accuracy: 0.59 - ETA: 1s - loss: 0.6765 - accuracy: 0.59 - ETA: 1s - loss: 0.6757 - accuracy: 0.59 - ETA: 1s - loss: 0.6750 - accuracy: 0.59 - ETA: 1s - loss: 0.6744 - accuracy: 0.59 - ETA: 1s - loss: 0.6738 - accuracy: 0.59 - ETA: 1s - loss: 0.6737 - accuracy: 0.59 - ETA: 1s - loss: 0.6729 - accuracy: 0.59 - ETA: 0s - loss: 0.6729 - accuracy: 0.59 - ETA: 0s - loss: 0.6726 - accuracy: 0.60 - ETA: 0s - loss: 0.6724 - accuracy: 0.60 - ETA: 0s - loss: 0.6720 - accuracy: 0.60 - ETA: 0s - loss: 0.6719 - accuracy: 0.60 - ETA: 0s - loss: 0.6712 - accuracy: 0.60 - ETA: 0s - loss: 0.6710 - accuracy: 0.60 - ETA: 0s - loss: 0.6707 - accuracy: 0.60 - ETA: 0s - loss: 0.6705 - accuracy: 0.60 - ETA: 0s - loss: 0.6699 - accuracy: 0.60 - ETA: 0s - loss: 0.6693 - accuracy: 0.60 - ETA: 0s - loss: 0.6689 - accuracy: 0.60 - ETA: 0s - loss: 0.6688 - accuracy: 0.60 - ETA: 0s - loss: 0.6684 - accuracy: 0.60 - ETA: 0s - loss: 0.6682 - accuracy: 0.61 - ETA: 0s - loss: 0.6675 - accuracy: 0.61 - ETA: 0s - loss: 0.6672 - accuracy: 0.61 - ETA: 0s - loss: 0.6668 - accuracy: 0.61 - ETA: 0s - loss: 0.6668 - accuracy: 0.61 - 3s 257us/step - loss: 0.6666 - accuracy: 0.6156 - val_loss: 0.6374 - val_accuracy: 0.7124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 104us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:04 - loss: 0.6993 - accuracy: 0.50 - ETA: 8s - loss: 0.6929 - accuracy: 0.5139 - ETA: 5s - loss: 0.6926 - accuracy: 0.51 - ETA: 4s - loss: 0.6916 - accuracy: 0.51 - ETA: 4s - loss: 0.6908 - accuracy: 0.52 - ETA: 3s - loss: 0.6888 - accuracy: 0.54 - ETA: 3s - loss: 0.6884 - accuracy: 0.53 - ETA: 3s - loss: 0.6871 - accuracy: 0.54 - ETA: 3s - loss: 0.6868 - accuracy: 0.54 - ETA: 2s - loss: 0.6861 - accuracy: 0.55 - ETA: 2s - loss: 0.6850 - accuracy: 0.55 - ETA: 2s - loss: 0.6849 - accuracy: 0.55 - ETA: 2s - loss: 0.6838 - accuracy: 0.56 - ETA: 2s - loss: 0.6826 - accuracy: 0.56 - ETA: 2s - loss: 0.6819 - accuracy: 0.57 - ETA: 2s - loss: 0.6814 - accuracy: 0.57 - ETA: 2s - loss: 0.6813 - accuracy: 0.57 - ETA: 2s - loss: 0.6810 - accuracy: 0.57 - ETA: 2s - loss: 0.6800 - accuracy: 0.57 - ETA: 2s - loss: 0.6789 - accuracy: 0.58 - ETA: 2s - loss: 0.6784 - accuracy: 0.58 - ETA: 1s - loss: 0.6779 - accuracy: 0.58 - ETA: 1s - loss: 0.6774 - accuracy: 0.58 - ETA: 1s - loss: 0.6773 - accuracy: 0.59 - ETA: 1s - loss: 0.6773 - accuracy: 0.58 - ETA: 1s - loss: 0.6761 - accuracy: 0.59 - ETA: 1s - loss: 0.6760 - accuracy: 0.59 - ETA: 1s - loss: 0.6756 - accuracy: 0.59 - ETA: 1s - loss: 0.6750 - accuracy: 0.59 - ETA: 1s - loss: 0.6745 - accuracy: 0.59 - ETA: 1s - loss: 0.6741 - accuracy: 0.59 - ETA: 1s - loss: 0.6733 - accuracy: 0.59 - ETA: 1s - loss: 0.6729 - accuracy: 0.60 - ETA: 1s - loss: 0.6719 - accuracy: 0.60 - ETA: 1s - loss: 0.6712 - accuracy: 0.60 - ETA: 1s - loss: 0.6709 - accuracy: 0.60 - ETA: 1s - loss: 0.6704 - accuracy: 0.60 - ETA: 1s - loss: 0.6696 - accuracy: 0.60 - ETA: 1s - loss: 0.6691 - accuracy: 0.60 - ETA: 0s - loss: 0.6687 - accuracy: 0.60 - ETA: 0s - loss: 0.6681 - accuracy: 0.60 - ETA: 0s - loss: 0.6675 - accuracy: 0.61 - ETA: 0s - loss: 0.6668 - accuracy: 0.61 - ETA: 0s - loss: 0.6662 - accuracy: 0.61 - ETA: 0s - loss: 0.6656 - accuracy: 0.61 - ETA: 0s - loss: 0.6654 - accuracy: 0.61 - ETA: 0s - loss: 0.6650 - accuracy: 0.61 - ETA: 0s - loss: 0.6646 - accuracy: 0.61 - ETA: 0s - loss: 0.6643 - accuracy: 0.61 - ETA: 0s - loss: 0.6636 - accuracy: 0.61 - ETA: 0s - loss: 0.6634 - accuracy: 0.61 - ETA: 0s - loss: 0.6623 - accuracy: 0.62 - ETA: 0s - loss: 0.6617 - accuracy: 0.62 - ETA: 0s - loss: 0.6614 - accuracy: 0.62 - ETA: 0s - loss: 0.6613 - accuracy: 0.62 - ETA: 0s - loss: 0.6610 - accuracy: 0.62 - ETA: 0s - loss: 0.6606 - accuracy: 0.62 - ETA: 0s - loss: 0.6604 - accuracy: 0.62 - 3s 252us/step - loss: 0.6604 - accuracy: 0.6244 - val_loss: 0.6286 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 100us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:07 - loss: 0.6889 - accuracy: 0.50 - ETA: 8s - loss: 0.6901 - accuracy: 0.5455 - ETA: 5s - loss: 0.6868 - accuracy: 0.53 - ETA: 4s - loss: 0.6884 - accuracy: 0.52 - ETA: 3s - loss: 0.6870 - accuracy: 0.53 - ETA: 3s - loss: 0.6876 - accuracy: 0.52 - ETA: 3s - loss: 0.6873 - accuracy: 0.53 - ETA: 3s - loss: 0.6874 - accuracy: 0.52 - ETA: 3s - loss: 0.6865 - accuracy: 0.53 - ETA: 2s - loss: 0.6864 - accuracy: 0.54 - ETA: 2s - loss: 0.6852 - accuracy: 0.55 - ETA: 2s - loss: 0.6841 - accuracy: 0.55 - ETA: 2s - loss: 0.6841 - accuracy: 0.55 - ETA: 2s - loss: 0.6829 - accuracy: 0.56 - ETA: 2s - loss: 0.6814 - accuracy: 0.57 - ETA: 2s - loss: 0.6813 - accuracy: 0.57 - ETA: 2s - loss: 0.6807 - accuracy: 0.57 - ETA: 2s - loss: 0.6800 - accuracy: 0.58 - ETA: 2s - loss: 0.6791 - accuracy: 0.58 - ETA: 2s - loss: 0.6776 - accuracy: 0.59 - ETA: 2s - loss: 0.6771 - accuracy: 0.59 - ETA: 1s - loss: 0.6764 - accuracy: 0.59 - ETA: 1s - loss: 0.6756 - accuracy: 0.59 - ETA: 1s - loss: 0.6748 - accuracy: 0.60 - ETA: 1s - loss: 0.6743 - accuracy: 0.60 - ETA: 1s - loss: 0.6736 - accuracy: 0.60 - ETA: 1s - loss: 0.6728 - accuracy: 0.60 - ETA: 1s - loss: 0.6722 - accuracy: 0.60 - ETA: 1s - loss: 0.6714 - accuracy: 0.61 - ETA: 1s - loss: 0.6708 - accuracy: 0.61 - ETA: 1s - loss: 0.6705 - accuracy: 0.61 - ETA: 1s - loss: 0.6697 - accuracy: 0.61 - ETA: 1s - loss: 0.6686 - accuracy: 0.61 - ETA: 1s - loss: 0.6684 - accuracy: 0.61 - ETA: 1s - loss: 0.6682 - accuracy: 0.61 - ETA: 1s - loss: 0.6675 - accuracy: 0.62 - ETA: 1s - loss: 0.6672 - accuracy: 0.62 - ETA: 1s - loss: 0.6664 - accuracy: 0.62 - ETA: 1s - loss: 0.6658 - accuracy: 0.62 - ETA: 0s - loss: 0.6655 - accuracy: 0.62 - ETA: 0s - loss: 0.6652 - accuracy: 0.62 - ETA: 0s - loss: 0.6645 - accuracy: 0.62 - ETA: 0s - loss: 0.6642 - accuracy: 0.62 - ETA: 0s - loss: 0.6633 - accuracy: 0.62 - ETA: 0s - loss: 0.6628 - accuracy: 0.63 - ETA: 0s - loss: 0.6624 - accuracy: 0.63 - ETA: 0s - loss: 0.6619 - accuracy: 0.63 - ETA: 0s - loss: 0.6607 - accuracy: 0.63 - ETA: 0s - loss: 0.6602 - accuracy: 0.63 - ETA: 0s - loss: 0.6598 - accuracy: 0.63 - ETA: 0s - loss: 0.6593 - accuracy: 0.63 - ETA: 0s - loss: 0.6585 - accuracy: 0.63 - ETA: 0s - loss: 0.6580 - accuracy: 0.63 - ETA: 0s - loss: 0.6578 - accuracy: 0.63 - ETA: 0s - loss: 0.6577 - accuracy: 0.63 - ETA: 0s - loss: 0.6573 - accuracy: 0.63 - ETA: 0s - loss: 0.6572 - accuracy: 0.63 - ETA: 0s - loss: 0.6567 - accuracy: 0.63 - 3s 253us/step - loss: 0.6564 - accuracy: 0.6399 - val_loss: 0.6216 - val_accuracy: 0.7152\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:10 - loss: 0.6911 - accuracy: 0.50 - ETA: 9s - loss: 0.6919 - accuracy: 0.5150 - ETA: 5s - loss: 0.6881 - accuracy: 0.54 - ETA: 4s - loss: 0.6908 - accuracy: 0.52 - ETA: 4s - loss: 0.6892 - accuracy: 0.54 - ETA: 4s - loss: 0.6891 - accuracy: 0.54 - ETA: 3s - loss: 0.6883 - accuracy: 0.54 - ETA: 3s - loss: 0.6889 - accuracy: 0.54 - ETA: 3s - loss: 0.6886 - accuracy: 0.54 - ETA: 3s - loss: 0.6882 - accuracy: 0.54 - ETA: 3s - loss: 0.6873 - accuracy: 0.54 - ETA: 2s - loss: 0.6866 - accuracy: 0.54 - ETA: 2s - loss: 0.6859 - accuracy: 0.55 - ETA: 2s - loss: 0.6853 - accuracy: 0.55 - ETA: 2s - loss: 0.6847 - accuracy: 0.56 - ETA: 2s - loss: 0.6842 - accuracy: 0.56 - ETA: 2s - loss: 0.6834 - accuracy: 0.56 - ETA: 2s - loss: 0.6833 - accuracy: 0.56 - ETA: 2s - loss: 0.6826 - accuracy: 0.57 - ETA: 2s - loss: 0.6821 - accuracy: 0.57 - ETA: 2s - loss: 0.6815 - accuracy: 0.57 - ETA: 2s - loss: 0.6811 - accuracy: 0.57 - ETA: 2s - loss: 0.6806 - accuracy: 0.57 - ETA: 1s - loss: 0.6803 - accuracy: 0.57 - ETA: 1s - loss: 0.6800 - accuracy: 0.57 - ETA: 1s - loss: 0.6799 - accuracy: 0.57 - ETA: 1s - loss: 0.6798 - accuracy: 0.57 - ETA: 1s - loss: 0.6793 - accuracy: 0.57 - ETA: 1s - loss: 0.6786 - accuracy: 0.58 - ETA: 1s - loss: 0.6781 - accuracy: 0.58 - ETA: 1s - loss: 0.6775 - accuracy: 0.58 - ETA: 1s - loss: 0.6771 - accuracy: 0.58 - ETA: 1s - loss: 0.6765 - accuracy: 0.58 - ETA: 1s - loss: 0.6759 - accuracy: 0.58 - ETA: 1s - loss: 0.6752 - accuracy: 0.58 - ETA: 1s - loss: 0.6752 - accuracy: 0.58 - ETA: 1s - loss: 0.6748 - accuracy: 0.58 - ETA: 1s - loss: 0.6741 - accuracy: 0.59 - ETA: 1s - loss: 0.6738 - accuracy: 0.59 - ETA: 1s - loss: 0.6736 - accuracy: 0.59 - ETA: 0s - loss: 0.6735 - accuracy: 0.59 - ETA: 0s - loss: 0.6735 - accuracy: 0.59 - ETA: 0s - loss: 0.6728 - accuracy: 0.59 - ETA: 0s - loss: 0.6728 - accuracy: 0.59 - ETA: 0s - loss: 0.6726 - accuracy: 0.59 - ETA: 0s - loss: 0.6721 - accuracy: 0.59 - ETA: 0s - loss: 0.6717 - accuracy: 0.59 - ETA: 0s - loss: 0.6713 - accuracy: 0.59 - ETA: 0s - loss: 0.6711 - accuracy: 0.59 - ETA: 0s - loss: 0.6705 - accuracy: 0.59 - ETA: 0s - loss: 0.6700 - accuracy: 0.60 - ETA: 0s - loss: 0.6695 - accuracy: 0.60 - ETA: 0s - loss: 0.6692 - accuracy: 0.60 - ETA: 0s - loss: 0.6688 - accuracy: 0.60 - ETA: 0s - loss: 0.6686 - accuracy: 0.60 - ETA: 0s - loss: 0.6682 - accuracy: 0.60 - ETA: 0s - loss: 0.6680 - accuracy: 0.60 - ETA: 0s - loss: 0.6677 - accuracy: 0.60 - ETA: 0s - loss: 0.6674 - accuracy: 0.60 - 3s 259us/step - loss: 0.6670 - accuracy: 0.6084 - val_loss: 0.6380 - val_accuracy: 0.7074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:07 - loss: 0.6929 - accuracy: 0.75 - ETA: 8s - loss: 0.6938 - accuracy: 0.5000 - ETA: 5s - loss: 0.6937 - accuracy: 0.50 - ETA: 4s - loss: 0.6933 - accuracy: 0.51 - ETA: 4s - loss: 0.6925 - accuracy: 0.52 - ETA: 3s - loss: 0.6914 - accuracy: 0.54 - ETA: 3s - loss: 0.6912 - accuracy: 0.55 - ETA: 3s - loss: 0.6903 - accuracy: 0.55 - ETA: 3s - loss: 0.6899 - accuracy: 0.55 - ETA: 3s - loss: 0.6891 - accuracy: 0.56 - ETA: 2s - loss: 0.6881 - accuracy: 0.57 - ETA: 2s - loss: 0.6874 - accuracy: 0.57 - ETA: 2s - loss: 0.6859 - accuracy: 0.58 - ETA: 2s - loss: 0.6850 - accuracy: 0.58 - ETA: 2s - loss: 0.6839 - accuracy: 0.58 - ETA: 2s - loss: 0.6827 - accuracy: 0.58 - ETA: 2s - loss: 0.6819 - accuracy: 0.58 - ETA: 2s - loss: 0.6821 - accuracy: 0.58 - ETA: 2s - loss: 0.6813 - accuracy: 0.58 - ETA: 2s - loss: 0.6811 - accuracy: 0.58 - ETA: 2s - loss: 0.6802 - accuracy: 0.58 - ETA: 2s - loss: 0.6804 - accuracy: 0.58 - ETA: 2s - loss: 0.6793 - accuracy: 0.58 - ETA: 1s - loss: 0.6784 - accuracy: 0.59 - ETA: 1s - loss: 0.6782 - accuracy: 0.59 - ETA: 1s - loss: 0.6781 - accuracy: 0.59 - ETA: 1s - loss: 0.6773 - accuracy: 0.59 - ETA: 1s - loss: 0.6768 - accuracy: 0.59 - ETA: 1s - loss: 0.6763 - accuracy: 0.59 - ETA: 1s - loss: 0.6754 - accuracy: 0.60 - ETA: 1s - loss: 0.6745 - accuracy: 0.60 - ETA: 1s - loss: 0.6743 - accuracy: 0.60 - ETA: 1s - loss: 0.6737 - accuracy: 0.60 - ETA: 1s - loss: 0.6734 - accuracy: 0.60 - ETA: 1s - loss: 0.6728 - accuracy: 0.61 - ETA: 1s - loss: 0.6725 - accuracy: 0.61 - ETA: 1s - loss: 0.6719 - accuracy: 0.61 - ETA: 1s - loss: 0.6713 - accuracy: 0.61 - ETA: 1s - loss: 0.6710 - accuracy: 0.61 - ETA: 1s - loss: 0.6703 - accuracy: 0.61 - ETA: 1s - loss: 0.6698 - accuracy: 0.61 - ETA: 0s - loss: 0.6695 - accuracy: 0.61 - ETA: 0s - loss: 0.6690 - accuracy: 0.61 - ETA: 0s - loss: 0.6685 - accuracy: 0.61 - ETA: 0s - loss: 0.6681 - accuracy: 0.61 - ETA: 0s - loss: 0.6673 - accuracy: 0.62 - ETA: 0s - loss: 0.6666 - accuracy: 0.62 - ETA: 0s - loss: 0.6658 - accuracy: 0.62 - ETA: 0s - loss: 0.6654 - accuracy: 0.62 - ETA: 0s - loss: 0.6647 - accuracy: 0.62 - ETA: 0s - loss: 0.6644 - accuracy: 0.62 - ETA: 0s - loss: 0.6639 - accuracy: 0.62 - ETA: 0s - loss: 0.6631 - accuracy: 0.62 - ETA: 0s - loss: 0.6632 - accuracy: 0.62 - ETA: 0s - loss: 0.6629 - accuracy: 0.62 - ETA: 0s - loss: 0.6625 - accuracy: 0.63 - ETA: 0s - loss: 0.6624 - accuracy: 0.62 - ETA: 0s - loss: 0.6617 - accuracy: 0.63 - ETA: 0s - loss: 0.6614 - accuracy: 0.63 - 3s 259us/step - loss: 0.6618 - accuracy: 0.6309 - val_loss: 0.6284 - val_accuracy: 0.7067\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:16 - loss: 0.7052 - accuracy: 0.0000e+ - ETA: 8s - loss: 0.6950 - accuracy: 0.4954     - ETA: 5s - loss: 0.6954 - accuracy: 0.49 - ETA: 4s - loss: 0.6920 - accuracy: 0.52 - ETA: 4s - loss: 0.6904 - accuracy: 0.53 - ETA: 3s - loss: 0.6896 - accuracy: 0.53 - ETA: 3s - loss: 0.6889 - accuracy: 0.53 - ETA: 3s - loss: 0.6867 - accuracy: 0.54 - ETA: 3s - loss: 0.6860 - accuracy: 0.54 - ETA: 2s - loss: 0.6844 - accuracy: 0.55 - ETA: 2s - loss: 0.6821 - accuracy: 0.56 - ETA: 2s - loss: 0.6816 - accuracy: 0.56 - ETA: 2s - loss: 0.6803 - accuracy: 0.57 - ETA: 2s - loss: 0.6794 - accuracy: 0.57 - ETA: 2s - loss: 0.6791 - accuracy: 0.57 - ETA: 2s - loss: 0.6781 - accuracy: 0.58 - ETA: 2s - loss: 0.6776 - accuracy: 0.58 - ETA: 2s - loss: 0.6774 - accuracy: 0.58 - ETA: 2s - loss: 0.6766 - accuracy: 0.59 - ETA: 2s - loss: 0.6761 - accuracy: 0.59 - ETA: 2s - loss: 0.6751 - accuracy: 0.59 - ETA: 1s - loss: 0.6743 - accuracy: 0.59 - ETA: 1s - loss: 0.6735 - accuracy: 0.60 - ETA: 1s - loss: 0.6733 - accuracy: 0.60 - ETA: 1s - loss: 0.6723 - accuracy: 0.60 - ETA: 1s - loss: 0.6718 - accuracy: 0.60 - ETA: 1s - loss: 0.6709 - accuracy: 0.61 - ETA: 1s - loss: 0.6702 - accuracy: 0.61 - ETA: 1s - loss: 0.6694 - accuracy: 0.61 - ETA: 1s - loss: 0.6685 - accuracy: 0.61 - ETA: 1s - loss: 0.6671 - accuracy: 0.62 - ETA: 1s - loss: 0.6662 - accuracy: 0.62 - ETA: 1s - loss: 0.6651 - accuracy: 0.62 - ETA: 1s - loss: 0.6641 - accuracy: 0.62 - ETA: 1s - loss: 0.6631 - accuracy: 0.62 - ETA: 1s - loss: 0.6620 - accuracy: 0.62 - ETA: 1s - loss: 0.6611 - accuracy: 0.63 - ETA: 1s - loss: 0.6602 - accuracy: 0.63 - ETA: 1s - loss: 0.6598 - accuracy: 0.63 - ETA: 0s - loss: 0.6591 - accuracy: 0.63 - ETA: 0s - loss: 0.6586 - accuracy: 0.63 - ETA: 0s - loss: 0.6582 - accuracy: 0.63 - ETA: 0s - loss: 0.6573 - accuracy: 0.63 - ETA: 0s - loss: 0.6568 - accuracy: 0.63 - ETA: 0s - loss: 0.6566 - accuracy: 0.63 - ETA: 0s - loss: 0.6559 - accuracy: 0.63 - ETA: 0s - loss: 0.6554 - accuracy: 0.64 - ETA: 0s - loss: 0.6547 - accuracy: 0.64 - ETA: 0s - loss: 0.6547 - accuracy: 0.64 - ETA: 0s - loss: 0.6535 - accuracy: 0.64 - ETA: 0s - loss: 0.6532 - accuracy: 0.64 - ETA: 0s - loss: 0.6530 - accuracy: 0.64 - ETA: 0s - loss: 0.6526 - accuracy: 0.64 - ETA: 0s - loss: 0.6522 - accuracy: 0.64 - ETA: 0s - loss: 0.6518 - accuracy: 0.64 - ETA: 0s - loss: 0.6509 - accuracy: 0.64 - ETA: 0s - loss: 0.6508 - accuracy: 0.64 - ETA: 0s - loss: 0.6507 - accuracy: 0.64 - 3s 254us/step - loss: 0.6503 - accuracy: 0.6476 - val_loss: 0.6190 - val_accuracy: 0.6925\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:04 - loss: 0.6915 - accuracy: 0.50 - ETA: 10s - loss: 0.6934 - accuracy: 0.5147 - ETA: 6s - loss: 0.6935 - accuracy: 0.512 - ETA: 5s - loss: 0.6928 - accuracy: 0.52 - ETA: 4s - loss: 0.6932 - accuracy: 0.52 - ETA: 4s - loss: 0.6933 - accuracy: 0.51 - ETA: 4s - loss: 0.6932 - accuracy: 0.51 - ETA: 3s - loss: 0.6924 - accuracy: 0.52 - ETA: 3s - loss: 0.6919 - accuracy: 0.52 - ETA: 3s - loss: 0.6913 - accuracy: 0.52 - ETA: 3s - loss: 0.6899 - accuracy: 0.53 - ETA: 3s - loss: 0.6903 - accuracy: 0.53 - ETA: 3s - loss: 0.6896 - accuracy: 0.54 - ETA: 2s - loss: 0.6888 - accuracy: 0.54 - ETA: 2s - loss: 0.6872 - accuracy: 0.54 - ETA: 2s - loss: 0.6872 - accuracy: 0.54 - ETA: 2s - loss: 0.6864 - accuracy: 0.54 - ETA: 2s - loss: 0.6858 - accuracy: 0.54 - ETA: 2s - loss: 0.6850 - accuracy: 0.55 - ETA: 2s - loss: 0.6839 - accuracy: 0.55 - ETA: 2s - loss: 0.6833 - accuracy: 0.55 - ETA: 2s - loss: 0.6824 - accuracy: 0.55 - ETA: 2s - loss: 0.6812 - accuracy: 0.55 - ETA: 2s - loss: 0.6811 - accuracy: 0.55 - ETA: 2s - loss: 0.6807 - accuracy: 0.55 - ETA: 2s - loss: 0.6798 - accuracy: 0.56 - ETA: 1s - loss: 0.6796 - accuracy: 0.56 - ETA: 1s - loss: 0.6789 - accuracy: 0.56 - ETA: 1s - loss: 0.6779 - accuracy: 0.56 - ETA: 1s - loss: 0.6774 - accuracy: 0.56 - ETA: 1s - loss: 0.6764 - accuracy: 0.56 - ETA: 1s - loss: 0.6761 - accuracy: 0.57 - ETA: 1s - loss: 0.6752 - accuracy: 0.57 - ETA: 1s - loss: 0.6746 - accuracy: 0.57 - ETA: 1s - loss: 0.6737 - accuracy: 0.57 - ETA: 1s - loss: 0.6731 - accuracy: 0.57 - ETA: 1s - loss: 0.6720 - accuracy: 0.57 - ETA: 1s - loss: 0.6712 - accuracy: 0.57 - ETA: 1s - loss: 0.6710 - accuracy: 0.57 - ETA: 1s - loss: 0.6700 - accuracy: 0.57 - ETA: 1s - loss: 0.6691 - accuracy: 0.57 - ETA: 1s - loss: 0.6680 - accuracy: 0.58 - ETA: 1s - loss: 0.6669 - accuracy: 0.58 - ETA: 1s - loss: 0.6659 - accuracy: 0.58 - ETA: 0s - loss: 0.6652 - accuracy: 0.58 - ETA: 0s - loss: 0.6647 - accuracy: 0.58 - ETA: 0s - loss: 0.6644 - accuracy: 0.58 - ETA: 0s - loss: 0.6635 - accuracy: 0.58 - ETA: 0s - loss: 0.6629 - accuracy: 0.58 - ETA: 0s - loss: 0.6618 - accuracy: 0.59 - ETA: 0s - loss: 0.6615 - accuracy: 0.59 - ETA: 0s - loss: 0.6612 - accuracy: 0.59 - ETA: 0s - loss: 0.6613 - accuracy: 0.59 - ETA: 0s - loss: 0.6606 - accuracy: 0.59 - ETA: 0s - loss: 0.6596 - accuracy: 0.59 - ETA: 0s - loss: 0.6590 - accuracy: 0.59 - ETA: 0s - loss: 0.6594 - accuracy: 0.59 - ETA: 0s - loss: 0.6594 - accuracy: 0.59 - ETA: 0s - loss: 0.6590 - accuracy: 0.59 - ETA: 0s - loss: 0.6584 - accuracy: 0.59 - ETA: 0s - loss: 0.6585 - accuracy: 0.59 - ETA: 0s - loss: 0.6582 - accuracy: 0.59 - ETA: 0s - loss: 0.6579 - accuracy: 0.59 - 3s 275us/step - loss: 0.6574 - accuracy: 0.6002 - val_loss: 0.6109 - val_accuracy: 0.7188\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 107us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:13 - loss: 0.6937 - accuracy: 0.25 - ETA: 9s - loss: 0.6941 - accuracy: 0.5046 - ETA: 6s - loss: 0.6933 - accuracy: 0.51 - ETA: 5s - loss: 0.6935 - accuracy: 0.51 - ETA: 4s - loss: 0.6931 - accuracy: 0.52 - ETA: 4s - loss: 0.6933 - accuracy: 0.52 - ETA: 3s - loss: 0.6935 - accuracy: 0.52 - ETA: 3s - loss: 0.6932 - accuracy: 0.52 - ETA: 3s - loss: 0.6929 - accuracy: 0.53 - ETA: 3s - loss: 0.6927 - accuracy: 0.52 - ETA: 3s - loss: 0.6918 - accuracy: 0.53 - ETA: 3s - loss: 0.6913 - accuracy: 0.53 - ETA: 3s - loss: 0.6900 - accuracy: 0.53 - ETA: 2s - loss: 0.6891 - accuracy: 0.53 - ETA: 2s - loss: 0.6895 - accuracy: 0.53 - ETA: 2s - loss: 0.6897 - accuracy: 0.53 - ETA: 2s - loss: 0.6890 - accuracy: 0.53 - ETA: 2s - loss: 0.6886 - accuracy: 0.54 - ETA: 2s - loss: 0.6876 - accuracy: 0.54 - ETA: 2s - loss: 0.6865 - accuracy: 0.54 - ETA: 2s - loss: 0.6852 - accuracy: 0.55 - ETA: 2s - loss: 0.6841 - accuracy: 0.55 - ETA: 2s - loss: 0.6830 - accuracy: 0.55 - ETA: 2s - loss: 0.6821 - accuracy: 0.55 - ETA: 2s - loss: 0.6812 - accuracy: 0.55 - ETA: 2s - loss: 0.6810 - accuracy: 0.56 - ETA: 1s - loss: 0.6803 - accuracy: 0.56 - ETA: 1s - loss: 0.6786 - accuracy: 0.56 - ETA: 1s - loss: 0.6774 - accuracy: 0.56 - ETA: 1s - loss: 0.6755 - accuracy: 0.57 - ETA: 1s - loss: 0.6741 - accuracy: 0.57 - ETA: 1s - loss: 0.6739 - accuracy: 0.57 - ETA: 1s - loss: 0.6732 - accuracy: 0.57 - ETA: 1s - loss: 0.6722 - accuracy: 0.57 - ETA: 1s - loss: 0.6714 - accuracy: 0.57 - ETA: 1s - loss: 0.6708 - accuracy: 0.57 - ETA: 1s - loss: 0.6701 - accuracy: 0.57 - ETA: 1s - loss: 0.6699 - accuracy: 0.58 - ETA: 1s - loss: 0.6694 - accuracy: 0.58 - ETA: 1s - loss: 0.6689 - accuracy: 0.58 - ETA: 1s - loss: 0.6670 - accuracy: 0.58 - ETA: 1s - loss: 0.6667 - accuracy: 0.58 - ETA: 1s - loss: 0.6662 - accuracy: 0.58 - ETA: 1s - loss: 0.6658 - accuracy: 0.58 - ETA: 0s - loss: 0.6647 - accuracy: 0.58 - ETA: 0s - loss: 0.6649 - accuracy: 0.58 - ETA: 0s - loss: 0.6638 - accuracy: 0.58 - ETA: 0s - loss: 0.6634 - accuracy: 0.58 - ETA: 0s - loss: 0.6627 - accuracy: 0.59 - ETA: 0s - loss: 0.6626 - accuracy: 0.59 - ETA: 0s - loss: 0.6623 - accuracy: 0.59 - ETA: 0s - loss: 0.6617 - accuracy: 0.59 - ETA: 0s - loss: 0.6611 - accuracy: 0.59 - ETA: 0s - loss: 0.6610 - accuracy: 0.59 - ETA: 0s - loss: 0.6607 - accuracy: 0.59 - ETA: 0s - loss: 0.6596 - accuracy: 0.59 - ETA: 0s - loss: 0.6595 - accuracy: 0.59 - ETA: 0s - loss: 0.6589 - accuracy: 0.59 - ETA: 0s - loss: 0.6588 - accuracy: 0.59 - ETA: 0s - loss: 0.6585 - accuracy: 0.59 - ETA: 0s - loss: 0.6581 - accuracy: 0.59 - ETA: 0s - loss: 0.6577 - accuracy: 0.59 - ETA: 0s - loss: 0.6572 - accuracy: 0.59 - 3s 274us/step - loss: 0.6570 - accuracy: 0.5998 - val_loss: 0.6033 - val_accuracy: 0.7188\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 7:07 - loss: 0.6899 - accuracy: 0.75 - ETA: 11s - loss: 0.6938 - accuracy: 0.4800 - ETA: 7s - loss: 0.6938 - accuracy: 0.477 - ETA: 5s - loss: 0.6939 - accuracy: 0.47 - ETA: 4s - loss: 0.6930 - accuracy: 0.49 - ETA: 4s - loss: 0.6910 - accuracy: 0.51 - ETA: 4s - loss: 0.6912 - accuracy: 0.51 - ETA: 3s - loss: 0.6910 - accuracy: 0.51 - ETA: 3s - loss: 0.6909 - accuracy: 0.51 - ETA: 3s - loss: 0.6903 - accuracy: 0.51 - ETA: 3s - loss: 0.6902 - accuracy: 0.51 - ETA: 3s - loss: 0.6902 - accuracy: 0.52 - ETA: 3s - loss: 0.6900 - accuracy: 0.51 - ETA: 2s - loss: 0.6898 - accuracy: 0.51 - ETA: 2s - loss: 0.6896 - accuracy: 0.51 - ETA: 2s - loss: 0.6894 - accuracy: 0.52 - ETA: 2s - loss: 0.6888 - accuracy: 0.52 - ETA: 2s - loss: 0.6877 - accuracy: 0.53 - ETA: 2s - loss: 0.6874 - accuracy: 0.53 - ETA: 2s - loss: 0.6863 - accuracy: 0.53 - ETA: 2s - loss: 0.6860 - accuracy: 0.53 - ETA: 2s - loss: 0.6855 - accuracy: 0.53 - ETA: 2s - loss: 0.6853 - accuracy: 0.53 - ETA: 2s - loss: 0.6847 - accuracy: 0.53 - ETA: 2s - loss: 0.6835 - accuracy: 0.54 - ETA: 2s - loss: 0.6827 - accuracy: 0.54 - ETA: 2s - loss: 0.6820 - accuracy: 0.54 - ETA: 1s - loss: 0.6807 - accuracy: 0.55 - ETA: 1s - loss: 0.6808 - accuracy: 0.54 - ETA: 1s - loss: 0.6798 - accuracy: 0.55 - ETA: 1s - loss: 0.6795 - accuracy: 0.55 - ETA: 1s - loss: 0.6790 - accuracy: 0.55 - ETA: 1s - loss: 0.6781 - accuracy: 0.55 - ETA: 1s - loss: 0.6770 - accuracy: 0.56 - ETA: 1s - loss: 0.6767 - accuracy: 0.56 - ETA: 1s - loss: 0.6765 - accuracy: 0.56 - ETA: 1s - loss: 0.6754 - accuracy: 0.56 - ETA: 1s - loss: 0.6749 - accuracy: 0.57 - ETA: 1s - loss: 0.6746 - accuracy: 0.57 - ETA: 1s - loss: 0.6736 - accuracy: 0.57 - ETA: 1s - loss: 0.6732 - accuracy: 0.57 - ETA: 1s - loss: 0.6723 - accuracy: 0.57 - ETA: 1s - loss: 0.6724 - accuracy: 0.57 - ETA: 1s - loss: 0.6724 - accuracy: 0.57 - ETA: 1s - loss: 0.6720 - accuracy: 0.58 - ETA: 0s - loss: 0.6719 - accuracy: 0.58 - ETA: 0s - loss: 0.6718 - accuracy: 0.58 - ETA: 0s - loss: 0.6715 - accuracy: 0.58 - ETA: 0s - loss: 0.6714 - accuracy: 0.58 - ETA: 0s - loss: 0.6712 - accuracy: 0.58 - ETA: 0s - loss: 0.6706 - accuracy: 0.58 - ETA: 0s - loss: 0.6705 - accuracy: 0.58 - ETA: 0s - loss: 0.6704 - accuracy: 0.58 - ETA: 0s - loss: 0.6698 - accuracy: 0.59 - ETA: 0s - loss: 0.6697 - accuracy: 0.59 - ETA: 0s - loss: 0.6695 - accuracy: 0.59 - ETA: 0s - loss: 0.6687 - accuracy: 0.59 - ETA: 0s - loss: 0.6690 - accuracy: 0.59 - ETA: 0s - loss: 0.6692 - accuracy: 0.59 - ETA: 0s - loss: 0.6691 - accuracy: 0.59 - ETA: 0s - loss: 0.6691 - accuracy: 0.59 - ETA: 0s - loss: 0.6689 - accuracy: 0.59 - ETA: 0s - loss: 0.6692 - accuracy: 0.59 - 4s 277us/step - loss: 0.6687 - accuracy: 0.5972 - val_loss: 0.6334 - val_accuracy: 0.7301\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 104us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:10 - loss: 0.6949 - accuracy: 0.50 - ETA: 10s - loss: 0.6936 - accuracy: 0.5192 - ETA: 6s - loss: 0.6933 - accuracy: 0.536 - ETA: 5s - loss: 0.6921 - accuracy: 0.56 - ETA: 5s - loss: 0.6916 - accuracy: 0.56 - ETA: 4s - loss: 0.6909 - accuracy: 0.57 - ETA: 4s - loss: 0.6912 - accuracy: 0.56 - ETA: 4s - loss: 0.6916 - accuracy: 0.55 - ETA: 3s - loss: 0.6918 - accuracy: 0.54 - ETA: 3s - loss: 0.6915 - accuracy: 0.55 - ETA: 3s - loss: 0.6911 - accuracy: 0.55 - ETA: 3s - loss: 0.6907 - accuracy: 0.55 - ETA: 3s - loss: 0.6904 - accuracy: 0.55 - ETA: 2s - loss: 0.6893 - accuracy: 0.56 - ETA: 2s - loss: 0.6887 - accuracy: 0.56 - ETA: 2s - loss: 0.6881 - accuracy: 0.56 - ETA: 2s - loss: 0.6869 - accuracy: 0.57 - ETA: 2s - loss: 0.6862 - accuracy: 0.57 - ETA: 2s - loss: 0.6855 - accuracy: 0.57 - ETA: 2s - loss: 0.6850 - accuracy: 0.58 - ETA: 2s - loss: 0.6839 - accuracy: 0.58 - ETA: 2s - loss: 0.6837 - accuracy: 0.58 - ETA: 2s - loss: 0.6827 - accuracy: 0.58 - ETA: 2s - loss: 0.6815 - accuracy: 0.59 - ETA: 2s - loss: 0.6795 - accuracy: 0.59 - ETA: 2s - loss: 0.6782 - accuracy: 0.59 - ETA: 1s - loss: 0.6781 - accuracy: 0.59 - ETA: 1s - loss: 0.6769 - accuracy: 0.60 - ETA: 1s - loss: 0.6761 - accuracy: 0.60 - ETA: 1s - loss: 0.6750 - accuracy: 0.60 - ETA: 1s - loss: 0.6749 - accuracy: 0.60 - ETA: 1s - loss: 0.6744 - accuracy: 0.60 - ETA: 1s - loss: 0.6739 - accuracy: 0.60 - ETA: 1s - loss: 0.6729 - accuracy: 0.60 - ETA: 1s - loss: 0.6721 - accuracy: 0.61 - ETA: 1s - loss: 0.6712 - accuracy: 0.61 - ETA: 1s - loss: 0.6699 - accuracy: 0.61 - ETA: 1s - loss: 0.6701 - accuracy: 0.61 - ETA: 1s - loss: 0.6696 - accuracy: 0.61 - ETA: 1s - loss: 0.6693 - accuracy: 0.61 - ETA: 1s - loss: 0.6685 - accuracy: 0.61 - ETA: 1s - loss: 0.6677 - accuracy: 0.62 - ETA: 1s - loss: 0.6673 - accuracy: 0.62 - ETA: 1s - loss: 0.6665 - accuracy: 0.62 - ETA: 0s - loss: 0.6661 - accuracy: 0.62 - ETA: 0s - loss: 0.6652 - accuracy: 0.62 - ETA: 0s - loss: 0.6638 - accuracy: 0.62 - ETA: 0s - loss: 0.6627 - accuracy: 0.62 - ETA: 0s - loss: 0.6626 - accuracy: 0.62 - ETA: 0s - loss: 0.6617 - accuracy: 0.62 - ETA: 0s - loss: 0.6608 - accuracy: 0.63 - ETA: 0s - loss: 0.6602 - accuracy: 0.63 - ETA: 0s - loss: 0.6600 - accuracy: 0.63 - ETA: 0s - loss: 0.6594 - accuracy: 0.63 - ETA: 0s - loss: 0.6588 - accuracy: 0.63 - ETA: 0s - loss: 0.6585 - accuracy: 0.63 - ETA: 0s - loss: 0.6582 - accuracy: 0.63 - ETA: 0s - loss: 0.6580 - accuracy: 0.63 - ETA: 0s - loss: 0.6571 - accuracy: 0.63 - ETA: 0s - loss: 0.6569 - accuracy: 0.63 - ETA: 0s - loss: 0.6563 - accuracy: 0.63 - ETA: 0s - loss: 0.6555 - accuracy: 0.63 - ETA: 0s - loss: 0.6548 - accuracy: 0.63 - 3s 275us/step - loss: 0.6545 - accuracy: 0.6362 - val_loss: 0.6105 - val_accuracy: 0.7152\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:07 - loss: 0.6936 - accuracy: 0.25 - ETA: 9s - loss: 0.6934 - accuracy: 0.5000 - ETA: 6s - loss: 0.6932 - accuracy: 0.51 - ETA: 5s - loss: 0.6930 - accuracy: 0.50 - ETA: 4s - loss: 0.6925 - accuracy: 0.50 - ETA: 4s - loss: 0.6926 - accuracy: 0.50 - ETA: 3s - loss: 0.6921 - accuracy: 0.51 - ETA: 3s - loss: 0.6915 - accuracy: 0.51 - ETA: 3s - loss: 0.6913 - accuracy: 0.51 - ETA: 3s - loss: 0.6901 - accuracy: 0.52 - ETA: 3s - loss: 0.6897 - accuracy: 0.52 - ETA: 3s - loss: 0.6889 - accuracy: 0.53 - ETA: 3s - loss: 0.6886 - accuracy: 0.53 - ETA: 2s - loss: 0.6874 - accuracy: 0.54 - ETA: 2s - loss: 0.6873 - accuracy: 0.54 - ETA: 2s - loss: 0.6860 - accuracy: 0.54 - ETA: 2s - loss: 0.6853 - accuracy: 0.54 - ETA: 2s - loss: 0.6848 - accuracy: 0.55 - ETA: 2s - loss: 0.6833 - accuracy: 0.55 - ETA: 2s - loss: 0.6831 - accuracy: 0.55 - ETA: 2s - loss: 0.6824 - accuracy: 0.56 - ETA: 2s - loss: 0.6817 - accuracy: 0.56 - ETA: 2s - loss: 0.6808 - accuracy: 0.56 - ETA: 2s - loss: 0.6800 - accuracy: 0.56 - ETA: 2s - loss: 0.6796 - accuracy: 0.56 - ETA: 2s - loss: 0.6786 - accuracy: 0.57 - ETA: 1s - loss: 0.6783 - accuracy: 0.57 - ETA: 1s - loss: 0.6777 - accuracy: 0.57 - ETA: 1s - loss: 0.6772 - accuracy: 0.57 - ETA: 1s - loss: 0.6767 - accuracy: 0.57 - ETA: 1s - loss: 0.6755 - accuracy: 0.57 - ETA: 1s - loss: 0.6748 - accuracy: 0.57 - ETA: 1s - loss: 0.6733 - accuracy: 0.57 - ETA: 1s - loss: 0.6725 - accuracy: 0.58 - ETA: 1s - loss: 0.6722 - accuracy: 0.58 - ETA: 1s - loss: 0.6719 - accuracy: 0.58 - ETA: 1s - loss: 0.6710 - accuracy: 0.58 - ETA: 1s - loss: 0.6704 - accuracy: 0.58 - ETA: 1s - loss: 0.6694 - accuracy: 0.58 - ETA: 1s - loss: 0.6682 - accuracy: 0.59 - ETA: 1s - loss: 0.6679 - accuracy: 0.59 - ETA: 1s - loss: 0.6677 - accuracy: 0.59 - ETA: 1s - loss: 0.6669 - accuracy: 0.59 - ETA: 1s - loss: 0.6661 - accuracy: 0.59 - ETA: 0s - loss: 0.6653 - accuracy: 0.59 - ETA: 0s - loss: 0.6648 - accuracy: 0.59 - ETA: 0s - loss: 0.6643 - accuracy: 0.59 - ETA: 0s - loss: 0.6632 - accuracy: 0.60 - ETA: 0s - loss: 0.6635 - accuracy: 0.60 - ETA: 0s - loss: 0.6633 - accuracy: 0.60 - ETA: 0s - loss: 0.6629 - accuracy: 0.60 - ETA: 0s - loss: 0.6627 - accuracy: 0.60 - ETA: 0s - loss: 0.6623 - accuracy: 0.60 - ETA: 0s - loss: 0.6616 - accuracy: 0.60 - ETA: 0s - loss: 0.6607 - accuracy: 0.60 - ETA: 0s - loss: 0.6598 - accuracy: 0.60 - ETA: 0s - loss: 0.6590 - accuracy: 0.60 - ETA: 0s - loss: 0.6588 - accuracy: 0.60 - ETA: 0s - loss: 0.6583 - accuracy: 0.61 - ETA: 0s - loss: 0.6572 - accuracy: 0.61 - ETA: 0s - loss: 0.6566 - accuracy: 0.61 - ETA: 0s - loss: 0.6555 - accuracy: 0.61 - ETA: 0s - loss: 0.6549 - accuracy: 0.61 - 3s 275us/step - loss: 0.6552 - accuracy: 0.6159 - val_loss: 0.6032 - val_accuracy: 0.7202\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:13 - loss: 0.6939 - accuracy: 0.25 - ETA: 10s - loss: 0.6938 - accuracy: 0.4856 - ETA: 6s - loss: 0.6930 - accuracy: 0.526 - ETA: 5s - loss: 0.6926 - accuracy: 0.52 - ETA: 4s - loss: 0.6927 - accuracy: 0.51 - ETA: 4s - loss: 0.6924 - accuracy: 0.52 - ETA: 3s - loss: 0.6927 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.50 - ETA: 3s - loss: 0.6914 - accuracy: 0.51 - ETA: 3s - loss: 0.6916 - accuracy: 0.51 - ETA: 3s - loss: 0.6907 - accuracy: 0.51 - ETA: 3s - loss: 0.6907 - accuracy: 0.51 - ETA: 3s - loss: 0.6902 - accuracy: 0.51 - ETA: 2s - loss: 0.6899 - accuracy: 0.51 - ETA: 2s - loss: 0.6894 - accuracy: 0.51 - ETA: 2s - loss: 0.6880 - accuracy: 0.52 - ETA: 2s - loss: 0.6875 - accuracy: 0.52 - ETA: 2s - loss: 0.6854 - accuracy: 0.52 - ETA: 2s - loss: 0.6839 - accuracy: 0.53 - ETA: 2s - loss: 0.6835 - accuracy: 0.53 - ETA: 2s - loss: 0.6824 - accuracy: 0.53 - ETA: 2s - loss: 0.6814 - accuracy: 0.53 - ETA: 2s - loss: 0.6817 - accuracy: 0.54 - ETA: 2s - loss: 0.6808 - accuracy: 0.54 - ETA: 2s - loss: 0.6806 - accuracy: 0.54 - ETA: 2s - loss: 0.6806 - accuracy: 0.54 - ETA: 2s - loss: 0.6795 - accuracy: 0.55 - ETA: 1s - loss: 0.6783 - accuracy: 0.55 - ETA: 1s - loss: 0.6773 - accuracy: 0.55 - ETA: 1s - loss: 0.6767 - accuracy: 0.56 - ETA: 1s - loss: 0.6764 - accuracy: 0.56 - ETA: 1s - loss: 0.6759 - accuracy: 0.56 - ETA: 1s - loss: 0.6759 - accuracy: 0.56 - ETA: 1s - loss: 0.6754 - accuracy: 0.56 - ETA: 1s - loss: 0.6750 - accuracy: 0.56 - ETA: 1s - loss: 0.6739 - accuracy: 0.57 - ETA: 1s - loss: 0.6732 - accuracy: 0.57 - ETA: 1s - loss: 0.6724 - accuracy: 0.57 - ETA: 1s - loss: 0.6724 - accuracy: 0.57 - ETA: 1s - loss: 0.6717 - accuracy: 0.58 - ETA: 1s - loss: 0.6709 - accuracy: 0.58 - ETA: 1s - loss: 0.6701 - accuracy: 0.58 - ETA: 1s - loss: 0.6690 - accuracy: 0.58 - ETA: 1s - loss: 0.6687 - accuracy: 0.58 - ETA: 1s - loss: 0.6683 - accuracy: 0.59 - ETA: 0s - loss: 0.6681 - accuracy: 0.59 - ETA: 0s - loss: 0.6672 - accuracy: 0.59 - ETA: 0s - loss: 0.6666 - accuracy: 0.59 - ETA: 0s - loss: 0.6663 - accuracy: 0.59 - ETA: 0s - loss: 0.6657 - accuracy: 0.59 - ETA: 0s - loss: 0.6653 - accuracy: 0.59 - ETA: 0s - loss: 0.6642 - accuracy: 0.59 - ETA: 0s - loss: 0.6631 - accuracy: 0.60 - ETA: 0s - loss: 0.6629 - accuracy: 0.60 - ETA: 0s - loss: 0.6628 - accuracy: 0.60 - ETA: 0s - loss: 0.6617 - accuracy: 0.60 - ETA: 0s - loss: 0.6614 - accuracy: 0.60 - ETA: 0s - loss: 0.6612 - accuracy: 0.60 - ETA: 0s - loss: 0.6606 - accuracy: 0.60 - ETA: 0s - loss: 0.6604 - accuracy: 0.60 - ETA: 0s - loss: 0.6601 - accuracy: 0.60 - ETA: 0s - loss: 0.6604 - accuracy: 0.60 - ETA: 0s - loss: 0.6596 - accuracy: 0.60 - ETA: 0s - loss: 0.6594 - accuracy: 0.60 - 4s 277us/step - loss: 0.6592 - accuracy: 0.6058 - val_loss: 0.6143 - val_accuracy: 0.7188\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 106us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 6:10 - loss: 0.7008 - accuracy: 0.25 - ETA: 9s - loss: 0.6947 - accuracy: 0.4764 - ETA: 6s - loss: 0.6942 - accuracy: 0.48 - ETA: 5s - loss: 0.6941 - accuracy: 0.50 - ETA: 4s - loss: 0.6940 - accuracy: 0.50 - ETA: 4s - loss: 0.6934 - accuracy: 0.52 - ETA: 3s - loss: 0.6931 - accuracy: 0.52 - ETA: 3s - loss: 0.6923 - accuracy: 0.53 - ETA: 3s - loss: 0.6914 - accuracy: 0.53 - ETA: 3s - loss: 0.6908 - accuracy: 0.53 - ETA: 3s - loss: 0.6894 - accuracy: 0.54 - ETA: 3s - loss: 0.6886 - accuracy: 0.55 - ETA: 2s - loss: 0.6865 - accuracy: 0.55 - ETA: 2s - loss: 0.6859 - accuracy: 0.55 - ETA: 2s - loss: 0.6843 - accuracy: 0.56 - ETA: 2s - loss: 0.6828 - accuracy: 0.56 - ETA: 2s - loss: 0.6814 - accuracy: 0.56 - ETA: 2s - loss: 0.6798 - accuracy: 0.57 - ETA: 2s - loss: 0.6789 - accuracy: 0.56 - ETA: 2s - loss: 0.6783 - accuracy: 0.56 - ETA: 2s - loss: 0.6772 - accuracy: 0.57 - ETA: 2s - loss: 0.6753 - accuracy: 0.57 - ETA: 2s - loss: 0.6739 - accuracy: 0.57 - ETA: 2s - loss: 0.6745 - accuracy: 0.57 - ETA: 2s - loss: 0.6733 - accuracy: 0.57 - ETA: 2s - loss: 0.6728 - accuracy: 0.57 - ETA: 1s - loss: 0.6713 - accuracy: 0.57 - ETA: 1s - loss: 0.6694 - accuracy: 0.58 - ETA: 1s - loss: 0.6675 - accuracy: 0.58 - ETA: 1s - loss: 0.6669 - accuracy: 0.58 - ETA: 1s - loss: 0.6658 - accuracy: 0.58 - ETA: 1s - loss: 0.6647 - accuracy: 0.59 - ETA: 1s - loss: 0.6641 - accuracy: 0.59 - ETA: 1s - loss: 0.6628 - accuracy: 0.59 - ETA: 1s - loss: 0.6626 - accuracy: 0.59 - ETA: 1s - loss: 0.6618 - accuracy: 0.59 - ETA: 1s - loss: 0.6608 - accuracy: 0.59 - ETA: 1s - loss: 0.6596 - accuracy: 0.60 - ETA: 1s - loss: 0.6597 - accuracy: 0.60 - ETA: 1s - loss: 0.6595 - accuracy: 0.60 - ETA: 1s - loss: 0.6585 - accuracy: 0.60 - ETA: 1s - loss: 0.6585 - accuracy: 0.60 - ETA: 1s - loss: 0.6572 - accuracy: 0.60 - ETA: 1s - loss: 0.6575 - accuracy: 0.60 - ETA: 0s - loss: 0.6571 - accuracy: 0.60 - ETA: 0s - loss: 0.6562 - accuracy: 0.60 - ETA: 0s - loss: 0.6561 - accuracy: 0.61 - ETA: 0s - loss: 0.6559 - accuracy: 0.61 - ETA: 0s - loss: 0.6557 - accuracy: 0.61 - ETA: 0s - loss: 0.6550 - accuracy: 0.61 - ETA: 0s - loss: 0.6546 - accuracy: 0.61 - ETA: 0s - loss: 0.6539 - accuracy: 0.61 - ETA: 0s - loss: 0.6528 - accuracy: 0.61 - ETA: 0s - loss: 0.6526 - accuracy: 0.61 - ETA: 0s - loss: 0.6522 - accuracy: 0.62 - ETA: 0s - loss: 0.6521 - accuracy: 0.62 - ETA: 0s - loss: 0.6517 - accuracy: 0.62 - ETA: 0s - loss: 0.6516 - accuracy: 0.62 - ETA: 0s - loss: 0.6517 - accuracy: 0.62 - ETA: 0s - loss: 0.6510 - accuracy: 0.62 - ETA: 0s - loss: 0.6501 - accuracy: 0.62 - ETA: 0s - loss: 0.6501 - accuracy: 0.62 - ETA: 0s - loss: 0.6496 - accuracy: 0.62 - 3s 273us/step - loss: 0.6496 - accuracy: 0.6282 - val_loss: 0.6013 - val_accuracy: 0.7202\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:32 - loss: 0.7005 - accuracy: 0.25 - ETA: 10s - loss: 0.6952 - accuracy: 0.5142 - ETA: 6s - loss: 0.6949 - accuracy: 0.495 - ETA: 5s - loss: 0.6947 - accuracy: 0.49 - ETA: 4s - loss: 0.6942 - accuracy: 0.50 - ETA: 4s - loss: 0.6938 - accuracy: 0.51 - ETA: 3s - loss: 0.6933 - accuracy: 0.50 - ETA: 3s - loss: 0.6923 - accuracy: 0.51 - ETA: 3s - loss: 0.6905 - accuracy: 0.51 - ETA: 3s - loss: 0.6906 - accuracy: 0.51 - ETA: 3s - loss: 0.6904 - accuracy: 0.51 - ETA: 3s - loss: 0.6894 - accuracy: 0.52 - ETA: 3s - loss: 0.6885 - accuracy: 0.52 - ETA: 2s - loss: 0.6879 - accuracy: 0.52 - ETA: 2s - loss: 0.6864 - accuracy: 0.53 - ETA: 2s - loss: 0.6857 - accuracy: 0.53 - ETA: 2s - loss: 0.6848 - accuracy: 0.53 - ETA: 2s - loss: 0.6845 - accuracy: 0.53 - ETA: 2s - loss: 0.6841 - accuracy: 0.53 - ETA: 2s - loss: 0.6833 - accuracy: 0.53 - ETA: 2s - loss: 0.6834 - accuracy: 0.53 - ETA: 2s - loss: 0.6824 - accuracy: 0.54 - ETA: 2s - loss: 0.6820 - accuracy: 0.54 - ETA: 2s - loss: 0.6809 - accuracy: 0.54 - ETA: 2s - loss: 0.6801 - accuracy: 0.54 - ETA: 2s - loss: 0.6789 - accuracy: 0.55 - ETA: 2s - loss: 0.6783 - accuracy: 0.55 - ETA: 1s - loss: 0.6772 - accuracy: 0.55 - ETA: 1s - loss: 0.6764 - accuracy: 0.56 - ETA: 1s - loss: 0.6758 - accuracy: 0.56 - ETA: 1s - loss: 0.6745 - accuracy: 0.56 - ETA: 1s - loss: 0.6735 - accuracy: 0.56 - ETA: 1s - loss: 0.6718 - accuracy: 0.57 - ETA: 1s - loss: 0.6699 - accuracy: 0.57 - ETA: 1s - loss: 0.6683 - accuracy: 0.57 - ETA: 1s - loss: 0.6676 - accuracy: 0.57 - ETA: 1s - loss: 0.6666 - accuracy: 0.57 - ETA: 1s - loss: 0.6657 - accuracy: 0.58 - ETA: 1s - loss: 0.6649 - accuracy: 0.58 - ETA: 1s - loss: 0.6642 - accuracy: 0.58 - ETA: 1s - loss: 0.6638 - accuracy: 0.58 - ETA: 1s - loss: 0.6631 - accuracy: 0.58 - ETA: 1s - loss: 0.6625 - accuracy: 0.58 - ETA: 1s - loss: 0.6620 - accuracy: 0.59 - ETA: 0s - loss: 0.6623 - accuracy: 0.59 - ETA: 0s - loss: 0.6617 - accuracy: 0.59 - ETA: 0s - loss: 0.6611 - accuracy: 0.59 - ETA: 0s - loss: 0.6605 - accuracy: 0.59 - ETA: 0s - loss: 0.6602 - accuracy: 0.59 - ETA: 0s - loss: 0.6599 - accuracy: 0.59 - ETA: 0s - loss: 0.6597 - accuracy: 0.59 - ETA: 0s - loss: 0.6600 - accuracy: 0.59 - ETA: 0s - loss: 0.6592 - accuracy: 0.60 - ETA: 0s - loss: 0.6583 - accuracy: 0.60 - ETA: 0s - loss: 0.6580 - accuracy: 0.60 - ETA: 0s - loss: 0.6571 - accuracy: 0.60 - ETA: 0s - loss: 0.6568 - accuracy: 0.60 - ETA: 0s - loss: 0.6558 - accuracy: 0.60 - ETA: 0s - loss: 0.6554 - accuracy: 0.61 - ETA: 0s - loss: 0.6549 - accuracy: 0.61 - ETA: 0s - loss: 0.6538 - accuracy: 0.61 - ETA: 0s - loss: 0.6536 - accuracy: 0.61 - ETA: 0s - loss: 0.6527 - accuracy: 0.61 - 3s 275us/step - loss: 0.6524 - accuracy: 0.6172 - val_loss: 0.5924 - val_accuracy: 0.7173\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 104us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:23 - loss: 0.6992 - accuracy: 0.0000e+ - ETA: 10s - loss: 0.6938 - accuracy: 0.5102     - ETA: 6s - loss: 0.6938 - accuracy: 0.509 - ETA: 5s - loss: 0.6934 - accuracy: 0.50 - ETA: 4s - loss: 0.6930 - accuracy: 0.52 - ETA: 4s - loss: 0.6927 - accuracy: 0.52 - ETA: 4s - loss: 0.6919 - accuracy: 0.53 - ETA: 3s - loss: 0.6904 - accuracy: 0.53 - ETA: 3s - loss: 0.6891 - accuracy: 0.53 - ETA: 3s - loss: 0.6880 - accuracy: 0.54 - ETA: 3s - loss: 0.6866 - accuracy: 0.54 - ETA: 3s - loss: 0.6858 - accuracy: 0.55 - ETA: 3s - loss: 0.6830 - accuracy: 0.56 - ETA: 3s - loss: 0.6821 - accuracy: 0.56 - ETA: 2s - loss: 0.6812 - accuracy: 0.56 - ETA: 2s - loss: 0.6805 - accuracy: 0.56 - ETA: 2s - loss: 0.6785 - accuracy: 0.57 - ETA: 2s - loss: 0.6791 - accuracy: 0.57 - ETA: 2s - loss: 0.6777 - accuracy: 0.57 - ETA: 2s - loss: 0.6753 - accuracy: 0.58 - ETA: 2s - loss: 0.6744 - accuracy: 0.58 - ETA: 2s - loss: 0.6738 - accuracy: 0.58 - ETA: 2s - loss: 0.6715 - accuracy: 0.59 - ETA: 2s - loss: 0.6704 - accuracy: 0.59 - ETA: 2s - loss: 0.6689 - accuracy: 0.59 - ETA: 2s - loss: 0.6662 - accuracy: 0.60 - ETA: 2s - loss: 0.6656 - accuracy: 0.60 - ETA: 2s - loss: 0.6645 - accuracy: 0.60 - ETA: 1s - loss: 0.6636 - accuracy: 0.60 - ETA: 1s - loss: 0.6628 - accuracy: 0.60 - ETA: 1s - loss: 0.6623 - accuracy: 0.60 - ETA: 1s - loss: 0.6608 - accuracy: 0.61 - ETA: 1s - loss: 0.6596 - accuracy: 0.61 - ETA: 1s - loss: 0.6588 - accuracy: 0.61 - ETA: 1s - loss: 0.6585 - accuracy: 0.61 - ETA: 1s - loss: 0.6588 - accuracy: 0.61 - ETA: 1s - loss: 0.6578 - accuracy: 0.61 - ETA: 1s - loss: 0.6573 - accuracy: 0.61 - ETA: 1s - loss: 0.6567 - accuracy: 0.61 - ETA: 1s - loss: 0.6561 - accuracy: 0.62 - ETA: 1s - loss: 0.6550 - accuracy: 0.62 - ETA: 1s - loss: 0.6542 - accuracy: 0.62 - ETA: 1s - loss: 0.6537 - accuracy: 0.62 - ETA: 1s - loss: 0.6539 - accuracy: 0.62 - ETA: 1s - loss: 0.6536 - accuracy: 0.62 - ETA: 0s - loss: 0.6530 - accuracy: 0.62 - ETA: 0s - loss: 0.6525 - accuracy: 0.62 - ETA: 0s - loss: 0.6519 - accuracy: 0.62 - ETA: 0s - loss: 0.6518 - accuracy: 0.62 - ETA: 0s - loss: 0.6515 - accuracy: 0.62 - ETA: 0s - loss: 0.6514 - accuracy: 0.62 - ETA: 0s - loss: 0.6504 - accuracy: 0.63 - ETA: 0s - loss: 0.6493 - accuracy: 0.63 - ETA: 0s - loss: 0.6485 - accuracy: 0.63 - ETA: 0s - loss: 0.6476 - accuracy: 0.63 - ETA: 0s - loss: 0.6461 - accuracy: 0.63 - ETA: 0s - loss: 0.6447 - accuracy: 0.63 - ETA: 0s - loss: 0.6440 - accuracy: 0.64 - ETA: 0s - loss: 0.6435 - accuracy: 0.64 - ETA: 0s - loss: 0.6427 - accuracy: 0.64 - ETA: 0s - loss: 0.6420 - accuracy: 0.64 - ETA: 0s - loss: 0.6415 - accuracy: 0.64 - ETA: 0s - loss: 0.6413 - accuracy: 0.64 - ETA: 0s - loss: 0.6412 - accuracy: 0.64 - 4s 280us/step - loss: 0.6412 - accuracy: 0.6460 - val_loss: 0.5856 - val_accuracy: 0.7237\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:10 - loss: 0.7051 - accuracy: 0.50 - ETA: 9s - loss: 0.6927 - accuracy: 0.5370 - ETA: 6s - loss: 0.6934 - accuracy: 0.52 - ETA: 5s - loss: 0.6926 - accuracy: 0.54 - ETA: 4s - loss: 0.6923 - accuracy: 0.53 - ETA: 4s - loss: 0.6899 - accuracy: 0.53 - ETA: 3s - loss: 0.6898 - accuracy: 0.53 - ETA: 3s - loss: 0.6890 - accuracy: 0.53 - ETA: 3s - loss: 0.6880 - accuracy: 0.54 - ETA: 3s - loss: 0.6852 - accuracy: 0.55 - ETA: 3s - loss: 0.6833 - accuracy: 0.56 - ETA: 3s - loss: 0.6818 - accuracy: 0.56 - ETA: 2s - loss: 0.6801 - accuracy: 0.56 - ETA: 2s - loss: 0.6780 - accuracy: 0.57 - ETA: 2s - loss: 0.6761 - accuracy: 0.57 - ETA: 2s - loss: 0.6747 - accuracy: 0.57 - ETA: 2s - loss: 0.6725 - accuracy: 0.58 - ETA: 2s - loss: 0.6718 - accuracy: 0.58 - ETA: 2s - loss: 0.6710 - accuracy: 0.58 - ETA: 2s - loss: 0.6704 - accuracy: 0.58 - ETA: 2s - loss: 0.6703 - accuracy: 0.58 - ETA: 2s - loss: 0.6683 - accuracy: 0.59 - ETA: 2s - loss: 0.6650 - accuracy: 0.59 - ETA: 2s - loss: 0.6637 - accuracy: 0.60 - ETA: 2s - loss: 0.6626 - accuracy: 0.60 - ETA: 2s - loss: 0.6608 - accuracy: 0.60 - ETA: 1s - loss: 0.6597 - accuracy: 0.60 - ETA: 1s - loss: 0.6592 - accuracy: 0.60 - ETA: 1s - loss: 0.6585 - accuracy: 0.60 - ETA: 1s - loss: 0.6575 - accuracy: 0.61 - ETA: 1s - loss: 0.6564 - accuracy: 0.61 - ETA: 1s - loss: 0.6552 - accuracy: 0.61 - ETA: 1s - loss: 0.6548 - accuracy: 0.61 - ETA: 1s - loss: 0.6538 - accuracy: 0.61 - ETA: 1s - loss: 0.6526 - accuracy: 0.62 - ETA: 1s - loss: 0.6510 - accuracy: 0.62 - ETA: 1s - loss: 0.6510 - accuracy: 0.62 - ETA: 1s - loss: 0.6508 - accuracy: 0.62 - ETA: 1s - loss: 0.6497 - accuracy: 0.62 - ETA: 1s - loss: 0.6485 - accuracy: 0.62 - ETA: 1s - loss: 0.6479 - accuracy: 0.62 - ETA: 1s - loss: 0.6478 - accuracy: 0.63 - ETA: 1s - loss: 0.6475 - accuracy: 0.63 - ETA: 1s - loss: 0.6462 - accuracy: 0.63 - ETA: 0s - loss: 0.6457 - accuracy: 0.63 - ETA: 0s - loss: 0.6449 - accuracy: 0.63 - ETA: 0s - loss: 0.6439 - accuracy: 0.63 - ETA: 0s - loss: 0.6423 - accuracy: 0.63 - ETA: 0s - loss: 0.6417 - accuracy: 0.64 - ETA: 0s - loss: 0.6407 - accuracy: 0.64 - ETA: 0s - loss: 0.6405 - accuracy: 0.64 - ETA: 0s - loss: 0.6395 - accuracy: 0.64 - ETA: 0s - loss: 0.6386 - accuracy: 0.64 - ETA: 0s - loss: 0.6379 - accuracy: 0.64 - ETA: 0s - loss: 0.6379 - accuracy: 0.64 - ETA: 0s - loss: 0.6373 - accuracy: 0.64 - ETA: 0s - loss: 0.6368 - accuracy: 0.64 - ETA: 0s - loss: 0.6363 - accuracy: 0.64 - ETA: 0s - loss: 0.6366 - accuracy: 0.64 - ETA: 0s - loss: 0.6360 - accuracy: 0.64 - ETA: 0s - loss: 0.6361 - accuracy: 0.64 - ETA: 0s - loss: 0.6360 - accuracy: 0.65 - ETA: 0s - loss: 0.6351 - accuracy: 0.65 - ETA: 0s - loss: 0.6347 - accuracy: 0.65 - 4s 277us/step - loss: 0.6347 - accuracy: 0.6511 - val_loss: 0.5842 - val_accuracy: 0.7223\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 97us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 6:16 - loss: 0.6974 - accuracy: 0.25 - ETA: 9s - loss: 0.6944 - accuracy: 0.5000 - ETA: 6s - loss: 0.6940 - accuracy: 0.51 - ETA: 5s - loss: 0.6923 - accuracy: 0.54 - ETA: 4s - loss: 0.6930 - accuracy: 0.53 - ETA: 4s - loss: 0.6933 - accuracy: 0.52 - ETA: 3s - loss: 0.6924 - accuracy: 0.53 - ETA: 3s - loss: 0.6917 - accuracy: 0.53 - ETA: 3s - loss: 0.6920 - accuracy: 0.52 - ETA: 3s - loss: 0.6918 - accuracy: 0.52 - ETA: 3s - loss: 0.6909 - accuracy: 0.53 - ETA: 3s - loss: 0.6897 - accuracy: 0.53 - ETA: 3s - loss: 0.6896 - accuracy: 0.53 - ETA: 2s - loss: 0.6881 - accuracy: 0.54 - ETA: 2s - loss: 0.6873 - accuracy: 0.54 - ETA: 2s - loss: 0.6868 - accuracy: 0.54 - ETA: 2s - loss: 0.6854 - accuracy: 0.54 - ETA: 2s - loss: 0.6844 - accuracy: 0.54 - ETA: 2s - loss: 0.6822 - accuracy: 0.55 - ETA: 2s - loss: 0.6811 - accuracy: 0.55 - ETA: 2s - loss: 0.6812 - accuracy: 0.55 - ETA: 2s - loss: 0.6796 - accuracy: 0.55 - ETA: 2s - loss: 0.6786 - accuracy: 0.55 - ETA: 2s - loss: 0.6778 - accuracy: 0.55 - ETA: 2s - loss: 0.6761 - accuracy: 0.56 - ETA: 2s - loss: 0.6749 - accuracy: 0.56 - ETA: 2s - loss: 0.6736 - accuracy: 0.56 - ETA: 1s - loss: 0.6731 - accuracy: 0.56 - ETA: 1s - loss: 0.6720 - accuracy: 0.56 - ETA: 1s - loss: 0.6716 - accuracy: 0.57 - ETA: 1s - loss: 0.6711 - accuracy: 0.57 - ETA: 1s - loss: 0.6708 - accuracy: 0.57 - ETA: 1s - loss: 0.6704 - accuracy: 0.57 - ETA: 1s - loss: 0.6699 - accuracy: 0.57 - ETA: 1s - loss: 0.6698 - accuracy: 0.57 - ETA: 1s - loss: 0.6698 - accuracy: 0.57 - ETA: 1s - loss: 0.6697 - accuracy: 0.57 - ETA: 1s - loss: 0.6696 - accuracy: 0.57 - ETA: 1s - loss: 0.6683 - accuracy: 0.58 - ETA: 1s - loss: 0.6673 - accuracy: 0.58 - ETA: 1s - loss: 0.6659 - accuracy: 0.58 - ETA: 1s - loss: 0.6656 - accuracy: 0.58 - ETA: 1s - loss: 0.6649 - accuracy: 0.58 - ETA: 1s - loss: 0.6639 - accuracy: 0.58 - ETA: 0s - loss: 0.6625 - accuracy: 0.58 - ETA: 0s - loss: 0.6621 - accuracy: 0.58 - ETA: 0s - loss: 0.6608 - accuracy: 0.59 - ETA: 0s - loss: 0.6600 - accuracy: 0.59 - ETA: 0s - loss: 0.6600 - accuracy: 0.59 - ETA: 0s - loss: 0.6601 - accuracy: 0.59 - ETA: 0s - loss: 0.6592 - accuracy: 0.59 - ETA: 0s - loss: 0.6585 - accuracy: 0.60 - ETA: 0s - loss: 0.6577 - accuracy: 0.60 - ETA: 0s - loss: 0.6575 - accuracy: 0.60 - ETA: 0s - loss: 0.6578 - accuracy: 0.60 - ETA: 0s - loss: 0.6574 - accuracy: 0.60 - ETA: 0s - loss: 0.6570 - accuracy: 0.60 - ETA: 0s - loss: 0.6561 - accuracy: 0.60 - ETA: 0s - loss: 0.6557 - accuracy: 0.60 - ETA: 0s - loss: 0.6559 - accuracy: 0.60 - ETA: 0s - loss: 0.6556 - accuracy: 0.60 - ETA: 0s - loss: 0.6551 - accuracy: 0.61 - ETA: 0s - loss: 0.6548 - accuracy: 0.61 - 3s 275us/step - loss: 0.6545 - accuracy: 0.6122 - val_loss: 0.5969 - val_accuracy: 0.7251\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:29 - loss: 0.6949 - accuracy: 0.0000e+ - ETA: 10s - loss: 0.6941 - accuracy: 0.5048     - ETA: 6s - loss: 0.6933 - accuracy: 0.502 - ETA: 5s - loss: 0.6933 - accuracy: 0.50 - ETA: 4s - loss: 0.6921 - accuracy: 0.51 - ETA: 4s - loss: 0.6913 - accuracy: 0.52 - ETA: 4s - loss: 0.6905 - accuracy: 0.53 - ETA: 3s - loss: 0.6895 - accuracy: 0.53 - ETA: 3s - loss: 0.6882 - accuracy: 0.54 - ETA: 3s - loss: 0.6858 - accuracy: 0.55 - ETA: 3s - loss: 0.6843 - accuracy: 0.55 - ETA: 3s - loss: 0.6837 - accuracy: 0.55 - ETA: 3s - loss: 0.6831 - accuracy: 0.55 - ETA: 3s - loss: 0.6807 - accuracy: 0.56 - ETA: 2s - loss: 0.6798 - accuracy: 0.56 - ETA: 2s - loss: 0.6771 - accuracy: 0.57 - ETA: 2s - loss: 0.6745 - accuracy: 0.57 - ETA: 2s - loss: 0.6734 - accuracy: 0.57 - ETA: 2s - loss: 0.6723 - accuracy: 0.58 - ETA: 2s - loss: 0.6699 - accuracy: 0.58 - ETA: 2s - loss: 0.6689 - accuracy: 0.58 - ETA: 2s - loss: 0.6669 - accuracy: 0.59 - ETA: 2s - loss: 0.6657 - accuracy: 0.59 - ETA: 2s - loss: 0.6639 - accuracy: 0.59 - ETA: 2s - loss: 0.6631 - accuracy: 0.59 - ETA: 2s - loss: 0.6613 - accuracy: 0.59 - ETA: 2s - loss: 0.6605 - accuracy: 0.59 - ETA: 2s - loss: 0.6594 - accuracy: 0.60 - ETA: 2s - loss: 0.6576 - accuracy: 0.60 - ETA: 1s - loss: 0.6576 - accuracy: 0.60 - ETA: 1s - loss: 0.6566 - accuracy: 0.60 - ETA: 1s - loss: 0.6554 - accuracy: 0.60 - ETA: 1s - loss: 0.6552 - accuracy: 0.60 - ETA: 1s - loss: 0.6552 - accuracy: 0.60 - ETA: 1s - loss: 0.6550 - accuracy: 0.60 - ETA: 1s - loss: 0.6539 - accuracy: 0.61 - ETA: 1s - loss: 0.6540 - accuracy: 0.61 - ETA: 1s - loss: 0.6513 - accuracy: 0.61 - ETA: 1s - loss: 0.6519 - accuracy: 0.61 - ETA: 1s - loss: 0.6517 - accuracy: 0.61 - ETA: 1s - loss: 0.6505 - accuracy: 0.61 - ETA: 1s - loss: 0.6484 - accuracy: 0.61 - ETA: 1s - loss: 0.6478 - accuracy: 0.62 - ETA: 1s - loss: 0.6475 - accuracy: 0.62 - ETA: 1s - loss: 0.6471 - accuracy: 0.62 - ETA: 1s - loss: 0.6460 - accuracy: 0.62 - ETA: 0s - loss: 0.6451 - accuracy: 0.62 - ETA: 0s - loss: 0.6446 - accuracy: 0.62 - ETA: 0s - loss: 0.6442 - accuracy: 0.62 - ETA: 0s - loss: 0.6434 - accuracy: 0.62 - ETA: 0s - loss: 0.6432 - accuracy: 0.62 - ETA: 0s - loss: 0.6426 - accuracy: 0.62 - ETA: 0s - loss: 0.6428 - accuracy: 0.62 - ETA: 0s - loss: 0.6433 - accuracy: 0.62 - ETA: 0s - loss: 0.6429 - accuracy: 0.62 - ETA: 0s - loss: 0.6427 - accuracy: 0.62 - ETA: 0s - loss: 0.6424 - accuracy: 0.62 - ETA: 0s - loss: 0.6418 - accuracy: 0.63 - ETA: 0s - loss: 0.6414 - accuracy: 0.63 - ETA: 0s - loss: 0.6408 - accuracy: 0.63 - ETA: 0s - loss: 0.6408 - accuracy: 0.63 - ETA: 0s - loss: 0.6412 - accuracy: 0.63 - ETA: 0s - loss: 0.6407 - accuracy: 0.63 - ETA: 0s - loss: 0.6404 - accuracy: 0.63 - ETA: 0s - loss: 0.6397 - accuracy: 0.63 - 4s 281us/step - loss: 0.6396 - accuracy: 0.6346 - val_loss: 0.5939 - val_accuracy: 0.7159\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 116us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:10 - loss: 0.6826 - accuracy: 0.75 - ETA: 10s - loss: 0.6939 - accuracy: 0.4856 - ETA: 6s - loss: 0.6922 - accuracy: 0.521 - ETA: 5s - loss: 0.6920 - accuracy: 0.52 - ETA: 4s - loss: 0.6918 - accuracy: 0.52 - ETA: 4s - loss: 0.6911 - accuracy: 0.53 - ETA: 3s - loss: 0.6901 - accuracy: 0.54 - ETA: 3s - loss: 0.6870 - accuracy: 0.55 - ETA: 3s - loss: 0.6858 - accuracy: 0.55 - ETA: 3s - loss: 0.6852 - accuracy: 0.56 - ETA: 3s - loss: 0.6830 - accuracy: 0.57 - ETA: 3s - loss: 0.6797 - accuracy: 0.58 - ETA: 2s - loss: 0.6785 - accuracy: 0.58 - ETA: 2s - loss: 0.6771 - accuracy: 0.59 - ETA: 2s - loss: 0.6744 - accuracy: 0.59 - ETA: 2s - loss: 0.6723 - accuracy: 0.60 - ETA: 2s - loss: 0.6710 - accuracy: 0.60 - ETA: 2s - loss: 0.6702 - accuracy: 0.60 - ETA: 2s - loss: 0.6702 - accuracy: 0.60 - ETA: 2s - loss: 0.6683 - accuracy: 0.61 - ETA: 2s - loss: 0.6663 - accuracy: 0.61 - ETA: 2s - loss: 0.6648 - accuracy: 0.61 - ETA: 2s - loss: 0.6630 - accuracy: 0.62 - ETA: 2s - loss: 0.6621 - accuracy: 0.62 - ETA: 2s - loss: 0.6609 - accuracy: 0.62 - ETA: 2s - loss: 0.6586 - accuracy: 0.62 - ETA: 1s - loss: 0.6567 - accuracy: 0.62 - ETA: 1s - loss: 0.6547 - accuracy: 0.63 - ETA: 1s - loss: 0.6540 - accuracy: 0.63 - ETA: 1s - loss: 0.6524 - accuracy: 0.63 - ETA: 1s - loss: 0.6515 - accuracy: 0.63 - ETA: 1s - loss: 0.6508 - accuracy: 0.63 - ETA: 1s - loss: 0.6505 - accuracy: 0.63 - ETA: 1s - loss: 0.6495 - accuracy: 0.63 - ETA: 1s - loss: 0.6491 - accuracy: 0.63 - ETA: 1s - loss: 0.6480 - accuracy: 0.64 - ETA: 1s - loss: 0.6474 - accuracy: 0.64 - ETA: 1s - loss: 0.6468 - accuracy: 0.64 - ETA: 1s - loss: 0.6460 - accuracy: 0.64 - ETA: 1s - loss: 0.6439 - accuracy: 0.64 - ETA: 1s - loss: 0.6442 - accuracy: 0.64 - ETA: 1s - loss: 0.6421 - accuracy: 0.64 - ETA: 1s - loss: 0.6412 - accuracy: 0.64 - ETA: 0s - loss: 0.6413 - accuracy: 0.65 - ETA: 0s - loss: 0.6407 - accuracy: 0.65 - ETA: 0s - loss: 0.6385 - accuracy: 0.65 - ETA: 0s - loss: 0.6383 - accuracy: 0.65 - ETA: 0s - loss: 0.6381 - accuracy: 0.65 - ETA: 0s - loss: 0.6368 - accuracy: 0.65 - ETA: 0s - loss: 0.6357 - accuracy: 0.65 - ETA: 0s - loss: 0.6355 - accuracy: 0.65 - ETA: 0s - loss: 0.6350 - accuracy: 0.65 - ETA: 0s - loss: 0.6343 - accuracy: 0.65 - ETA: 0s - loss: 0.6339 - accuracy: 0.65 - ETA: 0s - loss: 0.6339 - accuracy: 0.65 - ETA: 0s - loss: 0.6332 - accuracy: 0.65 - ETA: 0s - loss: 0.6331 - accuracy: 0.65 - ETA: 0s - loss: 0.6320 - accuracy: 0.66 - ETA: 0s - loss: 0.6309 - accuracy: 0.66 - ETA: 0s - loss: 0.6306 - accuracy: 0.66 - ETA: 0s - loss: 0.6301 - accuracy: 0.66 - 3s 268us/step - loss: 0.6303 - accuracy: 0.6613 - val_loss: 0.5845 - val_accuracy: 0.7266\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:04 - loss: 0.6975 - accuracy: 0.50 - ETA: 10s - loss: 0.6940 - accuracy: 0.5312 - ETA: 6s - loss: 0.6924 - accuracy: 0.535 - ETA: 5s - loss: 0.6886 - accuracy: 0.54 - ETA: 4s - loss: 0.6892 - accuracy: 0.53 - ETA: 4s - loss: 0.6898 - accuracy: 0.54 - ETA: 3s - loss: 0.6887 - accuracy: 0.55 - ETA: 3s - loss: 0.6881 - accuracy: 0.55 - ETA: 3s - loss: 0.6873 - accuracy: 0.55 - ETA: 3s - loss: 0.6858 - accuracy: 0.55 - ETA: 3s - loss: 0.6845 - accuracy: 0.56 - ETA: 3s - loss: 0.6845 - accuracy: 0.56 - ETA: 3s - loss: 0.6839 - accuracy: 0.57 - ETA: 2s - loss: 0.6812 - accuracy: 0.57 - ETA: 2s - loss: 0.6805 - accuracy: 0.58 - ETA: 2s - loss: 0.6784 - accuracy: 0.58 - ETA: 2s - loss: 0.6759 - accuracy: 0.59 - ETA: 2s - loss: 0.6723 - accuracy: 0.59 - ETA: 2s - loss: 0.6713 - accuracy: 0.59 - ETA: 2s - loss: 0.6696 - accuracy: 0.59 - ETA: 2s - loss: 0.6686 - accuracy: 0.60 - ETA: 2s - loss: 0.6670 - accuracy: 0.60 - ETA: 2s - loss: 0.6662 - accuracy: 0.60 - ETA: 2s - loss: 0.6640 - accuracy: 0.60 - ETA: 2s - loss: 0.6632 - accuracy: 0.61 - ETA: 2s - loss: 0.6618 - accuracy: 0.61 - ETA: 1s - loss: 0.6610 - accuracy: 0.61 - ETA: 1s - loss: 0.6591 - accuracy: 0.62 - ETA: 1s - loss: 0.6563 - accuracy: 0.62 - ETA: 1s - loss: 0.6555 - accuracy: 0.62 - ETA: 1s - loss: 0.6551 - accuracy: 0.62 - ETA: 1s - loss: 0.6531 - accuracy: 0.62 - ETA: 1s - loss: 0.6518 - accuracy: 0.63 - ETA: 1s - loss: 0.6503 - accuracy: 0.63 - ETA: 1s - loss: 0.6489 - accuracy: 0.63 - ETA: 1s - loss: 0.6486 - accuracy: 0.63 - ETA: 1s - loss: 0.6475 - accuracy: 0.63 - ETA: 1s - loss: 0.6469 - accuracy: 0.63 - ETA: 1s - loss: 0.6462 - accuracy: 0.64 - ETA: 1s - loss: 0.6455 - accuracy: 0.64 - ETA: 1s - loss: 0.6449 - accuracy: 0.64 - ETA: 1s - loss: 0.6436 - accuracy: 0.64 - ETA: 1s - loss: 0.6422 - accuracy: 0.64 - ETA: 1s - loss: 0.6409 - accuracy: 0.64 - ETA: 0s - loss: 0.6405 - accuracy: 0.64 - ETA: 0s - loss: 0.6400 - accuracy: 0.64 - ETA: 0s - loss: 0.6403 - accuracy: 0.64 - ETA: 0s - loss: 0.6397 - accuracy: 0.64 - ETA: 0s - loss: 0.6392 - accuracy: 0.64 - ETA: 0s - loss: 0.6384 - accuracy: 0.64 - ETA: 0s - loss: 0.6383 - accuracy: 0.65 - ETA: 0s - loss: 0.6380 - accuracy: 0.65 - ETA: 0s - loss: 0.6371 - accuracy: 0.65 - ETA: 0s - loss: 0.6370 - accuracy: 0.65 - ETA: 0s - loss: 0.6365 - accuracy: 0.65 - ETA: 0s - loss: 0.6365 - accuracy: 0.65 - ETA: 0s - loss: 0.6351 - accuracy: 0.65 - ETA: 0s - loss: 0.6347 - accuracy: 0.65 - ETA: 0s - loss: 0.6343 - accuracy: 0.65 - ETA: 0s - loss: 0.6338 - accuracy: 0.65 - ETA: 0s - loss: 0.6330 - accuracy: 0.65 - ETA: 0s - loss: 0.6331 - accuracy: 0.65 - ETA: 0s - loss: 0.6326 - accuracy: 0.65 - 3s 273us/step - loss: 0.6326 - accuracy: 0.6581 - val_loss: 0.5818 - val_accuracy: 0.7301\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 99us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 6:07 - loss: 0.6967 - accuracy: 0.25 - ETA: 9s - loss: 0.6954 - accuracy: 0.4904 - ETA: 6s - loss: 0.6950 - accuracy: 0.48 - ETA: 5s - loss: 0.6941 - accuracy: 0.52 - ETA: 4s - loss: 0.6932 - accuracy: 0.52 - ETA: 4s - loss: 0.6929 - accuracy: 0.52 - ETA: 3s - loss: 0.6902 - accuracy: 0.54 - ETA: 3s - loss: 0.6902 - accuracy: 0.54 - ETA: 3s - loss: 0.6896 - accuracy: 0.55 - ETA: 3s - loss: 0.6873 - accuracy: 0.55 - ETA: 3s - loss: 0.6853 - accuracy: 0.56 - ETA: 3s - loss: 0.6850 - accuracy: 0.56 - ETA: 3s - loss: 0.6827 - accuracy: 0.57 - ETA: 2s - loss: 0.6803 - accuracy: 0.57 - ETA: 2s - loss: 0.6793 - accuracy: 0.58 - ETA: 2s - loss: 0.6777 - accuracy: 0.58 - ETA: 2s - loss: 0.6756 - accuracy: 0.59 - ETA: 2s - loss: 0.6730 - accuracy: 0.59 - ETA: 2s - loss: 0.6714 - accuracy: 0.59 - ETA: 2s - loss: 0.6687 - accuracy: 0.60 - ETA: 2s - loss: 0.6646 - accuracy: 0.60 - ETA: 2s - loss: 0.6637 - accuracy: 0.61 - ETA: 2s - loss: 0.6621 - accuracy: 0.61 - ETA: 2s - loss: 0.6608 - accuracy: 0.61 - ETA: 2s - loss: 0.6587 - accuracy: 0.61 - ETA: 2s - loss: 0.6565 - accuracy: 0.62 - ETA: 2s - loss: 0.6545 - accuracy: 0.62 - ETA: 1s - loss: 0.6538 - accuracy: 0.62 - ETA: 1s - loss: 0.6534 - accuracy: 0.62 - ETA: 1s - loss: 0.6530 - accuracy: 0.62 - ETA: 1s - loss: 0.6511 - accuracy: 0.62 - ETA: 1s - loss: 0.6493 - accuracy: 0.63 - ETA: 1s - loss: 0.6484 - accuracy: 0.63 - ETA: 1s - loss: 0.6478 - accuracy: 0.63 - ETA: 1s - loss: 0.6459 - accuracy: 0.63 - ETA: 1s - loss: 0.6453 - accuracy: 0.63 - ETA: 1s - loss: 0.6449 - accuracy: 0.63 - ETA: 1s - loss: 0.6441 - accuracy: 0.64 - ETA: 1s - loss: 0.6427 - accuracy: 0.64 - ETA: 1s - loss: 0.6426 - accuracy: 0.64 - ETA: 1s - loss: 0.6417 - accuracy: 0.64 - ETA: 1s - loss: 0.6415 - accuracy: 0.64 - ETA: 1s - loss: 0.6402 - accuracy: 0.64 - ETA: 1s - loss: 0.6393 - accuracy: 0.64 - ETA: 1s - loss: 0.6377 - accuracy: 0.64 - ETA: 0s - loss: 0.6365 - accuracy: 0.65 - ETA: 0s - loss: 0.6358 - accuracy: 0.65 - ETA: 0s - loss: 0.6350 - accuracy: 0.65 - ETA: 0s - loss: 0.6346 - accuracy: 0.65 - ETA: 0s - loss: 0.6334 - accuracy: 0.65 - ETA: 0s - loss: 0.6332 - accuracy: 0.65 - ETA: 0s - loss: 0.6330 - accuracy: 0.65 - ETA: 0s - loss: 0.6321 - accuracy: 0.65 - ETA: 0s - loss: 0.6316 - accuracy: 0.65 - ETA: 0s - loss: 0.6307 - accuracy: 0.65 - ETA: 0s - loss: 0.6308 - accuracy: 0.65 - ETA: 0s - loss: 0.6303 - accuracy: 0.65 - ETA: 0s - loss: 0.6298 - accuracy: 0.65 - ETA: 0s - loss: 0.6293 - accuracy: 0.66 - ETA: 0s - loss: 0.6285 - accuracy: 0.66 - ETA: 0s - loss: 0.6283 - accuracy: 0.66 - ETA: 0s - loss: 0.6279 - accuracy: 0.66 - ETA: 0s - loss: 0.6277 - accuracy: 0.66 - ETA: 0s - loss: 0.6275 - accuracy: 0.66 - 4s 277us/step - loss: 0.6274 - accuracy: 0.6623 - val_loss: 0.5772 - val_accuracy: 0.7166\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:16 - loss: 0.6869 - accuracy: 0.75 - ETA: 10s - loss: 0.6936 - accuracy: 0.5433 - ETA: 6s - loss: 0.6921 - accuracy: 0.545 - ETA: 5s - loss: 0.6910 - accuracy: 0.54 - ETA: 4s - loss: 0.6891 - accuracy: 0.55 - ETA: 4s - loss: 0.6870 - accuracy: 0.56 - ETA: 3s - loss: 0.6862 - accuracy: 0.56 - ETA: 3s - loss: 0.6830 - accuracy: 0.57 - ETA: 3s - loss: 0.6791 - accuracy: 0.58 - ETA: 3s - loss: 0.6754 - accuracy: 0.58 - ETA: 3s - loss: 0.6750 - accuracy: 0.58 - ETA: 3s - loss: 0.6712 - accuracy: 0.59 - ETA: 2s - loss: 0.6704 - accuracy: 0.59 - ETA: 2s - loss: 0.6674 - accuracy: 0.60 - ETA: 2s - loss: 0.6643 - accuracy: 0.61 - ETA: 2s - loss: 0.6633 - accuracy: 0.61 - ETA: 2s - loss: 0.6618 - accuracy: 0.61 - ETA: 2s - loss: 0.6617 - accuracy: 0.62 - ETA: 2s - loss: 0.6610 - accuracy: 0.61 - ETA: 2s - loss: 0.6596 - accuracy: 0.62 - ETA: 2s - loss: 0.6581 - accuracy: 0.62 - ETA: 2s - loss: 0.6562 - accuracy: 0.62 - ETA: 2s - loss: 0.6544 - accuracy: 0.62 - ETA: 2s - loss: 0.6530 - accuracy: 0.63 - ETA: 2s - loss: 0.6505 - accuracy: 0.63 - ETA: 2s - loss: 0.6489 - accuracy: 0.63 - ETA: 1s - loss: 0.6475 - accuracy: 0.63 - ETA: 1s - loss: 0.6446 - accuracy: 0.64 - ETA: 1s - loss: 0.6423 - accuracy: 0.64 - ETA: 1s - loss: 0.6407 - accuracy: 0.64 - ETA: 1s - loss: 0.6400 - accuracy: 0.64 - ETA: 1s - loss: 0.6403 - accuracy: 0.64 - ETA: 1s - loss: 0.6382 - accuracy: 0.65 - ETA: 1s - loss: 0.6377 - accuracy: 0.65 - ETA: 1s - loss: 0.6370 - accuracy: 0.65 - ETA: 1s - loss: 0.6359 - accuracy: 0.65 - ETA: 1s - loss: 0.6358 - accuracy: 0.65 - ETA: 1s - loss: 0.6345 - accuracy: 0.65 - ETA: 1s - loss: 0.6352 - accuracy: 0.65 - ETA: 1s - loss: 0.6353 - accuracy: 0.65 - ETA: 1s - loss: 0.6347 - accuracy: 0.65 - ETA: 1s - loss: 0.6334 - accuracy: 0.65 - ETA: 1s - loss: 0.6332 - accuracy: 0.65 - ETA: 1s - loss: 0.6320 - accuracy: 0.66 - ETA: 0s - loss: 0.6322 - accuracy: 0.66 - ETA: 0s - loss: 0.6326 - accuracy: 0.66 - ETA: 0s - loss: 0.6313 - accuracy: 0.66 - ETA: 0s - loss: 0.6305 - accuracy: 0.66 - ETA: 0s - loss: 0.6298 - accuracy: 0.66 - ETA: 0s - loss: 0.6294 - accuracy: 0.66 - ETA: 0s - loss: 0.6287 - accuracy: 0.66 - ETA: 0s - loss: 0.6291 - accuracy: 0.66 - ETA: 0s - loss: 0.6287 - accuracy: 0.66 - ETA: 0s - loss: 0.6280 - accuracy: 0.66 - ETA: 0s - loss: 0.6275 - accuracy: 0.66 - ETA: 0s - loss: 0.6272 - accuracy: 0.66 - ETA: 0s - loss: 0.6265 - accuracy: 0.66 - ETA: 0s - loss: 0.6272 - accuracy: 0.66 - ETA: 0s - loss: 0.6266 - accuracy: 0.66 - ETA: 0s - loss: 0.6268 - accuracy: 0.66 - ETA: 0s - loss: 0.6269 - accuracy: 0.66 - ETA: 0s - loss: 0.6273 - accuracy: 0.66 - ETA: 0s - loss: 0.6272 - accuracy: 0.66 - 3s 273us/step - loss: 0.6272 - accuracy: 0.6681 - val_loss: 0.5814 - val_accuracy: 0.7294\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:07 - loss: 0.6955 - accuracy: 0.25 - ETA: 12s - loss: 0.6945 - accuracy: 0.5174 - ETA: 7s - loss: 0.6937 - accuracy: 0.536 - ETA: 6s - loss: 0.6944 - accuracy: 0.52 - ETA: 5s - loss: 0.6937 - accuracy: 0.52 - ETA: 5s - loss: 0.6929 - accuracy: 0.53 - ETA: 4s - loss: 0.6920 - accuracy: 0.53 - ETA: 4s - loss: 0.6908 - accuracy: 0.54 - ETA: 4s - loss: 0.6902 - accuracy: 0.54 - ETA: 3s - loss: 0.6883 - accuracy: 0.55 - ETA: 3s - loss: 0.6871 - accuracy: 0.56 - ETA: 3s - loss: 0.6861 - accuracy: 0.56 - ETA: 3s - loss: 0.6835 - accuracy: 0.57 - ETA: 3s - loss: 0.6807 - accuracy: 0.57 - ETA: 3s - loss: 0.6788 - accuracy: 0.58 - ETA: 3s - loss: 0.6769 - accuracy: 0.58 - ETA: 2s - loss: 0.6752 - accuracy: 0.58 - ETA: 2s - loss: 0.6736 - accuracy: 0.58 - ETA: 2s - loss: 0.6726 - accuracy: 0.59 - ETA: 2s - loss: 0.6704 - accuracy: 0.59 - ETA: 2s - loss: 0.6663 - accuracy: 0.60 - ETA: 2s - loss: 0.6647 - accuracy: 0.60 - ETA: 2s - loss: 0.6633 - accuracy: 0.60 - ETA: 2s - loss: 0.6623 - accuracy: 0.60 - ETA: 2s - loss: 0.6608 - accuracy: 0.61 - ETA: 2s - loss: 0.6597 - accuracy: 0.61 - ETA: 2s - loss: 0.6588 - accuracy: 0.61 - ETA: 2s - loss: 0.6566 - accuracy: 0.61 - ETA: 2s - loss: 0.6549 - accuracy: 0.62 - ETA: 1s - loss: 0.6537 - accuracy: 0.62 - ETA: 1s - loss: 0.6515 - accuracy: 0.62 - ETA: 1s - loss: 0.6514 - accuracy: 0.62 - ETA: 1s - loss: 0.6501 - accuracy: 0.62 - ETA: 1s - loss: 0.6485 - accuracy: 0.62 - ETA: 1s - loss: 0.6473 - accuracy: 0.62 - ETA: 1s - loss: 0.6476 - accuracy: 0.62 - ETA: 1s - loss: 0.6472 - accuracy: 0.62 - ETA: 1s - loss: 0.6463 - accuracy: 0.63 - ETA: 1s - loss: 0.6448 - accuracy: 0.63 - ETA: 1s - loss: 0.6436 - accuracy: 0.63 - ETA: 1s - loss: 0.6436 - accuracy: 0.63 - ETA: 1s - loss: 0.6427 - accuracy: 0.63 - ETA: 1s - loss: 0.6417 - accuracy: 0.63 - ETA: 1s - loss: 0.6413 - accuracy: 0.64 - ETA: 1s - loss: 0.6399 - accuracy: 0.64 - ETA: 1s - loss: 0.6394 - accuracy: 0.64 - ETA: 0s - loss: 0.6394 - accuracy: 0.64 - ETA: 0s - loss: 0.6385 - accuracy: 0.64 - ETA: 0s - loss: 0.6377 - accuracy: 0.64 - ETA: 0s - loss: 0.6367 - accuracy: 0.64 - ETA: 0s - loss: 0.6352 - accuracy: 0.64 - ETA: 0s - loss: 0.6340 - accuracy: 0.64 - ETA: 0s - loss: 0.6341 - accuracy: 0.64 - ETA: 0s - loss: 0.6330 - accuracy: 0.64 - ETA: 0s - loss: 0.6325 - accuracy: 0.64 - ETA: 0s - loss: 0.6319 - accuracy: 0.64 - ETA: 0s - loss: 0.6315 - accuracy: 0.64 - ETA: 0s - loss: 0.6316 - accuracy: 0.64 - ETA: 0s - loss: 0.6311 - accuracy: 0.65 - ETA: 0s - loss: 0.6297 - accuracy: 0.65 - ETA: 0s - loss: 0.6294 - accuracy: 0.65 - ETA: 0s - loss: 0.6285 - accuracy: 0.65 - ETA: 0s - loss: 0.6284 - accuracy: 0.65 - ETA: 0s - loss: 0.6279 - accuracy: 0.65 - ETA: 0s - loss: 0.6278 - accuracy: 0.65 - 4s 284us/step - loss: 0.6270 - accuracy: 0.6561 - val_loss: 0.5729 - val_accuracy: 0.7315\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 104us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:13 - loss: 0.6989 - accuracy: 0.50 - ETA: 11s - loss: 0.6958 - accuracy: 0.4946 - ETA: 6s - loss: 0.6949 - accuracy: 0.507 - ETA: 5s - loss: 0.6935 - accuracy: 0.52 - ETA: 4s - loss: 0.6937 - accuracy: 0.52 - ETA: 4s - loss: 0.6929 - accuracy: 0.52 - ETA: 4s - loss: 0.6926 - accuracy: 0.52 - ETA: 3s - loss: 0.6918 - accuracy: 0.53 - ETA: 3s - loss: 0.6913 - accuracy: 0.53 - ETA: 3s - loss: 0.6897 - accuracy: 0.54 - ETA: 3s - loss: 0.6888 - accuracy: 0.54 - ETA: 3s - loss: 0.6879 - accuracy: 0.55 - ETA: 3s - loss: 0.6868 - accuracy: 0.55 - ETA: 3s - loss: 0.6861 - accuracy: 0.55 - ETA: 2s - loss: 0.6845 - accuracy: 0.55 - ETA: 2s - loss: 0.6841 - accuracy: 0.55 - ETA: 2s - loss: 0.6829 - accuracy: 0.56 - ETA: 2s - loss: 0.6808 - accuracy: 0.56 - ETA: 2s - loss: 0.6795 - accuracy: 0.56 - ETA: 2s - loss: 0.6778 - accuracy: 0.57 - ETA: 2s - loss: 0.6765 - accuracy: 0.57 - ETA: 2s - loss: 0.6741 - accuracy: 0.58 - ETA: 2s - loss: 0.6712 - accuracy: 0.58 - ETA: 2s - loss: 0.6696 - accuracy: 0.59 - ETA: 2s - loss: 0.6674 - accuracy: 0.59 - ETA: 2s - loss: 0.6655 - accuracy: 0.59 - ETA: 2s - loss: 0.6657 - accuracy: 0.59 - ETA: 2s - loss: 0.6654 - accuracy: 0.59 - ETA: 1s - loss: 0.6642 - accuracy: 0.60 - ETA: 1s - loss: 0.6622 - accuracy: 0.60 - ETA: 1s - loss: 0.6623 - accuracy: 0.60 - ETA: 1s - loss: 0.6611 - accuracy: 0.60 - ETA: 1s - loss: 0.6598 - accuracy: 0.60 - ETA: 1s - loss: 0.6583 - accuracy: 0.61 - ETA: 1s - loss: 0.6572 - accuracy: 0.61 - ETA: 1s - loss: 0.6559 - accuracy: 0.61 - ETA: 1s - loss: 0.6555 - accuracy: 0.61 - ETA: 1s - loss: 0.6547 - accuracy: 0.61 - ETA: 1s - loss: 0.6537 - accuracy: 0.61 - ETA: 1s - loss: 0.6527 - accuracy: 0.62 - ETA: 1s - loss: 0.6510 - accuracy: 0.62 - ETA: 1s - loss: 0.6494 - accuracy: 0.62 - ETA: 1s - loss: 0.6483 - accuracy: 0.62 - ETA: 1s - loss: 0.6474 - accuracy: 0.62 - ETA: 1s - loss: 0.6462 - accuracy: 0.63 - ETA: 0s - loss: 0.6458 - accuracy: 0.63 - ETA: 0s - loss: 0.6446 - accuracy: 0.63 - ETA: 0s - loss: 0.6431 - accuracy: 0.63 - ETA: 0s - loss: 0.6417 - accuracy: 0.63 - ETA: 0s - loss: 0.6406 - accuracy: 0.63 - ETA: 0s - loss: 0.6397 - accuracy: 0.63 - ETA: 0s - loss: 0.6392 - accuracy: 0.64 - ETA: 0s - loss: 0.6383 - accuracy: 0.64 - ETA: 0s - loss: 0.6368 - accuracy: 0.64 - ETA: 0s - loss: 0.6360 - accuracy: 0.64 - ETA: 0s - loss: 0.6357 - accuracy: 0.64 - ETA: 0s - loss: 0.6357 - accuracy: 0.64 - ETA: 0s - loss: 0.6346 - accuracy: 0.64 - ETA: 0s - loss: 0.6337 - accuracy: 0.64 - ETA: 0s - loss: 0.6333 - accuracy: 0.64 - ETA: 0s - loss: 0.6332 - accuracy: 0.65 - ETA: 0s - loss: 0.6335 - accuracy: 0.65 - ETA: 0s - loss: 0.6337 - accuracy: 0.65 - ETA: 0s - loss: 0.6329 - accuracy: 0.65 - 4s 278us/step - loss: 0.6330 - accuracy: 0.6515 - val_loss: 0.5865 - val_accuracy: 0.7102\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 5:35 - loss: 0.6932 - accuracy: 0.50 - ETA: 8s - loss: 0.6942 - accuracy: 0.5509 - ETA: 6s - loss: 0.6931 - accuracy: 0.56 - ETA: 5s - loss: 0.6927 - accuracy: 0.57 - ETA: 4s - loss: 0.6930 - accuracy: 0.55 - ETA: 4s - loss: 0.6932 - accuracy: 0.53 - ETA: 3s - loss: 0.6927 - accuracy: 0.54 - ETA: 3s - loss: 0.6920 - accuracy: 0.53 - ETA: 3s - loss: 0.6921 - accuracy: 0.52 - ETA: 3s - loss: 0.6919 - accuracy: 0.52 - ETA: 3s - loss: 0.6911 - accuracy: 0.53 - ETA: 3s - loss: 0.6902 - accuracy: 0.53 - ETA: 2s - loss: 0.6896 - accuracy: 0.53 - ETA: 2s - loss: 0.6892 - accuracy: 0.53 - ETA: 2s - loss: 0.6885 - accuracy: 0.53 - ETA: 2s - loss: 0.6877 - accuracy: 0.54 - ETA: 2s - loss: 0.6873 - accuracy: 0.53 - ETA: 2s - loss: 0.6870 - accuracy: 0.53 - ETA: 2s - loss: 0.6865 - accuracy: 0.54 - ETA: 2s - loss: 0.6862 - accuracy: 0.54 - ETA: 2s - loss: 0.6857 - accuracy: 0.54 - ETA: 2s - loss: 0.6859 - accuracy: 0.53 - ETA: 2s - loss: 0.6850 - accuracy: 0.53 - ETA: 2s - loss: 0.6847 - accuracy: 0.53 - ETA: 2s - loss: 0.6844 - accuracy: 0.54 - ETA: 2s - loss: 0.6837 - accuracy: 0.54 - ETA: 1s - loss: 0.6831 - accuracy: 0.54 - ETA: 1s - loss: 0.6827 - accuracy: 0.54 - ETA: 1s - loss: 0.6820 - accuracy: 0.54 - ETA: 1s - loss: 0.6808 - accuracy: 0.55 - ETA: 1s - loss: 0.6802 - accuracy: 0.55 - ETA: 1s - loss: 0.6798 - accuracy: 0.55 - ETA: 1s - loss: 0.6789 - accuracy: 0.55 - ETA: 1s - loss: 0.6784 - accuracy: 0.55 - ETA: 1s - loss: 0.6780 - accuracy: 0.55 - ETA: 1s - loss: 0.6775 - accuracy: 0.56 - ETA: 1s - loss: 0.6764 - accuracy: 0.56 - ETA: 1s - loss: 0.6757 - accuracy: 0.56 - ETA: 1s - loss: 0.6747 - accuracy: 0.56 - ETA: 1s - loss: 0.6742 - accuracy: 0.56 - ETA: 1s - loss: 0.6738 - accuracy: 0.56 - ETA: 1s - loss: 0.6729 - accuracy: 0.56 - ETA: 1s - loss: 0.6725 - accuracy: 0.57 - ETA: 1s - loss: 0.6719 - accuracy: 0.57 - ETA: 0s - loss: 0.6713 - accuracy: 0.57 - ETA: 0s - loss: 0.6703 - accuracy: 0.57 - ETA: 0s - loss: 0.6697 - accuracy: 0.57 - ETA: 0s - loss: 0.6692 - accuracy: 0.57 - ETA: 0s - loss: 0.6690 - accuracy: 0.57 - ETA: 0s - loss: 0.6685 - accuracy: 0.57 - ETA: 0s - loss: 0.6680 - accuracy: 0.57 - ETA: 0s - loss: 0.6674 - accuracy: 0.58 - ETA: 0s - loss: 0.6671 - accuracy: 0.58 - ETA: 0s - loss: 0.6667 - accuracy: 0.58 - ETA: 0s - loss: 0.6662 - accuracy: 0.58 - ETA: 0s - loss: 0.6656 - accuracy: 0.58 - ETA: 0s - loss: 0.6648 - accuracy: 0.58 - ETA: 0s - loss: 0.6645 - accuracy: 0.58 - ETA: 0s - loss: 0.6639 - accuracy: 0.58 - ETA: 0s - loss: 0.6631 - accuracy: 0.58 - ETA: 0s - loss: 0.6628 - accuracy: 0.58 - ETA: 0s - loss: 0.6628 - accuracy: 0.58 - ETA: 0s - loss: 0.6622 - accuracy: 0.59 - 3s 273us/step - loss: 0.6620 - accuracy: 0.5909 - val_loss: 0.6098 - val_accuracy: 0.7237\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 98us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:38 - loss: 0.7033 - accuracy: 0.0000e+ - ETA: 9s - loss: 0.6929 - accuracy: 0.4954     - ETA: 5s - loss: 0.6931 - accuracy: 0.52 - ETA: 4s - loss: 0.6925 - accuracy: 0.52 - ETA: 4s - loss: 0.6924 - accuracy: 0.52 - ETA: 3s - loss: 0.6924 - accuracy: 0.51 - ETA: 3s - loss: 0.6915 - accuracy: 0.51 - ETA: 3s - loss: 0.6911 - accuracy: 0.51 - ETA: 3s - loss: 0.6914 - accuracy: 0.51 - ETA: 3s - loss: 0.6913 - accuracy: 0.51 - ETA: 3s - loss: 0.6902 - accuracy: 0.52 - ETA: 2s - loss: 0.6894 - accuracy: 0.52 - ETA: 2s - loss: 0.6879 - accuracy: 0.53 - ETA: 2s - loss: 0.6868 - accuracy: 0.53 - ETA: 2s - loss: 0.6866 - accuracy: 0.53 - ETA: 2s - loss: 0.6866 - accuracy: 0.53 - ETA: 2s - loss: 0.6856 - accuracy: 0.53 - ETA: 2s - loss: 0.6854 - accuracy: 0.53 - ETA: 2s - loss: 0.6849 - accuracy: 0.53 - ETA: 2s - loss: 0.6838 - accuracy: 0.54 - ETA: 2s - loss: 0.6837 - accuracy: 0.54 - ETA: 2s - loss: 0.6822 - accuracy: 0.54 - ETA: 2s - loss: 0.6818 - accuracy: 0.54 - ETA: 2s - loss: 0.6810 - accuracy: 0.55 - ETA: 2s - loss: 0.6807 - accuracy: 0.55 - ETA: 1s - loss: 0.6799 - accuracy: 0.55 - ETA: 1s - loss: 0.6797 - accuracy: 0.56 - ETA: 1s - loss: 0.6791 - accuracy: 0.56 - ETA: 1s - loss: 0.6787 - accuracy: 0.56 - ETA: 1s - loss: 0.6784 - accuracy: 0.56 - ETA: 1s - loss: 0.6774 - accuracy: 0.57 - ETA: 1s - loss: 0.6765 - accuracy: 0.57 - ETA: 1s - loss: 0.6753 - accuracy: 0.57 - ETA: 1s - loss: 0.6756 - accuracy: 0.58 - ETA: 1s - loss: 0.6756 - accuracy: 0.58 - ETA: 1s - loss: 0.6753 - accuracy: 0.58 - ETA: 1s - loss: 0.6744 - accuracy: 0.58 - ETA: 1s - loss: 0.6739 - accuracy: 0.58 - ETA: 1s - loss: 0.6727 - accuracy: 0.58 - ETA: 1s - loss: 0.6721 - accuracy: 0.59 - ETA: 1s - loss: 0.6711 - accuracy: 0.59 - ETA: 1s - loss: 0.6706 - accuracy: 0.59 - ETA: 1s - loss: 0.6697 - accuracy: 0.59 - ETA: 0s - loss: 0.6695 - accuracy: 0.59 - ETA: 0s - loss: 0.6686 - accuracy: 0.59 - ETA: 0s - loss: 0.6684 - accuracy: 0.60 - ETA: 0s - loss: 0.6684 - accuracy: 0.60 - ETA: 0s - loss: 0.6681 - accuracy: 0.60 - ETA: 0s - loss: 0.6678 - accuracy: 0.60 - ETA: 0s - loss: 0.6677 - accuracy: 0.60 - ETA: 0s - loss: 0.6675 - accuracy: 0.60 - ETA: 0s - loss: 0.6673 - accuracy: 0.60 - ETA: 0s - loss: 0.6671 - accuracy: 0.60 - ETA: 0s - loss: 0.6667 - accuracy: 0.60 - ETA: 0s - loss: 0.6664 - accuracy: 0.60 - ETA: 0s - loss: 0.6659 - accuracy: 0.61 - ETA: 0s - loss: 0.6660 - accuracy: 0.61 - ETA: 0s - loss: 0.6656 - accuracy: 0.61 - ETA: 0s - loss: 0.6657 - accuracy: 0.61 - ETA: 0s - loss: 0.6657 - accuracy: 0.61 - ETA: 0s - loss: 0.6648 - accuracy: 0.61 - ETA: 0s - loss: 0.6642 - accuracy: 0.61 - 3s 271us/step - loss: 0.6636 - accuracy: 0.6192 - val_loss: 0.6215 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:35 - loss: 0.6929 - accuracy: 0.50 - ETA: 8s - loss: 0.6928 - accuracy: 0.5370 - ETA: 5s - loss: 0.6925 - accuracy: 0.50 - ETA: 4s - loss: 0.6919 - accuracy: 0.52 - ETA: 4s - loss: 0.6913 - accuracy: 0.51 - ETA: 3s - loss: 0.6916 - accuracy: 0.51 - ETA: 3s - loss: 0.6916 - accuracy: 0.51 - ETA: 3s - loss: 0.6920 - accuracy: 0.50 - ETA: 3s - loss: 0.6911 - accuracy: 0.51 - ETA: 3s - loss: 0.6913 - accuracy: 0.51 - ETA: 3s - loss: 0.6910 - accuracy: 0.51 - ETA: 2s - loss: 0.6904 - accuracy: 0.51 - ETA: 2s - loss: 0.6894 - accuracy: 0.51 - ETA: 2s - loss: 0.6893 - accuracy: 0.51 - ETA: 2s - loss: 0.6890 - accuracy: 0.51 - ETA: 2s - loss: 0.6889 - accuracy: 0.51 - ETA: 2s - loss: 0.6887 - accuracy: 0.51 - ETA: 2s - loss: 0.6881 - accuracy: 0.51 - ETA: 2s - loss: 0.6877 - accuracy: 0.51 - ETA: 2s - loss: 0.6879 - accuracy: 0.51 - ETA: 2s - loss: 0.6877 - accuracy: 0.52 - ETA: 2s - loss: 0.6873 - accuracy: 0.53 - ETA: 2s - loss: 0.6871 - accuracy: 0.53 - ETA: 2s - loss: 0.6865 - accuracy: 0.53 - ETA: 1s - loss: 0.6858 - accuracy: 0.54 - ETA: 1s - loss: 0.6852 - accuracy: 0.54 - ETA: 1s - loss: 0.6850 - accuracy: 0.54 - ETA: 1s - loss: 0.6844 - accuracy: 0.54 - ETA: 1s - loss: 0.6835 - accuracy: 0.55 - ETA: 1s - loss: 0.6828 - accuracy: 0.55 - ETA: 1s - loss: 0.6823 - accuracy: 0.55 - ETA: 1s - loss: 0.6824 - accuracy: 0.55 - ETA: 1s - loss: 0.6816 - accuracy: 0.55 - ETA: 1s - loss: 0.6809 - accuracy: 0.55 - ETA: 1s - loss: 0.6808 - accuracy: 0.55 - ETA: 1s - loss: 0.6804 - accuracy: 0.55 - ETA: 1s - loss: 0.6803 - accuracy: 0.55 - ETA: 1s - loss: 0.6800 - accuracy: 0.56 - ETA: 1s - loss: 0.6794 - accuracy: 0.56 - ETA: 1s - loss: 0.6785 - accuracy: 0.56 - ETA: 1s - loss: 0.6784 - accuracy: 0.56 - ETA: 0s - loss: 0.6783 - accuracy: 0.56 - ETA: 0s - loss: 0.6780 - accuracy: 0.56 - ETA: 0s - loss: 0.6775 - accuracy: 0.57 - ETA: 0s - loss: 0.6770 - accuracy: 0.57 - ETA: 0s - loss: 0.6768 - accuracy: 0.57 - ETA: 0s - loss: 0.6765 - accuracy: 0.57 - ETA: 0s - loss: 0.6759 - accuracy: 0.57 - ETA: 0s - loss: 0.6756 - accuracy: 0.57 - ETA: 0s - loss: 0.6758 - accuracy: 0.57 - ETA: 0s - loss: 0.6754 - accuracy: 0.57 - ETA: 0s - loss: 0.6754 - accuracy: 0.57 - ETA: 0s - loss: 0.6750 - accuracy: 0.57 - ETA: 0s - loss: 0.6748 - accuracy: 0.58 - ETA: 0s - loss: 0.6748 - accuracy: 0.58 - ETA: 0s - loss: 0.6741 - accuracy: 0.58 - ETA: 0s - loss: 0.6738 - accuracy: 0.58 - ETA: 0s - loss: 0.6734 - accuracy: 0.58 - ETA: 0s - loss: 0.6731 - accuracy: 0.58 - ETA: 0s - loss: 0.6727 - accuracy: 0.58 - ETA: 0s - loss: 0.6724 - accuracy: 0.58 - ETA: 0s - loss: 0.6725 - accuracy: 0.58 - ETA: 0s - loss: 0.6725 - accuracy: 0.58 - ETA: 0s - loss: 0.6723 - accuracy: 0.58 - 4s 278us/step - loss: 0.6720 - accuracy: 0.5865 - val_loss: 0.6345 - val_accuracy: 0.7074\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 107us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:26 - loss: 0.6942 - accuracy: 0.50 - ETA: 8s - loss: 0.6936 - accuracy: 0.5182 - ETA: 5s - loss: 0.6936 - accuracy: 0.51 - ETA: 4s - loss: 0.6935 - accuracy: 0.50 - ETA: 4s - loss: 0.6931 - accuracy: 0.52 - ETA: 3s - loss: 0.6931 - accuracy: 0.51 - ETA: 3s - loss: 0.6932 - accuracy: 0.50 - ETA: 3s - loss: 0.6928 - accuracy: 0.51 - ETA: 3s - loss: 0.6924 - accuracy: 0.52 - ETA: 3s - loss: 0.6916 - accuracy: 0.52 - ETA: 3s - loss: 0.6908 - accuracy: 0.53 - ETA: 2s - loss: 0.6903 - accuracy: 0.53 - ETA: 2s - loss: 0.6902 - accuracy: 0.53 - ETA: 2s - loss: 0.6901 - accuracy: 0.53 - ETA: 2s - loss: 0.6898 - accuracy: 0.53 - ETA: 2s - loss: 0.6897 - accuracy: 0.53 - ETA: 2s - loss: 0.6894 - accuracy: 0.53 - ETA: 2s - loss: 0.6883 - accuracy: 0.53 - ETA: 2s - loss: 0.6874 - accuracy: 0.54 - ETA: 2s - loss: 0.6867 - accuracy: 0.54 - ETA: 2s - loss: 0.6868 - accuracy: 0.54 - ETA: 2s - loss: 0.6861 - accuracy: 0.54 - ETA: 2s - loss: 0.6857 - accuracy: 0.54 - ETA: 2s - loss: 0.6851 - accuracy: 0.54 - ETA: 2s - loss: 0.6848 - accuracy: 0.54 - ETA: 1s - loss: 0.6836 - accuracy: 0.55 - ETA: 1s - loss: 0.6830 - accuracy: 0.55 - ETA: 1s - loss: 0.6822 - accuracy: 0.55 - ETA: 1s - loss: 0.6818 - accuracy: 0.55 - ETA: 1s - loss: 0.6815 - accuracy: 0.55 - ETA: 1s - loss: 0.6804 - accuracy: 0.55 - ETA: 1s - loss: 0.6800 - accuracy: 0.55 - ETA: 1s - loss: 0.6798 - accuracy: 0.56 - ETA: 1s - loss: 0.6790 - accuracy: 0.56 - ETA: 1s - loss: 0.6785 - accuracy: 0.56 - ETA: 1s - loss: 0.6782 - accuracy: 0.56 - ETA: 1s - loss: 0.6772 - accuracy: 0.56 - ETA: 1s - loss: 0.6766 - accuracy: 0.56 - ETA: 1s - loss: 0.6762 - accuracy: 0.56 - ETA: 1s - loss: 0.6753 - accuracy: 0.56 - ETA: 1s - loss: 0.6748 - accuracy: 0.57 - ETA: 1s - loss: 0.6743 - accuracy: 0.57 - ETA: 1s - loss: 0.6739 - accuracy: 0.57 - ETA: 0s - loss: 0.6732 - accuracy: 0.57 - ETA: 0s - loss: 0.6722 - accuracy: 0.57 - ETA: 0s - loss: 0.6719 - accuracy: 0.57 - ETA: 0s - loss: 0.6718 - accuracy: 0.57 - ETA: 0s - loss: 0.6708 - accuracy: 0.57 - ETA: 0s - loss: 0.6707 - accuracy: 0.57 - ETA: 0s - loss: 0.6699 - accuracy: 0.57 - ETA: 0s - loss: 0.6694 - accuracy: 0.57 - ETA: 0s - loss: 0.6686 - accuracy: 0.57 - ETA: 0s - loss: 0.6685 - accuracy: 0.58 - ETA: 0s - loss: 0.6676 - accuracy: 0.58 - ETA: 0s - loss: 0.6665 - accuracy: 0.58 - ETA: 0s - loss: 0.6655 - accuracy: 0.58 - ETA: 0s - loss: 0.6647 - accuracy: 0.58 - ETA: 0s - loss: 0.6641 - accuracy: 0.58 - ETA: 0s - loss: 0.6635 - accuracy: 0.58 - ETA: 0s - loss: 0.6638 - accuracy: 0.58 - ETA: 0s - loss: 0.6639 - accuracy: 0.58 - ETA: 0s - loss: 0.6634 - accuracy: 0.58 - 3s 268us/step - loss: 0.6634 - accuracy: 0.5884 - val_loss: 0.6118 - val_accuracy: 0.7053\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 5:29 - loss: 0.6928 - accuracy: 0.50 - ETA: 8s - loss: 0.6939 - accuracy: 0.4954 - ETA: 5s - loss: 0.6937 - accuracy: 0.51 - ETA: 4s - loss: 0.6931 - accuracy: 0.53 - ETA: 4s - loss: 0.6929 - accuracy: 0.53 - ETA: 3s - loss: 0.6925 - accuracy: 0.53 - ETA: 3s - loss: 0.6925 - accuracy: 0.52 - ETA: 3s - loss: 0.6922 - accuracy: 0.53 - ETA: 3s - loss: 0.6922 - accuracy: 0.53 - ETA: 3s - loss: 0.6918 - accuracy: 0.53 - ETA: 3s - loss: 0.6918 - accuracy: 0.53 - ETA: 2s - loss: 0.6913 - accuracy: 0.53 - ETA: 2s - loss: 0.6911 - accuracy: 0.54 - ETA: 2s - loss: 0.6908 - accuracy: 0.54 - ETA: 2s - loss: 0.6906 - accuracy: 0.54 - ETA: 2s - loss: 0.6901 - accuracy: 0.55 - ETA: 2s - loss: 0.6897 - accuracy: 0.56 - ETA: 2s - loss: 0.6891 - accuracy: 0.56 - ETA: 2s - loss: 0.6887 - accuracy: 0.56 - ETA: 2s - loss: 0.6881 - accuracy: 0.57 - ETA: 2s - loss: 0.6879 - accuracy: 0.57 - ETA: 2s - loss: 0.6872 - accuracy: 0.57 - ETA: 2s - loss: 0.6864 - accuracy: 0.57 - ETA: 2s - loss: 0.6863 - accuracy: 0.57 - ETA: 1s - loss: 0.6856 - accuracy: 0.57 - ETA: 1s - loss: 0.6849 - accuracy: 0.58 - ETA: 1s - loss: 0.6841 - accuracy: 0.58 - ETA: 1s - loss: 0.6835 - accuracy: 0.58 - ETA: 1s - loss: 0.6824 - accuracy: 0.58 - ETA: 1s - loss: 0.6821 - accuracy: 0.58 - ETA: 1s - loss: 0.6812 - accuracy: 0.58 - ETA: 1s - loss: 0.6808 - accuracy: 0.59 - ETA: 1s - loss: 0.6800 - accuracy: 0.59 - ETA: 1s - loss: 0.6794 - accuracy: 0.59 - ETA: 1s - loss: 0.6788 - accuracy: 0.59 - ETA: 1s - loss: 0.6786 - accuracy: 0.59 - ETA: 1s - loss: 0.6775 - accuracy: 0.59 - ETA: 1s - loss: 0.6769 - accuracy: 0.59 - ETA: 1s - loss: 0.6759 - accuracy: 0.59 - ETA: 1s - loss: 0.6752 - accuracy: 0.59 - ETA: 1s - loss: 0.6749 - accuracy: 0.59 - ETA: 0s - loss: 0.6745 - accuracy: 0.59 - ETA: 0s - loss: 0.6742 - accuracy: 0.59 - ETA: 0s - loss: 0.6730 - accuracy: 0.60 - ETA: 0s - loss: 0.6732 - accuracy: 0.59 - ETA: 0s - loss: 0.6723 - accuracy: 0.60 - ETA: 0s - loss: 0.6718 - accuracy: 0.60 - ETA: 0s - loss: 0.6714 - accuracy: 0.60 - ETA: 0s - loss: 0.6705 - accuracy: 0.60 - ETA: 0s - loss: 0.6698 - accuracy: 0.60 - ETA: 0s - loss: 0.6689 - accuracy: 0.60 - ETA: 0s - loss: 0.6682 - accuracy: 0.60 - ETA: 0s - loss: 0.6682 - accuracy: 0.60 - ETA: 0s - loss: 0.6676 - accuracy: 0.60 - ETA: 0s - loss: 0.6671 - accuracy: 0.60 - ETA: 0s - loss: 0.6667 - accuracy: 0.60 - ETA: 0s - loss: 0.6658 - accuracy: 0.61 - ETA: 0s - loss: 0.6654 - accuracy: 0.61 - ETA: 0s - loss: 0.6646 - accuracy: 0.61 - ETA: 0s - loss: 0.6638 - accuracy: 0.61 - 3s 261us/step - loss: 0.6632 - accuracy: 0.6136 - val_loss: 0.6064 - val_accuracy: 0.7188\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:38 - loss: 0.6937 - accuracy: 0.75 - ETA: 8s - loss: 0.6926 - accuracy: 0.5580 - ETA: 5s - loss: 0.6921 - accuracy: 0.54 - ETA: 4s - loss: 0.6928 - accuracy: 0.51 - ETA: 4s - loss: 0.6925 - accuracy: 0.52 - ETA: 3s - loss: 0.6922 - accuracy: 0.52 - ETA: 3s - loss: 0.6920 - accuracy: 0.53 - ETA: 3s - loss: 0.6920 - accuracy: 0.53 - ETA: 3s - loss: 0.6916 - accuracy: 0.54 - ETA: 3s - loss: 0.6911 - accuracy: 0.54 - ETA: 2s - loss: 0.6908 - accuracy: 0.55 - ETA: 2s - loss: 0.6898 - accuracy: 0.55 - ETA: 2s - loss: 0.6891 - accuracy: 0.56 - ETA: 2s - loss: 0.6886 - accuracy: 0.56 - ETA: 2s - loss: 0.6877 - accuracy: 0.56 - ETA: 2s - loss: 0.6874 - accuracy: 0.56 - ETA: 2s - loss: 0.6869 - accuracy: 0.56 - ETA: 2s - loss: 0.6862 - accuracy: 0.57 - ETA: 2s - loss: 0.6853 - accuracy: 0.57 - ETA: 2s - loss: 0.6845 - accuracy: 0.57 - ETA: 2s - loss: 0.6842 - accuracy: 0.57 - ETA: 2s - loss: 0.6833 - accuracy: 0.58 - ETA: 2s - loss: 0.6829 - accuracy: 0.58 - ETA: 1s - loss: 0.6817 - accuracy: 0.58 - ETA: 1s - loss: 0.6813 - accuracy: 0.58 - ETA: 1s - loss: 0.6809 - accuracy: 0.58 - ETA: 1s - loss: 0.6796 - accuracy: 0.58 - ETA: 1s - loss: 0.6796 - accuracy: 0.58 - ETA: 1s - loss: 0.6790 - accuracy: 0.58 - ETA: 1s - loss: 0.6788 - accuracy: 0.58 - ETA: 1s - loss: 0.6784 - accuracy: 0.58 - ETA: 1s - loss: 0.6774 - accuracy: 0.58 - ETA: 1s - loss: 0.6770 - accuracy: 0.58 - ETA: 1s - loss: 0.6762 - accuracy: 0.58 - ETA: 1s - loss: 0.6760 - accuracy: 0.59 - ETA: 1s - loss: 0.6749 - accuracy: 0.59 - ETA: 1s - loss: 0.6743 - accuracy: 0.59 - ETA: 1s - loss: 0.6738 - accuracy: 0.59 - ETA: 1s - loss: 0.6735 - accuracy: 0.59 - ETA: 1s - loss: 0.6728 - accuracy: 0.59 - ETA: 0s - loss: 0.6727 - accuracy: 0.59 - ETA: 0s - loss: 0.6719 - accuracy: 0.59 - ETA: 0s - loss: 0.6711 - accuracy: 0.59 - ETA: 0s - loss: 0.6706 - accuracy: 0.59 - ETA: 0s - loss: 0.6698 - accuracy: 0.59 - ETA: 0s - loss: 0.6693 - accuracy: 0.59 - ETA: 0s - loss: 0.6694 - accuracy: 0.59 - ETA: 0s - loss: 0.6689 - accuracy: 0.59 - ETA: 0s - loss: 0.6682 - accuracy: 0.60 - ETA: 0s - loss: 0.6672 - accuracy: 0.60 - ETA: 0s - loss: 0.6664 - accuracy: 0.60 - ETA: 0s - loss: 0.6654 - accuracy: 0.60 - ETA: 0s - loss: 0.6650 - accuracy: 0.60 - ETA: 0s - loss: 0.6645 - accuracy: 0.60 - ETA: 0s - loss: 0.6642 - accuracy: 0.60 - ETA: 0s - loss: 0.6635 - accuracy: 0.60 - ETA: 0s - loss: 0.6629 - accuracy: 0.60 - ETA: 0s - loss: 0.6620 - accuracy: 0.60 - ETA: 0s - loss: 0.6611 - accuracy: 0.60 - 3s 257us/step - loss: 0.6611 - accuracy: 0.6096 - val_loss: 0.6111 - val_accuracy: 0.7159\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:23 - loss: 0.6905 - accuracy: 0.75 - ETA: 8s - loss: 0.6950 - accuracy: 0.4769 - ETA: 5s - loss: 0.6942 - accuracy: 0.49 - ETA: 4s - loss: 0.6942 - accuracy: 0.47 - ETA: 4s - loss: 0.6937 - accuracy: 0.49 - ETA: 3s - loss: 0.6934 - accuracy: 0.50 - ETA: 3s - loss: 0.6931 - accuracy: 0.51 - ETA: 3s - loss: 0.6928 - accuracy: 0.51 - ETA: 3s - loss: 0.6923 - accuracy: 0.51 - ETA: 3s - loss: 0.6921 - accuracy: 0.52 - ETA: 2s - loss: 0.6912 - accuracy: 0.52 - ETA: 2s - loss: 0.6905 - accuracy: 0.53 - ETA: 2s - loss: 0.6902 - accuracy: 0.53 - ETA: 2s - loss: 0.6893 - accuracy: 0.53 - ETA: 2s - loss: 0.6886 - accuracy: 0.54 - ETA: 2s - loss: 0.6880 - accuracy: 0.54 - ETA: 2s - loss: 0.6872 - accuracy: 0.55 - ETA: 2s - loss: 0.6867 - accuracy: 0.55 - ETA: 2s - loss: 0.6855 - accuracy: 0.56 - ETA: 2s - loss: 0.6848 - accuracy: 0.56 - ETA: 2s - loss: 0.6843 - accuracy: 0.56 - ETA: 2s - loss: 0.6841 - accuracy: 0.56 - ETA: 1s - loss: 0.6833 - accuracy: 0.57 - ETA: 1s - loss: 0.6823 - accuracy: 0.57 - ETA: 1s - loss: 0.6811 - accuracy: 0.57 - ETA: 1s - loss: 0.6804 - accuracy: 0.57 - ETA: 1s - loss: 0.6792 - accuracy: 0.57 - ETA: 1s - loss: 0.6779 - accuracy: 0.58 - ETA: 1s - loss: 0.6770 - accuracy: 0.58 - ETA: 1s - loss: 0.6766 - accuracy: 0.58 - ETA: 1s - loss: 0.6757 - accuracy: 0.58 - ETA: 1s - loss: 0.6742 - accuracy: 0.58 - ETA: 1s - loss: 0.6735 - accuracy: 0.58 - ETA: 1s - loss: 0.6726 - accuracy: 0.59 - ETA: 1s - loss: 0.6717 - accuracy: 0.59 - ETA: 1s - loss: 0.6709 - accuracy: 0.59 - ETA: 1s - loss: 0.6700 - accuracy: 0.59 - ETA: 1s - loss: 0.6691 - accuracy: 0.59 - ETA: 1s - loss: 0.6685 - accuracy: 0.59 - ETA: 0s - loss: 0.6678 - accuracy: 0.59 - ETA: 0s - loss: 0.6673 - accuracy: 0.60 - ETA: 0s - loss: 0.6662 - accuracy: 0.60 - ETA: 0s - loss: 0.6653 - accuracy: 0.60 - ETA: 0s - loss: 0.6651 - accuracy: 0.60 - ETA: 0s - loss: 0.6646 - accuracy: 0.60 - ETA: 0s - loss: 0.6635 - accuracy: 0.60 - ETA: 0s - loss: 0.6630 - accuracy: 0.60 - ETA: 0s - loss: 0.6626 - accuracy: 0.60 - ETA: 0s - loss: 0.6623 - accuracy: 0.60 - ETA: 0s - loss: 0.6616 - accuracy: 0.61 - ETA: 0s - loss: 0.6608 - accuracy: 0.61 - ETA: 0s - loss: 0.6601 - accuracy: 0.61 - ETA: 0s - loss: 0.6598 - accuracy: 0.61 - ETA: 0s - loss: 0.6591 - accuracy: 0.61 - ETA: 0s - loss: 0.6585 - accuracy: 0.61 - ETA: 0s - loss: 0.6576 - accuracy: 0.61 - ETA: 0s - loss: 0.6565 - accuracy: 0.61 - ETA: 0s - loss: 0.6547 - accuracy: 0.62 - 3s 253us/step - loss: 0.6544 - accuracy: 0.6214 - val_loss: 0.5885 - val_accuracy: 0.7188\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:26 - loss: 0.6955 - accuracy: 0.75 - ETA: 8s - loss: 0.6936 - accuracy: 0.5136 - ETA: 5s - loss: 0.6914 - accuracy: 0.53 - ETA: 4s - loss: 0.6931 - accuracy: 0.52 - ETA: 4s - loss: 0.6920 - accuracy: 0.52 - ETA: 3s - loss: 0.6920 - accuracy: 0.52 - ETA: 3s - loss: 0.6919 - accuracy: 0.52 - ETA: 3s - loss: 0.6911 - accuracy: 0.52 - ETA: 3s - loss: 0.6914 - accuracy: 0.51 - ETA: 3s - loss: 0.6913 - accuracy: 0.51 - ETA: 3s - loss: 0.6909 - accuracy: 0.50 - ETA: 2s - loss: 0.6905 - accuracy: 0.51 - ETA: 2s - loss: 0.6903 - accuracy: 0.51 - ETA: 2s - loss: 0.6897 - accuracy: 0.51 - ETA: 2s - loss: 0.6896 - accuracy: 0.51 - ETA: 2s - loss: 0.6891 - accuracy: 0.51 - ETA: 2s - loss: 0.6890 - accuracy: 0.52 - ETA: 2s - loss: 0.6889 - accuracy: 0.52 - ETA: 2s - loss: 0.6881 - accuracy: 0.52 - ETA: 2s - loss: 0.6865 - accuracy: 0.53 - ETA: 2s - loss: 0.6851 - accuracy: 0.53 - ETA: 2s - loss: 0.6843 - accuracy: 0.53 - ETA: 2s - loss: 0.6839 - accuracy: 0.53 - ETA: 2s - loss: 0.6832 - accuracy: 0.54 - ETA: 2s - loss: 0.6821 - accuracy: 0.54 - ETA: 1s - loss: 0.6814 - accuracy: 0.55 - ETA: 1s - loss: 0.6811 - accuracy: 0.55 - ETA: 1s - loss: 0.6803 - accuracy: 0.55 - ETA: 1s - loss: 0.6796 - accuracy: 0.56 - ETA: 1s - loss: 0.6791 - accuracy: 0.56 - ETA: 1s - loss: 0.6781 - accuracy: 0.56 - ETA: 1s - loss: 0.6771 - accuracy: 0.57 - ETA: 1s - loss: 0.6767 - accuracy: 0.57 - ETA: 1s - loss: 0.6760 - accuracy: 0.57 - ETA: 1s - loss: 0.6755 - accuracy: 0.57 - ETA: 1s - loss: 0.6748 - accuracy: 0.58 - ETA: 1s - loss: 0.6738 - accuracy: 0.58 - ETA: 1s - loss: 0.6726 - accuracy: 0.58 - ETA: 1s - loss: 0.6722 - accuracy: 0.59 - ETA: 1s - loss: 0.6723 - accuracy: 0.59 - ETA: 1s - loss: 0.6720 - accuracy: 0.59 - ETA: 1s - loss: 0.6716 - accuracy: 0.59 - ETA: 1s - loss: 0.6709 - accuracy: 0.59 - ETA: 0s - loss: 0.6705 - accuracy: 0.59 - ETA: 0s - loss: 0.6706 - accuracy: 0.59 - ETA: 0s - loss: 0.6701 - accuracy: 0.59 - ETA: 0s - loss: 0.6695 - accuracy: 0.60 - ETA: 0s - loss: 0.6690 - accuracy: 0.60 - ETA: 0s - loss: 0.6685 - accuracy: 0.60 - ETA: 0s - loss: 0.6683 - accuracy: 0.60 - ETA: 0s - loss: 0.6679 - accuracy: 0.60 - ETA: 0s - loss: 0.6675 - accuracy: 0.60 - ETA: 0s - loss: 0.6673 - accuracy: 0.60 - ETA: 0s - loss: 0.6667 - accuracy: 0.61 - ETA: 0s - loss: 0.6663 - accuracy: 0.61 - ETA: 0s - loss: 0.6655 - accuracy: 0.61 - ETA: 0s - loss: 0.6651 - accuracy: 0.61 - ETA: 0s - loss: 0.6638 - accuracy: 0.61 - ETA: 0s - loss: 0.6631 - accuracy: 0.61 - ETA: 0s - loss: 0.6630 - accuracy: 0.62 - ETA: 0s - loss: 0.6627 - accuracy: 0.62 - ETA: 0s - loss: 0.6620 - accuracy: 0.62 - 3s 271us/step - loss: 0.6617 - accuracy: 0.6242 - val_loss: 0.6149 - val_accuracy: 0.7024\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:26 - loss: 0.6856 - accuracy: 1.00 - ETA: 8s - loss: 0.6943 - accuracy: 0.5318 - ETA: 5s - loss: 0.6936 - accuracy: 0.52 - ETA: 4s - loss: 0.6941 - accuracy: 0.50 - ETA: 4s - loss: 0.6936 - accuracy: 0.50 - ETA: 3s - loss: 0.6933 - accuracy: 0.50 - ETA: 3s - loss: 0.6924 - accuracy: 0.51 - ETA: 3s - loss: 0.6918 - accuracy: 0.52 - ETA: 3s - loss: 0.6918 - accuracy: 0.52 - ETA: 3s - loss: 0.6910 - accuracy: 0.53 - ETA: 2s - loss: 0.6904 - accuracy: 0.54 - ETA: 2s - loss: 0.6891 - accuracy: 0.54 - ETA: 2s - loss: 0.6892 - accuracy: 0.54 - ETA: 2s - loss: 0.6881 - accuracy: 0.54 - ETA: 2s - loss: 0.6867 - accuracy: 0.55 - ETA: 2s - loss: 0.6858 - accuracy: 0.55 - ETA: 2s - loss: 0.6850 - accuracy: 0.55 - ETA: 2s - loss: 0.6835 - accuracy: 0.56 - ETA: 2s - loss: 0.6830 - accuracy: 0.56 - ETA: 2s - loss: 0.6824 - accuracy: 0.57 - ETA: 2s - loss: 0.6813 - accuracy: 0.57 - ETA: 2s - loss: 0.6808 - accuracy: 0.57 - ETA: 1s - loss: 0.6797 - accuracy: 0.58 - ETA: 1s - loss: 0.6785 - accuracy: 0.58 - ETA: 1s - loss: 0.6768 - accuracy: 0.58 - ETA: 1s - loss: 0.6773 - accuracy: 0.58 - ETA: 1s - loss: 0.6767 - accuracy: 0.59 - ETA: 1s - loss: 0.6757 - accuracy: 0.59 - ETA: 1s - loss: 0.6753 - accuracy: 0.59 - ETA: 1s - loss: 0.6749 - accuracy: 0.59 - ETA: 1s - loss: 0.6731 - accuracy: 0.60 - ETA: 1s - loss: 0.6720 - accuracy: 0.60 - ETA: 1s - loss: 0.6700 - accuracy: 0.60 - ETA: 1s - loss: 0.6686 - accuracy: 0.60 - ETA: 1s - loss: 0.6679 - accuracy: 0.61 - ETA: 1s - loss: 0.6678 - accuracy: 0.60 - ETA: 1s - loss: 0.6668 - accuracy: 0.61 - ETA: 1s - loss: 0.6663 - accuracy: 0.61 - ETA: 1s - loss: 0.6657 - accuracy: 0.61 - ETA: 1s - loss: 0.6647 - accuracy: 0.61 - ETA: 1s - loss: 0.6645 - accuracy: 0.61 - ETA: 0s - loss: 0.6635 - accuracy: 0.61 - ETA: 0s - loss: 0.6627 - accuracy: 0.61 - ETA: 0s - loss: 0.6621 - accuracy: 0.61 - ETA: 0s - loss: 0.6617 - accuracy: 0.62 - ETA: 0s - loss: 0.6608 - accuracy: 0.62 - ETA: 0s - loss: 0.6597 - accuracy: 0.62 - ETA: 0s - loss: 0.6588 - accuracy: 0.62 - ETA: 0s - loss: 0.6585 - accuracy: 0.62 - ETA: 0s - loss: 0.6582 - accuracy: 0.62 - ETA: 0s - loss: 0.6571 - accuracy: 0.62 - ETA: 0s - loss: 0.6569 - accuracy: 0.62 - ETA: 0s - loss: 0.6559 - accuracy: 0.63 - ETA: 0s - loss: 0.6546 - accuracy: 0.63 - ETA: 0s - loss: 0.6541 - accuracy: 0.63 - ETA: 0s - loss: 0.6536 - accuracy: 0.63 - ETA: 0s - loss: 0.6527 - accuracy: 0.63 - ETA: 0s - loss: 0.6521 - accuracy: 0.63 - ETA: 0s - loss: 0.6517 - accuracy: 0.63 - ETA: 0s - loss: 0.6506 - accuracy: 0.64 - 3s 261us/step - loss: 0.6505 - accuracy: 0.6414 - val_loss: 0.5946 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 5:26 - loss: 0.6950 - accuracy: 0.75 - ETA: 8s - loss: 0.6929 - accuracy: 0.5727 - ETA: 5s - loss: 0.6927 - accuracy: 0.55 - ETA: 4s - loss: 0.6923 - accuracy: 0.53 - ETA: 4s - loss: 0.6922 - accuracy: 0.53 - ETA: 3s - loss: 0.6925 - accuracy: 0.52 - ETA: 3s - loss: 0.6911 - accuracy: 0.53 - ETA: 3s - loss: 0.6898 - accuracy: 0.54 - ETA: 3s - loss: 0.6907 - accuracy: 0.53 - ETA: 3s - loss: 0.6900 - accuracy: 0.53 - ETA: 3s - loss: 0.6890 - accuracy: 0.54 - ETA: 2s - loss: 0.6885 - accuracy: 0.54 - ETA: 2s - loss: 0.6880 - accuracy: 0.54 - ETA: 2s - loss: 0.6869 - accuracy: 0.54 - ETA: 2s - loss: 0.6867 - accuracy: 0.54 - ETA: 2s - loss: 0.6850 - accuracy: 0.55 - ETA: 2s - loss: 0.6848 - accuracy: 0.55 - ETA: 2s - loss: 0.6840 - accuracy: 0.55 - ETA: 2s - loss: 0.6836 - accuracy: 0.55 - ETA: 2s - loss: 0.6832 - accuracy: 0.55 - ETA: 2s - loss: 0.6830 - accuracy: 0.55 - ETA: 2s - loss: 0.6823 - accuracy: 0.55 - ETA: 2s - loss: 0.6811 - accuracy: 0.56 - ETA: 2s - loss: 0.6794 - accuracy: 0.56 - ETA: 1s - loss: 0.6781 - accuracy: 0.57 - ETA: 1s - loss: 0.6773 - accuracy: 0.57 - ETA: 1s - loss: 0.6766 - accuracy: 0.57 - ETA: 1s - loss: 0.6750 - accuracy: 0.57 - ETA: 1s - loss: 0.6735 - accuracy: 0.57 - ETA: 1s - loss: 0.6728 - accuracy: 0.58 - ETA: 1s - loss: 0.6713 - accuracy: 0.58 - ETA: 1s - loss: 0.6707 - accuracy: 0.58 - ETA: 1s - loss: 0.6695 - accuracy: 0.58 - ETA: 1s - loss: 0.6696 - accuracy: 0.58 - ETA: 1s - loss: 0.6684 - accuracy: 0.58 - ETA: 1s - loss: 0.6674 - accuracy: 0.58 - ETA: 1s - loss: 0.6665 - accuracy: 0.59 - ETA: 1s - loss: 0.6656 - accuracy: 0.59 - ETA: 1s - loss: 0.6644 - accuracy: 0.59 - ETA: 1s - loss: 0.6636 - accuracy: 0.59 - ETA: 1s - loss: 0.6619 - accuracy: 0.59 - ETA: 1s - loss: 0.6610 - accuracy: 0.60 - ETA: 0s - loss: 0.6601 - accuracy: 0.60 - ETA: 0s - loss: 0.6597 - accuracy: 0.60 - ETA: 0s - loss: 0.6592 - accuracy: 0.60 - ETA: 0s - loss: 0.6582 - accuracy: 0.60 - ETA: 0s - loss: 0.6568 - accuracy: 0.60 - ETA: 0s - loss: 0.6561 - accuracy: 0.61 - ETA: 0s - loss: 0.6559 - accuracy: 0.61 - ETA: 0s - loss: 0.6551 - accuracy: 0.61 - ETA: 0s - loss: 0.6547 - accuracy: 0.61 - ETA: 0s - loss: 0.6546 - accuracy: 0.61 - ETA: 0s - loss: 0.6538 - accuracy: 0.61 - ETA: 0s - loss: 0.6539 - accuracy: 0.61 - ETA: 0s - loss: 0.6538 - accuracy: 0.61 - ETA: 0s - loss: 0.6524 - accuracy: 0.61 - ETA: 0s - loss: 0.6516 - accuracy: 0.61 - ETA: 0s - loss: 0.6516 - accuracy: 0.62 - ETA: 0s - loss: 0.6507 - accuracy: 0.62 - ETA: 0s - loss: 0.6505 - accuracy: 0.62 - ETA: 0s - loss: 0.6500 - accuracy: 0.62 - 3s 265us/step - loss: 0.6496 - accuracy: 0.6234 - val_loss: 0.5947 - val_accuracy: 0.7095\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:26 - loss: 0.6827 - accuracy: 1.00 - ETA: 8s - loss: 0.6910 - accuracy: 0.5613 - ETA: 5s - loss: 0.6906 - accuracy: 0.53 - ETA: 4s - loss: 0.6902 - accuracy: 0.52 - ETA: 4s - loss: 0.6909 - accuracy: 0.51 - ETA: 3s - loss: 0.6911 - accuracy: 0.51 - ETA: 3s - loss: 0.6907 - accuracy: 0.52 - ETA: 3s - loss: 0.6905 - accuracy: 0.52 - ETA: 3s - loss: 0.6891 - accuracy: 0.53 - ETA: 3s - loss: 0.6879 - accuracy: 0.54 - ETA: 3s - loss: 0.6870 - accuracy: 0.54 - ETA: 2s - loss: 0.6862 - accuracy: 0.54 - ETA: 2s - loss: 0.6861 - accuracy: 0.55 - ETA: 2s - loss: 0.6850 - accuracy: 0.55 - ETA: 2s - loss: 0.6837 - accuracy: 0.56 - ETA: 2s - loss: 0.6829 - accuracy: 0.56 - ETA: 2s - loss: 0.6820 - accuracy: 0.56 - ETA: 2s - loss: 0.6816 - accuracy: 0.56 - ETA: 2s - loss: 0.6807 - accuracy: 0.57 - ETA: 2s - loss: 0.6790 - accuracy: 0.57 - ETA: 2s - loss: 0.6778 - accuracy: 0.57 - ETA: 2s - loss: 0.6766 - accuracy: 0.58 - ETA: 2s - loss: 0.6752 - accuracy: 0.58 - ETA: 2s - loss: 0.6742 - accuracy: 0.58 - ETA: 2s - loss: 0.6733 - accuracy: 0.58 - ETA: 1s - loss: 0.6719 - accuracy: 0.58 - ETA: 1s - loss: 0.6714 - accuracy: 0.59 - ETA: 1s - loss: 0.6709 - accuracy: 0.59 - ETA: 1s - loss: 0.6705 - accuracy: 0.59 - ETA: 1s - loss: 0.6687 - accuracy: 0.59 - ETA: 1s - loss: 0.6675 - accuracy: 0.59 - ETA: 1s - loss: 0.6668 - accuracy: 0.59 - ETA: 1s - loss: 0.6664 - accuracy: 0.60 - ETA: 1s - loss: 0.6662 - accuracy: 0.60 - ETA: 1s - loss: 0.6646 - accuracy: 0.60 - ETA: 1s - loss: 0.6635 - accuracy: 0.60 - ETA: 1s - loss: 0.6618 - accuracy: 0.61 - ETA: 1s - loss: 0.6618 - accuracy: 0.61 - ETA: 1s - loss: 0.6611 - accuracy: 0.61 - ETA: 1s - loss: 0.6605 - accuracy: 0.61 - ETA: 1s - loss: 0.6593 - accuracy: 0.61 - ETA: 1s - loss: 0.6581 - accuracy: 0.61 - ETA: 1s - loss: 0.6578 - accuracy: 0.61 - ETA: 0s - loss: 0.6576 - accuracy: 0.61 - ETA: 0s - loss: 0.6573 - accuracy: 0.62 - ETA: 0s - loss: 0.6564 - accuracy: 0.62 - ETA: 0s - loss: 0.6566 - accuracy: 0.62 - ETA: 0s - loss: 0.6562 - accuracy: 0.62 - ETA: 0s - loss: 0.6556 - accuracy: 0.62 - ETA: 0s - loss: 0.6558 - accuracy: 0.62 - ETA: 0s - loss: 0.6552 - accuracy: 0.62 - ETA: 0s - loss: 0.6542 - accuracy: 0.62 - ETA: 0s - loss: 0.6533 - accuracy: 0.62 - ETA: 0s - loss: 0.6520 - accuracy: 0.63 - ETA: 0s - loss: 0.6514 - accuracy: 0.63 - ETA: 0s - loss: 0.6507 - accuracy: 0.63 - ETA: 0s - loss: 0.6505 - accuracy: 0.63 - ETA: 0s - loss: 0.6501 - accuracy: 0.63 - ETA: 0s - loss: 0.6500 - accuracy: 0.63 - ETA: 0s - loss: 0.6492 - accuracy: 0.63 - ETA: 0s - loss: 0.6492 - accuracy: 0.63 - ETA: 0s - loss: 0.6487 - accuracy: 0.63 - 3s 269us/step - loss: 0.6486 - accuracy: 0.6335 - val_loss: 0.5905 - val_accuracy: 0.7138\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 104us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:32 - loss: 0.6942 - accuracy: 0.75 - ETA: 8s - loss: 0.6940 - accuracy: 0.4907 - ETA: 5s - loss: 0.6934 - accuracy: 0.53 - ETA: 4s - loss: 0.6931 - accuracy: 0.53 - ETA: 4s - loss: 0.6929 - accuracy: 0.53 - ETA: 3s - loss: 0.6926 - accuracy: 0.53 - ETA: 3s - loss: 0.6924 - accuracy: 0.53 - ETA: 3s - loss: 0.6923 - accuracy: 0.53 - ETA: 3s - loss: 0.6918 - accuracy: 0.54 - ETA: 3s - loss: 0.6914 - accuracy: 0.54 - ETA: 3s - loss: 0.6912 - accuracy: 0.54 - ETA: 2s - loss: 0.6903 - accuracy: 0.55 - ETA: 2s - loss: 0.6894 - accuracy: 0.56 - ETA: 2s - loss: 0.6883 - accuracy: 0.57 - ETA: 2s - loss: 0.6873 - accuracy: 0.57 - ETA: 2s - loss: 0.6869 - accuracy: 0.57 - ETA: 2s - loss: 0.6868 - accuracy: 0.57 - ETA: 2s - loss: 0.6856 - accuracy: 0.57 - ETA: 2s - loss: 0.6844 - accuracy: 0.58 - ETA: 2s - loss: 0.6835 - accuracy: 0.58 - ETA: 2s - loss: 0.6820 - accuracy: 0.59 - ETA: 2s - loss: 0.6810 - accuracy: 0.59 - ETA: 2s - loss: 0.6796 - accuracy: 0.59 - ETA: 2s - loss: 0.6784 - accuracy: 0.59 - ETA: 2s - loss: 0.6770 - accuracy: 0.60 - ETA: 1s - loss: 0.6750 - accuracy: 0.60 - ETA: 1s - loss: 0.6745 - accuracy: 0.60 - ETA: 1s - loss: 0.6730 - accuracy: 0.60 - ETA: 1s - loss: 0.6721 - accuracy: 0.60 - ETA: 1s - loss: 0.6711 - accuracy: 0.60 - ETA: 1s - loss: 0.6698 - accuracy: 0.61 - ETA: 1s - loss: 0.6677 - accuracy: 0.61 - ETA: 1s - loss: 0.6656 - accuracy: 0.61 - ETA: 1s - loss: 0.6646 - accuracy: 0.61 - ETA: 1s - loss: 0.6639 - accuracy: 0.62 - ETA: 1s - loss: 0.6628 - accuracy: 0.62 - ETA: 1s - loss: 0.6620 - accuracy: 0.62 - ETA: 1s - loss: 0.6613 - accuracy: 0.62 - ETA: 1s - loss: 0.6605 - accuracy: 0.62 - ETA: 1s - loss: 0.6593 - accuracy: 0.62 - ETA: 1s - loss: 0.6579 - accuracy: 0.62 - ETA: 0s - loss: 0.6574 - accuracy: 0.62 - ETA: 0s - loss: 0.6566 - accuracy: 0.63 - ETA: 0s - loss: 0.6557 - accuracy: 0.63 - ETA: 0s - loss: 0.6551 - accuracy: 0.63 - ETA: 0s - loss: 0.6537 - accuracy: 0.63 - ETA: 0s - loss: 0.6518 - accuracy: 0.63 - ETA: 0s - loss: 0.6511 - accuracy: 0.63 - ETA: 0s - loss: 0.6507 - accuracy: 0.63 - ETA: 0s - loss: 0.6502 - accuracy: 0.63 - ETA: 0s - loss: 0.6489 - accuracy: 0.63 - ETA: 0s - loss: 0.6485 - accuracy: 0.64 - ETA: 0s - loss: 0.6483 - accuracy: 0.64 - ETA: 0s - loss: 0.6478 - accuracy: 0.64 - ETA: 0s - loss: 0.6472 - accuracy: 0.64 - ETA: 0s - loss: 0.6464 - accuracy: 0.64 - ETA: 0s - loss: 0.6458 - accuracy: 0.64 - ETA: 0s - loss: 0.6457 - accuracy: 0.64 - ETA: 0s - loss: 0.6453 - accuracy: 0.64 - 3s 258us/step - loss: 0.6447 - accuracy: 0.6461 - val_loss: 0.5901 - val_accuracy: 0.7180\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 109us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:38 - loss: 0.6952 - accuracy: 0.50 - ETA: 9s - loss: 0.6928 - accuracy: 0.5566 - ETA: 6s - loss: 0.6924 - accuracy: 0.52 - ETA: 4s - loss: 0.6919 - accuracy: 0.53 - ETA: 4s - loss: 0.6920 - accuracy: 0.53 - ETA: 4s - loss: 0.6916 - accuracy: 0.53 - ETA: 3s - loss: 0.6907 - accuracy: 0.53 - ETA: 3s - loss: 0.6897 - accuracy: 0.54 - ETA: 3s - loss: 0.6891 - accuracy: 0.54 - ETA: 3s - loss: 0.6872 - accuracy: 0.55 - ETA: 3s - loss: 0.6864 - accuracy: 0.55 - ETA: 3s - loss: 0.6858 - accuracy: 0.55 - ETA: 3s - loss: 0.6840 - accuracy: 0.56 - ETA: 2s - loss: 0.6833 - accuracy: 0.56 - ETA: 2s - loss: 0.6830 - accuracy: 0.56 - ETA: 2s - loss: 0.6826 - accuracy: 0.56 - ETA: 2s - loss: 0.6826 - accuracy: 0.56 - ETA: 2s - loss: 0.6812 - accuracy: 0.56 - ETA: 2s - loss: 0.6807 - accuracy: 0.57 - ETA: 2s - loss: 0.6803 - accuracy: 0.57 - ETA: 2s - loss: 0.6794 - accuracy: 0.57 - ETA: 2s - loss: 0.6776 - accuracy: 0.58 - ETA: 2s - loss: 0.6767 - accuracy: 0.58 - ETA: 2s - loss: 0.6753 - accuracy: 0.58 - ETA: 2s - loss: 0.6741 - accuracy: 0.58 - ETA: 2s - loss: 0.6732 - accuracy: 0.59 - ETA: 2s - loss: 0.6728 - accuracy: 0.59 - ETA: 1s - loss: 0.6716 - accuracy: 0.59 - ETA: 1s - loss: 0.6706 - accuracy: 0.60 - ETA: 1s - loss: 0.6691 - accuracy: 0.60 - ETA: 1s - loss: 0.6681 - accuracy: 0.60 - ETA: 1s - loss: 0.6680 - accuracy: 0.60 - ETA: 1s - loss: 0.6671 - accuracy: 0.61 - ETA: 1s - loss: 0.6658 - accuracy: 0.61 - ETA: 1s - loss: 0.6645 - accuracy: 0.61 - ETA: 1s - loss: 0.6637 - accuracy: 0.61 - ETA: 1s - loss: 0.6623 - accuracy: 0.61 - ETA: 1s - loss: 0.6613 - accuracy: 0.61 - ETA: 1s - loss: 0.6601 - accuracy: 0.62 - ETA: 1s - loss: 0.6599 - accuracy: 0.62 - ETA: 1s - loss: 0.6591 - accuracy: 0.62 - ETA: 1s - loss: 0.6582 - accuracy: 0.62 - ETA: 1s - loss: 0.6575 - accuracy: 0.62 - ETA: 0s - loss: 0.6572 - accuracy: 0.62 - ETA: 0s - loss: 0.6562 - accuracy: 0.62 - ETA: 0s - loss: 0.6556 - accuracy: 0.62 - ETA: 0s - loss: 0.6547 - accuracy: 0.63 - ETA: 0s - loss: 0.6540 - accuracy: 0.63 - ETA: 0s - loss: 0.6535 - accuracy: 0.63 - ETA: 0s - loss: 0.6529 - accuracy: 0.63 - ETA: 0s - loss: 0.6524 - accuracy: 0.63 - ETA: 0s - loss: 0.6524 - accuracy: 0.63 - ETA: 0s - loss: 0.6519 - accuracy: 0.63 - ETA: 0s - loss: 0.6519 - accuracy: 0.63 - ETA: 0s - loss: 0.6520 - accuracy: 0.63 - ETA: 0s - loss: 0.6516 - accuracy: 0.63 - ETA: 0s - loss: 0.6510 - accuracy: 0.63 - ETA: 0s - loss: 0.6504 - accuracy: 0.63 - ETA: 0s - loss: 0.6496 - accuracy: 0.64 - ETA: 0s - loss: 0.6493 - accuracy: 0.64 - ETA: 0s - loss: 0.6482 - accuracy: 0.64 - ETA: 0s - loss: 0.6479 - accuracy: 0.64 - 3s 269us/step - loss: 0.6477 - accuracy: 0.6434 - val_loss: 0.5888 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 5:23 - loss: 0.6891 - accuracy: 1.00 - ETA: 8s - loss: 0.6946 - accuracy: 0.5189 - ETA: 5s - loss: 0.6935 - accuracy: 0.54 - ETA: 4s - loss: 0.6930 - accuracy: 0.53 - ETA: 4s - loss: 0.6915 - accuracy: 0.53 - ETA: 3s - loss: 0.6917 - accuracy: 0.53 - ETA: 3s - loss: 0.6914 - accuracy: 0.53 - ETA: 3s - loss: 0.6915 - accuracy: 0.53 - ETA: 3s - loss: 0.6905 - accuracy: 0.53 - ETA: 3s - loss: 0.6896 - accuracy: 0.53 - ETA: 3s - loss: 0.6880 - accuracy: 0.54 - ETA: 3s - loss: 0.6881 - accuracy: 0.54 - ETA: 2s - loss: 0.6872 - accuracy: 0.54 - ETA: 2s - loss: 0.6867 - accuracy: 0.55 - ETA: 2s - loss: 0.6849 - accuracy: 0.56 - ETA: 2s - loss: 0.6847 - accuracy: 0.55 - ETA: 2s - loss: 0.6831 - accuracy: 0.56 - ETA: 2s - loss: 0.6822 - accuracy: 0.56 - ETA: 2s - loss: 0.6810 - accuracy: 0.57 - ETA: 2s - loss: 0.6801 - accuracy: 0.57 - ETA: 2s - loss: 0.6794 - accuracy: 0.57 - ETA: 2s - loss: 0.6786 - accuracy: 0.58 - ETA: 2s - loss: 0.6772 - accuracy: 0.58 - ETA: 2s - loss: 0.6767 - accuracy: 0.58 - ETA: 2s - loss: 0.6756 - accuracy: 0.58 - ETA: 1s - loss: 0.6736 - accuracy: 0.59 - ETA: 1s - loss: 0.6732 - accuracy: 0.59 - ETA: 1s - loss: 0.6708 - accuracy: 0.59 - ETA: 1s - loss: 0.6695 - accuracy: 0.60 - ETA: 1s - loss: 0.6683 - accuracy: 0.60 - ETA: 1s - loss: 0.6672 - accuracy: 0.60 - ETA: 1s - loss: 0.6657 - accuracy: 0.60 - ETA: 1s - loss: 0.6646 - accuracy: 0.60 - ETA: 1s - loss: 0.6633 - accuracy: 0.60 - ETA: 1s - loss: 0.6620 - accuracy: 0.61 - ETA: 1s - loss: 0.6619 - accuracy: 0.61 - ETA: 1s - loss: 0.6612 - accuracy: 0.61 - ETA: 1s - loss: 0.6605 - accuracy: 0.61 - ETA: 1s - loss: 0.6597 - accuracy: 0.61 - ETA: 1s - loss: 0.6581 - accuracy: 0.61 - ETA: 1s - loss: 0.6567 - accuracy: 0.61 - ETA: 1s - loss: 0.6542 - accuracy: 0.62 - ETA: 0s - loss: 0.6536 - accuracy: 0.62 - ETA: 0s - loss: 0.6544 - accuracy: 0.62 - ETA: 0s - loss: 0.6546 - accuracy: 0.62 - ETA: 0s - loss: 0.6535 - accuracy: 0.62 - ETA: 0s - loss: 0.6532 - accuracy: 0.62 - ETA: 0s - loss: 0.6529 - accuracy: 0.62 - ETA: 0s - loss: 0.6518 - accuracy: 0.62 - ETA: 0s - loss: 0.6505 - accuracy: 0.62 - ETA: 0s - loss: 0.6498 - accuracy: 0.62 - ETA: 0s - loss: 0.6485 - accuracy: 0.63 - ETA: 0s - loss: 0.6479 - accuracy: 0.63 - ETA: 0s - loss: 0.6463 - accuracy: 0.63 - ETA: 0s - loss: 0.6459 - accuracy: 0.63 - ETA: 0s - loss: 0.6458 - accuracy: 0.63 - ETA: 0s - loss: 0.6447 - accuracy: 0.63 - ETA: 0s - loss: 0.6448 - accuracy: 0.63 - ETA: 0s - loss: 0.6438 - accuracy: 0.63 - ETA: 0s - loss: 0.6436 - accuracy: 0.63 - ETA: 0s - loss: 0.6427 - accuracy: 0.63 - 3s 267us/step - loss: 0.6425 - accuracy: 0.6378 - val_loss: 0.5820 - val_accuracy: 0.7180\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:23 - loss: 0.6791 - accuracy: 1.00 - ETA: 8s - loss: 0.6963 - accuracy: 0.4861 - ETA: 5s - loss: 0.6946 - accuracy: 0.50 - ETA: 4s - loss: 0.6907 - accuracy: 0.53 - ETA: 4s - loss: 0.6905 - accuracy: 0.52 - ETA: 3s - loss: 0.6890 - accuracy: 0.52 - ETA: 3s - loss: 0.6882 - accuracy: 0.52 - ETA: 3s - loss: 0.6856 - accuracy: 0.53 - ETA: 3s - loss: 0.6851 - accuracy: 0.53 - ETA: 3s - loss: 0.6835 - accuracy: 0.54 - ETA: 3s - loss: 0.6824 - accuracy: 0.54 - ETA: 2s - loss: 0.6818 - accuracy: 0.54 - ETA: 2s - loss: 0.6809 - accuracy: 0.55 - ETA: 2s - loss: 0.6800 - accuracy: 0.56 - ETA: 2s - loss: 0.6792 - accuracy: 0.56 - ETA: 2s - loss: 0.6775 - accuracy: 0.57 - ETA: 2s - loss: 0.6760 - accuracy: 0.57 - ETA: 2s - loss: 0.6755 - accuracy: 0.57 - ETA: 2s - loss: 0.6747 - accuracy: 0.58 - ETA: 2s - loss: 0.6736 - accuracy: 0.58 - ETA: 2s - loss: 0.6727 - accuracy: 0.58 - ETA: 2s - loss: 0.6710 - accuracy: 0.59 - ETA: 2s - loss: 0.6700 - accuracy: 0.59 - ETA: 2s - loss: 0.6689 - accuracy: 0.59 - ETA: 1s - loss: 0.6671 - accuracy: 0.60 - ETA: 1s - loss: 0.6660 - accuracy: 0.60 - ETA: 1s - loss: 0.6642 - accuracy: 0.60 - ETA: 1s - loss: 0.6631 - accuracy: 0.60 - ETA: 1s - loss: 0.6613 - accuracy: 0.61 - ETA: 1s - loss: 0.6600 - accuracy: 0.61 - ETA: 1s - loss: 0.6585 - accuracy: 0.61 - ETA: 1s - loss: 0.6577 - accuracy: 0.61 - ETA: 1s - loss: 0.6574 - accuracy: 0.61 - ETA: 1s - loss: 0.6569 - accuracy: 0.61 - ETA: 1s - loss: 0.6564 - accuracy: 0.61 - ETA: 1s - loss: 0.6548 - accuracy: 0.61 - ETA: 1s - loss: 0.6536 - accuracy: 0.62 - ETA: 1s - loss: 0.6526 - accuracy: 0.62 - ETA: 1s - loss: 0.6512 - accuracy: 0.62 - ETA: 1s - loss: 0.6507 - accuracy: 0.62 - ETA: 1s - loss: 0.6495 - accuracy: 0.62 - ETA: 1s - loss: 0.6477 - accuracy: 0.62 - ETA: 0s - loss: 0.6483 - accuracy: 0.62 - ETA: 0s - loss: 0.6476 - accuracy: 0.63 - ETA: 0s - loss: 0.6486 - accuracy: 0.62 - ETA: 0s - loss: 0.6472 - accuracy: 0.63 - ETA: 0s - loss: 0.6463 - accuracy: 0.63 - ETA: 0s - loss: 0.6452 - accuracy: 0.63 - ETA: 0s - loss: 0.6433 - accuracy: 0.63 - ETA: 0s - loss: 0.6427 - accuracy: 0.63 - ETA: 0s - loss: 0.6425 - accuracy: 0.63 - ETA: 0s - loss: 0.6427 - accuracy: 0.63 - ETA: 0s - loss: 0.6414 - accuracy: 0.64 - ETA: 0s - loss: 0.6410 - accuracy: 0.64 - ETA: 0s - loss: 0.6407 - accuracy: 0.64 - ETA: 0s - loss: 0.6395 - accuracy: 0.64 - ETA: 0s - loss: 0.6390 - accuracy: 0.64 - ETA: 0s - loss: 0.6381 - accuracy: 0.64 - ETA: 0s - loss: 0.6363 - accuracy: 0.64 - ETA: 0s - loss: 0.6359 - accuracy: 0.64 - ETA: 0s - loss: 0.6351 - accuracy: 0.64 - 3s 264us/step - loss: 0.6349 - accuracy: 0.6477 - val_loss: 0.5788 - val_accuracy: 0.7173\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:23 - loss: 0.6925 - accuracy: 0.50 - ETA: 8s - loss: 0.6952 - accuracy: 0.4858 - ETA: 5s - loss: 0.6944 - accuracy: 0.50 - ETA: 4s - loss: 0.6937 - accuracy: 0.52 - ETA: 4s - loss: 0.6911 - accuracy: 0.54 - ETA: 3s - loss: 0.6909 - accuracy: 0.53 - ETA: 3s - loss: 0.6877 - accuracy: 0.55 - ETA: 3s - loss: 0.6876 - accuracy: 0.54 - ETA: 3s - loss: 0.6880 - accuracy: 0.54 - ETA: 3s - loss: 0.6874 - accuracy: 0.55 - ETA: 3s - loss: 0.6868 - accuracy: 0.55 - ETA: 2s - loss: 0.6849 - accuracy: 0.57 - ETA: 2s - loss: 0.6830 - accuracy: 0.57 - ETA: 2s - loss: 0.6817 - accuracy: 0.57 - ETA: 2s - loss: 0.6792 - accuracy: 0.58 - ETA: 2s - loss: 0.6782 - accuracy: 0.58 - ETA: 2s - loss: 0.6770 - accuracy: 0.58 - ETA: 2s - loss: 0.6753 - accuracy: 0.58 - ETA: 2s - loss: 0.6737 - accuracy: 0.59 - ETA: 2s - loss: 0.6732 - accuracy: 0.59 - ETA: 2s - loss: 0.6721 - accuracy: 0.59 - ETA: 2s - loss: 0.6697 - accuracy: 0.60 - ETA: 2s - loss: 0.6688 - accuracy: 0.60 - ETA: 2s - loss: 0.6677 - accuracy: 0.60 - ETA: 1s - loss: 0.6678 - accuracy: 0.60 - ETA: 1s - loss: 0.6670 - accuracy: 0.60 - ETA: 1s - loss: 0.6670 - accuracy: 0.60 - ETA: 1s - loss: 0.6657 - accuracy: 0.61 - ETA: 1s - loss: 0.6652 - accuracy: 0.61 - ETA: 1s - loss: 0.6646 - accuracy: 0.61 - ETA: 1s - loss: 0.6643 - accuracy: 0.61 - ETA: 1s - loss: 0.6621 - accuracy: 0.61 - ETA: 1s - loss: 0.6613 - accuracy: 0.61 - ETA: 1s - loss: 0.6592 - accuracy: 0.62 - ETA: 1s - loss: 0.6572 - accuracy: 0.62 - ETA: 1s - loss: 0.6555 - accuracy: 0.62 - ETA: 1s - loss: 0.6538 - accuracy: 0.62 - ETA: 1s - loss: 0.6527 - accuracy: 0.62 - ETA: 1s - loss: 0.6516 - accuracy: 0.62 - ETA: 1s - loss: 0.6504 - accuracy: 0.63 - ETA: 1s - loss: 0.6505 - accuracy: 0.63 - ETA: 1s - loss: 0.6502 - accuracy: 0.63 - ETA: 0s - loss: 0.6488 - accuracy: 0.63 - ETA: 0s - loss: 0.6469 - accuracy: 0.63 - ETA: 0s - loss: 0.6460 - accuracy: 0.63 - ETA: 0s - loss: 0.6453 - accuracy: 0.63 - ETA: 0s - loss: 0.6440 - accuracy: 0.63 - ETA: 0s - loss: 0.6431 - accuracy: 0.63 - ETA: 0s - loss: 0.6428 - accuracy: 0.64 - ETA: 0s - loss: 0.6417 - accuracy: 0.64 - ETA: 0s - loss: 0.6416 - accuracy: 0.64 - ETA: 0s - loss: 0.6410 - accuracy: 0.64 - ETA: 0s - loss: 0.6402 - accuracy: 0.64 - ETA: 0s - loss: 0.6399 - accuracy: 0.64 - ETA: 0s - loss: 0.6395 - accuracy: 0.64 - ETA: 0s - loss: 0.6384 - accuracy: 0.64 - ETA: 0s - loss: 0.6379 - accuracy: 0.64 - ETA: 0s - loss: 0.6372 - accuracy: 0.64 - ETA: 0s - loss: 0.6368 - accuracy: 0.64 - ETA: 0s - loss: 0.6367 - accuracy: 0.64 - ETA: 0s - loss: 0.6360 - accuracy: 0.65 - ETA: 0s - loss: 0.6357 - accuracy: 0.65 - 3s 268us/step - loss: 0.6358 - accuracy: 0.6516 - val_loss: 0.5817 - val_accuracy: 0.7195\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 100us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:38 - loss: 0.6894 - accuracy: 1.00 - ETA: 8s - loss: 0.6952 - accuracy: 0.4821 - ETA: 5s - loss: 0.6938 - accuracy: 0.50 - ETA: 4s - loss: 0.6942 - accuracy: 0.49 - ETA: 4s - loss: 0.6939 - accuracy: 0.50 - ETA: 3s - loss: 0.6932 - accuracy: 0.52 - ETA: 3s - loss: 0.6926 - accuracy: 0.53 - ETA: 3s - loss: 0.6920 - accuracy: 0.54 - ETA: 3s - loss: 0.6917 - accuracy: 0.55 - ETA: 3s - loss: 0.6906 - accuracy: 0.55 - ETA: 2s - loss: 0.6894 - accuracy: 0.56 - ETA: 2s - loss: 0.6881 - accuracy: 0.57 - ETA: 2s - loss: 0.6873 - accuracy: 0.57 - ETA: 2s - loss: 0.6858 - accuracy: 0.58 - ETA: 2s - loss: 0.6843 - accuracy: 0.58 - ETA: 2s - loss: 0.6834 - accuracy: 0.58 - ETA: 2s - loss: 0.6827 - accuracy: 0.58 - ETA: 2s - loss: 0.6808 - accuracy: 0.59 - ETA: 2s - loss: 0.6802 - accuracy: 0.59 - ETA: 2s - loss: 0.6785 - accuracy: 0.60 - ETA: 2s - loss: 0.6769 - accuracy: 0.60 - ETA: 2s - loss: 0.6748 - accuracy: 0.60 - ETA: 2s - loss: 0.6733 - accuracy: 0.61 - ETA: 1s - loss: 0.6713 - accuracy: 0.61 - ETA: 1s - loss: 0.6685 - accuracy: 0.61 - ETA: 1s - loss: 0.6671 - accuracy: 0.61 - ETA: 1s - loss: 0.6660 - accuracy: 0.62 - ETA: 1s - loss: 0.6645 - accuracy: 0.62 - ETA: 1s - loss: 0.6635 - accuracy: 0.62 - ETA: 1s - loss: 0.6631 - accuracy: 0.62 - ETA: 1s - loss: 0.6622 - accuracy: 0.62 - ETA: 1s - loss: 0.6607 - accuracy: 0.62 - ETA: 1s - loss: 0.6592 - accuracy: 0.63 - ETA: 1s - loss: 0.6593 - accuracy: 0.63 - ETA: 1s - loss: 0.6590 - accuracy: 0.63 - ETA: 1s - loss: 0.6571 - accuracy: 0.63 - ETA: 1s - loss: 0.6561 - accuracy: 0.63 - ETA: 1s - loss: 0.6547 - accuracy: 0.63 - ETA: 1s - loss: 0.6533 - accuracy: 0.63 - ETA: 1s - loss: 0.6535 - accuracy: 0.63 - ETA: 1s - loss: 0.6524 - accuracy: 0.63 - ETA: 0s - loss: 0.6511 - accuracy: 0.64 - ETA: 0s - loss: 0.6497 - accuracy: 0.64 - ETA: 0s - loss: 0.6500 - accuracy: 0.64 - ETA: 0s - loss: 0.6490 - accuracy: 0.64 - ETA: 0s - loss: 0.6476 - accuracy: 0.64 - ETA: 0s - loss: 0.6465 - accuracy: 0.64 - ETA: 0s - loss: 0.6462 - accuracy: 0.64 - ETA: 0s - loss: 0.6452 - accuracy: 0.64 - ETA: 0s - loss: 0.6445 - accuracy: 0.64 - ETA: 0s - loss: 0.6439 - accuracy: 0.65 - ETA: 0s - loss: 0.6430 - accuracy: 0.65 - ETA: 0s - loss: 0.6409 - accuracy: 0.65 - ETA: 0s - loss: 0.6401 - accuracy: 0.65 - ETA: 0s - loss: 0.6400 - accuracy: 0.65 - ETA: 0s - loss: 0.6391 - accuracy: 0.65 - ETA: 0s - loss: 0.6389 - accuracy: 0.65 - ETA: 0s - loss: 0.6383 - accuracy: 0.65 - ETA: 0s - loss: 0.6379 - accuracy: 0.65 - ETA: 0s - loss: 0.6377 - accuracy: 0.65 - 3s 261us/step - loss: 0.6378 - accuracy: 0.6576 - val_loss: 0.5798 - val_accuracy: 0.7188\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 110us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 5:26 - loss: 0.7085 - accuracy: 0.25 - ETA: 8s - loss: 0.6946 - accuracy: 0.5318 - ETA: 5s - loss: 0.6938 - accuracy: 0.52 - ETA: 4s - loss: 0.6937 - accuracy: 0.52 - ETA: 4s - loss: 0.6935 - accuracy: 0.53 - ETA: 3s - loss: 0.6928 - accuracy: 0.54 - ETA: 3s - loss: 0.6914 - accuracy: 0.54 - ETA: 3s - loss: 0.6908 - accuracy: 0.54 - ETA: 3s - loss: 0.6897 - accuracy: 0.55 - ETA: 3s - loss: 0.6888 - accuracy: 0.55 - ETA: 3s - loss: 0.6875 - accuracy: 0.56 - ETA: 3s - loss: 0.6864 - accuracy: 0.57 - ETA: 2s - loss: 0.6853 - accuracy: 0.57 - ETA: 2s - loss: 0.6839 - accuracy: 0.58 - ETA: 2s - loss: 0.6829 - accuracy: 0.58 - ETA: 2s - loss: 0.6811 - accuracy: 0.58 - ETA: 2s - loss: 0.6795 - accuracy: 0.59 - ETA: 2s - loss: 0.6779 - accuracy: 0.59 - ETA: 2s - loss: 0.6772 - accuracy: 0.60 - ETA: 2s - loss: 0.6760 - accuracy: 0.60 - ETA: 2s - loss: 0.6745 - accuracy: 0.60 - ETA: 2s - loss: 0.6738 - accuracy: 0.60 - ETA: 2s - loss: 0.6713 - accuracy: 0.60 - ETA: 2s - loss: 0.6695 - accuracy: 0.61 - ETA: 2s - loss: 0.6683 - accuracy: 0.61 - ETA: 1s - loss: 0.6680 - accuracy: 0.61 - ETA: 1s - loss: 0.6664 - accuracy: 0.61 - ETA: 1s - loss: 0.6650 - accuracy: 0.61 - ETA: 1s - loss: 0.6635 - accuracy: 0.61 - ETA: 1s - loss: 0.6617 - accuracy: 0.61 - ETA: 1s - loss: 0.6610 - accuracy: 0.62 - ETA: 1s - loss: 0.6596 - accuracy: 0.62 - ETA: 1s - loss: 0.6583 - accuracy: 0.62 - ETA: 1s - loss: 0.6570 - accuracy: 0.62 - ETA: 1s - loss: 0.6555 - accuracy: 0.62 - ETA: 1s - loss: 0.6533 - accuracy: 0.63 - ETA: 1s - loss: 0.6524 - accuracy: 0.63 - ETA: 1s - loss: 0.6510 - accuracy: 0.63 - ETA: 1s - loss: 0.6491 - accuracy: 0.63 - ETA: 1s - loss: 0.6479 - accuracy: 0.63 - ETA: 1s - loss: 0.6483 - accuracy: 0.63 - ETA: 1s - loss: 0.6480 - accuracy: 0.63 - ETA: 1s - loss: 0.6472 - accuracy: 0.63 - ETA: 0s - loss: 0.6463 - accuracy: 0.64 - ETA: 0s - loss: 0.6458 - accuracy: 0.64 - ETA: 0s - loss: 0.6454 - accuracy: 0.64 - ETA: 0s - loss: 0.6446 - accuracy: 0.64 - ETA: 0s - loss: 0.6440 - accuracy: 0.64 - ETA: 0s - loss: 0.6432 - accuracy: 0.64 - ETA: 0s - loss: 0.6421 - accuracy: 0.64 - ETA: 0s - loss: 0.6413 - accuracy: 0.64 - ETA: 0s - loss: 0.6404 - accuracy: 0.64 - ETA: 0s - loss: 0.6396 - accuracy: 0.64 - ETA: 0s - loss: 0.6394 - accuracy: 0.64 - ETA: 0s - loss: 0.6392 - accuracy: 0.64 - ETA: 0s - loss: 0.6387 - accuracy: 0.65 - ETA: 0s - loss: 0.6385 - accuracy: 0.65 - ETA: 0s - loss: 0.6377 - accuracy: 0.65 - ETA: 0s - loss: 0.6376 - accuracy: 0.65 - ETA: 0s - loss: 0.6369 - accuracy: 0.65 - ETA: 0s - loss: 0.6363 - accuracy: 0.65 - ETA: 0s - loss: 0.6360 - accuracy: 0.65 - 3s 269us/step - loss: 0.6356 - accuracy: 0.6544 - val_loss: 0.5839 - val_accuracy: 0.7195\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 104us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 4:45 - loss: 0.6942 - accuracy: 0.50 - ETA: 7s - loss: 0.6935 - accuracy: 0.5259 - ETA: 5s - loss: 0.6935 - accuracy: 0.51 - ETA: 4s - loss: 0.6924 - accuracy: 0.54 - ETA: 3s - loss: 0.6933 - accuracy: 0.52 - ETA: 3s - loss: 0.6936 - accuracy: 0.50 - ETA: 3s - loss: 0.6935 - accuracy: 0.51 - ETA: 3s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6928 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6923 - accuracy: 0.51 - ETA: 1s - loss: 0.6922 - accuracy: 0.51 - ETA: 1s - loss: 0.6921 - accuracy: 0.51 - ETA: 1s - loss: 0.6921 - accuracy: 0.51 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 0s - loss: 0.6916 - accuracy: 0.52 - ETA: 0s - loss: 0.6916 - accuracy: 0.52 - ETA: 0s - loss: 0.6915 - accuracy: 0.52 - ETA: 0s - loss: 0.6912 - accuracy: 0.52 - ETA: 0s - loss: 0.6911 - accuracy: 0.52 - ETA: 0s - loss: 0.6911 - accuracy: 0.52 - ETA: 0s - loss: 0.6909 - accuracy: 0.52 - ETA: 0s - loss: 0.6905 - accuracy: 0.52 - ETA: 0s - loss: 0.6904 - accuracy: 0.52 - ETA: 0s - loss: 0.6905 - accuracy: 0.52 - ETA: 0s - loss: 0.6904 - accuracy: 0.52 - ETA: 0s - loss: 0.6902 - accuracy: 0.52 - ETA: 0s - loss: 0.6899 - accuracy: 0.52 - ETA: 0s - loss: 0.6897 - accuracy: 0.52 - ETA: 0s - loss: 0.6895 - accuracy: 0.52 - ETA: 0s - loss: 0.6894 - accuracy: 0.52 - ETA: 0s - loss: 0.6891 - accuracy: 0.53 - ETA: 0s - loss: 0.6888 - accuracy: 0.53 - ETA: 0s - loss: 0.6887 - accuracy: 0.53 - ETA: 0s - loss: 0.6886 - accuracy: 0.53 - 3s 251us/step - loss: 0.6885 - accuracy: 0.5311 - val_loss: 0.6692 - val_accuracy: 0.6712\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 4:48 - loss: 0.6907 - accuracy: 0.50 - ETA: 7s - loss: 0.6962 - accuracy: 0.4598 - ETA: 5s - loss: 0.6938 - accuracy: 0.51 - ETA: 4s - loss: 0.6942 - accuracy: 0.50 - ETA: 3s - loss: 0.6945 - accuracy: 0.50 - ETA: 3s - loss: 0.6943 - accuracy: 0.49 - ETA: 3s - loss: 0.6942 - accuracy: 0.49 - ETA: 3s - loss: 0.6941 - accuracy: 0.49 - ETA: 3s - loss: 0.6940 - accuracy: 0.49 - ETA: 2s - loss: 0.6940 - accuracy: 0.50 - ETA: 2s - loss: 0.6934 - accuracy: 0.51 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6929 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6928 - accuracy: 0.51 - ETA: 2s - loss: 0.6927 - accuracy: 0.51 - ETA: 2s - loss: 0.6924 - accuracy: 0.52 - ETA: 2s - loss: 0.6923 - accuracy: 0.52 - ETA: 2s - loss: 0.6923 - accuracy: 0.52 - ETA: 2s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6917 - accuracy: 0.52 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6917 - accuracy: 0.52 - ETA: 1s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6915 - accuracy: 0.53 - ETA: 1s - loss: 0.6915 - accuracy: 0.53 - ETA: 1s - loss: 0.6913 - accuracy: 0.53 - ETA: 1s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6909 - accuracy: 0.53 - ETA: 1s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 1s - loss: 0.6907 - accuracy: 0.53 - ETA: 0s - loss: 0.6906 - accuracy: 0.54 - ETA: 0s - loss: 0.6904 - accuracy: 0.54 - ETA: 0s - loss: 0.6904 - accuracy: 0.54 - ETA: 0s - loss: 0.6903 - accuracy: 0.54 - ETA: 0s - loss: 0.6901 - accuracy: 0.54 - ETA: 0s - loss: 0.6901 - accuracy: 0.54 - ETA: 0s - loss: 0.6900 - accuracy: 0.54 - ETA: 0s - loss: 0.6899 - accuracy: 0.54 - ETA: 0s - loss: 0.6897 - accuracy: 0.54 - ETA: 0s - loss: 0.6894 - accuracy: 0.55 - ETA: 0s - loss: 0.6892 - accuracy: 0.55 - ETA: 0s - loss: 0.6891 - accuracy: 0.55 - ETA: 0s - loss: 0.6888 - accuracy: 0.55 - ETA: 0s - loss: 0.6885 - accuracy: 0.55 - ETA: 0s - loss: 0.6882 - accuracy: 0.55 - ETA: 0s - loss: 0.6879 - accuracy: 0.55 - ETA: 0s - loss: 0.6878 - accuracy: 0.55 - ETA: 0s - loss: 0.6875 - accuracy: 0.55 - ETA: 0s - loss: 0.6874 - accuracy: 0.56 - 3s 254us/step - loss: 0.6872 - accuracy: 0.5609 - val_loss: 0.6688 - val_accuracy: 0.7074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 4:54 - loss: 0.6923 - accuracy: 0.75 - ETA: 7s - loss: 0.6942 - accuracy: 0.4693 - ETA: 5s - loss: 0.6940 - accuracy: 0.47 - ETA: 4s - loss: 0.6941 - accuracy: 0.48 - ETA: 3s - loss: 0.6941 - accuracy: 0.48 - ETA: 3s - loss: 0.6939 - accuracy: 0.49 - ETA: 3s - loss: 0.6939 - accuracy: 0.49 - ETA: 3s - loss: 0.6937 - accuracy: 0.50 - ETA: 3s - loss: 0.6935 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 2s - loss: 0.6935 - accuracy: 0.50 - ETA: 2s - loss: 0.6936 - accuracy: 0.50 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.50 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6922 - accuracy: 0.52 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - ETA: 0s - loss: 0.6919 - accuracy: 0.52 - ETA: 0s - loss: 0.6918 - accuracy: 0.52 - ETA: 0s - loss: 0.6916 - accuracy: 0.52 - ETA: 0s - loss: 0.6916 - accuracy: 0.52 - 3s 252us/step - loss: 0.6916 - accuracy: 0.5286 - val_loss: 0.6823 - val_accuracy: 0.6847\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 108us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:48 - loss: 0.6921 - accuracy: 0.50 - ETA: 7s - loss: 0.6925 - accuracy: 0.5670 - ETA: 5s - loss: 0.6942 - accuracy: 0.50 - ETA: 4s - loss: 0.6941 - accuracy: 0.48 - ETA: 3s - loss: 0.6940 - accuracy: 0.49 - ETA: 3s - loss: 0.6941 - accuracy: 0.49 - ETA: 3s - loss: 0.6940 - accuracy: 0.49 - ETA: 3s - loss: 0.6937 - accuracy: 0.50 - ETA: 2s - loss: 0.6937 - accuracy: 0.50 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.50 - ETA: 2s - loss: 0.6936 - accuracy: 0.50 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6931 - accuracy: 0.51 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6919 - accuracy: 0.51 - ETA: 0s - loss: 0.6919 - accuracy: 0.51 - ETA: 0s - loss: 0.6917 - accuracy: 0.51 - ETA: 0s - loss: 0.6917 - accuracy: 0.51 - ETA: 0s - loss: 0.6917 - accuracy: 0.51 - ETA: 0s - loss: 0.6915 - accuracy: 0.51 - ETA: 0s - loss: 0.6914 - accuracy: 0.51 - ETA: 0s - loss: 0.6912 - accuracy: 0.51 - ETA: 0s - loss: 0.6908 - accuracy: 0.51 - ETA: 0s - loss: 0.6907 - accuracy: 0.51 - 3s 250us/step - loss: 0.6905 - accuracy: 0.5184 - val_loss: 0.6779 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 106us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:48 - loss: 0.6865 - accuracy: 0.75 - ETA: 7s - loss: 0.6932 - accuracy: 0.5045 - ETA: 5s - loss: 0.6934 - accuracy: 0.50 - ETA: 4s - loss: 0.6935 - accuracy: 0.50 - ETA: 3s - loss: 0.6936 - accuracy: 0.50 - ETA: 3s - loss: 0.6933 - accuracy: 0.51 - ETA: 3s - loss: 0.6933 - accuracy: 0.51 - ETA: 3s - loss: 0.6928 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6928 - accuracy: 0.51 - ETA: 2s - loss: 0.6929 - accuracy: 0.51 - ETA: 2s - loss: 0.6931 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6928 - accuracy: 0.51 - ETA: 2s - loss: 0.6928 - accuracy: 0.51 - ETA: 2s - loss: 0.6929 - accuracy: 0.51 - ETA: 2s - loss: 0.6929 - accuracy: 0.51 - ETA: 2s - loss: 0.6928 - accuracy: 0.51 - ETA: 1s - loss: 0.6928 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6922 - accuracy: 0.52 - ETA: 1s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6917 - accuracy: 0.52 - ETA: 1s - loss: 0.6915 - accuracy: 0.52 - ETA: 1s - loss: 0.6914 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 1s - loss: 0.6904 - accuracy: 0.53 - ETA: 1s - loss: 0.6907 - accuracy: 0.53 - ETA: 1s - loss: 0.6905 - accuracy: 0.53 - ETA: 1s - loss: 0.6904 - accuracy: 0.53 - ETA: 1s - loss: 0.6903 - accuracy: 0.53 - ETA: 0s - loss: 0.6902 - accuracy: 0.53 - ETA: 0s - loss: 0.6903 - accuracy: 0.53 - ETA: 0s - loss: 0.6900 - accuracy: 0.53 - ETA: 0s - loss: 0.6900 - accuracy: 0.53 - ETA: 0s - loss: 0.6898 - accuracy: 0.53 - ETA: 0s - loss: 0.6896 - accuracy: 0.53 - ETA: 0s - loss: 0.6894 - accuracy: 0.53 - ETA: 0s - loss: 0.6892 - accuracy: 0.53 - ETA: 0s - loss: 0.6891 - accuracy: 0.53 - ETA: 0s - loss: 0.6888 - accuracy: 0.53 - ETA: 0s - loss: 0.6885 - accuracy: 0.54 - ETA: 0s - loss: 0.6884 - accuracy: 0.54 - ETA: 0s - loss: 0.6882 - accuracy: 0.54 - ETA: 0s - loss: 0.6879 - accuracy: 0.54 - ETA: 0s - loss: 0.6876 - accuracy: 0.54 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6871 - accuracy: 0.54 - ETA: 0s - loss: 0.6868 - accuracy: 0.54 - 3s 256us/step - loss: 0.6868 - accuracy: 0.5443 - val_loss: 0.6659 - val_accuracy: 0.6960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 110us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:16 - loss: 0.6981 - accuracy: 0.50 - ETA: 8s - loss: 0.6944 - accuracy: 0.4712 - ETA: 6s - loss: 0.6943 - accuracy: 0.46 - ETA: 5s - loss: 0.6944 - accuracy: 0.46 - ETA: 4s - loss: 0.6943 - accuracy: 0.46 - ETA: 4s - loss: 0.6939 - accuracy: 0.48 - ETA: 4s - loss: 0.6935 - accuracy: 0.49 - ETA: 4s - loss: 0.6938 - accuracy: 0.49 - ETA: 4s - loss: 0.6938 - accuracy: 0.49 - ETA: 3s - loss: 0.6937 - accuracy: 0.49 - ETA: 3s - loss: 0.6934 - accuracy: 0.50 - ETA: 3s - loss: 0.6936 - accuracy: 0.50 - ETA: 3s - loss: 0.6937 - accuracy: 0.50 - ETA: 3s - loss: 0.6937 - accuracy: 0.50 - ETA: 3s - loss: 0.6937 - accuracy: 0.50 - ETA: 2s - loss: 0.6939 - accuracy: 0.49 - ETA: 2s - loss: 0.6939 - accuracy: 0.49 - ETA: 2s - loss: 0.6939 - accuracy: 0.49 - ETA: 2s - loss: 0.6939 - accuracy: 0.49 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.49 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 2s - loss: 0.6937 - accuracy: 0.50 - ETA: 2s - loss: 0.6937 - accuracy: 0.50 - ETA: 2s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6932 - accuracy: 0.50 - ETA: 0s - loss: 0.6932 - accuracy: 0.50 - ETA: 0s - loss: 0.6932 - accuracy: 0.50 - ETA: 0s - loss: 0.6932 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - 3s 269us/step - loss: 0.6927 - accuracy: 0.5143 - val_loss: 0.6882 - val_accuracy: 0.5128\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 4:54 - loss: 0.7005 - accuracy: 0.25 - ETA: 7s - loss: 0.6951 - accuracy: 0.4554 - ETA: 5s - loss: 0.6952 - accuracy: 0.46 - ETA: 4s - loss: 0.6950 - accuracy: 0.45 - ETA: 3s - loss: 0.6950 - accuracy: 0.46 - ETA: 3s - loss: 0.6947 - accuracy: 0.48 - ETA: 3s - loss: 0.6946 - accuracy: 0.48 - ETA: 3s - loss: 0.6946 - accuracy: 0.49 - ETA: 3s - loss: 0.6944 - accuracy: 0.49 - ETA: 2s - loss: 0.6943 - accuracy: 0.50 - ETA: 2s - loss: 0.6940 - accuracy: 0.50 - ETA: 2s - loss: 0.6942 - accuracy: 0.50 - ETA: 2s - loss: 0.6939 - accuracy: 0.50 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6929 - accuracy: 0.51 - ETA: 2s - loss: 0.6928 - accuracy: 0.51 - ETA: 2s - loss: 0.6929 - accuracy: 0.51 - ETA: 2s - loss: 0.6925 - accuracy: 0.52 - ETA: 2s - loss: 0.6922 - accuracy: 0.52 - ETA: 1s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6922 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - ETA: 0s - loss: 0.6919 - accuracy: 0.52 - ETA: 0s - loss: 0.6918 - accuracy: 0.52 - ETA: 0s - loss: 0.6915 - accuracy: 0.52 - ETA: 0s - loss: 0.6915 - accuracy: 0.52 - ETA: 0s - loss: 0.6914 - accuracy: 0.52 - ETA: 0s - loss: 0.6913 - accuracy: 0.52 - ETA: 0s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6910 - accuracy: 0.53 - ETA: 0s - loss: 0.6908 - accuracy: 0.53 - ETA: 0s - loss: 0.6905 - accuracy: 0.53 - ETA: 0s - loss: 0.6903 - accuracy: 0.53 - ETA: 0s - loss: 0.6902 - accuracy: 0.53 - ETA: 0s - loss: 0.6899 - accuracy: 0.53 - ETA: 0s - loss: 0.6899 - accuracy: 0.53 - ETA: 0s - loss: 0.6896 - accuracy: 0.53 - ETA: 0s - loss: 0.6893 - accuracy: 0.53 - 3s 254us/step - loss: 0.6892 - accuracy: 0.5399 - val_loss: 0.6712 - val_accuracy: 0.6811\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:04 - loss: 0.6859 - accuracy: 0.75 - ETA: 8s - loss: 0.6929 - accuracy: 0.5648 - ETA: 5s - loss: 0.6931 - accuracy: 0.53 - ETA: 4s - loss: 0.6932 - accuracy: 0.52 - ETA: 4s - loss: 0.6933 - accuracy: 0.51 - ETA: 3s - loss: 0.6936 - accuracy: 0.51 - ETA: 3s - loss: 0.6934 - accuracy: 0.52 - ETA: 3s - loss: 0.6933 - accuracy: 0.51 - ETA: 3s - loss: 0.6931 - accuracy: 0.51 - ETA: 3s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6931 - accuracy: 0.51 - ETA: 2s - loss: 0.6929 - accuracy: 0.51 - ETA: 2s - loss: 0.6925 - accuracy: 0.51 - ETA: 2s - loss: 0.6928 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6924 - accuracy: 0.52 - ETA: 2s - loss: 0.6917 - accuracy: 0.52 - ETA: 2s - loss: 0.6918 - accuracy: 0.52 - ETA: 2s - loss: 0.6917 - accuracy: 0.52 - ETA: 2s - loss: 0.6917 - accuracy: 0.51 - ETA: 2s - loss: 0.6915 - accuracy: 0.51 - ETA: 2s - loss: 0.6911 - accuracy: 0.52 - ETA: 2s - loss: 0.6912 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.52 - ETA: 1s - loss: 0.6908 - accuracy: 0.52 - ETA: 1s - loss: 0.6904 - accuracy: 0.52 - ETA: 1s - loss: 0.6898 - accuracy: 0.52 - ETA: 1s - loss: 0.6896 - accuracy: 0.52 - ETA: 1s - loss: 0.6896 - accuracy: 0.52 - ETA: 1s - loss: 0.6895 - accuracy: 0.52 - ETA: 1s - loss: 0.6895 - accuracy: 0.52 - ETA: 1s - loss: 0.6893 - accuracy: 0.52 - ETA: 1s - loss: 0.6890 - accuracy: 0.52 - ETA: 1s - loss: 0.6892 - accuracy: 0.52 - ETA: 1s - loss: 0.6889 - accuracy: 0.53 - ETA: 1s - loss: 0.6888 - accuracy: 0.53 - ETA: 1s - loss: 0.6884 - accuracy: 0.53 - ETA: 1s - loss: 0.6881 - accuracy: 0.53 - ETA: 1s - loss: 0.6878 - accuracy: 0.53 - ETA: 0s - loss: 0.6876 - accuracy: 0.53 - ETA: 0s - loss: 0.6873 - accuracy: 0.53 - ETA: 0s - loss: 0.6871 - accuracy: 0.53 - ETA: 0s - loss: 0.6866 - accuracy: 0.53 - ETA: 0s - loss: 0.6866 - accuracy: 0.53 - ETA: 0s - loss: 0.6863 - accuracy: 0.53 - ETA: 0s - loss: 0.6861 - accuracy: 0.53 - ETA: 0s - loss: 0.6856 - accuracy: 0.54 - ETA: 0s - loss: 0.6855 - accuracy: 0.54 - ETA: 0s - loss: 0.6852 - accuracy: 0.54 - ETA: 0s - loss: 0.6852 - accuracy: 0.54 - ETA: 0s - loss: 0.6848 - accuracy: 0.54 - ETA: 0s - loss: 0.6846 - accuracy: 0.54 - ETA: 0s - loss: 0.6845 - accuracy: 0.54 - ETA: 0s - loss: 0.6842 - accuracy: 0.54 - ETA: 0s - loss: 0.6841 - accuracy: 0.54 - ETA: 0s - loss: 0.6836 - accuracy: 0.54 - ETA: 0s - loss: 0.6835 - accuracy: 0.54 - ETA: 0s - loss: 0.6829 - accuracy: 0.55 - 3s 257us/step - loss: 0.6828 - accuracy: 0.5510 - val_loss: 0.6506 - val_accuracy: 0.6797\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:07 - loss: 0.7029 - accuracy: 0.0000e+ - ETA: 8s - loss: 0.6947 - accuracy: 0.4732     - ETA: 5s - loss: 0.6944 - accuracy: 0.49 - ETA: 4s - loss: 0.6942 - accuracy: 0.50 - ETA: 3s - loss: 0.6942 - accuracy: 0.49 - ETA: 3s - loss: 0.6941 - accuracy: 0.50 - ETA: 3s - loss: 0.6940 - accuracy: 0.50 - ETA: 3s - loss: 0.6941 - accuracy: 0.50 - ETA: 3s - loss: 0.6940 - accuracy: 0.50 - ETA: 3s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6940 - accuracy: 0.51 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.52 - ETA: 2s - loss: 0.6932 - accuracy: 0.52 - ETA: 2s - loss: 0.6930 - accuracy: 0.52 - ETA: 2s - loss: 0.6927 - accuracy: 0.52 - ETA: 2s - loss: 0.6926 - accuracy: 0.52 - ETA: 2s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6922 - accuracy: 0.52 - ETA: 1s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6917 - accuracy: 0.52 - ETA: 1s - loss: 0.6913 - accuracy: 0.53 - ETA: 1s - loss: 0.6913 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.52 - ETA: 1s - loss: 0.6911 - accuracy: 0.52 - ETA: 1s - loss: 0.6909 - accuracy: 0.52 - ETA: 1s - loss: 0.6907 - accuracy: 0.52 - ETA: 1s - loss: 0.6906 - accuracy: 0.52 - ETA: 1s - loss: 0.6902 - accuracy: 0.53 - ETA: 1s - loss: 0.6900 - accuracy: 0.53 - ETA: 1s - loss: 0.6899 - accuracy: 0.53 - ETA: 1s - loss: 0.6896 - accuracy: 0.53 - ETA: 0s - loss: 0.6895 - accuracy: 0.53 - ETA: 0s - loss: 0.6891 - accuracy: 0.53 - ETA: 0s - loss: 0.6888 - accuracy: 0.53 - ETA: 0s - loss: 0.6886 - accuracy: 0.53 - ETA: 0s - loss: 0.6882 - accuracy: 0.53 - ETA: 0s - loss: 0.6881 - accuracy: 0.54 - ETA: 0s - loss: 0.6879 - accuracy: 0.54 - ETA: 0s - loss: 0.6877 - accuracy: 0.54 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6870 - accuracy: 0.54 - ETA: 0s - loss: 0.6869 - accuracy: 0.54 - ETA: 0s - loss: 0.6865 - accuracy: 0.54 - ETA: 0s - loss: 0.6863 - accuracy: 0.54 - ETA: 0s - loss: 0.6861 - accuracy: 0.54 - ETA: 0s - loss: 0.6857 - accuracy: 0.54 - ETA: 0s - loss: 0.6856 - accuracy: 0.54 - ETA: 0s - loss: 0.6854 - accuracy: 0.54 - ETA: 0s - loss: 0.6854 - accuracy: 0.54 - ETA: 0s - loss: 0.6851 - accuracy: 0.54 - 3s 260us/step - loss: 0.6851 - accuracy: 0.5495 - val_loss: 0.6592 - val_accuracy: 0.7060\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 113us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 5:00 - loss: 0.6940 - accuracy: 0.75 - ETA: 7s - loss: 0.6939 - accuracy: 0.5044 - ETA: 5s - loss: 0.6940 - accuracy: 0.50 - ETA: 4s - loss: 0.6943 - accuracy: 0.50 - ETA: 3s - loss: 0.6943 - accuracy: 0.50 - ETA: 3s - loss: 0.6944 - accuracy: 0.50 - ETA: 3s - loss: 0.6941 - accuracy: 0.51 - ETA: 3s - loss: 0.6943 - accuracy: 0.50 - ETA: 3s - loss: 0.6942 - accuracy: 0.50 - ETA: 2s - loss: 0.6941 - accuracy: 0.50 - ETA: 2s - loss: 0.6940 - accuracy: 0.51 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.52 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 2s - loss: 0.6928 - accuracy: 0.52 - ETA: 2s - loss: 0.6925 - accuracy: 0.52 - ETA: 2s - loss: 0.6919 - accuracy: 0.52 - ETA: 2s - loss: 0.6918 - accuracy: 0.52 - ETA: 2s - loss: 0.6916 - accuracy: 0.52 - ETA: 2s - loss: 0.6918 - accuracy: 0.52 - ETA: 2s - loss: 0.6917 - accuracy: 0.52 - ETA: 2s - loss: 0.6918 - accuracy: 0.52 - ETA: 2s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6917 - accuracy: 0.52 - ETA: 1s - loss: 0.6917 - accuracy: 0.52 - ETA: 1s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6915 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.52 - ETA: 1s - loss: 0.6907 - accuracy: 0.52 - ETA: 1s - loss: 0.6907 - accuracy: 0.52 - ETA: 1s - loss: 0.6906 - accuracy: 0.52 - ETA: 1s - loss: 0.6903 - accuracy: 0.52 - ETA: 1s - loss: 0.6904 - accuracy: 0.52 - ETA: 1s - loss: 0.6903 - accuracy: 0.52 - ETA: 1s - loss: 0.6903 - accuracy: 0.52 - ETA: 1s - loss: 0.6902 - accuracy: 0.52 - ETA: 1s - loss: 0.6901 - accuracy: 0.52 - ETA: 1s - loss: 0.6900 - accuracy: 0.52 - ETA: 0s - loss: 0.6897 - accuracy: 0.53 - ETA: 0s - loss: 0.6897 - accuracy: 0.53 - ETA: 0s - loss: 0.6895 - accuracy: 0.53 - ETA: 0s - loss: 0.6893 - accuracy: 0.53 - ETA: 0s - loss: 0.6887 - accuracy: 0.53 - ETA: 0s - loss: 0.6881 - accuracy: 0.53 - ETA: 0s - loss: 0.6882 - accuracy: 0.53 - ETA: 0s - loss: 0.6879 - accuracy: 0.53 - ETA: 0s - loss: 0.6875 - accuracy: 0.53 - ETA: 0s - loss: 0.6872 - accuracy: 0.53 - ETA: 0s - loss: 0.6869 - accuracy: 0.53 - ETA: 0s - loss: 0.6869 - accuracy: 0.53 - ETA: 0s - loss: 0.6868 - accuracy: 0.53 - ETA: 0s - loss: 0.6865 - accuracy: 0.54 - ETA: 0s - loss: 0.6863 - accuracy: 0.54 - ETA: 0s - loss: 0.6859 - accuracy: 0.54 - ETA: 0s - loss: 0.6859 - accuracy: 0.54 - ETA: 0s - loss: 0.6857 - accuracy: 0.54 - ETA: 0s - loss: 0.6856 - accuracy: 0.54 - 3s 262us/step - loss: 0.6854 - accuracy: 0.5464 - val_loss: 0.6615 - val_accuracy: 0.6804\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 115us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:29 - loss: 0.6934 - accuracy: 1.00 - ETA: 9s - loss: 0.6931 - accuracy: 0.5637 - ETA: 6s - loss: 0.6939 - accuracy: 0.52 - ETA: 5s - loss: 0.6941 - accuracy: 0.51 - ETA: 4s - loss: 0.6938 - accuracy: 0.51 - ETA: 4s - loss: 0.6939 - accuracy: 0.50 - ETA: 3s - loss: 0.6935 - accuracy: 0.51 - ETA: 3s - loss: 0.6934 - accuracy: 0.51 - ETA: 3s - loss: 0.6929 - accuracy: 0.52 - ETA: 3s - loss: 0.6919 - accuracy: 0.53 - ETA: 3s - loss: 0.6921 - accuracy: 0.53 - ETA: 2s - loss: 0.6920 - accuracy: 0.53 - ETA: 2s - loss: 0.6923 - accuracy: 0.52 - ETA: 2s - loss: 0.6924 - accuracy: 0.52 - ETA: 2s - loss: 0.6926 - accuracy: 0.52 - ETA: 2s - loss: 0.6924 - accuracy: 0.52 - ETA: 2s - loss: 0.6926 - accuracy: 0.52 - ETA: 2s - loss: 0.6928 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6927 - accuracy: 0.51 - ETA: 2s - loss: 0.6927 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.51 - ETA: 1s - loss: 0.6921 - accuracy: 0.51 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6914 - accuracy: 0.52 - ETA: 1s - loss: 0.6914 - accuracy: 0.52 - ETA: 1s - loss: 0.6913 - accuracy: 0.52 - ETA: 1s - loss: 0.6914 - accuracy: 0.52 - ETA: 1s - loss: 0.6911 - accuracy: 0.52 - ETA: 1s - loss: 0.6911 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.52 - ETA: 1s - loss: 0.6909 - accuracy: 0.52 - ETA: 1s - loss: 0.6909 - accuracy: 0.52 - ETA: 1s - loss: 0.6909 - accuracy: 0.52 - ETA: 1s - loss: 0.6908 - accuracy: 0.52 - ETA: 1s - loss: 0.6906 - accuracy: 0.52 - ETA: 1s - loss: 0.6903 - accuracy: 0.52 - ETA: 1s - loss: 0.6902 - accuracy: 0.53 - ETA: 0s - loss: 0.6900 - accuracy: 0.53 - ETA: 0s - loss: 0.6900 - accuracy: 0.53 - ETA: 0s - loss: 0.6899 - accuracy: 0.53 - ETA: 0s - loss: 0.6897 - accuracy: 0.53 - ETA: 0s - loss: 0.6894 - accuracy: 0.53 - ETA: 0s - loss: 0.6891 - accuracy: 0.53 - ETA: 0s - loss: 0.6891 - accuracy: 0.53 - ETA: 0s - loss: 0.6888 - accuracy: 0.53 - ETA: 0s - loss: 0.6886 - accuracy: 0.54 - ETA: 0s - loss: 0.6885 - accuracy: 0.54 - ETA: 0s - loss: 0.6882 - accuracy: 0.54 - ETA: 0s - loss: 0.6879 - accuracy: 0.54 - ETA: 0s - loss: 0.6876 - accuracy: 0.54 - ETA: 0s - loss: 0.6873 - accuracy: 0.54 - ETA: 0s - loss: 0.6870 - accuracy: 0.54 - ETA: 0s - loss: 0.6870 - accuracy: 0.54 - ETA: 0s - loss: 0.6866 - accuracy: 0.55 - ETA: 0s - loss: 0.6862 - accuracy: 0.55 - ETA: 0s - loss: 0.6862 - accuracy: 0.55 - ETA: 0s - loss: 0.6859 - accuracy: 0.55 - 3s 266us/step - loss: 0.6858 - accuracy: 0.5544 - val_loss: 0.6582 - val_accuracy: 0.6605\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 109us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:54 - loss: 0.6870 - accuracy: 1.00 - ETA: 8s - loss: 0.6946 - accuracy: 0.4954 - ETA: 6s - loss: 0.6947 - accuracy: 0.47 - ETA: 4s - loss: 0.6947 - accuracy: 0.48 - ETA: 4s - loss: 0.6945 - accuracy: 0.49 - ETA: 3s - loss: 0.6947 - accuracy: 0.47 - ETA: 3s - loss: 0.6947 - accuracy: 0.47 - ETA: 3s - loss: 0.6946 - accuracy: 0.48 - ETA: 3s - loss: 0.6946 - accuracy: 0.48 - ETA: 3s - loss: 0.6945 - accuracy: 0.48 - ETA: 3s - loss: 0.6945 - accuracy: 0.49 - ETA: 2s - loss: 0.6943 - accuracy: 0.49 - ETA: 2s - loss: 0.6944 - accuracy: 0.49 - ETA: 2s - loss: 0.6943 - accuracy: 0.49 - ETA: 2s - loss: 0.6943 - accuracy: 0.49 - ETA: 2s - loss: 0.6943 - accuracy: 0.49 - ETA: 2s - loss: 0.6943 - accuracy: 0.49 - ETA: 2s - loss: 0.6942 - accuracy: 0.49 - ETA: 2s - loss: 0.6942 - accuracy: 0.49 - ETA: 2s - loss: 0.6942 - accuracy: 0.49 - ETA: 2s - loss: 0.6941 - accuracy: 0.50 - ETA: 2s - loss: 0.6940 - accuracy: 0.50 - ETA: 2s - loss: 0.6940 - accuracy: 0.50 - ETA: 1s - loss: 0.6939 - accuracy: 0.50 - ETA: 1s - loss: 0.6940 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6928 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - ETA: 0s - loss: 0.6917 - accuracy: 0.51 - ETA: 0s - loss: 0.6916 - accuracy: 0.51 - ETA: 0s - loss: 0.6916 - accuracy: 0.51 - ETA: 0s - loss: 0.6914 - accuracy: 0.52 - ETA: 0s - loss: 0.6913 - accuracy: 0.52 - ETA: 0s - loss: 0.6911 - accuracy: 0.52 - ETA: 0s - loss: 0.6909 - accuracy: 0.52 - ETA: 0s - loss: 0.6907 - accuracy: 0.52 - ETA: 0s - loss: 0.6906 - accuracy: 0.52 - ETA: 0s - loss: 0.6904 - accuracy: 0.52 - ETA: 0s - loss: 0.6902 - accuracy: 0.52 - ETA: 0s - loss: 0.6900 - accuracy: 0.52 - ETA: 0s - loss: 0.6897 - accuracy: 0.53 - ETA: 0s - loss: 0.6893 - accuracy: 0.53 - ETA: 0s - loss: 0.6890 - accuracy: 0.53 - 3s 257us/step - loss: 0.6887 - accuracy: 0.5320 - val_loss: 0.6714 - val_accuracy: 0.5803\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 4:57 - loss: 0.6986 - accuracy: 0.25 - ETA: 8s - loss: 0.6946 - accuracy: 0.5343 - ETA: 5s - loss: 0.6948 - accuracy: 0.50 - ETA: 4s - loss: 0.6944 - accuracy: 0.51 - ETA: 4s - loss: 0.6941 - accuracy: 0.52 - ETA: 3s - loss: 0.6935 - accuracy: 0.52 - ETA: 3s - loss: 0.6933 - accuracy: 0.52 - ETA: 3s - loss: 0.6934 - accuracy: 0.52 - ETA: 3s - loss: 0.6927 - accuracy: 0.52 - ETA: 2s - loss: 0.6928 - accuracy: 0.52 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 2s - loss: 0.6930 - accuracy: 0.52 - ETA: 2s - loss: 0.6929 - accuracy: 0.52 - ETA: 2s - loss: 0.6926 - accuracy: 0.52 - ETA: 2s - loss: 0.6923 - accuracy: 0.52 - ETA: 2s - loss: 0.6922 - accuracy: 0.52 - ETA: 2s - loss: 0.6919 - accuracy: 0.53 - ETA: 2s - loss: 0.6916 - accuracy: 0.52 - ETA: 2s - loss: 0.6913 - accuracy: 0.53 - ETA: 2s - loss: 0.6907 - accuracy: 0.53 - ETA: 2s - loss: 0.6903 - accuracy: 0.53 - ETA: 2s - loss: 0.6904 - accuracy: 0.53 - ETA: 1s - loss: 0.6901 - accuracy: 0.53 - ETA: 1s - loss: 0.6898 - accuracy: 0.54 - ETA: 1s - loss: 0.6894 - accuracy: 0.54 - ETA: 1s - loss: 0.6891 - accuracy: 0.54 - ETA: 1s - loss: 0.6889 - accuracy: 0.54 - ETA: 1s - loss: 0.6885 - accuracy: 0.54 - ETA: 1s - loss: 0.6883 - accuracy: 0.54 - ETA: 1s - loss: 0.6878 - accuracy: 0.54 - ETA: 1s - loss: 0.6874 - accuracy: 0.54 - ETA: 1s - loss: 0.6866 - accuracy: 0.55 - ETA: 1s - loss: 0.6864 - accuracy: 0.55 - ETA: 1s - loss: 0.6860 - accuracy: 0.55 - ETA: 1s - loss: 0.6855 - accuracy: 0.55 - ETA: 1s - loss: 0.6855 - accuracy: 0.55 - ETA: 1s - loss: 0.6853 - accuracy: 0.55 - ETA: 1s - loss: 0.6849 - accuracy: 0.56 - ETA: 1s - loss: 0.6842 - accuracy: 0.56 - ETA: 0s - loss: 0.6839 - accuracy: 0.56 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6830 - accuracy: 0.56 - ETA: 0s - loss: 0.6826 - accuracy: 0.56 - ETA: 0s - loss: 0.6822 - accuracy: 0.57 - ETA: 0s - loss: 0.6820 - accuracy: 0.57 - ETA: 0s - loss: 0.6821 - accuracy: 0.57 - ETA: 0s - loss: 0.6814 - accuracy: 0.57 - ETA: 0s - loss: 0.6808 - accuracy: 0.57 - ETA: 0s - loss: 0.6806 - accuracy: 0.57 - ETA: 0s - loss: 0.6801 - accuracy: 0.57 - ETA: 0s - loss: 0.6798 - accuracy: 0.57 - ETA: 0s - loss: 0.6794 - accuracy: 0.57 - ETA: 0s - loss: 0.6788 - accuracy: 0.58 - ETA: 0s - loss: 0.6785 - accuracy: 0.58 - ETA: 0s - loss: 0.6783 - accuracy: 0.58 - ETA: 0s - loss: 0.6778 - accuracy: 0.58 - ETA: 0s - loss: 0.6772 - accuracy: 0.58 - ETA: 0s - loss: 0.6769 - accuracy: 0.58 - ETA: 0s - loss: 0.6762 - accuracy: 0.58 - 3s 255us/step - loss: 0.6762 - accuracy: 0.5863 - val_loss: 0.6316 - val_accuracy: 0.6960\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 112us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 4:48 - loss: 0.6931 - accuracy: 0.50 - ETA: 7s - loss: 0.6932 - accuracy: 0.5491 - ETA: 5s - loss: 0.6918 - accuracy: 0.56 - ETA: 4s - loss: 0.6914 - accuracy: 0.56 - ETA: 3s - loss: 0.6926 - accuracy: 0.54 - ETA: 3s - loss: 0.6931 - accuracy: 0.53 - ETA: 3s - loss: 0.6931 - accuracy: 0.53 - ETA: 3s - loss: 0.6932 - accuracy: 0.52 - ETA: 3s - loss: 0.6936 - accuracy: 0.52 - ETA: 3s - loss: 0.6933 - accuracy: 0.52 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 2s - loss: 0.6927 - accuracy: 0.53 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 2s - loss: 0.6932 - accuracy: 0.52 - ETA: 2s - loss: 0.6932 - accuracy: 0.52 - ETA: 2s - loss: 0.6930 - accuracy: 0.52 - ETA: 2s - loss: 0.6927 - accuracy: 0.53 - ETA: 2s - loss: 0.6926 - accuracy: 0.53 - ETA: 2s - loss: 0.6921 - accuracy: 0.53 - ETA: 2s - loss: 0.6921 - accuracy: 0.53 - ETA: 2s - loss: 0.6921 - accuracy: 0.53 - ETA: 2s - loss: 0.6917 - accuracy: 0.53 - ETA: 1s - loss: 0.6918 - accuracy: 0.53 - ETA: 1s - loss: 0.6914 - accuracy: 0.53 - ETA: 1s - loss: 0.6916 - accuracy: 0.53 - ETA: 1s - loss: 0.6915 - accuracy: 0.53 - ETA: 1s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6909 - accuracy: 0.53 - ETA: 1s - loss: 0.6907 - accuracy: 0.53 - ETA: 1s - loss: 0.6905 - accuracy: 0.54 - ETA: 1s - loss: 0.6903 - accuracy: 0.54 - ETA: 1s - loss: 0.6899 - accuracy: 0.54 - ETA: 1s - loss: 0.6897 - accuracy: 0.54 - ETA: 1s - loss: 0.6897 - accuracy: 0.54 - ETA: 1s - loss: 0.6893 - accuracy: 0.54 - ETA: 1s - loss: 0.6889 - accuracy: 0.54 - ETA: 1s - loss: 0.6885 - accuracy: 0.55 - ETA: 1s - loss: 0.6883 - accuracy: 0.55 - ETA: 1s - loss: 0.6880 - accuracy: 0.55 - ETA: 1s - loss: 0.6878 - accuracy: 0.55 - ETA: 1s - loss: 0.6872 - accuracy: 0.55 - ETA: 0s - loss: 0.6866 - accuracy: 0.55 - ETA: 0s - loss: 0.6863 - accuracy: 0.56 - ETA: 0s - loss: 0.6858 - accuracy: 0.56 - ETA: 0s - loss: 0.6852 - accuracy: 0.56 - ETA: 0s - loss: 0.6847 - accuracy: 0.56 - ETA: 0s - loss: 0.6845 - accuracy: 0.56 - ETA: 0s - loss: 0.6843 - accuracy: 0.56 - ETA: 0s - loss: 0.6837 - accuracy: 0.56 - ETA: 0s - loss: 0.6829 - accuracy: 0.57 - ETA: 0s - loss: 0.6823 - accuracy: 0.57 - ETA: 0s - loss: 0.6820 - accuracy: 0.57 - ETA: 0s - loss: 0.6816 - accuracy: 0.57 - ETA: 0s - loss: 0.6811 - accuracy: 0.57 - ETA: 0s - loss: 0.6806 - accuracy: 0.57 - ETA: 0s - loss: 0.6802 - accuracy: 0.57 - ETA: 0s - loss: 0.6799 - accuracy: 0.57 - ETA: 0s - loss: 0.6796 - accuracy: 0.57 - ETA: 0s - loss: 0.6792 - accuracy: 0.58 - ETA: 0s - loss: 0.6788 - accuracy: 0.58 - 3s 265us/step - loss: 0.6783 - accuracy: 0.5831 - val_loss: 0.6352 - val_accuracy: 0.7102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 111us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:04 - loss: 0.6936 - accuracy: 0.75 - ETA: 8s - loss: 0.6942 - accuracy: 0.5602 - ETA: 5s - loss: 0.6931 - accuracy: 0.56 - ETA: 4s - loss: 0.6934 - accuracy: 0.54 - ETA: 4s - loss: 0.6937 - accuracy: 0.52 - ETA: 3s - loss: 0.6939 - accuracy: 0.52 - ETA: 3s - loss: 0.6934 - accuracy: 0.53 - ETA: 3s - loss: 0.6933 - accuracy: 0.52 - ETA: 3s - loss: 0.6932 - accuracy: 0.52 - ETA: 3s - loss: 0.6932 - accuracy: 0.52 - ETA: 2s - loss: 0.6932 - accuracy: 0.52 - ETA: 2s - loss: 0.6930 - accuracy: 0.52 - ETA: 2s - loss: 0.6927 - accuracy: 0.52 - ETA: 2s - loss: 0.6925 - accuracy: 0.52 - ETA: 2s - loss: 0.6924 - accuracy: 0.52 - ETA: 2s - loss: 0.6919 - accuracy: 0.52 - ETA: 2s - loss: 0.6921 - accuracy: 0.52 - ETA: 2s - loss: 0.6918 - accuracy: 0.52 - ETA: 2s - loss: 0.6916 - accuracy: 0.52 - ETA: 2s - loss: 0.6916 - accuracy: 0.52 - ETA: 2s - loss: 0.6914 - accuracy: 0.52 - ETA: 2s - loss: 0.6912 - accuracy: 0.52 - ETA: 2s - loss: 0.6909 - accuracy: 0.52 - ETA: 2s - loss: 0.6907 - accuracy: 0.52 - ETA: 2s - loss: 0.6908 - accuracy: 0.52 - ETA: 1s - loss: 0.6905 - accuracy: 0.53 - ETA: 1s - loss: 0.6902 - accuracy: 0.53 - ETA: 1s - loss: 0.6899 - accuracy: 0.53 - ETA: 1s - loss: 0.6898 - accuracy: 0.53 - ETA: 1s - loss: 0.6896 - accuracy: 0.53 - ETA: 1s - loss: 0.6892 - accuracy: 0.54 - ETA: 1s - loss: 0.6889 - accuracy: 0.54 - ETA: 1s - loss: 0.6885 - accuracy: 0.54 - ETA: 1s - loss: 0.6881 - accuracy: 0.54 - ETA: 1s - loss: 0.6877 - accuracy: 0.54 - ETA: 1s - loss: 0.6874 - accuracy: 0.54 - ETA: 1s - loss: 0.6871 - accuracy: 0.55 - ETA: 1s - loss: 0.6866 - accuracy: 0.55 - ETA: 1s - loss: 0.6859 - accuracy: 0.55 - ETA: 1s - loss: 0.6857 - accuracy: 0.55 - ETA: 1s - loss: 0.6855 - accuracy: 0.55 - ETA: 1s - loss: 0.6849 - accuracy: 0.56 - ETA: 1s - loss: 0.6841 - accuracy: 0.56 - ETA: 1s - loss: 0.6836 - accuracy: 0.56 - ETA: 0s - loss: 0.6832 - accuracy: 0.56 - ETA: 0s - loss: 0.6823 - accuracy: 0.57 - ETA: 0s - loss: 0.6821 - accuracy: 0.57 - ETA: 0s - loss: 0.6812 - accuracy: 0.57 - ETA: 0s - loss: 0.6807 - accuracy: 0.57 - ETA: 0s - loss: 0.6801 - accuracy: 0.57 - ETA: 0s - loss: 0.6791 - accuracy: 0.58 - ETA: 0s - loss: 0.6790 - accuracy: 0.58 - ETA: 0s - loss: 0.6789 - accuracy: 0.58 - ETA: 0s - loss: 0.6786 - accuracy: 0.58 - ETA: 0s - loss: 0.6777 - accuracy: 0.58 - ETA: 0s - loss: 0.6771 - accuracy: 0.58 - ETA: 0s - loss: 0.6763 - accuracy: 0.58 - ETA: 0s - loss: 0.6758 - accuracy: 0.58 - ETA: 0s - loss: 0.6754 - accuracy: 0.58 - ETA: 0s - loss: 0.6750 - accuracy: 0.58 - ETA: 0s - loss: 0.6744 - accuracy: 0.59 - ETA: 0s - loss: 0.6737 - accuracy: 0.59 - 3s 268us/step - loss: 0.6737 - accuracy: 0.5925 - val_loss: 0.6279 - val_accuracy: 0.7180\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 119us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:39 - loss: 0.7005 - accuracy: 0.50 - ETA: 10s - loss: 0.6939 - accuracy: 0.5450 - ETA: 7s - loss: 0.6956 - accuracy: 0.505 - ETA: 6s - loss: 0.6955 - accuracy: 0.49 - ETA: 5s - loss: 0.6952 - accuracy: 0.50 - ETA: 5s - loss: 0.6953 - accuracy: 0.49 - ETA: 5s - loss: 0.6953 - accuracy: 0.50 - ETA: 5s - loss: 0.6952 - accuracy: 0.50 - ETA: 4s - loss: 0.6949 - accuracy: 0.51 - ETA: 4s - loss: 0.6948 - accuracy: 0.51 - ETA: 4s - loss: 0.6946 - accuracy: 0.51 - ETA: 3s - loss: 0.6944 - accuracy: 0.51 - ETA: 3s - loss: 0.6944 - accuracy: 0.51 - ETA: 3s - loss: 0.6944 - accuracy: 0.51 - ETA: 3s - loss: 0.6943 - accuracy: 0.51 - ETA: 3s - loss: 0.6943 - accuracy: 0.51 - ETA: 3s - loss: 0.6944 - accuracy: 0.51 - ETA: 3s - loss: 0.6944 - accuracy: 0.51 - ETA: 3s - loss: 0.6942 - accuracy: 0.51 - ETA: 3s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.52 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.51 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 2s - loss: 0.6930 - accuracy: 0.52 - ETA: 2s - loss: 0.6927 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6915 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.52 - ETA: 1s - loss: 0.6907 - accuracy: 0.52 - ETA: 1s - loss: 0.6906 - accuracy: 0.52 - ETA: 1s - loss: 0.6907 - accuracy: 0.52 - ETA: 1s - loss: 0.6905 - accuracy: 0.52 - ETA: 1s - loss: 0.6903 - accuracy: 0.52 - ETA: 1s - loss: 0.6900 - accuracy: 0.52 - ETA: 1s - loss: 0.6898 - accuracy: 0.53 - ETA: 1s - loss: 0.6896 - accuracy: 0.53 - ETA: 0s - loss: 0.6895 - accuracy: 0.53 - ETA: 0s - loss: 0.6890 - accuracy: 0.53 - ETA: 0s - loss: 0.6885 - accuracy: 0.53 - ETA: 0s - loss: 0.6882 - accuracy: 0.54 - ETA: 0s - loss: 0.6877 - accuracy: 0.54 - ETA: 0s - loss: 0.6876 - accuracy: 0.54 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6870 - accuracy: 0.54 - ETA: 0s - loss: 0.6864 - accuracy: 0.54 - ETA: 0s - loss: 0.6859 - accuracy: 0.55 - ETA: 0s - loss: 0.6855 - accuracy: 0.55 - ETA: 0s - loss: 0.6851 - accuracy: 0.55 - ETA: 0s - loss: 0.6848 - accuracy: 0.55 - ETA: 0s - loss: 0.6848 - accuracy: 0.55 - ETA: 0s - loss: 0.6844 - accuracy: 0.55 - ETA: 0s - loss: 0.6841 - accuracy: 0.55 - ETA: 0s - loss: 0.6836 - accuracy: 0.56 - 3s 266us/step - loss: 0.6835 - accuracy: 0.5617 - val_loss: 0.6498 - val_accuracy: 0.6413\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 106us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:48 - loss: 0.6929 - accuracy: 0.50 - ETA: 7s - loss: 0.6962 - accuracy: 0.4545 - ETA: 5s - loss: 0.6951 - accuracy: 0.50 - ETA: 4s - loss: 0.6950 - accuracy: 0.50 - ETA: 3s - loss: 0.6949 - accuracy: 0.50 - ETA: 3s - loss: 0.6950 - accuracy: 0.50 - ETA: 3s - loss: 0.6949 - accuracy: 0.50 - ETA: 3s - loss: 0.6944 - accuracy: 0.51 - ETA: 3s - loss: 0.6937 - accuracy: 0.52 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6942 - accuracy: 0.51 - ETA: 2s - loss: 0.6941 - accuracy: 0.51 - ETA: 2s - loss: 0.6941 - accuracy: 0.51 - ETA: 2s - loss: 0.6941 - accuracy: 0.51 - ETA: 2s - loss: 0.6941 - accuracy: 0.51 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6922 - accuracy: 0.52 - ETA: 1s - loss: 0.6922 - accuracy: 0.51 - ETA: 1s - loss: 0.6921 - accuracy: 0.51 - ETA: 1s - loss: 0.6920 - accuracy: 0.51 - ETA: 1s - loss: 0.6921 - accuracy: 0.52 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6914 - accuracy: 0.53 - ETA: 1s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 1s - loss: 0.6906 - accuracy: 0.53 - ETA: 1s - loss: 0.6903 - accuracy: 0.54 - ETA: 0s - loss: 0.6901 - accuracy: 0.54 - ETA: 0s - loss: 0.6895 - accuracy: 0.54 - ETA: 0s - loss: 0.6894 - accuracy: 0.54 - ETA: 0s - loss: 0.6890 - accuracy: 0.54 - ETA: 0s - loss: 0.6885 - accuracy: 0.55 - ETA: 0s - loss: 0.6883 - accuracy: 0.55 - ETA: 0s - loss: 0.6881 - accuracy: 0.55 - ETA: 0s - loss: 0.6877 - accuracy: 0.55 - ETA: 0s - loss: 0.6875 - accuracy: 0.55 - ETA: 0s - loss: 0.6872 - accuracy: 0.55 - ETA: 0s - loss: 0.6870 - accuracy: 0.55 - ETA: 0s - loss: 0.6868 - accuracy: 0.55 - ETA: 0s - loss: 0.6868 - accuracy: 0.55 - ETA: 0s - loss: 0.6863 - accuracy: 0.56 - ETA: 0s - loss: 0.6859 - accuracy: 0.56 - ETA: 0s - loss: 0.6855 - accuracy: 0.56 - ETA: 0s - loss: 0.6849 - accuracy: 0.56 - ETA: 0s - loss: 0.6845 - accuracy: 0.56 - ETA: 0s - loss: 0.6836 - accuracy: 0.56 - 3s 251us/step - loss: 0.6835 - accuracy: 0.5693 - val_loss: 0.6480 - val_accuracy: 0.6548\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:54 - loss: 0.6905 - accuracy: 0.50 - ETA: 7s - loss: 0.6948 - accuracy: 0.5307 - ETA: 5s - loss: 0.6949 - accuracy: 0.51 - ETA: 4s - loss: 0.6949 - accuracy: 0.50 - ETA: 4s - loss: 0.6948 - accuracy: 0.51 - ETA: 3s - loss: 0.6944 - accuracy: 0.51 - ETA: 3s - loss: 0.6940 - accuracy: 0.51 - ETA: 3s - loss: 0.6939 - accuracy: 0.51 - ETA: 3s - loss: 0.6937 - accuracy: 0.51 - ETA: 3s - loss: 0.6942 - accuracy: 0.51 - ETA: 2s - loss: 0.6943 - accuracy: 0.51 - ETA: 2s - loss: 0.6942 - accuracy: 0.50 - ETA: 2s - loss: 0.6940 - accuracy: 0.51 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.52 - ETA: 2s - loss: 0.6930 - accuracy: 0.52 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 2s - loss: 0.6930 - accuracy: 0.52 - ETA: 2s - loss: 0.6930 - accuracy: 0.52 - ETA: 2s - loss: 0.6929 - accuracy: 0.52 - ETA: 2s - loss: 0.6929 - accuracy: 0.52 - ETA: 2s - loss: 0.6928 - accuracy: 0.52 - ETA: 2s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6922 - accuracy: 0.53 - ETA: 1s - loss: 0.6922 - accuracy: 0.53 - ETA: 1s - loss: 0.6922 - accuracy: 0.53 - ETA: 1s - loss: 0.6920 - accuracy: 0.53 - ETA: 1s - loss: 0.6919 - accuracy: 0.53 - ETA: 1s - loss: 0.6917 - accuracy: 0.53 - ETA: 1s - loss: 0.6916 - accuracy: 0.54 - ETA: 1s - loss: 0.6915 - accuracy: 0.54 - ETA: 1s - loss: 0.6914 - accuracy: 0.54 - ETA: 1s - loss: 0.6909 - accuracy: 0.54 - ETA: 1s - loss: 0.6909 - accuracy: 0.54 - ETA: 1s - loss: 0.6906 - accuracy: 0.54 - ETA: 1s - loss: 0.6903 - accuracy: 0.54 - ETA: 1s - loss: 0.6900 - accuracy: 0.54 - ETA: 1s - loss: 0.6897 - accuracy: 0.55 - ETA: 1s - loss: 0.6895 - accuracy: 0.55 - ETA: 1s - loss: 0.6892 - accuracy: 0.55 - ETA: 0s - loss: 0.6888 - accuracy: 0.55 - ETA: 0s - loss: 0.6886 - accuracy: 0.55 - ETA: 0s - loss: 0.6880 - accuracy: 0.55 - ETA: 0s - loss: 0.6874 - accuracy: 0.55 - ETA: 0s - loss: 0.6871 - accuracy: 0.55 - ETA: 0s - loss: 0.6868 - accuracy: 0.55 - ETA: 0s - loss: 0.6865 - accuracy: 0.55 - ETA: 0s - loss: 0.6862 - accuracy: 0.55 - ETA: 0s - loss: 0.6859 - accuracy: 0.56 - ETA: 0s - loss: 0.6858 - accuracy: 0.56 - ETA: 0s - loss: 0.6853 - accuracy: 0.56 - ETA: 0s - loss: 0.6850 - accuracy: 0.56 - ETA: 0s - loss: 0.6845 - accuracy: 0.56 - ETA: 0s - loss: 0.6845 - accuracy: 0.56 - ETA: 0s - loss: 0.6841 - accuracy: 0.56 - ETA: 0s - loss: 0.6839 - accuracy: 0.56 - ETA: 0s - loss: 0.6833 - accuracy: 0.56 - ETA: 0s - loss: 0.6829 - accuracy: 0.56 - ETA: 0s - loss: 0.6825 - accuracy: 0.57 - 3s 260us/step - loss: 0.6823 - accuracy: 0.5708 - val_loss: 0.6494 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 100us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 6:13 - loss: 0.6938 - accuracy: 0.75 - ETA: 9s - loss: 0.6937 - accuracy: 0.4722 - ETA: 6s - loss: 0.6935 - accuracy: 0.49 - ETA: 5s - loss: 0.6936 - accuracy: 0.48 - ETA: 4s - loss: 0.6934 - accuracy: 0.49 - ETA: 4s - loss: 0.6930 - accuracy: 0.51 - ETA: 3s - loss: 0.6929 - accuracy: 0.51 - ETA: 3s - loss: 0.6927 - accuracy: 0.52 - ETA: 3s - loss: 0.6923 - accuracy: 0.53 - ETA: 3s - loss: 0.6921 - accuracy: 0.53 - ETA: 3s - loss: 0.6918 - accuracy: 0.54 - ETA: 3s - loss: 0.6913 - accuracy: 0.54 - ETA: 2s - loss: 0.6901 - accuracy: 0.55 - ETA: 2s - loss: 0.6897 - accuracy: 0.54 - ETA: 2s - loss: 0.6891 - accuracy: 0.55 - ETA: 2s - loss: 0.6885 - accuracy: 0.55 - ETA: 2s - loss: 0.6874 - accuracy: 0.56 - ETA: 2s - loss: 0.6855 - accuracy: 0.56 - ETA: 2s - loss: 0.6842 - accuracy: 0.57 - ETA: 2s - loss: 0.6835 - accuracy: 0.57 - ETA: 2s - loss: 0.6830 - accuracy: 0.57 - ETA: 2s - loss: 0.6814 - accuracy: 0.57 - ETA: 2s - loss: 0.6807 - accuracy: 0.57 - ETA: 2s - loss: 0.6793 - accuracy: 0.57 - ETA: 2s - loss: 0.6786 - accuracy: 0.57 - ETA: 1s - loss: 0.6776 - accuracy: 0.57 - ETA: 1s - loss: 0.6767 - accuracy: 0.58 - ETA: 1s - loss: 0.6763 - accuracy: 0.58 - ETA: 1s - loss: 0.6760 - accuracy: 0.58 - ETA: 1s - loss: 0.6747 - accuracy: 0.58 - ETA: 1s - loss: 0.6726 - accuracy: 0.58 - ETA: 1s - loss: 0.6711 - accuracy: 0.58 - ETA: 1s - loss: 0.6717 - accuracy: 0.58 - ETA: 1s - loss: 0.6712 - accuracy: 0.58 - ETA: 1s - loss: 0.6707 - accuracy: 0.58 - ETA: 1s - loss: 0.6701 - accuracy: 0.58 - ETA: 1s - loss: 0.6693 - accuracy: 0.58 - ETA: 1s - loss: 0.6691 - accuracy: 0.59 - ETA: 1s - loss: 0.6683 - accuracy: 0.59 - ETA: 1s - loss: 0.6677 - accuracy: 0.59 - ETA: 1s - loss: 0.6670 - accuracy: 0.59 - ETA: 1s - loss: 0.6663 - accuracy: 0.59 - ETA: 1s - loss: 0.6660 - accuracy: 0.59 - ETA: 0s - loss: 0.6656 - accuracy: 0.59 - ETA: 0s - loss: 0.6652 - accuracy: 0.59 - ETA: 0s - loss: 0.6644 - accuracy: 0.59 - ETA: 0s - loss: 0.6635 - accuracy: 0.59 - ETA: 0s - loss: 0.6634 - accuracy: 0.59 - ETA: 0s - loss: 0.6631 - accuracy: 0.60 - ETA: 0s - loss: 0.6618 - accuracy: 0.60 - ETA: 0s - loss: 0.6613 - accuracy: 0.60 - ETA: 0s - loss: 0.6609 - accuracy: 0.60 - ETA: 0s - loss: 0.6599 - accuracy: 0.60 - ETA: 0s - loss: 0.6590 - accuracy: 0.60 - ETA: 0s - loss: 0.6589 - accuracy: 0.60 - ETA: 0s - loss: 0.6592 - accuracy: 0.60 - ETA: 0s - loss: 0.6586 - accuracy: 0.60 - ETA: 0s - loss: 0.6579 - accuracy: 0.60 - ETA: 0s - loss: 0.6576 - accuracy: 0.60 - ETA: 0s - loss: 0.6569 - accuracy: 0.60 - ETA: 0s - loss: 0.6566 - accuracy: 0.60 - ETA: 0s - loss: 0.6563 - accuracy: 0.61 - 3s 270us/step - loss: 0.6559 - accuracy: 0.6110 - val_loss: 0.6033 - val_accuracy: 0.7152\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 104us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:29 - loss: 0.7065 - accuracy: 0.25 - ETA: 10s - loss: 0.6933 - accuracy: 0.4856 - ETA: 6s - loss: 0.6937 - accuracy: 0.507 - ETA: 5s - loss: 0.6935 - accuracy: 0.51 - ETA: 4s - loss: 0.6926 - accuracy: 0.52 - ETA: 4s - loss: 0.6922 - accuracy: 0.52 - ETA: 4s - loss: 0.6925 - accuracy: 0.51 - ETA: 3s - loss: 0.6925 - accuracy: 0.51 - ETA: 3s - loss: 0.6921 - accuracy: 0.51 - ETA: 3s - loss: 0.6919 - accuracy: 0.51 - ETA: 3s - loss: 0.6916 - accuracy: 0.52 - ETA: 3s - loss: 0.6908 - accuracy: 0.52 - ETA: 3s - loss: 0.6904 - accuracy: 0.53 - ETA: 3s - loss: 0.6896 - accuracy: 0.53 - ETA: 2s - loss: 0.6892 - accuracy: 0.54 - ETA: 2s - loss: 0.6885 - accuracy: 0.54 - ETA: 2s - loss: 0.6882 - accuracy: 0.54 - ETA: 2s - loss: 0.6875 - accuracy: 0.55 - ETA: 2s - loss: 0.6867 - accuracy: 0.55 - ETA: 2s - loss: 0.6860 - accuracy: 0.55 - ETA: 2s - loss: 0.6849 - accuracy: 0.56 - ETA: 2s - loss: 0.6843 - accuracy: 0.56 - ETA: 2s - loss: 0.6836 - accuracy: 0.56 - ETA: 2s - loss: 0.6833 - accuracy: 0.56 - ETA: 2s - loss: 0.6821 - accuracy: 0.56 - ETA: 2s - loss: 0.6811 - accuracy: 0.57 - ETA: 2s - loss: 0.6808 - accuracy: 0.57 - ETA: 2s - loss: 0.6802 - accuracy: 0.57 - ETA: 2s - loss: 0.6790 - accuracy: 0.57 - ETA: 1s - loss: 0.6775 - accuracy: 0.57 - ETA: 1s - loss: 0.6770 - accuracy: 0.57 - ETA: 1s - loss: 0.6757 - accuracy: 0.58 - ETA: 1s - loss: 0.6749 - accuracy: 0.58 - ETA: 1s - loss: 0.6744 - accuracy: 0.58 - ETA: 1s - loss: 0.6740 - accuracy: 0.58 - ETA: 1s - loss: 0.6731 - accuracy: 0.58 - ETA: 1s - loss: 0.6723 - accuracy: 0.58 - ETA: 1s - loss: 0.6719 - accuracy: 0.58 - ETA: 1s - loss: 0.6719 - accuracy: 0.58 - ETA: 1s - loss: 0.6710 - accuracy: 0.59 - ETA: 1s - loss: 0.6697 - accuracy: 0.59 - ETA: 1s - loss: 0.6693 - accuracy: 0.59 - ETA: 1s - loss: 0.6686 - accuracy: 0.59 - ETA: 1s - loss: 0.6679 - accuracy: 0.59 - ETA: 1s - loss: 0.6673 - accuracy: 0.59 - ETA: 1s - loss: 0.6662 - accuracy: 0.59 - ETA: 0s - loss: 0.6658 - accuracy: 0.60 - ETA: 0s - loss: 0.6654 - accuracy: 0.60 - ETA: 0s - loss: 0.6650 - accuracy: 0.60 - ETA: 0s - loss: 0.6639 - accuracy: 0.60 - ETA: 0s - loss: 0.6637 - accuracy: 0.60 - ETA: 0s - loss: 0.6637 - accuracy: 0.60 - ETA: 0s - loss: 0.6630 - accuracy: 0.60 - ETA: 0s - loss: 0.6624 - accuracy: 0.60 - ETA: 0s - loss: 0.6617 - accuracy: 0.60 - ETA: 0s - loss: 0.6613 - accuracy: 0.60 - ETA: 0s - loss: 0.6610 - accuracy: 0.60 - ETA: 0s - loss: 0.6607 - accuracy: 0.60 - ETA: 0s - loss: 0.6600 - accuracy: 0.60 - ETA: 0s - loss: 0.6595 - accuracy: 0.60 - ETA: 0s - loss: 0.6592 - accuracy: 0.60 - ETA: 0s - loss: 0.6589 - accuracy: 0.60 - ETA: 0s - loss: 0.6581 - accuracy: 0.61 - ETA: 0s - loss: 0.6578 - accuracy: 0.61 - 4s 281us/step - loss: 0.6576 - accuracy: 0.6122 - val_loss: 0.6104 - val_accuracy: 0.7145\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 115us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 7:39 - loss: 0.6884 - accuracy: 1.00 - ETA: 14s - loss: 0.6930 - accuracy: 0.5000 - ETA: 9s - loss: 0.6930 - accuracy: 0.521 - ETA: 7s - loss: 0.6932 - accuracy: 0.51 - ETA: 6s - loss: 0.6922 - accuracy: 0.53 - ETA: 5s - loss: 0.6922 - accuracy: 0.52 - ETA: 4s - loss: 0.6903 - accuracy: 0.54 - ETA: 4s - loss: 0.6902 - accuracy: 0.54 - ETA: 4s - loss: 0.6896 - accuracy: 0.54 - ETA: 3s - loss: 0.6901 - accuracy: 0.53 - ETA: 3s - loss: 0.6894 - accuracy: 0.53 - ETA: 3s - loss: 0.6878 - accuracy: 0.54 - ETA: 3s - loss: 0.6865 - accuracy: 0.55 - ETA: 3s - loss: 0.6856 - accuracy: 0.55 - ETA: 2s - loss: 0.6847 - accuracy: 0.55 - ETA: 2s - loss: 0.6840 - accuracy: 0.55 - ETA: 2s - loss: 0.6841 - accuracy: 0.55 - ETA: 2s - loss: 0.6830 - accuracy: 0.56 - ETA: 2s - loss: 0.6821 - accuracy: 0.56 - ETA: 2s - loss: 0.6810 - accuracy: 0.56 - ETA: 2s - loss: 0.6803 - accuracy: 0.56 - ETA: 2s - loss: 0.6795 - accuracy: 0.56 - ETA: 2s - loss: 0.6779 - accuracy: 0.57 - ETA: 2s - loss: 0.6773 - accuracy: 0.57 - ETA: 2s - loss: 0.6756 - accuracy: 0.57 - ETA: 1s - loss: 0.6760 - accuracy: 0.57 - ETA: 1s - loss: 0.6755 - accuracy: 0.57 - ETA: 1s - loss: 0.6750 - accuracy: 0.57 - ETA: 1s - loss: 0.6740 - accuracy: 0.58 - ETA: 1s - loss: 0.6730 - accuracy: 0.58 - ETA: 1s - loss: 0.6722 - accuracy: 0.58 - ETA: 1s - loss: 0.6720 - accuracy: 0.58 - ETA: 1s - loss: 0.6712 - accuracy: 0.58 - ETA: 1s - loss: 0.6713 - accuracy: 0.58 - ETA: 1s - loss: 0.6698 - accuracy: 0.59 - ETA: 1s - loss: 0.6693 - accuracy: 0.59 - ETA: 1s - loss: 0.6685 - accuracy: 0.59 - ETA: 1s - loss: 0.6680 - accuracy: 0.59 - ETA: 1s - loss: 0.6678 - accuracy: 0.59 - ETA: 1s - loss: 0.6681 - accuracy: 0.59 - ETA: 1s - loss: 0.6670 - accuracy: 0.59 - ETA: 1s - loss: 0.6666 - accuracy: 0.59 - ETA: 0s - loss: 0.6653 - accuracy: 0.59 - ETA: 0s - loss: 0.6642 - accuracy: 0.59 - ETA: 0s - loss: 0.6633 - accuracy: 0.60 - ETA: 0s - loss: 0.6624 - accuracy: 0.60 - ETA: 0s - loss: 0.6627 - accuracy: 0.60 - ETA: 0s - loss: 0.6622 - accuracy: 0.60 - ETA: 0s - loss: 0.6620 - accuracy: 0.60 - ETA: 0s - loss: 0.6615 - accuracy: 0.60 - ETA: 0s - loss: 0.6606 - accuracy: 0.60 - ETA: 0s - loss: 0.6599 - accuracy: 0.60 - ETA: 0s - loss: 0.6598 - accuracy: 0.60 - ETA: 0s - loss: 0.6590 - accuracy: 0.60 - ETA: 0s - loss: 0.6576 - accuracy: 0.60 - ETA: 0s - loss: 0.6572 - accuracy: 0.60 - ETA: 0s - loss: 0.6567 - accuracy: 0.60 - ETA: 0s - loss: 0.6562 - accuracy: 0.60 - ETA: 0s - loss: 0.6556 - accuracy: 0.60 - ETA: 0s - loss: 0.6551 - accuracy: 0.61 - 3s 266us/step - loss: 0.6550 - accuracy: 0.6108 - val_loss: 0.6094 - val_accuracy: 0.7216\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:23 - loss: 0.6966 - accuracy: 0.25 - ETA: 10s - loss: 0.6924 - accuracy: 0.5735 - ETA: 6s - loss: 0.6925 - accuracy: 0.553 - ETA: 5s - loss: 0.6927 - accuracy: 0.53 - ETA: 4s - loss: 0.6927 - accuracy: 0.53 - ETA: 4s - loss: 0.6926 - accuracy: 0.53 - ETA: 4s - loss: 0.6928 - accuracy: 0.53 - ETA: 3s - loss: 0.6923 - accuracy: 0.54 - ETA: 3s - loss: 0.6922 - accuracy: 0.55 - ETA: 3s - loss: 0.6922 - accuracy: 0.55 - ETA: 3s - loss: 0.6920 - accuracy: 0.55 - ETA: 3s - loss: 0.6917 - accuracy: 0.55 - ETA: 3s - loss: 0.6915 - accuracy: 0.55 - ETA: 3s - loss: 0.6916 - accuracy: 0.54 - ETA: 2s - loss: 0.6914 - accuracy: 0.54 - ETA: 2s - loss: 0.6911 - accuracy: 0.54 - ETA: 2s - loss: 0.6909 - accuracy: 0.55 - ETA: 2s - loss: 0.6908 - accuracy: 0.55 - ETA: 2s - loss: 0.6903 - accuracy: 0.55 - ETA: 2s - loss: 0.6897 - accuracy: 0.56 - ETA: 2s - loss: 0.6893 - accuracy: 0.56 - ETA: 2s - loss: 0.6889 - accuracy: 0.56 - ETA: 2s - loss: 0.6881 - accuracy: 0.56 - ETA: 2s - loss: 0.6875 - accuracy: 0.56 - ETA: 2s - loss: 0.6872 - accuracy: 0.56 - ETA: 2s - loss: 0.6867 - accuracy: 0.57 - ETA: 2s - loss: 0.6862 - accuracy: 0.57 - ETA: 1s - loss: 0.6858 - accuracy: 0.57 - ETA: 1s - loss: 0.6854 - accuracy: 0.57 - ETA: 1s - loss: 0.6850 - accuracy: 0.57 - ETA: 1s - loss: 0.6843 - accuracy: 0.57 - ETA: 1s - loss: 0.6837 - accuracy: 0.58 - ETA: 1s - loss: 0.6830 - accuracy: 0.58 - ETA: 1s - loss: 0.6825 - accuracy: 0.58 - ETA: 1s - loss: 0.6817 - accuracy: 0.58 - ETA: 1s - loss: 0.6813 - accuracy: 0.58 - ETA: 1s - loss: 0.6809 - accuracy: 0.59 - ETA: 1s - loss: 0.6803 - accuracy: 0.59 - ETA: 1s - loss: 0.6797 - accuracy: 0.59 - ETA: 1s - loss: 0.6793 - accuracy: 0.59 - ETA: 1s - loss: 0.6787 - accuracy: 0.59 - ETA: 1s - loss: 0.6784 - accuracy: 0.59 - ETA: 1s - loss: 0.6778 - accuracy: 0.59 - ETA: 1s - loss: 0.6770 - accuracy: 0.59 - ETA: 0s - loss: 0.6767 - accuracy: 0.59 - ETA: 0s - loss: 0.6766 - accuracy: 0.59 - ETA: 0s - loss: 0.6764 - accuracy: 0.60 - ETA: 0s - loss: 0.6756 - accuracy: 0.60 - ETA: 0s - loss: 0.6749 - accuracy: 0.60 - ETA: 0s - loss: 0.6746 - accuracy: 0.60 - ETA: 0s - loss: 0.6740 - accuracy: 0.60 - ETA: 0s - loss: 0.6735 - accuracy: 0.60 - ETA: 0s - loss: 0.6729 - accuracy: 0.60 - ETA: 0s - loss: 0.6724 - accuracy: 0.60 - ETA: 0s - loss: 0.6717 - accuracy: 0.60 - ETA: 0s - loss: 0.6715 - accuracy: 0.60 - ETA: 0s - loss: 0.6709 - accuracy: 0.61 - ETA: 0s - loss: 0.6707 - accuracy: 0.61 - ETA: 0s - loss: 0.6700 - accuracy: 0.61 - ETA: 0s - loss: 0.6700 - accuracy: 0.61 - ETA: 0s - loss: 0.6692 - accuracy: 0.61 - ETA: 0s - loss: 0.6688 - accuracy: 0.61 - 3s 270us/step - loss: 0.6688 - accuracy: 0.6129 - val_loss: 0.6256 - val_accuracy: 0.7180\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 104us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 6:16 - loss: 0.7072 - accuracy: 0.25 - ETA: 10s - loss: 0.6941 - accuracy: 0.5245 - ETA: 6s - loss: 0.6922 - accuracy: 0.546 - ETA: 5s - loss: 0.6933 - accuracy: 0.51 - ETA: 4s - loss: 0.6930 - accuracy: 0.53 - ETA: 4s - loss: 0.6927 - accuracy: 0.53 - ETA: 3s - loss: 0.6915 - accuracy: 0.54 - ETA: 3s - loss: 0.6908 - accuracy: 0.54 - ETA: 3s - loss: 0.6901 - accuracy: 0.55 - ETA: 3s - loss: 0.6900 - accuracy: 0.55 - ETA: 3s - loss: 0.6889 - accuracy: 0.56 - ETA: 3s - loss: 0.6874 - accuracy: 0.56 - ETA: 3s - loss: 0.6867 - accuracy: 0.56 - ETA: 2s - loss: 0.6859 - accuracy: 0.57 - ETA: 2s - loss: 0.6856 - accuracy: 0.57 - ETA: 2s - loss: 0.6851 - accuracy: 0.57 - ETA: 2s - loss: 0.6842 - accuracy: 0.57 - ETA: 2s - loss: 0.6838 - accuracy: 0.57 - ETA: 2s - loss: 0.6834 - accuracy: 0.57 - ETA: 2s - loss: 0.6826 - accuracy: 0.57 - ETA: 2s - loss: 0.6814 - accuracy: 0.58 - ETA: 2s - loss: 0.6809 - accuracy: 0.58 - ETA: 2s - loss: 0.6796 - accuracy: 0.58 - ETA: 2s - loss: 0.6784 - accuracy: 0.58 - ETA: 2s - loss: 0.6786 - accuracy: 0.58 - ETA: 2s - loss: 0.6780 - accuracy: 0.58 - ETA: 2s - loss: 0.6762 - accuracy: 0.59 - ETA: 1s - loss: 0.6745 - accuracy: 0.59 - ETA: 1s - loss: 0.6728 - accuracy: 0.59 - ETA: 1s - loss: 0.6724 - accuracy: 0.59 - ETA: 1s - loss: 0.6719 - accuracy: 0.59 - ETA: 1s - loss: 0.6714 - accuracy: 0.59 - ETA: 1s - loss: 0.6697 - accuracy: 0.59 - ETA: 1s - loss: 0.6683 - accuracy: 0.60 - ETA: 1s - loss: 0.6676 - accuracy: 0.60 - ETA: 1s - loss: 0.6673 - accuracy: 0.60 - ETA: 1s - loss: 0.6667 - accuracy: 0.60 - ETA: 1s - loss: 0.6656 - accuracy: 0.60 - ETA: 1s - loss: 0.6656 - accuracy: 0.60 - ETA: 1s - loss: 0.6646 - accuracy: 0.60 - ETA: 1s - loss: 0.6644 - accuracy: 0.60 - ETA: 1s - loss: 0.6645 - accuracy: 0.60 - ETA: 1s - loss: 0.6638 - accuracy: 0.60 - ETA: 1s - loss: 0.6633 - accuracy: 0.60 - ETA: 1s - loss: 0.6623 - accuracy: 0.60 - ETA: 0s - loss: 0.6619 - accuracy: 0.60 - ETA: 0s - loss: 0.6615 - accuracy: 0.60 - ETA: 0s - loss: 0.6607 - accuracy: 0.61 - ETA: 0s - loss: 0.6600 - accuracy: 0.61 - ETA: 0s - loss: 0.6591 - accuracy: 0.61 - ETA: 0s - loss: 0.6581 - accuracy: 0.61 - ETA: 0s - loss: 0.6580 - accuracy: 0.61 - ETA: 0s - loss: 0.6578 - accuracy: 0.61 - ETA: 0s - loss: 0.6576 - accuracy: 0.61 - ETA: 0s - loss: 0.6570 - accuracy: 0.61 - ETA: 0s - loss: 0.6563 - accuracy: 0.61 - ETA: 0s - loss: 0.6559 - accuracy: 0.61 - ETA: 0s - loss: 0.6554 - accuracy: 0.61 - ETA: 0s - loss: 0.6547 - accuracy: 0.62 - ETA: 0s - loss: 0.6539 - accuracy: 0.62 - ETA: 0s - loss: 0.6532 - accuracy: 0.62 - ETA: 0s - loss: 0.6528 - accuracy: 0.62 - ETA: 0s - loss: 0.6523 - accuracy: 0.62 - 3s 274us/step - loss: 0.6524 - accuracy: 0.6240 - val_loss: 0.6020 - val_accuracy: 0.7209\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:13 - loss: 0.6961 - accuracy: 0.50 - ETA: 10s - loss: 0.6928 - accuracy: 0.4904 - ETA: 6s - loss: 0.6929 - accuracy: 0.519 - ETA: 5s - loss: 0.6918 - accuracy: 0.54 - ETA: 4s - loss: 0.6918 - accuracy: 0.53 - ETA: 4s - loss: 0.6908 - accuracy: 0.53 - ETA: 3s - loss: 0.6905 - accuracy: 0.52 - ETA: 3s - loss: 0.6898 - accuracy: 0.52 - ETA: 3s - loss: 0.6892 - accuracy: 0.52 - ETA: 3s - loss: 0.6888 - accuracy: 0.53 - ETA: 3s - loss: 0.6882 - accuracy: 0.54 - ETA: 3s - loss: 0.6870 - accuracy: 0.55 - ETA: 3s - loss: 0.6865 - accuracy: 0.55 - ETA: 2s - loss: 0.6858 - accuracy: 0.56 - ETA: 2s - loss: 0.6856 - accuracy: 0.56 - ETA: 2s - loss: 0.6843 - accuracy: 0.56 - ETA: 2s - loss: 0.6835 - accuracy: 0.56 - ETA: 2s - loss: 0.6826 - accuracy: 0.57 - ETA: 2s - loss: 0.6813 - accuracy: 0.57 - ETA: 2s - loss: 0.6802 - accuracy: 0.58 - ETA: 2s - loss: 0.6803 - accuracy: 0.58 - ETA: 2s - loss: 0.6792 - accuracy: 0.58 - ETA: 2s - loss: 0.6786 - accuracy: 0.58 - ETA: 2s - loss: 0.6771 - accuracy: 0.58 - ETA: 2s - loss: 0.6758 - accuracy: 0.59 - ETA: 2s - loss: 0.6744 - accuracy: 0.59 - ETA: 1s - loss: 0.6730 - accuracy: 0.59 - ETA: 1s - loss: 0.6720 - accuracy: 0.59 - ETA: 1s - loss: 0.6707 - accuracy: 0.59 - ETA: 1s - loss: 0.6687 - accuracy: 0.60 - ETA: 1s - loss: 0.6681 - accuracy: 0.60 - ETA: 1s - loss: 0.6680 - accuracy: 0.60 - ETA: 1s - loss: 0.6683 - accuracy: 0.60 - ETA: 1s - loss: 0.6673 - accuracy: 0.60 - ETA: 1s - loss: 0.6666 - accuracy: 0.60 - ETA: 1s - loss: 0.6660 - accuracy: 0.60 - ETA: 1s - loss: 0.6651 - accuracy: 0.60 - ETA: 1s - loss: 0.6640 - accuracy: 0.60 - ETA: 1s - loss: 0.6628 - accuracy: 0.61 - ETA: 1s - loss: 0.6617 - accuracy: 0.61 - ETA: 1s - loss: 0.6611 - accuracy: 0.61 - ETA: 1s - loss: 0.6607 - accuracy: 0.61 - ETA: 1s - loss: 0.6603 - accuracy: 0.61 - ETA: 1s - loss: 0.6599 - accuracy: 0.61 - ETA: 0s - loss: 0.6592 - accuracy: 0.61 - ETA: 0s - loss: 0.6588 - accuracy: 0.62 - ETA: 0s - loss: 0.6576 - accuracy: 0.62 - ETA: 0s - loss: 0.6567 - accuracy: 0.62 - ETA: 0s - loss: 0.6562 - accuracy: 0.62 - ETA: 0s - loss: 0.6552 - accuracy: 0.62 - ETA: 0s - loss: 0.6543 - accuracy: 0.62 - ETA: 0s - loss: 0.6538 - accuracy: 0.62 - ETA: 0s - loss: 0.6532 - accuracy: 0.62 - ETA: 0s - loss: 0.6525 - accuracy: 0.62 - ETA: 0s - loss: 0.6519 - accuracy: 0.63 - ETA: 0s - loss: 0.6512 - accuracy: 0.63 - ETA: 0s - loss: 0.6513 - accuracy: 0.63 - ETA: 0s - loss: 0.6507 - accuracy: 0.63 - ETA: 0s - loss: 0.6502 - accuracy: 0.63 - ETA: 0s - loss: 0.6502 - accuracy: 0.63 - ETA: 0s - loss: 0.6500 - accuracy: 0.63 - ETA: 0s - loss: 0.6489 - accuracy: 0.63 - 3s 271us/step - loss: 0.6484 - accuracy: 0.6355 - val_loss: 0.6004 - val_accuracy: 0.7045\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 107us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:13 - loss: 0.6952 - accuracy: 0.50 - ETA: 9s - loss: 0.6942 - accuracy: 0.5093 - ETA: 6s - loss: 0.6940 - accuracy: 0.48 - ETA: 5s - loss: 0.6939 - accuracy: 0.48 - ETA: 4s - loss: 0.6929 - accuracy: 0.50 - ETA: 4s - loss: 0.6923 - accuracy: 0.50 - ETA: 3s - loss: 0.6922 - accuracy: 0.50 - ETA: 3s - loss: 0.6924 - accuracy: 0.50 - ETA: 3s - loss: 0.6918 - accuracy: 0.50 - ETA: 3s - loss: 0.6912 - accuracy: 0.50 - ETA: 3s - loss: 0.6903 - accuracy: 0.51 - ETA: 3s - loss: 0.6896 - accuracy: 0.51 - ETA: 3s - loss: 0.6892 - accuracy: 0.52 - ETA: 2s - loss: 0.6888 - accuracy: 0.52 - ETA: 2s - loss: 0.6879 - accuracy: 0.52 - ETA: 2s - loss: 0.6866 - accuracy: 0.53 - ETA: 2s - loss: 0.6855 - accuracy: 0.53 - ETA: 2s - loss: 0.6835 - accuracy: 0.53 - ETA: 2s - loss: 0.6837 - accuracy: 0.53 - ETA: 2s - loss: 0.6833 - accuracy: 0.53 - ETA: 2s - loss: 0.6828 - accuracy: 0.54 - ETA: 2s - loss: 0.6815 - accuracy: 0.54 - ETA: 2s - loss: 0.6815 - accuracy: 0.54 - ETA: 2s - loss: 0.6805 - accuracy: 0.54 - ETA: 2s - loss: 0.6791 - accuracy: 0.55 - ETA: 2s - loss: 0.6790 - accuracy: 0.55 - ETA: 2s - loss: 0.6782 - accuracy: 0.55 - ETA: 2s - loss: 0.6768 - accuracy: 0.55 - ETA: 2s - loss: 0.6758 - accuracy: 0.55 - ETA: 1s - loss: 0.6753 - accuracy: 0.56 - ETA: 1s - loss: 0.6740 - accuracy: 0.56 - ETA: 1s - loss: 0.6727 - accuracy: 0.56 - ETA: 1s - loss: 0.6716 - accuracy: 0.56 - ETA: 1s - loss: 0.6704 - accuracy: 0.57 - ETA: 1s - loss: 0.6699 - accuracy: 0.57 - ETA: 1s - loss: 0.6689 - accuracy: 0.57 - ETA: 1s - loss: 0.6678 - accuracy: 0.57 - ETA: 1s - loss: 0.6660 - accuracy: 0.58 - ETA: 1s - loss: 0.6647 - accuracy: 0.58 - ETA: 1s - loss: 0.6641 - accuracy: 0.58 - ETA: 1s - loss: 0.6630 - accuracy: 0.58 - ETA: 1s - loss: 0.6620 - accuracy: 0.58 - ETA: 1s - loss: 0.6612 - accuracy: 0.58 - ETA: 1s - loss: 0.6611 - accuracy: 0.58 - ETA: 1s - loss: 0.6607 - accuracy: 0.58 - ETA: 0s - loss: 0.6603 - accuracy: 0.59 - ETA: 0s - loss: 0.6596 - accuracy: 0.59 - ETA: 0s - loss: 0.6585 - accuracy: 0.59 - ETA: 0s - loss: 0.6577 - accuracy: 0.59 - ETA: 0s - loss: 0.6576 - accuracy: 0.59 - ETA: 0s - loss: 0.6575 - accuracy: 0.59 - ETA: 0s - loss: 0.6577 - accuracy: 0.59 - ETA: 0s - loss: 0.6574 - accuracy: 0.59 - ETA: 0s - loss: 0.6563 - accuracy: 0.59 - ETA: 0s - loss: 0.6552 - accuracy: 0.59 - ETA: 0s - loss: 0.6543 - accuracy: 0.59 - ETA: 0s - loss: 0.6540 - accuracy: 0.60 - ETA: 0s - loss: 0.6533 - accuracy: 0.60 - ETA: 0s - loss: 0.6520 - accuracy: 0.60 - ETA: 0s - loss: 0.6523 - accuracy: 0.60 - ETA: 0s - loss: 0.6518 - accuracy: 0.60 - ETA: 0s - loss: 0.6517 - accuracy: 0.60 - ETA: 0s - loss: 0.6511 - accuracy: 0.60 - 3s 274us/step - loss: 0.6505 - accuracy: 0.6049 - val_loss: 0.5960 - val_accuracy: 0.7173\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 98us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:39 - loss: 0.6866 - accuracy: 1.00 - ETA: 10s - loss: 0.6949 - accuracy: 0.4904 - ETA: 6s - loss: 0.6940 - accuracy: 0.504 - ETA: 5s - loss: 0.6937 - accuracy: 0.50 - ETA: 4s - loss: 0.6916 - accuracy: 0.52 - ETA: 4s - loss: 0.6913 - accuracy: 0.52 - ETA: 4s - loss: 0.6911 - accuracy: 0.51 - ETA: 3s - loss: 0.6911 - accuracy: 0.52 - ETA: 3s - loss: 0.6909 - accuracy: 0.52 - ETA: 3s - loss: 0.6905 - accuracy: 0.52 - ETA: 3s - loss: 0.6898 - accuracy: 0.53 - ETA: 3s - loss: 0.6891 - accuracy: 0.54 - ETA: 3s - loss: 0.6884 - accuracy: 0.54 - ETA: 3s - loss: 0.6873 - accuracy: 0.54 - ETA: 2s - loss: 0.6867 - accuracy: 0.55 - ETA: 2s - loss: 0.6861 - accuracy: 0.55 - ETA: 2s - loss: 0.6852 - accuracy: 0.56 - ETA: 2s - loss: 0.6840 - accuracy: 0.56 - ETA: 2s - loss: 0.6830 - accuracy: 0.56 - ETA: 2s - loss: 0.6827 - accuracy: 0.56 - ETA: 2s - loss: 0.6810 - accuracy: 0.57 - ETA: 2s - loss: 0.6799 - accuracy: 0.57 - ETA: 2s - loss: 0.6796 - accuracy: 0.57 - ETA: 2s - loss: 0.6789 - accuracy: 0.58 - ETA: 2s - loss: 0.6772 - accuracy: 0.58 - ETA: 2s - loss: 0.6764 - accuracy: 0.58 - ETA: 2s - loss: 0.6755 - accuracy: 0.58 - ETA: 2s - loss: 0.6748 - accuracy: 0.58 - ETA: 2s - loss: 0.6738 - accuracy: 0.59 - ETA: 1s - loss: 0.6726 - accuracy: 0.59 - ETA: 1s - loss: 0.6706 - accuracy: 0.59 - ETA: 1s - loss: 0.6699 - accuracy: 0.59 - ETA: 1s - loss: 0.6690 - accuracy: 0.59 - ETA: 1s - loss: 0.6682 - accuracy: 0.59 - ETA: 1s - loss: 0.6670 - accuracy: 0.60 - ETA: 1s - loss: 0.6664 - accuracy: 0.60 - ETA: 1s - loss: 0.6661 - accuracy: 0.60 - ETA: 1s - loss: 0.6659 - accuracy: 0.60 - ETA: 1s - loss: 0.6654 - accuracy: 0.60 - ETA: 1s - loss: 0.6643 - accuracy: 0.60 - ETA: 1s - loss: 0.6642 - accuracy: 0.60 - ETA: 1s - loss: 0.6630 - accuracy: 0.60 - ETA: 1s - loss: 0.6621 - accuracy: 0.60 - ETA: 1s - loss: 0.6604 - accuracy: 0.60 - ETA: 1s - loss: 0.6603 - accuracy: 0.60 - ETA: 1s - loss: 0.6587 - accuracy: 0.61 - ETA: 0s - loss: 0.6580 - accuracy: 0.61 - ETA: 0s - loss: 0.6574 - accuracy: 0.61 - ETA: 0s - loss: 0.6569 - accuracy: 0.61 - ETA: 0s - loss: 0.6562 - accuracy: 0.61 - ETA: 0s - loss: 0.6559 - accuracy: 0.61 - ETA: 0s - loss: 0.6548 - accuracy: 0.61 - ETA: 0s - loss: 0.6534 - accuracy: 0.61 - ETA: 0s - loss: 0.6525 - accuracy: 0.62 - ETA: 0s - loss: 0.6519 - accuracy: 0.62 - ETA: 0s - loss: 0.6514 - accuracy: 0.62 - ETA: 0s - loss: 0.6506 - accuracy: 0.62 - ETA: 0s - loss: 0.6509 - accuracy: 0.62 - ETA: 0s - loss: 0.6499 - accuracy: 0.62 - ETA: 0s - loss: 0.6488 - accuracy: 0.62 - ETA: 0s - loss: 0.6486 - accuracy: 0.62 - ETA: 0s - loss: 0.6481 - accuracy: 0.62 - ETA: 0s - loss: 0.6477 - accuracy: 0.62 - ETA: 0s - loss: 0.6470 - accuracy: 0.62 - 4s 280us/step - loss: 0.6461 - accuracy: 0.6308 - val_loss: 0.5869 - val_accuracy: 0.7188\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 108us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 6:10 - loss: 0.6974 - accuracy: 0.50 - ETA: 10s - loss: 0.6934 - accuracy: 0.5343 - ETA: 6s - loss: 0.6932 - accuracy: 0.526 - ETA: 5s - loss: 0.6936 - accuracy: 0.51 - ETA: 4s - loss: 0.6929 - accuracy: 0.54 - ETA: 4s - loss: 0.6921 - accuracy: 0.55 - ETA: 4s - loss: 0.6912 - accuracy: 0.55 - ETA: 3s - loss: 0.6905 - accuracy: 0.55 - ETA: 3s - loss: 0.6886 - accuracy: 0.56 - ETA: 3s - loss: 0.6877 - accuracy: 0.56 - ETA: 3s - loss: 0.6871 - accuracy: 0.56 - ETA: 3s - loss: 0.6858 - accuracy: 0.56 - ETA: 3s - loss: 0.6853 - accuracy: 0.56 - ETA: 2s - loss: 0.6842 - accuracy: 0.57 - ETA: 2s - loss: 0.6832 - accuracy: 0.57 - ETA: 2s - loss: 0.6821 - accuracy: 0.57 - ETA: 2s - loss: 0.6804 - accuracy: 0.58 - ETA: 2s - loss: 0.6786 - accuracy: 0.58 - ETA: 2s - loss: 0.6780 - accuracy: 0.58 - ETA: 2s - loss: 0.6767 - accuracy: 0.58 - ETA: 2s - loss: 0.6757 - accuracy: 0.58 - ETA: 2s - loss: 0.6742 - accuracy: 0.59 - ETA: 2s - loss: 0.6732 - accuracy: 0.59 - ETA: 2s - loss: 0.6723 - accuracy: 0.59 - ETA: 2s - loss: 0.6710 - accuracy: 0.59 - ETA: 2s - loss: 0.6695 - accuracy: 0.60 - ETA: 1s - loss: 0.6681 - accuracy: 0.60 - ETA: 1s - loss: 0.6671 - accuracy: 0.60 - ETA: 1s - loss: 0.6661 - accuracy: 0.60 - ETA: 1s - loss: 0.6651 - accuracy: 0.60 - ETA: 1s - loss: 0.6638 - accuracy: 0.60 - ETA: 1s - loss: 0.6616 - accuracy: 0.61 - ETA: 1s - loss: 0.6601 - accuracy: 0.61 - ETA: 1s - loss: 0.6587 - accuracy: 0.61 - ETA: 1s - loss: 0.6576 - accuracy: 0.61 - ETA: 1s - loss: 0.6562 - accuracy: 0.62 - ETA: 1s - loss: 0.6551 - accuracy: 0.62 - ETA: 1s - loss: 0.6553 - accuracy: 0.62 - ETA: 1s - loss: 0.6539 - accuracy: 0.62 - ETA: 1s - loss: 0.6534 - accuracy: 0.62 - ETA: 1s - loss: 0.6536 - accuracy: 0.62 - ETA: 1s - loss: 0.6527 - accuracy: 0.62 - ETA: 1s - loss: 0.6524 - accuracy: 0.62 - ETA: 1s - loss: 0.6513 - accuracy: 0.62 - ETA: 0s - loss: 0.6505 - accuracy: 0.63 - ETA: 0s - loss: 0.6496 - accuracy: 0.63 - ETA: 0s - loss: 0.6486 - accuracy: 0.63 - ETA: 0s - loss: 0.6475 - accuracy: 0.63 - ETA: 0s - loss: 0.6465 - accuracy: 0.63 - ETA: 0s - loss: 0.6462 - accuracy: 0.63 - ETA: 0s - loss: 0.6450 - accuracy: 0.63 - ETA: 0s - loss: 0.6439 - accuracy: 0.64 - ETA: 0s - loss: 0.6433 - accuracy: 0.64 - ETA: 0s - loss: 0.6430 - accuracy: 0.64 - ETA: 0s - loss: 0.6430 - accuracy: 0.64 - ETA: 0s - loss: 0.6419 - accuracy: 0.64 - ETA: 0s - loss: 0.6409 - accuracy: 0.64 - ETA: 0s - loss: 0.6405 - accuracy: 0.64 - ETA: 0s - loss: 0.6404 - accuracy: 0.64 - ETA: 0s - loss: 0.6396 - accuracy: 0.64 - ETA: 0s - loss: 0.6383 - accuracy: 0.64 - ETA: 0s - loss: 0.6374 - accuracy: 0.65 - ETA: 0s - loss: 0.6364 - accuracy: 0.65 - 3s 275us/step - loss: 0.6356 - accuracy: 0.6526 - val_loss: 0.5804 - val_accuracy: 0.7173\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 107us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:13 - loss: 0.6946 - accuracy: 0.50 - ETA: 9s - loss: 0.6941 - accuracy: 0.5093 - ETA: 6s - loss: 0.6938 - accuracy: 0.50 - ETA: 5s - loss: 0.6933 - accuracy: 0.52 - ETA: 4s - loss: 0.6911 - accuracy: 0.54 - ETA: 4s - loss: 0.6874 - accuracy: 0.55 - ETA: 3s - loss: 0.6865 - accuracy: 0.55 - ETA: 3s - loss: 0.6873 - accuracy: 0.54 - ETA: 3s - loss: 0.6867 - accuracy: 0.54 - ETA: 3s - loss: 0.6874 - accuracy: 0.54 - ETA: 3s - loss: 0.6866 - accuracy: 0.54 - ETA: 2s - loss: 0.6859 - accuracy: 0.55 - ETA: 2s - loss: 0.6852 - accuracy: 0.55 - ETA: 2s - loss: 0.6836 - accuracy: 0.55 - ETA: 2s - loss: 0.6829 - accuracy: 0.56 - ETA: 2s - loss: 0.6820 - accuracy: 0.56 - ETA: 2s - loss: 0.6804 - accuracy: 0.56 - ETA: 2s - loss: 0.6791 - accuracy: 0.56 - ETA: 2s - loss: 0.6778 - accuracy: 0.57 - ETA: 2s - loss: 0.6771 - accuracy: 0.57 - ETA: 2s - loss: 0.6771 - accuracy: 0.57 - ETA: 2s - loss: 0.6759 - accuracy: 0.58 - ETA: 2s - loss: 0.6747 - accuracy: 0.58 - ETA: 2s - loss: 0.6736 - accuracy: 0.58 - ETA: 1s - loss: 0.6728 - accuracy: 0.58 - ETA: 1s - loss: 0.6718 - accuracy: 0.59 - ETA: 1s - loss: 0.6700 - accuracy: 0.59 - ETA: 1s - loss: 0.6693 - accuracy: 0.59 - ETA: 1s - loss: 0.6686 - accuracy: 0.59 - ETA: 1s - loss: 0.6680 - accuracy: 0.60 - ETA: 1s - loss: 0.6669 - accuracy: 0.60 - ETA: 1s - loss: 0.6657 - accuracy: 0.60 - ETA: 1s - loss: 0.6655 - accuracy: 0.60 - ETA: 1s - loss: 0.6643 - accuracy: 0.60 - ETA: 1s - loss: 0.6629 - accuracy: 0.61 - ETA: 1s - loss: 0.6623 - accuracy: 0.61 - ETA: 1s - loss: 0.6617 - accuracy: 0.61 - ETA: 1s - loss: 0.6606 - accuracy: 0.61 - ETA: 1s - loss: 0.6600 - accuracy: 0.61 - ETA: 1s - loss: 0.6589 - accuracy: 0.61 - ETA: 1s - loss: 0.6582 - accuracy: 0.62 - ETA: 1s - loss: 0.6567 - accuracy: 0.62 - ETA: 0s - loss: 0.6557 - accuracy: 0.62 - ETA: 0s - loss: 0.6557 - accuracy: 0.62 - ETA: 0s - loss: 0.6544 - accuracy: 0.62 - ETA: 0s - loss: 0.6535 - accuracy: 0.62 - ETA: 0s - loss: 0.6530 - accuracy: 0.62 - ETA: 0s - loss: 0.6527 - accuracy: 0.62 - ETA: 0s - loss: 0.6516 - accuracy: 0.62 - ETA: 0s - loss: 0.6509 - accuracy: 0.63 - ETA: 0s - loss: 0.6501 - accuracy: 0.63 - ETA: 0s - loss: 0.6495 - accuracy: 0.63 - ETA: 0s - loss: 0.6498 - accuracy: 0.63 - ETA: 0s - loss: 0.6494 - accuracy: 0.63 - ETA: 0s - loss: 0.6486 - accuracy: 0.63 - ETA: 0s - loss: 0.6476 - accuracy: 0.63 - ETA: 0s - loss: 0.6462 - accuracy: 0.63 - ETA: 0s - loss: 0.6466 - accuracy: 0.63 - ETA: 0s - loss: 0.6464 - accuracy: 0.63 - ETA: 0s - loss: 0.6466 - accuracy: 0.63 - ETA: 0s - loss: 0.6462 - accuracy: 0.63 - 3s 267us/step - loss: 0.6461 - accuracy: 0.6370 - val_loss: 0.5922 - val_accuracy: 0.7237\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 99us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:07 - loss: 0.6944 - accuracy: 0.50 - ETA: 9s - loss: 0.6943 - accuracy: 0.4623 - ETA: 6s - loss: 0.6940 - accuracy: 0.49 - ETA: 5s - loss: 0.6927 - accuracy: 0.50 - ETA: 4s - loss: 0.6905 - accuracy: 0.52 - ETA: 4s - loss: 0.6891 - accuracy: 0.52 - ETA: 3s - loss: 0.6891 - accuracy: 0.52 - ETA: 3s - loss: 0.6890 - accuracy: 0.53 - ETA: 3s - loss: 0.6889 - accuracy: 0.53 - ETA: 3s - loss: 0.6885 - accuracy: 0.53 - ETA: 3s - loss: 0.6869 - accuracy: 0.54 - ETA: 3s - loss: 0.6864 - accuracy: 0.54 - ETA: 3s - loss: 0.6852 - accuracy: 0.54 - ETA: 2s - loss: 0.6842 - accuracy: 0.54 - ETA: 2s - loss: 0.6838 - accuracy: 0.54 - ETA: 2s - loss: 0.6824 - accuracy: 0.55 - ETA: 2s - loss: 0.6815 - accuracy: 0.55 - ETA: 2s - loss: 0.6801 - accuracy: 0.56 - ETA: 2s - loss: 0.6793 - accuracy: 0.56 - ETA: 2s - loss: 0.6779 - accuracy: 0.57 - ETA: 2s - loss: 0.6782 - accuracy: 0.57 - ETA: 2s - loss: 0.6769 - accuracy: 0.57 - ETA: 2s - loss: 0.6753 - accuracy: 0.58 - ETA: 2s - loss: 0.6748 - accuracy: 0.58 - ETA: 2s - loss: 0.6739 - accuracy: 0.58 - ETA: 2s - loss: 0.6727 - accuracy: 0.58 - ETA: 2s - loss: 0.6717 - accuracy: 0.59 - ETA: 1s - loss: 0.6699 - accuracy: 0.59 - ETA: 1s - loss: 0.6694 - accuracy: 0.59 - ETA: 1s - loss: 0.6690 - accuracy: 0.59 - ETA: 1s - loss: 0.6677 - accuracy: 0.59 - ETA: 1s - loss: 0.6678 - accuracy: 0.59 - ETA: 1s - loss: 0.6660 - accuracy: 0.60 - ETA: 1s - loss: 0.6653 - accuracy: 0.60 - ETA: 1s - loss: 0.6651 - accuracy: 0.60 - ETA: 1s - loss: 0.6640 - accuracy: 0.60 - ETA: 1s - loss: 0.6633 - accuracy: 0.60 - ETA: 1s - loss: 0.6624 - accuracy: 0.60 - ETA: 1s - loss: 0.6612 - accuracy: 0.61 - ETA: 1s - loss: 0.6601 - accuracy: 0.61 - ETA: 1s - loss: 0.6591 - accuracy: 0.61 - ETA: 1s - loss: 0.6582 - accuracy: 0.61 - ETA: 1s - loss: 0.6577 - accuracy: 0.61 - ETA: 1s - loss: 0.6567 - accuracy: 0.61 - ETA: 0s - loss: 0.6550 - accuracy: 0.62 - ETA: 0s - loss: 0.6537 - accuracy: 0.62 - ETA: 0s - loss: 0.6530 - accuracy: 0.62 - ETA: 0s - loss: 0.6520 - accuracy: 0.62 - ETA: 0s - loss: 0.6513 - accuracy: 0.62 - ETA: 0s - loss: 0.6518 - accuracy: 0.62 - ETA: 0s - loss: 0.6510 - accuracy: 0.62 - ETA: 0s - loss: 0.6510 - accuracy: 0.62 - ETA: 0s - loss: 0.6510 - accuracy: 0.62 - ETA: 0s - loss: 0.6504 - accuracy: 0.62 - ETA: 0s - loss: 0.6503 - accuracy: 0.63 - ETA: 0s - loss: 0.6496 - accuracy: 0.63 - ETA: 0s - loss: 0.6490 - accuracy: 0.63 - ETA: 0s - loss: 0.6487 - accuracy: 0.63 - ETA: 0s - loss: 0.6480 - accuracy: 0.63 - ETA: 0s - loss: 0.6473 - accuracy: 0.63 - ETA: 0s - loss: 0.6470 - accuracy: 0.63 - ETA: 0s - loss: 0.6467 - accuracy: 0.63 - ETA: 0s - loss: 0.6457 - accuracy: 0.63 - 3s 274us/step - loss: 0.6457 - accuracy: 0.6368 - val_loss: 0.5946 - val_accuracy: 0.7259\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:10 - loss: 0.6945 - accuracy: 0.50 - ETA: 10s - loss: 0.6940 - accuracy: 0.4808 - ETA: 6s - loss: 0.6936 - accuracy: 0.524 - ETA: 5s - loss: 0.6930 - accuracy: 0.53 - ETA: 4s - loss: 0.6927 - accuracy: 0.53 - ETA: 4s - loss: 0.6925 - accuracy: 0.53 - ETA: 3s - loss: 0.6922 - accuracy: 0.53 - ETA: 3s - loss: 0.6914 - accuracy: 0.54 - ETA: 3s - loss: 0.6911 - accuracy: 0.54 - ETA: 3s - loss: 0.6899 - accuracy: 0.54 - ETA: 3s - loss: 0.6897 - accuracy: 0.54 - ETA: 3s - loss: 0.6891 - accuracy: 0.54 - ETA: 3s - loss: 0.6882 - accuracy: 0.55 - ETA: 2s - loss: 0.6868 - accuracy: 0.55 - ETA: 2s - loss: 0.6861 - accuracy: 0.55 - ETA: 2s - loss: 0.6843 - accuracy: 0.56 - ETA: 2s - loss: 0.6835 - accuracy: 0.56 - ETA: 2s - loss: 0.6826 - accuracy: 0.56 - ETA: 2s - loss: 0.6813 - accuracy: 0.57 - ETA: 2s - loss: 0.6801 - accuracy: 0.57 - ETA: 2s - loss: 0.6799 - accuracy: 0.57 - ETA: 2s - loss: 0.6790 - accuracy: 0.57 - ETA: 2s - loss: 0.6780 - accuracy: 0.58 - ETA: 2s - loss: 0.6766 - accuracy: 0.58 - ETA: 2s - loss: 0.6742 - accuracy: 0.59 - ETA: 2s - loss: 0.6711 - accuracy: 0.59 - ETA: 2s - loss: 0.6696 - accuracy: 0.59 - ETA: 1s - loss: 0.6679 - accuracy: 0.59 - ETA: 1s - loss: 0.6674 - accuracy: 0.60 - ETA: 1s - loss: 0.6677 - accuracy: 0.59 - ETA: 1s - loss: 0.6663 - accuracy: 0.60 - ETA: 1s - loss: 0.6648 - accuracy: 0.60 - ETA: 1s - loss: 0.6643 - accuracy: 0.60 - ETA: 1s - loss: 0.6626 - accuracy: 0.60 - ETA: 1s - loss: 0.6607 - accuracy: 0.60 - ETA: 1s - loss: 0.6593 - accuracy: 0.61 - ETA: 1s - loss: 0.6582 - accuracy: 0.61 - ETA: 1s - loss: 0.6570 - accuracy: 0.61 - ETA: 1s - loss: 0.6571 - accuracy: 0.61 - ETA: 1s - loss: 0.6566 - accuracy: 0.61 - ETA: 1s - loss: 0.6566 - accuracy: 0.61 - ETA: 1s - loss: 0.6566 - accuracy: 0.61 - ETA: 1s - loss: 0.6566 - accuracy: 0.61 - ETA: 1s - loss: 0.6560 - accuracy: 0.61 - ETA: 1s - loss: 0.6550 - accuracy: 0.62 - ETA: 0s - loss: 0.6547 - accuracy: 0.62 - ETA: 0s - loss: 0.6544 - accuracy: 0.62 - ETA: 0s - loss: 0.6540 - accuracy: 0.62 - ETA: 0s - loss: 0.6542 - accuracy: 0.62 - ETA: 0s - loss: 0.6537 - accuracy: 0.62 - ETA: 0s - loss: 0.6535 - accuracy: 0.62 - ETA: 0s - loss: 0.6531 - accuracy: 0.62 - ETA: 0s - loss: 0.6532 - accuracy: 0.62 - ETA: 0s - loss: 0.6529 - accuracy: 0.62 - ETA: 0s - loss: 0.6526 - accuracy: 0.62 - ETA: 0s - loss: 0.6515 - accuracy: 0.62 - ETA: 0s - loss: 0.6511 - accuracy: 0.62 - ETA: 0s - loss: 0.6506 - accuracy: 0.63 - ETA: 0s - loss: 0.6498 - accuracy: 0.63 - ETA: 0s - loss: 0.6487 - accuracy: 0.63 - ETA: 0s - loss: 0.6482 - accuracy: 0.63 - ETA: 0s - loss: 0.6473 - accuracy: 0.63 - ETA: 0s - loss: 0.6464 - accuracy: 0.63 - 3s 275us/step - loss: 0.6464 - accuracy: 0.6355 - val_loss: 0.5907 - val_accuracy: 0.7244\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 100us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 6:10 - loss: 0.6899 - accuracy: 0.50 - ETA: 10s - loss: 0.6942 - accuracy: 0.5052 - ETA: 6s - loss: 0.6927 - accuracy: 0.530 - ETA: 5s - loss: 0.6925 - accuracy: 0.53 - ETA: 4s - loss: 0.6906 - accuracy: 0.55 - ETA: 4s - loss: 0.6905 - accuracy: 0.55 - ETA: 4s - loss: 0.6897 - accuracy: 0.56 - ETA: 3s - loss: 0.6892 - accuracy: 0.56 - ETA: 3s - loss: 0.6885 - accuracy: 0.56 - ETA: 3s - loss: 0.6874 - accuracy: 0.56 - ETA: 3s - loss: 0.6866 - accuracy: 0.56 - ETA: 3s - loss: 0.6852 - accuracy: 0.57 - ETA: 3s - loss: 0.6836 - accuracy: 0.57 - ETA: 3s - loss: 0.6829 - accuracy: 0.58 - ETA: 3s - loss: 0.6812 - accuracy: 0.59 - ETA: 2s - loss: 0.6802 - accuracy: 0.59 - ETA: 2s - loss: 0.6789 - accuracy: 0.59 - ETA: 2s - loss: 0.6758 - accuracy: 0.60 - ETA: 2s - loss: 0.6751 - accuracy: 0.60 - ETA: 2s - loss: 0.6729 - accuracy: 0.60 - ETA: 2s - loss: 0.6715 - accuracy: 0.60 - ETA: 2s - loss: 0.6701 - accuracy: 0.60 - ETA: 2s - loss: 0.6690 - accuracy: 0.61 - ETA: 2s - loss: 0.6672 - accuracy: 0.61 - ETA: 2s - loss: 0.6666 - accuracy: 0.61 - ETA: 2s - loss: 0.6657 - accuracy: 0.61 - ETA: 2s - loss: 0.6649 - accuracy: 0.61 - ETA: 2s - loss: 0.6639 - accuracy: 0.61 - ETA: 2s - loss: 0.6631 - accuracy: 0.62 - ETA: 1s - loss: 0.6621 - accuracy: 0.62 - ETA: 1s - loss: 0.6614 - accuracy: 0.62 - ETA: 1s - loss: 0.6607 - accuracy: 0.62 - ETA: 1s - loss: 0.6593 - accuracy: 0.62 - ETA: 1s - loss: 0.6589 - accuracy: 0.62 - ETA: 1s - loss: 0.6583 - accuracy: 0.62 - ETA: 1s - loss: 0.6574 - accuracy: 0.62 - ETA: 1s - loss: 0.6567 - accuracy: 0.62 - ETA: 1s - loss: 0.6562 - accuracy: 0.62 - ETA: 1s - loss: 0.6561 - accuracy: 0.63 - ETA: 1s - loss: 0.6553 - accuracy: 0.63 - ETA: 1s - loss: 0.6540 - accuracy: 0.63 - ETA: 1s - loss: 0.6528 - accuracy: 0.63 - ETA: 1s - loss: 0.6517 - accuracy: 0.63 - ETA: 1s - loss: 0.6511 - accuracy: 0.63 - ETA: 1s - loss: 0.6516 - accuracy: 0.63 - ETA: 1s - loss: 0.6501 - accuracy: 0.63 - ETA: 0s - loss: 0.6499 - accuracy: 0.63 - ETA: 0s - loss: 0.6490 - accuracy: 0.64 - ETA: 0s - loss: 0.6481 - accuracy: 0.64 - ETA: 0s - loss: 0.6466 - accuracy: 0.64 - ETA: 0s - loss: 0.6452 - accuracy: 0.64 - ETA: 0s - loss: 0.6440 - accuracy: 0.64 - ETA: 0s - loss: 0.6430 - accuracy: 0.64 - ETA: 0s - loss: 0.6421 - accuracy: 0.64 - ETA: 0s - loss: 0.6420 - accuracy: 0.64 - ETA: 0s - loss: 0.6411 - accuracy: 0.65 - ETA: 0s - loss: 0.6405 - accuracy: 0.65 - ETA: 0s - loss: 0.6406 - accuracy: 0.65 - ETA: 0s - loss: 0.6397 - accuracy: 0.65 - ETA: 0s - loss: 0.6388 - accuracy: 0.65 - ETA: 0s - loss: 0.6385 - accuracy: 0.65 - ETA: 0s - loss: 0.6382 - accuracy: 0.65 - ETA: 0s - loss: 0.6376 - accuracy: 0.65 - ETA: 0s - loss: 0.6366 - accuracy: 0.65 - 4s 278us/step - loss: 0.6368 - accuracy: 0.6559 - val_loss: 0.5760 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:13 - loss: 0.7180 - accuracy: 0.25 - ETA: 10s - loss: 0.6956 - accuracy: 0.4902 - ETA: 6s - loss: 0.6959 - accuracy: 0.502 - ETA: 5s - loss: 0.6950 - accuracy: 0.50 - ETA: 4s - loss: 0.6939 - accuracy: 0.51 - ETA: 4s - loss: 0.6934 - accuracy: 0.52 - ETA: 3s - loss: 0.6931 - accuracy: 0.52 - ETA: 3s - loss: 0.6927 - accuracy: 0.53 - ETA: 3s - loss: 0.6909 - accuracy: 0.53 - ETA: 3s - loss: 0.6893 - accuracy: 0.53 - ETA: 3s - loss: 0.6884 - accuracy: 0.53 - ETA: 3s - loss: 0.6877 - accuracy: 0.54 - ETA: 2s - loss: 0.6856 - accuracy: 0.55 - ETA: 2s - loss: 0.6849 - accuracy: 0.55 - ETA: 2s - loss: 0.6838 - accuracy: 0.56 - ETA: 2s - loss: 0.6817 - accuracy: 0.56 - ETA: 2s - loss: 0.6800 - accuracy: 0.57 - ETA: 2s - loss: 0.6785 - accuracy: 0.57 - ETA: 2s - loss: 0.6768 - accuracy: 0.57 - ETA: 2s - loss: 0.6754 - accuracy: 0.58 - ETA: 2s - loss: 0.6729 - accuracy: 0.58 - ETA: 2s - loss: 0.6716 - accuracy: 0.59 - ETA: 2s - loss: 0.6710 - accuracy: 0.59 - ETA: 2s - loss: 0.6694 - accuracy: 0.59 - ETA: 2s - loss: 0.6674 - accuracy: 0.60 - ETA: 2s - loss: 0.6674 - accuracy: 0.60 - ETA: 1s - loss: 0.6655 - accuracy: 0.60 - ETA: 1s - loss: 0.6647 - accuracy: 0.60 - ETA: 1s - loss: 0.6634 - accuracy: 0.60 - ETA: 1s - loss: 0.6626 - accuracy: 0.61 - ETA: 1s - loss: 0.6612 - accuracy: 0.61 - ETA: 1s - loss: 0.6593 - accuracy: 0.61 - ETA: 1s - loss: 0.6589 - accuracy: 0.61 - ETA: 1s - loss: 0.6588 - accuracy: 0.61 - ETA: 1s - loss: 0.6570 - accuracy: 0.61 - ETA: 1s - loss: 0.6556 - accuracy: 0.62 - ETA: 1s - loss: 0.6542 - accuracy: 0.62 - ETA: 1s - loss: 0.6525 - accuracy: 0.62 - ETA: 1s - loss: 0.6516 - accuracy: 0.62 - ETA: 1s - loss: 0.6494 - accuracy: 0.63 - ETA: 1s - loss: 0.6490 - accuracy: 0.63 - ETA: 1s - loss: 0.6486 - accuracy: 0.63 - ETA: 1s - loss: 0.6485 - accuracy: 0.63 - ETA: 0s - loss: 0.6485 - accuracy: 0.63 - ETA: 0s - loss: 0.6476 - accuracy: 0.63 - ETA: 0s - loss: 0.6469 - accuracy: 0.63 - ETA: 0s - loss: 0.6456 - accuracy: 0.63 - ETA: 0s - loss: 0.6443 - accuracy: 0.64 - ETA: 0s - loss: 0.6449 - accuracy: 0.64 - ETA: 0s - loss: 0.6445 - accuracy: 0.64 - ETA: 0s - loss: 0.6441 - accuracy: 0.64 - ETA: 0s - loss: 0.6437 - accuracy: 0.64 - ETA: 0s - loss: 0.6428 - accuracy: 0.64 - ETA: 0s - loss: 0.6420 - accuracy: 0.64 - ETA: 0s - loss: 0.6412 - accuracy: 0.64 - ETA: 0s - loss: 0.6406 - accuracy: 0.64 - ETA: 0s - loss: 0.6408 - accuracy: 0.64 - ETA: 0s - loss: 0.6399 - accuracy: 0.64 - ETA: 0s - loss: 0.6397 - accuracy: 0.64 - ETA: 0s - loss: 0.6392 - accuracy: 0.64 - ETA: 0s - loss: 0.6381 - accuracy: 0.64 - ETA: 0s - loss: 0.6377 - accuracy: 0.65 - 3s 271us/step - loss: 0.6375 - accuracy: 0.6512 - val_loss: 0.5822 - val_accuracy: 0.7273\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:10 - loss: 0.6858 - accuracy: 0.75 - ETA: 9s - loss: 0.6905 - accuracy: 0.5991 - ETA: 6s - loss: 0.6914 - accuracy: 0.56 - ETA: 5s - loss: 0.6911 - accuracy: 0.57 - ETA: 4s - loss: 0.6900 - accuracy: 0.56 - ETA: 4s - loss: 0.6888 - accuracy: 0.57 - ETA: 3s - loss: 0.6878 - accuracy: 0.57 - ETA: 3s - loss: 0.6863 - accuracy: 0.57 - ETA: 3s - loss: 0.6843 - accuracy: 0.58 - ETA: 3s - loss: 0.6839 - accuracy: 0.58 - ETA: 3s - loss: 0.6818 - accuracy: 0.59 - ETA: 3s - loss: 0.6813 - accuracy: 0.59 - ETA: 3s - loss: 0.6798 - accuracy: 0.60 - ETA: 2s - loss: 0.6794 - accuracy: 0.60 - ETA: 2s - loss: 0.6782 - accuracy: 0.60 - ETA: 2s - loss: 0.6764 - accuracy: 0.61 - ETA: 2s - loss: 0.6739 - accuracy: 0.61 - ETA: 2s - loss: 0.6717 - accuracy: 0.61 - ETA: 2s - loss: 0.6698 - accuracy: 0.62 - ETA: 2s - loss: 0.6699 - accuracy: 0.62 - ETA: 2s - loss: 0.6690 - accuracy: 0.62 - ETA: 2s - loss: 0.6671 - accuracy: 0.62 - ETA: 2s - loss: 0.6653 - accuracy: 0.63 - ETA: 2s - loss: 0.6639 - accuracy: 0.63 - ETA: 2s - loss: 0.6626 - accuracy: 0.63 - ETA: 2s - loss: 0.6615 - accuracy: 0.63 - ETA: 1s - loss: 0.6609 - accuracy: 0.63 - ETA: 1s - loss: 0.6591 - accuracy: 0.63 - ETA: 1s - loss: 0.6583 - accuracy: 0.63 - ETA: 1s - loss: 0.6560 - accuracy: 0.64 - ETA: 1s - loss: 0.6549 - accuracy: 0.64 - ETA: 1s - loss: 0.6541 - accuracy: 0.64 - ETA: 1s - loss: 0.6523 - accuracy: 0.64 - ETA: 1s - loss: 0.6510 - accuracy: 0.64 - ETA: 1s - loss: 0.6501 - accuracy: 0.64 - ETA: 1s - loss: 0.6496 - accuracy: 0.64 - ETA: 1s - loss: 0.6492 - accuracy: 0.64 - ETA: 1s - loss: 0.6485 - accuracy: 0.64 - ETA: 1s - loss: 0.6482 - accuracy: 0.65 - ETA: 1s - loss: 0.6468 - accuracy: 0.65 - ETA: 1s - loss: 0.6451 - accuracy: 0.65 - ETA: 1s - loss: 0.6450 - accuracy: 0.65 - ETA: 1s - loss: 0.6432 - accuracy: 0.65 - ETA: 1s - loss: 0.6423 - accuracy: 0.65 - ETA: 0s - loss: 0.6410 - accuracy: 0.65 - ETA: 0s - loss: 0.6400 - accuracy: 0.65 - ETA: 0s - loss: 0.6388 - accuracy: 0.66 - ETA: 0s - loss: 0.6376 - accuracy: 0.66 - ETA: 0s - loss: 0.6371 - accuracy: 0.66 - ETA: 0s - loss: 0.6367 - accuracy: 0.66 - ETA: 0s - loss: 0.6365 - accuracy: 0.66 - ETA: 0s - loss: 0.6363 - accuracy: 0.66 - ETA: 0s - loss: 0.6356 - accuracy: 0.66 - ETA: 0s - loss: 0.6352 - accuracy: 0.66 - ETA: 0s - loss: 0.6348 - accuracy: 0.66 - ETA: 0s - loss: 0.6348 - accuracy: 0.66 - ETA: 0s - loss: 0.6345 - accuracy: 0.66 - ETA: 0s - loss: 0.6334 - accuracy: 0.66 - ETA: 0s - loss: 0.6326 - accuracy: 0.66 - ETA: 0s - loss: 0.6324 - accuracy: 0.66 - ETA: 0s - loss: 0.6318 - accuracy: 0.66 - ETA: 0s - loss: 0.6317 - accuracy: 0.66 - ETA: 0s - loss: 0.6307 - accuracy: 0.66 - 3s 276us/step - loss: 0.6299 - accuracy: 0.6691 - val_loss: 0.5771 - val_accuracy: 0.7209\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:20 - loss: 0.6960 - accuracy: 0.50 - ETA: 10s - loss: 0.6944 - accuracy: 0.5000 - ETA: 6s - loss: 0.6937 - accuracy: 0.495 - ETA: 5s - loss: 0.6927 - accuracy: 0.49 - ETA: 4s - loss: 0.6919 - accuracy: 0.50 - ETA: 4s - loss: 0.6923 - accuracy: 0.49 - ETA: 4s - loss: 0.6918 - accuracy: 0.49 - ETA: 3s - loss: 0.6917 - accuracy: 0.50 - ETA: 3s - loss: 0.6911 - accuracy: 0.51 - ETA: 3s - loss: 0.6909 - accuracy: 0.52 - ETA: 3s - loss: 0.6899 - accuracy: 0.52 - ETA: 3s - loss: 0.6888 - accuracy: 0.53 - ETA: 3s - loss: 0.6873 - accuracy: 0.53 - ETA: 2s - loss: 0.6873 - accuracy: 0.54 - ETA: 2s - loss: 0.6857 - accuracy: 0.54 - ETA: 2s - loss: 0.6844 - accuracy: 0.55 - ETA: 2s - loss: 0.6840 - accuracy: 0.55 - ETA: 2s - loss: 0.6828 - accuracy: 0.55 - ETA: 2s - loss: 0.6813 - accuracy: 0.56 - ETA: 2s - loss: 0.6800 - accuracy: 0.56 - ETA: 2s - loss: 0.6797 - accuracy: 0.56 - ETA: 2s - loss: 0.6789 - accuracy: 0.56 - ETA: 2s - loss: 0.6774 - accuracy: 0.56 - ETA: 2s - loss: 0.6756 - accuracy: 0.57 - ETA: 2s - loss: 0.6743 - accuracy: 0.57 - ETA: 2s - loss: 0.6736 - accuracy: 0.57 - ETA: 2s - loss: 0.6727 - accuracy: 0.57 - ETA: 1s - loss: 0.6711 - accuracy: 0.58 - ETA: 1s - loss: 0.6706 - accuracy: 0.58 - ETA: 1s - loss: 0.6695 - accuracy: 0.58 - ETA: 1s - loss: 0.6678 - accuracy: 0.59 - ETA: 1s - loss: 0.6675 - accuracy: 0.59 - ETA: 1s - loss: 0.6668 - accuracy: 0.59 - ETA: 1s - loss: 0.6656 - accuracy: 0.59 - ETA: 1s - loss: 0.6643 - accuracy: 0.59 - ETA: 1s - loss: 0.6626 - accuracy: 0.60 - ETA: 1s - loss: 0.6613 - accuracy: 0.60 - ETA: 1s - loss: 0.6605 - accuracy: 0.60 - ETA: 1s - loss: 0.6598 - accuracy: 0.60 - ETA: 1s - loss: 0.6586 - accuracy: 0.60 - ETA: 1s - loss: 0.6571 - accuracy: 0.61 - ETA: 1s - loss: 0.6562 - accuracy: 0.61 - ETA: 1s - loss: 0.6548 - accuracy: 0.61 - ETA: 1s - loss: 0.6548 - accuracy: 0.61 - ETA: 0s - loss: 0.6547 - accuracy: 0.61 - ETA: 0s - loss: 0.6538 - accuracy: 0.62 - ETA: 0s - loss: 0.6528 - accuracy: 0.62 - ETA: 0s - loss: 0.6526 - accuracy: 0.62 - ETA: 0s - loss: 0.6519 - accuracy: 0.62 - ETA: 0s - loss: 0.6512 - accuracy: 0.62 - ETA: 0s - loss: 0.6501 - accuracy: 0.62 - ETA: 0s - loss: 0.6497 - accuracy: 0.62 - ETA: 0s - loss: 0.6488 - accuracy: 0.63 - ETA: 0s - loss: 0.6488 - accuracy: 0.63 - ETA: 0s - loss: 0.6474 - accuracy: 0.63 - ETA: 0s - loss: 0.6471 - accuracy: 0.63 - ETA: 0s - loss: 0.6463 - accuracy: 0.63 - ETA: 0s - loss: 0.6460 - accuracy: 0.63 - ETA: 0s - loss: 0.6460 - accuracy: 0.63 - ETA: 0s - loss: 0.6458 - accuracy: 0.63 - ETA: 0s - loss: 0.6453 - accuracy: 0.63 - ETA: 0s - loss: 0.6454 - accuracy: 0.63 - ETA: 0s - loss: 0.6443 - accuracy: 0.63 - 3s 276us/step - loss: 0.6440 - accuracy: 0.6400 - val_loss: 0.5885 - val_accuracy: 0.7202\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 100us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 6:23 - loss: 0.7056 - accuracy: 0.25 - ETA: 10s - loss: 0.6969 - accuracy: 0.4300 - ETA: 6s - loss: 0.6955 - accuracy: 0.495 - ETA: 5s - loss: 0.6949 - accuracy: 0.50 - ETA: 4s - loss: 0.6946 - accuracy: 0.50 - ETA: 4s - loss: 0.6947 - accuracy: 0.50 - ETA: 4s - loss: 0.6941 - accuracy: 0.52 - ETA: 3s - loss: 0.6938 - accuracy: 0.52 - ETA: 3s - loss: 0.6938 - accuracy: 0.52 - ETA: 3s - loss: 0.6933 - accuracy: 0.53 - ETA: 3s - loss: 0.6928 - accuracy: 0.54 - ETA: 3s - loss: 0.6919 - accuracy: 0.55 - ETA: 3s - loss: 0.6909 - accuracy: 0.55 - ETA: 3s - loss: 0.6902 - accuracy: 0.56 - ETA: 2s - loss: 0.6890 - accuracy: 0.56 - ETA: 2s - loss: 0.6879 - accuracy: 0.56 - ETA: 2s - loss: 0.6866 - accuracy: 0.57 - ETA: 2s - loss: 0.6857 - accuracy: 0.57 - ETA: 2s - loss: 0.6844 - accuracy: 0.57 - ETA: 2s - loss: 0.6827 - accuracy: 0.58 - ETA: 2s - loss: 0.6819 - accuracy: 0.58 - ETA: 2s - loss: 0.6812 - accuracy: 0.58 - ETA: 2s - loss: 0.6804 - accuracy: 0.58 - ETA: 2s - loss: 0.6801 - accuracy: 0.58 - ETA: 2s - loss: 0.6788 - accuracy: 0.59 - ETA: 2s - loss: 0.6790 - accuracy: 0.58 - ETA: 2s - loss: 0.6786 - accuracy: 0.59 - ETA: 2s - loss: 0.6775 - accuracy: 0.59 - ETA: 2s - loss: 0.6763 - accuracy: 0.59 - ETA: 2s - loss: 0.6755 - accuracy: 0.59 - ETA: 1s - loss: 0.6746 - accuracy: 0.60 - ETA: 1s - loss: 0.6729 - accuracy: 0.60 - ETA: 1s - loss: 0.6720 - accuracy: 0.60 - ETA: 1s - loss: 0.6691 - accuracy: 0.60 - ETA: 1s - loss: 0.6688 - accuracy: 0.61 - ETA: 1s - loss: 0.6672 - accuracy: 0.61 - ETA: 1s - loss: 0.6664 - accuracy: 0.61 - ETA: 1s - loss: 0.6658 - accuracy: 0.61 - ETA: 1s - loss: 0.6648 - accuracy: 0.61 - ETA: 1s - loss: 0.6642 - accuracy: 0.61 - ETA: 1s - loss: 0.6631 - accuracy: 0.61 - ETA: 1s - loss: 0.6626 - accuracy: 0.61 - ETA: 1s - loss: 0.6617 - accuracy: 0.61 - ETA: 1s - loss: 0.6597 - accuracy: 0.62 - ETA: 1s - loss: 0.6588 - accuracy: 0.62 - ETA: 1s - loss: 0.6585 - accuracy: 0.62 - ETA: 0s - loss: 0.6579 - accuracy: 0.62 - ETA: 0s - loss: 0.6571 - accuracy: 0.62 - ETA: 0s - loss: 0.6562 - accuracy: 0.62 - ETA: 0s - loss: 0.6553 - accuracy: 0.63 - ETA: 0s - loss: 0.6539 - accuracy: 0.63 - ETA: 0s - loss: 0.6534 - accuracy: 0.63 - ETA: 0s - loss: 0.6528 - accuracy: 0.63 - ETA: 0s - loss: 0.6521 - accuracy: 0.63 - ETA: 0s - loss: 0.6513 - accuracy: 0.63 - ETA: 0s - loss: 0.6504 - accuracy: 0.63 - ETA: 0s - loss: 0.6490 - accuracy: 0.63 - ETA: 0s - loss: 0.6485 - accuracy: 0.64 - ETA: 0s - loss: 0.6486 - accuracy: 0.64 - ETA: 0s - loss: 0.6481 - accuracy: 0.64 - ETA: 0s - loss: 0.6477 - accuracy: 0.64 - ETA: 0s - loss: 0.6472 - accuracy: 0.64 - ETA: 0s - loss: 0.6473 - accuracy: 0.64 - ETA: 0s - loss: 0.6472 - accuracy: 0.64 - ETA: 0s - loss: 0.6461 - accuracy: 0.64 - 4s 282us/step - loss: 0.6461 - accuracy: 0.6455 - val_loss: 0.5916 - val_accuracy: 0.7202\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:04 - loss: 0.7148 - accuracy: 0.0000e+ - ETA: 9s - loss: 0.6936 - accuracy: 0.5324     - ETA: 6s - loss: 0.6940 - accuracy: 0.51 - ETA: 5s - loss: 0.6936 - accuracy: 0.52 - ETA: 4s - loss: 0.6929 - accuracy: 0.53 - ETA: 4s - loss: 0.6922 - accuracy: 0.55 - ETA: 3s - loss: 0.6916 - accuracy: 0.55 - ETA: 3s - loss: 0.6908 - accuracy: 0.56 - ETA: 3s - loss: 0.6888 - accuracy: 0.57 - ETA: 3s - loss: 0.6877 - accuracy: 0.58 - ETA: 3s - loss: 0.6855 - accuracy: 0.58 - ETA: 3s - loss: 0.6837 - accuracy: 0.59 - ETA: 2s - loss: 0.6826 - accuracy: 0.59 - ETA: 2s - loss: 0.6809 - accuracy: 0.59 - ETA: 2s - loss: 0.6800 - accuracy: 0.59 - ETA: 2s - loss: 0.6785 - accuracy: 0.59 - ETA: 2s - loss: 0.6777 - accuracy: 0.60 - ETA: 2s - loss: 0.6762 - accuracy: 0.60 - ETA: 2s - loss: 0.6737 - accuracy: 0.61 - ETA: 2s - loss: 0.6723 - accuracy: 0.61 - ETA: 2s - loss: 0.6710 - accuracy: 0.61 - ETA: 2s - loss: 0.6693 - accuracy: 0.61 - ETA: 2s - loss: 0.6675 - accuracy: 0.61 - ETA: 2s - loss: 0.6662 - accuracy: 0.62 - ETA: 2s - loss: 0.6660 - accuracy: 0.62 - ETA: 1s - loss: 0.6636 - accuracy: 0.62 - ETA: 1s - loss: 0.6623 - accuracy: 0.62 - ETA: 1s - loss: 0.6604 - accuracy: 0.62 - ETA: 1s - loss: 0.6585 - accuracy: 0.63 - ETA: 1s - loss: 0.6551 - accuracy: 0.63 - ETA: 1s - loss: 0.6541 - accuracy: 0.63 - ETA: 1s - loss: 0.6537 - accuracy: 0.63 - ETA: 1s - loss: 0.6533 - accuracy: 0.63 - ETA: 1s - loss: 0.6530 - accuracy: 0.64 - ETA: 1s - loss: 0.6514 - accuracy: 0.64 - ETA: 1s - loss: 0.6500 - accuracy: 0.64 - ETA: 1s - loss: 0.6495 - accuracy: 0.64 - ETA: 1s - loss: 0.6492 - accuracy: 0.64 - ETA: 1s - loss: 0.6487 - accuracy: 0.64 - ETA: 1s - loss: 0.6484 - accuracy: 0.64 - ETA: 1s - loss: 0.6481 - accuracy: 0.64 - ETA: 1s - loss: 0.6471 - accuracy: 0.64 - ETA: 1s - loss: 0.6468 - accuracy: 0.64 - ETA: 0s - loss: 0.6448 - accuracy: 0.65 - ETA: 0s - loss: 0.6447 - accuracy: 0.65 - ETA: 0s - loss: 0.6442 - accuracy: 0.65 - ETA: 0s - loss: 0.6442 - accuracy: 0.65 - ETA: 0s - loss: 0.6429 - accuracy: 0.65 - ETA: 0s - loss: 0.6423 - accuracy: 0.65 - ETA: 0s - loss: 0.6414 - accuracy: 0.65 - ETA: 0s - loss: 0.6414 - accuracy: 0.65 - ETA: 0s - loss: 0.6404 - accuracy: 0.65 - ETA: 0s - loss: 0.6399 - accuracy: 0.65 - ETA: 0s - loss: 0.6395 - accuracy: 0.65 - ETA: 0s - loss: 0.6386 - accuracy: 0.65 - ETA: 0s - loss: 0.6382 - accuracy: 0.66 - ETA: 0s - loss: 0.6374 - accuracy: 0.66 - ETA: 0s - loss: 0.6363 - accuracy: 0.66 - ETA: 0s - loss: 0.6360 - accuracy: 0.66 - ETA: 0s - loss: 0.6350 - accuracy: 0.66 - ETA: 0s - loss: 0.6346 - accuracy: 0.66 - ETA: 0s - loss: 0.6344 - accuracy: 0.66 - 3s 272us/step - loss: 0.6330 - accuracy: 0.6664 - val_loss: 0.5810 - val_accuracy: 0.7294\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:10 - loss: 0.6936 - accuracy: 0.50 - ETA: 8s - loss: 0.6916 - accuracy: 0.4727 - ETA: 5s - loss: 0.6904 - accuracy: 0.50 - ETA: 4s - loss: 0.6889 - accuracy: 0.51 - ETA: 4s - loss: 0.6889 - accuracy: 0.51 - ETA: 3s - loss: 0.6896 - accuracy: 0.51 - ETA: 3s - loss: 0.6900 - accuracy: 0.51 - ETA: 3s - loss: 0.6902 - accuracy: 0.51 - ETA: 3s - loss: 0.6898 - accuracy: 0.52 - ETA: 3s - loss: 0.6893 - accuracy: 0.53 - ETA: 2s - loss: 0.6892 - accuracy: 0.53 - ETA: 2s - loss: 0.6891 - accuracy: 0.53 - ETA: 2s - loss: 0.6884 - accuracy: 0.53 - ETA: 2s - loss: 0.6880 - accuracy: 0.53 - ETA: 2s - loss: 0.6878 - accuracy: 0.53 - ETA: 2s - loss: 0.6875 - accuracy: 0.54 - ETA: 2s - loss: 0.6869 - accuracy: 0.54 - ETA: 2s - loss: 0.6868 - accuracy: 0.54 - ETA: 2s - loss: 0.6864 - accuracy: 0.54 - ETA: 2s - loss: 0.6856 - accuracy: 0.54 - ETA: 2s - loss: 0.6849 - accuracy: 0.55 - ETA: 2s - loss: 0.6843 - accuracy: 0.55 - ETA: 2s - loss: 0.6843 - accuracy: 0.55 - ETA: 1s - loss: 0.6839 - accuracy: 0.55 - ETA: 1s - loss: 0.6839 - accuracy: 0.55 - ETA: 1s - loss: 0.6838 - accuracy: 0.55 - ETA: 1s - loss: 0.6837 - accuracy: 0.55 - ETA: 1s - loss: 0.6836 - accuracy: 0.55 - ETA: 1s - loss: 0.6836 - accuracy: 0.55 - ETA: 1s - loss: 0.6830 - accuracy: 0.55 - ETA: 1s - loss: 0.6828 - accuracy: 0.56 - ETA: 1s - loss: 0.6823 - accuracy: 0.56 - ETA: 1s - loss: 0.6817 - accuracy: 0.56 - ETA: 1s - loss: 0.6817 - accuracy: 0.56 - ETA: 1s - loss: 0.6815 - accuracy: 0.56 - ETA: 1s - loss: 0.6812 - accuracy: 0.56 - ETA: 1s - loss: 0.6809 - accuracy: 0.56 - ETA: 1s - loss: 0.6806 - accuracy: 0.56 - ETA: 1s - loss: 0.6801 - accuracy: 0.56 - ETA: 1s - loss: 0.6798 - accuracy: 0.56 - ETA: 0s - loss: 0.6798 - accuracy: 0.56 - ETA: 0s - loss: 0.6795 - accuracy: 0.56 - ETA: 0s - loss: 0.6791 - accuracy: 0.56 - ETA: 0s - loss: 0.6788 - accuracy: 0.56 - ETA: 0s - loss: 0.6786 - accuracy: 0.57 - ETA: 0s - loss: 0.6783 - accuracy: 0.57 - ETA: 0s - loss: 0.6781 - accuracy: 0.57 - ETA: 0s - loss: 0.6778 - accuracy: 0.57 - ETA: 0s - loss: 0.6776 - accuracy: 0.57 - ETA: 0s - loss: 0.6776 - accuracy: 0.57 - ETA: 0s - loss: 0.6773 - accuracy: 0.57 - ETA: 0s - loss: 0.6774 - accuracy: 0.57 - ETA: 0s - loss: 0.6770 - accuracy: 0.57 - ETA: 0s - loss: 0.6770 - accuracy: 0.57 - ETA: 0s - loss: 0.6765 - accuracy: 0.57 - ETA: 0s - loss: 0.6765 - accuracy: 0.57 - ETA: 0s - loss: 0.6765 - accuracy: 0.57 - ETA: 0s - loss: 0.6765 - accuracy: 0.57 - ETA: 0s - loss: 0.6764 - accuracy: 0.57 - 3s 258us/step - loss: 0.6764 - accuracy: 0.5744 - val_loss: 0.6578 - val_accuracy: 0.7088\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 104us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:07 - loss: 0.6949 - accuracy: 0.50 - ETA: 8s - loss: 0.6930 - accuracy: 0.5091 - ETA: 5s - loss: 0.6916 - accuracy: 0.52 - ETA: 4s - loss: 0.6914 - accuracy: 0.51 - ETA: 4s - loss: 0.6905 - accuracy: 0.53 - ETA: 3s - loss: 0.6903 - accuracy: 0.52 - ETA: 3s - loss: 0.6902 - accuracy: 0.52 - ETA: 3s - loss: 0.6903 - accuracy: 0.53 - ETA: 3s - loss: 0.6901 - accuracy: 0.53 - ETA: 2s - loss: 0.6894 - accuracy: 0.54 - ETA: 2s - loss: 0.6895 - accuracy: 0.53 - ETA: 2s - loss: 0.6888 - accuracy: 0.55 - ETA: 2s - loss: 0.6885 - accuracy: 0.55 - ETA: 2s - loss: 0.6878 - accuracy: 0.55 - ETA: 2s - loss: 0.6877 - accuracy: 0.55 - ETA: 2s - loss: 0.6873 - accuracy: 0.56 - ETA: 2s - loss: 0.6869 - accuracy: 0.56 - ETA: 2s - loss: 0.6864 - accuracy: 0.56 - ETA: 2s - loss: 0.6862 - accuracy: 0.56 - ETA: 2s - loss: 0.6858 - accuracy: 0.56 - ETA: 2s - loss: 0.6856 - accuracy: 0.56 - ETA: 2s - loss: 0.6854 - accuracy: 0.56 - ETA: 1s - loss: 0.6847 - accuracy: 0.57 - ETA: 1s - loss: 0.6845 - accuracy: 0.57 - ETA: 1s - loss: 0.6846 - accuracy: 0.57 - ETA: 1s - loss: 0.6845 - accuracy: 0.57 - ETA: 1s - loss: 0.6838 - accuracy: 0.57 - ETA: 1s - loss: 0.6835 - accuracy: 0.57 - ETA: 1s - loss: 0.6830 - accuracy: 0.57 - ETA: 1s - loss: 0.6826 - accuracy: 0.57 - ETA: 1s - loss: 0.6823 - accuracy: 0.57 - ETA: 1s - loss: 0.6819 - accuracy: 0.57 - ETA: 1s - loss: 0.6817 - accuracy: 0.57 - ETA: 1s - loss: 0.6817 - accuracy: 0.57 - ETA: 1s - loss: 0.6814 - accuracy: 0.57 - ETA: 1s - loss: 0.6811 - accuracy: 0.57 - ETA: 1s - loss: 0.6808 - accuracy: 0.58 - ETA: 1s - loss: 0.6806 - accuracy: 0.58 - ETA: 1s - loss: 0.6806 - accuracy: 0.58 - ETA: 1s - loss: 0.6804 - accuracy: 0.58 - ETA: 1s - loss: 0.6801 - accuracy: 0.58 - ETA: 0s - loss: 0.6796 - accuracy: 0.58 - ETA: 0s - loss: 0.6793 - accuracy: 0.58 - ETA: 0s - loss: 0.6786 - accuracy: 0.58 - ETA: 0s - loss: 0.6781 - accuracy: 0.58 - ETA: 0s - loss: 0.6782 - accuracy: 0.58 - ETA: 0s - loss: 0.6782 - accuracy: 0.58 - ETA: 0s - loss: 0.6779 - accuracy: 0.58 - ETA: 0s - loss: 0.6777 - accuracy: 0.58 - ETA: 0s - loss: 0.6774 - accuracy: 0.59 - ETA: 0s - loss: 0.6771 - accuracy: 0.59 - ETA: 0s - loss: 0.6768 - accuracy: 0.59 - ETA: 0s - loss: 0.6765 - accuracy: 0.59 - ETA: 0s - loss: 0.6763 - accuracy: 0.59 - ETA: 0s - loss: 0.6759 - accuracy: 0.59 - ETA: 0s - loss: 0.6753 - accuracy: 0.59 - ETA: 0s - loss: 0.6749 - accuracy: 0.59 - ETA: 0s - loss: 0.6746 - accuracy: 0.59 - ETA: 0s - loss: 0.6743 - accuracy: 0.59 - ETA: 0s - loss: 0.6738 - accuracy: 0.59 - 3s 260us/step - loss: 0.6738 - accuracy: 0.5978 - val_loss: 0.6521 - val_accuracy: 0.7124\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:10 - loss: 0.6935 - accuracy: 0.50 - ETA: 8s - loss: 0.6941 - accuracy: 0.5357 - ETA: 5s - loss: 0.6919 - accuracy: 0.55 - ETA: 4s - loss: 0.6928 - accuracy: 0.52 - ETA: 4s - loss: 0.6932 - accuracy: 0.51 - ETA: 3s - loss: 0.6929 - accuracy: 0.51 - ETA: 3s - loss: 0.6928 - accuracy: 0.51 - ETA: 3s - loss: 0.6928 - accuracy: 0.50 - ETA: 3s - loss: 0.6923 - accuracy: 0.51 - ETA: 3s - loss: 0.6921 - accuracy: 0.50 - ETA: 2s - loss: 0.6918 - accuracy: 0.50 - ETA: 2s - loss: 0.6909 - accuracy: 0.51 - ETA: 2s - loss: 0.6909 - accuracy: 0.51 - ETA: 2s - loss: 0.6906 - accuracy: 0.51 - ETA: 2s - loss: 0.6904 - accuracy: 0.51 - ETA: 2s - loss: 0.6901 - accuracy: 0.51 - ETA: 2s - loss: 0.6899 - accuracy: 0.51 - ETA: 2s - loss: 0.6896 - accuracy: 0.51 - ETA: 2s - loss: 0.6893 - accuracy: 0.51 - ETA: 2s - loss: 0.6893 - accuracy: 0.51 - ETA: 2s - loss: 0.6890 - accuracy: 0.51 - ETA: 2s - loss: 0.6887 - accuracy: 0.51 - ETA: 1s - loss: 0.6882 - accuracy: 0.51 - ETA: 1s - loss: 0.6880 - accuracy: 0.51 - ETA: 1s - loss: 0.6879 - accuracy: 0.51 - ETA: 1s - loss: 0.6878 - accuracy: 0.51 - ETA: 1s - loss: 0.6875 - accuracy: 0.51 - ETA: 1s - loss: 0.6874 - accuracy: 0.51 - ETA: 1s - loss: 0.6870 - accuracy: 0.51 - ETA: 1s - loss: 0.6865 - accuracy: 0.52 - ETA: 1s - loss: 0.6862 - accuracy: 0.52 - ETA: 1s - loss: 0.6866 - accuracy: 0.51 - ETA: 1s - loss: 0.6863 - accuracy: 0.52 - ETA: 1s - loss: 0.6860 - accuracy: 0.52 - ETA: 1s - loss: 0.6856 - accuracy: 0.52 - ETA: 1s - loss: 0.6853 - accuracy: 0.52 - ETA: 1s - loss: 0.6851 - accuracy: 0.52 - ETA: 1s - loss: 0.6847 - accuracy: 0.53 - ETA: 1s - loss: 0.6846 - accuracy: 0.52 - ETA: 1s - loss: 0.6843 - accuracy: 0.53 - ETA: 0s - loss: 0.6840 - accuracy: 0.53 - ETA: 0s - loss: 0.6836 - accuracy: 0.53 - ETA: 0s - loss: 0.6839 - accuracy: 0.53 - ETA: 0s - loss: 0.6837 - accuracy: 0.53 - ETA: 0s - loss: 0.6836 - accuracy: 0.53 - ETA: 0s - loss: 0.6830 - accuracy: 0.53 - ETA: 0s - loss: 0.6827 - accuracy: 0.54 - ETA: 0s - loss: 0.6822 - accuracy: 0.54 - ETA: 0s - loss: 0.6822 - accuracy: 0.54 - ETA: 0s - loss: 0.6821 - accuracy: 0.54 - ETA: 0s - loss: 0.6818 - accuracy: 0.54 - ETA: 0s - loss: 0.6818 - accuracy: 0.54 - ETA: 0s - loss: 0.6816 - accuracy: 0.54 - ETA: 0s - loss: 0.6812 - accuracy: 0.54 - ETA: 0s - loss: 0.6810 - accuracy: 0.54 - ETA: 0s - loss: 0.6809 - accuracy: 0.54 - ETA: 0s - loss: 0.6809 - accuracy: 0.54 - ETA: 0s - loss: 0.6807 - accuracy: 0.54 - ETA: 0s - loss: 0.6805 - accuracy: 0.54 - 3s 256us/step - loss: 0.6803 - accuracy: 0.5490 - val_loss: 0.6645 - val_accuracy: 0.6300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:13 - loss: 0.6927 - accuracy: 0.50 - ETA: 8s - loss: 0.6937 - accuracy: 0.4676 - ETA: 5s - loss: 0.6937 - accuracy: 0.47 - ETA: 4s - loss: 0.6936 - accuracy: 0.46 - ETA: 4s - loss: 0.6935 - accuracy: 0.48 - ETA: 3s - loss: 0.6934 - accuracy: 0.48 - ETA: 3s - loss: 0.6933 - accuracy: 0.48 - ETA: 3s - loss: 0.6932 - accuracy: 0.50 - ETA: 3s - loss: 0.6928 - accuracy: 0.51 - ETA: 2s - loss: 0.6929 - accuracy: 0.51 - ETA: 2s - loss: 0.6927 - accuracy: 0.52 - ETA: 2s - loss: 0.6925 - accuracy: 0.52 - ETA: 2s - loss: 0.6925 - accuracy: 0.52 - ETA: 2s - loss: 0.6922 - accuracy: 0.53 - ETA: 2s - loss: 0.6920 - accuracy: 0.53 - ETA: 2s - loss: 0.6917 - accuracy: 0.54 - ETA: 2s - loss: 0.6913 - accuracy: 0.54 - ETA: 2s - loss: 0.6909 - accuracy: 0.54 - ETA: 2s - loss: 0.6905 - accuracy: 0.54 - ETA: 2s - loss: 0.6903 - accuracy: 0.54 - ETA: 2s - loss: 0.6902 - accuracy: 0.54 - ETA: 2s - loss: 0.6900 - accuracy: 0.55 - ETA: 1s - loss: 0.6897 - accuracy: 0.55 - ETA: 1s - loss: 0.6895 - accuracy: 0.55 - ETA: 1s - loss: 0.6893 - accuracy: 0.55 - ETA: 1s - loss: 0.6890 - accuracy: 0.56 - ETA: 1s - loss: 0.6888 - accuracy: 0.56 - ETA: 1s - loss: 0.6886 - accuracy: 0.56 - ETA: 1s - loss: 0.6884 - accuracy: 0.56 - ETA: 1s - loss: 0.6881 - accuracy: 0.57 - ETA: 1s - loss: 0.6877 - accuracy: 0.57 - ETA: 1s - loss: 0.6874 - accuracy: 0.57 - ETA: 1s - loss: 0.6871 - accuracy: 0.57 - ETA: 1s - loss: 0.6870 - accuracy: 0.57 - ETA: 1s - loss: 0.6868 - accuracy: 0.58 - ETA: 1s - loss: 0.6866 - accuracy: 0.58 - ETA: 1s - loss: 0.6864 - accuracy: 0.58 - ETA: 1s - loss: 0.6862 - accuracy: 0.58 - ETA: 1s - loss: 0.6860 - accuracy: 0.58 - ETA: 0s - loss: 0.6856 - accuracy: 0.58 - ETA: 0s - loss: 0.6851 - accuracy: 0.58 - ETA: 0s - loss: 0.6851 - accuracy: 0.58 - ETA: 0s - loss: 0.6848 - accuracy: 0.59 - ETA: 0s - loss: 0.6846 - accuracy: 0.59 - ETA: 0s - loss: 0.6846 - accuracy: 0.59 - ETA: 0s - loss: 0.6843 - accuracy: 0.59 - ETA: 0s - loss: 0.6838 - accuracy: 0.59 - ETA: 0s - loss: 0.6836 - accuracy: 0.59 - ETA: 0s - loss: 0.6835 - accuracy: 0.59 - ETA: 0s - loss: 0.6832 - accuracy: 0.60 - ETA: 0s - loss: 0.6829 - accuracy: 0.60 - ETA: 0s - loss: 0.6828 - accuracy: 0.60 - ETA: 0s - loss: 0.6826 - accuracy: 0.60 - ETA: 0s - loss: 0.6826 - accuracy: 0.60 - ETA: 0s - loss: 0.6822 - accuracy: 0.60 - ETA: 0s - loss: 0.6821 - accuracy: 0.60 - ETA: 0s - loss: 0.6821 - accuracy: 0.60 - ETA: 0s - loss: 0.6819 - accuracy: 0.60 - 3s 255us/step - loss: 0.6816 - accuracy: 0.6070 - val_loss: 0.6661 - val_accuracy: 0.6939\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:04 - loss: 0.6929 - accuracy: 0.75 - ETA: 8s - loss: 0.6939 - accuracy: 0.4955 - ETA: 5s - loss: 0.6934 - accuracy: 0.50 - ETA: 4s - loss: 0.6936 - accuracy: 0.49 - ETA: 4s - loss: 0.6934 - accuracy: 0.49 - ETA: 3s - loss: 0.6933 - accuracy: 0.49 - ETA: 3s - loss: 0.6930 - accuracy: 0.50 - ETA: 3s - loss: 0.6928 - accuracy: 0.50 - ETA: 3s - loss: 0.6925 - accuracy: 0.50 - ETA: 3s - loss: 0.6922 - accuracy: 0.50 - ETA: 2s - loss: 0.6924 - accuracy: 0.50 - ETA: 2s - loss: 0.6925 - accuracy: 0.50 - ETA: 2s - loss: 0.6922 - accuracy: 0.50 - ETA: 2s - loss: 0.6920 - accuracy: 0.50 - ETA: 2s - loss: 0.6921 - accuracy: 0.50 - ETA: 2s - loss: 0.6921 - accuracy: 0.50 - ETA: 2s - loss: 0.6919 - accuracy: 0.50 - ETA: 2s - loss: 0.6920 - accuracy: 0.50 - ETA: 2s - loss: 0.6918 - accuracy: 0.49 - ETA: 2s - loss: 0.6916 - accuracy: 0.49 - ETA: 2s - loss: 0.6914 - accuracy: 0.49 - ETA: 2s - loss: 0.6915 - accuracy: 0.49 - ETA: 2s - loss: 0.6915 - accuracy: 0.49 - ETA: 1s - loss: 0.6914 - accuracy: 0.49 - ETA: 1s - loss: 0.6915 - accuracy: 0.49 - ETA: 1s - loss: 0.6915 - accuracy: 0.49 - ETA: 1s - loss: 0.6914 - accuracy: 0.49 - ETA: 1s - loss: 0.6914 - accuracy: 0.49 - ETA: 1s - loss: 0.6913 - accuracy: 0.49 - ETA: 1s - loss: 0.6912 - accuracy: 0.49 - ETA: 1s - loss: 0.6912 - accuracy: 0.49 - ETA: 1s - loss: 0.6912 - accuracy: 0.49 - ETA: 1s - loss: 0.6912 - accuracy: 0.49 - ETA: 1s - loss: 0.6912 - accuracy: 0.49 - ETA: 1s - loss: 0.6911 - accuracy: 0.49 - ETA: 1s - loss: 0.6911 - accuracy: 0.49 - ETA: 1s - loss: 0.6910 - accuracy: 0.49 - ETA: 1s - loss: 0.6910 - accuracy: 0.49 - ETA: 1s - loss: 0.6909 - accuracy: 0.49 - ETA: 1s - loss: 0.6909 - accuracy: 0.49 - ETA: 1s - loss: 0.6909 - accuracy: 0.49 - ETA: 1s - loss: 0.6908 - accuracy: 0.50 - ETA: 1s - loss: 0.6907 - accuracy: 0.50 - ETA: 1s - loss: 0.6906 - accuracy: 0.50 - ETA: 0s - loss: 0.6905 - accuracy: 0.50 - ETA: 0s - loss: 0.6904 - accuracy: 0.50 - ETA: 0s - loss: 0.6901 - accuracy: 0.50 - ETA: 0s - loss: 0.6901 - accuracy: 0.50 - ETA: 0s - loss: 0.6900 - accuracy: 0.50 - ETA: 0s - loss: 0.6900 - accuracy: 0.50 - ETA: 0s - loss: 0.6899 - accuracy: 0.50 - ETA: 0s - loss: 0.6899 - accuracy: 0.50 - ETA: 0s - loss: 0.6898 - accuracy: 0.50 - ETA: 0s - loss: 0.6897 - accuracy: 0.50 - ETA: 0s - loss: 0.6896 - accuracy: 0.50 - ETA: 0s - loss: 0.6896 - accuracy: 0.50 - ETA: 0s - loss: 0.6895 - accuracy: 0.50 - ETA: 0s - loss: 0.6894 - accuracy: 0.50 - ETA: 0s - loss: 0.6895 - accuracy: 0.50 - ETA: 0s - loss: 0.6895 - accuracy: 0.50 - ETA: 0s - loss: 0.6896 - accuracy: 0.50 - ETA: 0s - loss: 0.6896 - accuracy: 0.50 - ETA: 0s - loss: 0.6896 - accuracy: 0.50 - ETA: 0s - loss: 0.6895 - accuracy: 0.50 - ETA: 0s - loss: 0.6893 - accuracy: 0.50 - ETA: 0s - loss: 0.6893 - accuracy: 0.50 - ETA: 0s - loss: 0.6892 - accuracy: 0.50 - 4s 291us/step - loss: 0.6892 - accuracy: 0.5042 - val_loss: 0.6827 - val_accuracy: 0.5689\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 128us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:10 - loss: 0.6925 - accuracy: 0.75 - ETA: 8s - loss: 0.6942 - accuracy: 0.5139 - ETA: 5s - loss: 0.6935 - accuracy: 0.52 - ETA: 4s - loss: 0.6924 - accuracy: 0.54 - ETA: 4s - loss: 0.6926 - accuracy: 0.53 - ETA: 3s - loss: 0.6926 - accuracy: 0.53 - ETA: 3s - loss: 0.6925 - accuracy: 0.52 - ETA: 3s - loss: 0.6927 - accuracy: 0.52 - ETA: 3s - loss: 0.6927 - accuracy: 0.52 - ETA: 3s - loss: 0.6926 - accuracy: 0.52 - ETA: 2s - loss: 0.6928 - accuracy: 0.51 - ETA: 2s - loss: 0.6927 - accuracy: 0.51 - ETA: 2s - loss: 0.6928 - accuracy: 0.51 - ETA: 2s - loss: 0.6927 - accuracy: 0.51 - ETA: 2s - loss: 0.6928 - accuracy: 0.51 - ETA: 2s - loss: 0.6927 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6925 - accuracy: 0.51 - ETA: 2s - loss: 0.6925 - accuracy: 0.51 - ETA: 2s - loss: 0.6925 - accuracy: 0.50 - ETA: 2s - loss: 0.6925 - accuracy: 0.51 - ETA: 2s - loss: 0.6924 - accuracy: 0.50 - ETA: 1s - loss: 0.6924 - accuracy: 0.50 - ETA: 1s - loss: 0.6923 - accuracy: 0.50 - ETA: 1s - loss: 0.6923 - accuracy: 0.50 - ETA: 1s - loss: 0.6922 - accuracy: 0.50 - ETA: 1s - loss: 0.6921 - accuracy: 0.50 - ETA: 1s - loss: 0.6921 - accuracy: 0.50 - ETA: 1s - loss: 0.6922 - accuracy: 0.50 - ETA: 1s - loss: 0.6922 - accuracy: 0.50 - ETA: 1s - loss: 0.6922 - accuracy: 0.50 - ETA: 1s - loss: 0.6919 - accuracy: 0.50 - ETA: 1s - loss: 0.6919 - accuracy: 0.50 - ETA: 1s - loss: 0.6919 - accuracy: 0.50 - ETA: 1s - loss: 0.6917 - accuracy: 0.50 - ETA: 1s - loss: 0.6917 - accuracy: 0.50 - ETA: 1s - loss: 0.6916 - accuracy: 0.50 - ETA: 1s - loss: 0.6914 - accuracy: 0.51 - ETA: 1s - loss: 0.6913 - accuracy: 0.51 - ETA: 1s - loss: 0.6912 - accuracy: 0.51 - ETA: 0s - loss: 0.6910 - accuracy: 0.51 - ETA: 0s - loss: 0.6907 - accuracy: 0.51 - ETA: 0s - loss: 0.6905 - accuracy: 0.51 - ETA: 0s - loss: 0.6904 - accuracy: 0.51 - ETA: 0s - loss: 0.6902 - accuracy: 0.51 - ETA: 0s - loss: 0.6902 - accuracy: 0.51 - ETA: 0s - loss: 0.6900 - accuracy: 0.51 - ETA: 0s - loss: 0.6897 - accuracy: 0.52 - ETA: 0s - loss: 0.6895 - accuracy: 0.52 - ETA: 0s - loss: 0.6895 - accuracy: 0.52 - ETA: 0s - loss: 0.6894 - accuracy: 0.52 - ETA: 0s - loss: 0.6892 - accuracy: 0.52 - ETA: 0s - loss: 0.6891 - accuracy: 0.52 - ETA: 0s - loss: 0.6890 - accuracy: 0.52 - ETA: 0s - loss: 0.6889 - accuracy: 0.52 - ETA: 0s - loss: 0.6888 - accuracy: 0.52 - ETA: 0s - loss: 0.6887 - accuracy: 0.52 - ETA: 0s - loss: 0.6886 - accuracy: 0.52 - ETA: 0s - loss: 0.6884 - accuracy: 0.53 - 3s 256us/step - loss: 0.6884 - accuracy: 0.5300 - val_loss: 0.6784 - val_accuracy: 0.6541\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 106us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:10 - loss: 0.6948 - accuracy: 0.50 - ETA: 8s - loss: 0.6929 - accuracy: 0.5273 - ETA: 5s - loss: 0.6932 - accuracy: 0.52 - ETA: 4s - loss: 0.6913 - accuracy: 0.53 - ETA: 4s - loss: 0.6907 - accuracy: 0.53 - ETA: 3s - loss: 0.6905 - accuracy: 0.53 - ETA: 3s - loss: 0.6903 - accuracy: 0.53 - ETA: 3s - loss: 0.6909 - accuracy: 0.53 - ETA: 3s - loss: 0.6901 - accuracy: 0.53 - ETA: 2s - loss: 0.6899 - accuracy: 0.53 - ETA: 2s - loss: 0.6895 - accuracy: 0.54 - ETA: 2s - loss: 0.6896 - accuracy: 0.53 - ETA: 2s - loss: 0.6891 - accuracy: 0.54 - ETA: 2s - loss: 0.6887 - accuracy: 0.54 - ETA: 2s - loss: 0.6885 - accuracy: 0.54 - ETA: 2s - loss: 0.6881 - accuracy: 0.55 - ETA: 2s - loss: 0.6881 - accuracy: 0.55 - ETA: 2s - loss: 0.6878 - accuracy: 0.54 - ETA: 2s - loss: 0.6873 - accuracy: 0.55 - ETA: 2s - loss: 0.6868 - accuracy: 0.55 - ETA: 2s - loss: 0.6867 - accuracy: 0.55 - ETA: 1s - loss: 0.6867 - accuracy: 0.55 - ETA: 1s - loss: 0.6859 - accuracy: 0.55 - ETA: 1s - loss: 0.6859 - accuracy: 0.55 - ETA: 1s - loss: 0.6854 - accuracy: 0.56 - ETA: 1s - loss: 0.6853 - accuracy: 0.56 - ETA: 1s - loss: 0.6849 - accuracy: 0.56 - ETA: 1s - loss: 0.6849 - accuracy: 0.56 - ETA: 1s - loss: 0.6845 - accuracy: 0.56 - ETA: 1s - loss: 0.6840 - accuracy: 0.56 - ETA: 1s - loss: 0.6835 - accuracy: 0.56 - ETA: 1s - loss: 0.6830 - accuracy: 0.56 - ETA: 1s - loss: 0.6827 - accuracy: 0.56 - ETA: 1s - loss: 0.6827 - accuracy: 0.56 - ETA: 1s - loss: 0.6823 - accuracy: 0.56 - ETA: 1s - loss: 0.6821 - accuracy: 0.56 - ETA: 1s - loss: 0.6819 - accuracy: 0.57 - ETA: 1s - loss: 0.6814 - accuracy: 0.57 - ETA: 1s - loss: 0.6811 - accuracy: 0.57 - ETA: 1s - loss: 0.6807 - accuracy: 0.57 - ETA: 0s - loss: 0.6804 - accuracy: 0.57 - ETA: 0s - loss: 0.6799 - accuracy: 0.57 - ETA: 0s - loss: 0.6795 - accuracy: 0.57 - ETA: 0s - loss: 0.6790 - accuracy: 0.57 - ETA: 0s - loss: 0.6786 - accuracy: 0.57 - ETA: 0s - loss: 0.6782 - accuracy: 0.57 - ETA: 0s - loss: 0.6779 - accuracy: 0.57 - ETA: 0s - loss: 0.6775 - accuracy: 0.57 - ETA: 0s - loss: 0.6770 - accuracy: 0.58 - ETA: 0s - loss: 0.6767 - accuracy: 0.57 - ETA: 0s - loss: 0.6762 - accuracy: 0.58 - ETA: 0s - loss: 0.6761 - accuracy: 0.58 - ETA: 0s - loss: 0.6755 - accuracy: 0.58 - ETA: 0s - loss: 0.6753 - accuracy: 0.58 - ETA: 0s - loss: 0.6752 - accuracy: 0.58 - ETA: 0s - loss: 0.6751 - accuracy: 0.58 - ETA: 0s - loss: 0.6747 - accuracy: 0.58 - ETA: 0s - loss: 0.6746 - accuracy: 0.58 - 3s 255us/step - loss: 0.6744 - accuracy: 0.5839 - val_loss: 0.6509 - val_accuracy: 0.6989\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:07 - loss: 0.6943 - accuracy: 0.50 - ETA: 8s - loss: 0.6935 - accuracy: 0.5091 - ETA: 5s - loss: 0.6906 - accuracy: 0.54 - ETA: 4s - loss: 0.6883 - accuracy: 0.55 - ETA: 4s - loss: 0.6881 - accuracy: 0.55 - ETA: 3s - loss: 0.6880 - accuracy: 0.55 - ETA: 3s - loss: 0.6887 - accuracy: 0.54 - ETA: 3s - loss: 0.6881 - accuracy: 0.55 - ETA: 3s - loss: 0.6873 - accuracy: 0.56 - ETA: 3s - loss: 0.6875 - accuracy: 0.55 - ETA: 2s - loss: 0.6871 - accuracy: 0.56 - ETA: 2s - loss: 0.6866 - accuracy: 0.56 - ETA: 2s - loss: 0.6860 - accuracy: 0.56 - ETA: 2s - loss: 0.6858 - accuracy: 0.56 - ETA: 2s - loss: 0.6857 - accuracy: 0.56 - ETA: 2s - loss: 0.6855 - accuracy: 0.56 - ETA: 2s - loss: 0.6847 - accuracy: 0.56 - ETA: 2s - loss: 0.6844 - accuracy: 0.56 - ETA: 2s - loss: 0.6840 - accuracy: 0.56 - ETA: 2s - loss: 0.6839 - accuracy: 0.56 - ETA: 2s - loss: 0.6833 - accuracy: 0.56 - ETA: 2s - loss: 0.6827 - accuracy: 0.56 - ETA: 2s - loss: 0.6825 - accuracy: 0.57 - ETA: 1s - loss: 0.6822 - accuracy: 0.57 - ETA: 1s - loss: 0.6819 - accuracy: 0.57 - ETA: 1s - loss: 0.6813 - accuracy: 0.57 - ETA: 1s - loss: 0.6812 - accuracy: 0.57 - ETA: 1s - loss: 0.6809 - accuracy: 0.57 - ETA: 1s - loss: 0.6805 - accuracy: 0.57 - ETA: 1s - loss: 0.6799 - accuracy: 0.57 - ETA: 1s - loss: 0.6796 - accuracy: 0.57 - ETA: 1s - loss: 0.6796 - accuracy: 0.57 - ETA: 1s - loss: 0.6793 - accuracy: 0.57 - ETA: 1s - loss: 0.6792 - accuracy: 0.57 - ETA: 1s - loss: 0.6788 - accuracy: 0.58 - ETA: 1s - loss: 0.6785 - accuracy: 0.58 - ETA: 1s - loss: 0.6785 - accuracy: 0.58 - ETA: 1s - loss: 0.6781 - accuracy: 0.58 - ETA: 1s - loss: 0.6778 - accuracy: 0.58 - ETA: 1s - loss: 0.6775 - accuracy: 0.58 - ETA: 0s - loss: 0.6773 - accuracy: 0.58 - ETA: 0s - loss: 0.6771 - accuracy: 0.58 - ETA: 0s - loss: 0.6766 - accuracy: 0.58 - ETA: 0s - loss: 0.6760 - accuracy: 0.58 - ETA: 0s - loss: 0.6759 - accuracy: 0.58 - ETA: 0s - loss: 0.6756 - accuracy: 0.58 - ETA: 0s - loss: 0.6752 - accuracy: 0.58 - ETA: 0s - loss: 0.6747 - accuracy: 0.59 - ETA: 0s - loss: 0.6742 - accuracy: 0.59 - ETA: 0s - loss: 0.6740 - accuracy: 0.59 - ETA: 0s - loss: 0.6735 - accuracy: 0.59 - ETA: 0s - loss: 0.6734 - accuracy: 0.59 - ETA: 0s - loss: 0.6730 - accuracy: 0.59 - ETA: 0s - loss: 0.6726 - accuracy: 0.59 - ETA: 0s - loss: 0.6723 - accuracy: 0.59 - ETA: 0s - loss: 0.6719 - accuracy: 0.59 - ETA: 0s - loss: 0.6717 - accuracy: 0.59 - ETA: 0s - loss: 0.6713 - accuracy: 0.59 - ETA: 0s - loss: 0.6708 - accuracy: 0.59 - 3s 258us/step - loss: 0.6708 - accuracy: 0.5956 - val_loss: 0.6452 - val_accuracy: 0.7095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:00 - loss: 0.6960 - accuracy: 0.75 - ETA: 8s - loss: 0.6945 - accuracy: 0.5046 - ETA: 5s - loss: 0.6934 - accuracy: 0.50 - ETA: 4s - loss: 0.6925 - accuracy: 0.50 - ETA: 4s - loss: 0.6909 - accuracy: 0.52 - ETA: 3s - loss: 0.6910 - accuracy: 0.51 - ETA: 3s - loss: 0.6907 - accuracy: 0.52 - ETA: 3s - loss: 0.6905 - accuracy: 0.52 - ETA: 3s - loss: 0.6906 - accuracy: 0.52 - ETA: 3s - loss: 0.6903 - accuracy: 0.52 - ETA: 2s - loss: 0.6900 - accuracy: 0.52 - ETA: 2s - loss: 0.6898 - accuracy: 0.52 - ETA: 2s - loss: 0.6893 - accuracy: 0.53 - ETA: 2s - loss: 0.6889 - accuracy: 0.53 - ETA: 2s - loss: 0.6884 - accuracy: 0.54 - ETA: 2s - loss: 0.6881 - accuracy: 0.54 - ETA: 2s - loss: 0.6873 - accuracy: 0.55 - ETA: 2s - loss: 0.6867 - accuracy: 0.55 - ETA: 2s - loss: 0.6862 - accuracy: 0.56 - ETA: 2s - loss: 0.6855 - accuracy: 0.56 - ETA: 2s - loss: 0.6852 - accuracy: 0.56 - ETA: 2s - loss: 0.6851 - accuracy: 0.56 - ETA: 2s - loss: 0.6848 - accuracy: 0.56 - ETA: 1s - loss: 0.6843 - accuracy: 0.57 - ETA: 1s - loss: 0.6836 - accuracy: 0.57 - ETA: 1s - loss: 0.6832 - accuracy: 0.57 - ETA: 1s - loss: 0.6829 - accuracy: 0.57 - ETA: 1s - loss: 0.6823 - accuracy: 0.57 - ETA: 1s - loss: 0.6820 - accuracy: 0.57 - ETA: 1s - loss: 0.6811 - accuracy: 0.58 - ETA: 1s - loss: 0.6808 - accuracy: 0.58 - ETA: 1s - loss: 0.6804 - accuracy: 0.58 - ETA: 1s - loss: 0.6799 - accuracy: 0.58 - ETA: 1s - loss: 0.6793 - accuracy: 0.58 - ETA: 1s - loss: 0.6789 - accuracy: 0.58 - ETA: 1s - loss: 0.6785 - accuracy: 0.58 - ETA: 1s - loss: 0.6777 - accuracy: 0.59 - ETA: 1s - loss: 0.6774 - accuracy: 0.59 - ETA: 1s - loss: 0.6772 - accuracy: 0.59 - ETA: 1s - loss: 0.6766 - accuracy: 0.59 - ETA: 1s - loss: 0.6761 - accuracy: 0.59 - ETA: 0s - loss: 0.6756 - accuracy: 0.59 - ETA: 0s - loss: 0.6749 - accuracy: 0.59 - ETA: 0s - loss: 0.6744 - accuracy: 0.60 - ETA: 0s - loss: 0.6734 - accuracy: 0.60 - ETA: 0s - loss: 0.6728 - accuracy: 0.60 - ETA: 0s - loss: 0.6722 - accuracy: 0.60 - ETA: 0s - loss: 0.6718 - accuracy: 0.60 - ETA: 0s - loss: 0.6712 - accuracy: 0.60 - ETA: 0s - loss: 0.6713 - accuracy: 0.60 - ETA: 0s - loss: 0.6709 - accuracy: 0.60 - ETA: 0s - loss: 0.6705 - accuracy: 0.60 - ETA: 0s - loss: 0.6702 - accuracy: 0.60 - ETA: 0s - loss: 0.6697 - accuracy: 0.60 - ETA: 0s - loss: 0.6692 - accuracy: 0.61 - ETA: 0s - loss: 0.6692 - accuracy: 0.61 - ETA: 0s - loss: 0.6687 - accuracy: 0.61 - ETA: 0s - loss: 0.6682 - accuracy: 0.61 - ETA: 0s - loss: 0.6680 - accuracy: 0.61 - ETA: 0s - loss: 0.6676 - accuracy: 0.61 - 3s 260us/step - loss: 0.6673 - accuracy: 0.6151 - val_loss: 0.6360 - val_accuracy: 0.7045\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:07 - loss: 0.7017 - accuracy: 0.50 - ETA: 8s - loss: 0.6894 - accuracy: 0.5409 - ETA: 5s - loss: 0.6914 - accuracy: 0.52 - ETA: 4s - loss: 0.6908 - accuracy: 0.52 - ETA: 4s - loss: 0.6893 - accuracy: 0.53 - ETA: 3s - loss: 0.6897 - accuracy: 0.53 - ETA: 3s - loss: 0.6901 - accuracy: 0.53 - ETA: 3s - loss: 0.6905 - accuracy: 0.52 - ETA: 3s - loss: 0.6900 - accuracy: 0.53 - ETA: 3s - loss: 0.6894 - accuracy: 0.53 - ETA: 2s - loss: 0.6892 - accuracy: 0.53 - ETA: 2s - loss: 0.6890 - accuracy: 0.53 - ETA: 2s - loss: 0.6883 - accuracy: 0.54 - ETA: 2s - loss: 0.6878 - accuracy: 0.54 - ETA: 2s - loss: 0.6879 - accuracy: 0.54 - ETA: 2s - loss: 0.6877 - accuracy: 0.54 - ETA: 2s - loss: 0.6877 - accuracy: 0.54 - ETA: 2s - loss: 0.6877 - accuracy: 0.54 - ETA: 2s - loss: 0.6875 - accuracy: 0.54 - ETA: 2s - loss: 0.6872 - accuracy: 0.54 - ETA: 2s - loss: 0.6871 - accuracy: 0.54 - ETA: 2s - loss: 0.6870 - accuracy: 0.54 - ETA: 2s - loss: 0.6867 - accuracy: 0.55 - ETA: 1s - loss: 0.6862 - accuracy: 0.55 - ETA: 1s - loss: 0.6861 - accuracy: 0.55 - ETA: 1s - loss: 0.6859 - accuracy: 0.55 - ETA: 1s - loss: 0.6855 - accuracy: 0.55 - ETA: 1s - loss: 0.6848 - accuracy: 0.56 - ETA: 1s - loss: 0.6847 - accuracy: 0.56 - ETA: 1s - loss: 0.6845 - accuracy: 0.56 - ETA: 1s - loss: 0.6841 - accuracy: 0.56 - ETA: 1s - loss: 0.6840 - accuracy: 0.56 - ETA: 1s - loss: 0.6837 - accuracy: 0.56 - ETA: 1s - loss: 0.6833 - accuracy: 0.56 - ETA: 1s - loss: 0.6831 - accuracy: 0.56 - ETA: 1s - loss: 0.6829 - accuracy: 0.56 - ETA: 1s - loss: 0.6825 - accuracy: 0.56 - ETA: 1s - loss: 0.6825 - accuracy: 0.56 - ETA: 1s - loss: 0.6823 - accuracy: 0.56 - ETA: 1s - loss: 0.6821 - accuracy: 0.56 - ETA: 1s - loss: 0.6818 - accuracy: 0.56 - ETA: 0s - loss: 0.6816 - accuracy: 0.57 - ETA: 0s - loss: 0.6815 - accuracy: 0.57 - ETA: 0s - loss: 0.6810 - accuracy: 0.57 - ETA: 0s - loss: 0.6804 - accuracy: 0.57 - ETA: 0s - loss: 0.6799 - accuracy: 0.57 - ETA: 0s - loss: 0.6794 - accuracy: 0.57 - ETA: 0s - loss: 0.6790 - accuracy: 0.57 - ETA: 0s - loss: 0.6789 - accuracy: 0.57 - ETA: 0s - loss: 0.6785 - accuracy: 0.57 - ETA: 0s - loss: 0.6783 - accuracy: 0.57 - ETA: 0s - loss: 0.6778 - accuracy: 0.58 - ETA: 0s - loss: 0.6776 - accuracy: 0.58 - ETA: 0s - loss: 0.6774 - accuracy: 0.58 - ETA: 0s - loss: 0.6773 - accuracy: 0.58 - ETA: 0s - loss: 0.6770 - accuracy: 0.58 - ETA: 0s - loss: 0.6770 - accuracy: 0.58 - ETA: 0s - loss: 0.6766 - accuracy: 0.58 - ETA: 0s - loss: 0.6761 - accuracy: 0.58 - ETA: 0s - loss: 0.6758 - accuracy: 0.58 - 3s 261us/step - loss: 0.6757 - accuracy: 0.5837 - val_loss: 0.6530 - val_accuracy: 0.7017\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 109us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:00 - loss: 0.6926 - accuracy: 0.50 - ETA: 8s - loss: 0.6941 - accuracy: 0.5045 - ETA: 5s - loss: 0.6933 - accuracy: 0.53 - ETA: 4s - loss: 0.6919 - accuracy: 0.54 - ETA: 3s - loss: 0.6906 - accuracy: 0.53 - ETA: 3s - loss: 0.6913 - accuracy: 0.52 - ETA: 3s - loss: 0.6910 - accuracy: 0.52 - ETA: 3s - loss: 0.6901 - accuracy: 0.52 - ETA: 3s - loss: 0.6900 - accuracy: 0.53 - ETA: 3s - loss: 0.6904 - accuracy: 0.53 - ETA: 2s - loss: 0.6899 - accuracy: 0.53 - ETA: 2s - loss: 0.6897 - accuracy: 0.53 - ETA: 2s - loss: 0.6892 - accuracy: 0.54 - ETA: 2s - loss: 0.6889 - accuracy: 0.54 - ETA: 2s - loss: 0.6887 - accuracy: 0.54 - ETA: 2s - loss: 0.6880 - accuracy: 0.54 - ETA: 2s - loss: 0.6879 - accuracy: 0.54 - ETA: 2s - loss: 0.6875 - accuracy: 0.55 - ETA: 2s - loss: 0.6869 - accuracy: 0.55 - ETA: 2s - loss: 0.6865 - accuracy: 0.55 - ETA: 2s - loss: 0.6861 - accuracy: 0.55 - ETA: 2s - loss: 0.6857 - accuracy: 0.55 - ETA: 2s - loss: 0.6854 - accuracy: 0.56 - ETA: 1s - loss: 0.6850 - accuracy: 0.56 - ETA: 1s - loss: 0.6841 - accuracy: 0.56 - ETA: 1s - loss: 0.6838 - accuracy: 0.56 - ETA: 1s - loss: 0.6837 - accuracy: 0.56 - ETA: 1s - loss: 0.6832 - accuracy: 0.57 - ETA: 1s - loss: 0.6829 - accuracy: 0.57 - ETA: 1s - loss: 0.6821 - accuracy: 0.57 - ETA: 1s - loss: 0.6816 - accuracy: 0.57 - ETA: 1s - loss: 0.6812 - accuracy: 0.57 - ETA: 1s - loss: 0.6811 - accuracy: 0.57 - ETA: 1s - loss: 0.6808 - accuracy: 0.57 - ETA: 1s - loss: 0.6802 - accuracy: 0.57 - ETA: 1s - loss: 0.6800 - accuracy: 0.57 - ETA: 1s - loss: 0.6797 - accuracy: 0.57 - ETA: 1s - loss: 0.6796 - accuracy: 0.57 - ETA: 1s - loss: 0.6793 - accuracy: 0.57 - ETA: 1s - loss: 0.6790 - accuracy: 0.57 - ETA: 0s - loss: 0.6790 - accuracy: 0.57 - ETA: 0s - loss: 0.6786 - accuracy: 0.57 - ETA: 0s - loss: 0.6781 - accuracy: 0.58 - ETA: 0s - loss: 0.6774 - accuracy: 0.58 - ETA: 0s - loss: 0.6768 - accuracy: 0.58 - ETA: 0s - loss: 0.6767 - accuracy: 0.58 - ETA: 0s - loss: 0.6763 - accuracy: 0.58 - ETA: 0s - loss: 0.6758 - accuracy: 0.58 - ETA: 0s - loss: 0.6754 - accuracy: 0.58 - ETA: 0s - loss: 0.6750 - accuracy: 0.58 - ETA: 0s - loss: 0.6747 - accuracy: 0.58 - ETA: 0s - loss: 0.6741 - accuracy: 0.58 - ETA: 0s - loss: 0.6738 - accuracy: 0.58 - ETA: 0s - loss: 0.6732 - accuracy: 0.58 - ETA: 0s - loss: 0.6726 - accuracy: 0.58 - ETA: 0s - loss: 0.6725 - accuracy: 0.58 - ETA: 0s - loss: 0.6723 - accuracy: 0.58 - ETA: 0s - loss: 0.6718 - accuracy: 0.58 - ETA: 0s - loss: 0.6717 - accuracy: 0.58 - 3s 259us/step - loss: 0.6714 - accuracy: 0.5897 - val_loss: 0.6451 - val_accuracy: 0.7131\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 105us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:13 - loss: 0.6946 - accuracy: 0.25 - ETA: 8s - loss: 0.6917 - accuracy: 0.4777 - ETA: 5s - loss: 0.6924 - accuracy: 0.48 - ETA: 4s - loss: 0.6926 - accuracy: 0.47 - ETA: 4s - loss: 0.6923 - accuracy: 0.47 - ETA: 3s - loss: 0.6925 - accuracy: 0.48 - ETA: 3s - loss: 0.6919 - accuracy: 0.49 - ETA: 3s - loss: 0.6917 - accuracy: 0.50 - ETA: 3s - loss: 0.6909 - accuracy: 0.51 - ETA: 2s - loss: 0.6903 - accuracy: 0.51 - ETA: 2s - loss: 0.6892 - accuracy: 0.52 - ETA: 2s - loss: 0.6885 - accuracy: 0.52 - ETA: 2s - loss: 0.6873 - accuracy: 0.53 - ETA: 2s - loss: 0.6861 - accuracy: 0.54 - ETA: 2s - loss: 0.6859 - accuracy: 0.54 - ETA: 2s - loss: 0.6853 - accuracy: 0.54 - ETA: 2s - loss: 0.6849 - accuracy: 0.54 - ETA: 2s - loss: 0.6839 - accuracy: 0.55 - ETA: 2s - loss: 0.6840 - accuracy: 0.55 - ETA: 2s - loss: 0.6843 - accuracy: 0.55 - ETA: 2s - loss: 0.6841 - accuracy: 0.55 - ETA: 2s - loss: 0.6840 - accuracy: 0.55 - ETA: 1s - loss: 0.6835 - accuracy: 0.55 - ETA: 1s - loss: 0.6829 - accuracy: 0.55 - ETA: 1s - loss: 0.6825 - accuracy: 0.55 - ETA: 1s - loss: 0.6817 - accuracy: 0.56 - ETA: 1s - loss: 0.6813 - accuracy: 0.56 - ETA: 1s - loss: 0.6811 - accuracy: 0.56 - ETA: 1s - loss: 0.6804 - accuracy: 0.56 - ETA: 1s - loss: 0.6797 - accuracy: 0.56 - ETA: 1s - loss: 0.6791 - accuracy: 0.57 - ETA: 1s - loss: 0.6789 - accuracy: 0.57 - ETA: 1s - loss: 0.6787 - accuracy: 0.57 - ETA: 1s - loss: 0.6783 - accuracy: 0.57 - ETA: 1s - loss: 0.6775 - accuracy: 0.57 - ETA: 1s - loss: 0.6773 - accuracy: 0.57 - ETA: 1s - loss: 0.6769 - accuracy: 0.57 - ETA: 1s - loss: 0.6765 - accuracy: 0.57 - ETA: 1s - loss: 0.6762 - accuracy: 0.57 - ETA: 1s - loss: 0.6754 - accuracy: 0.57 - ETA: 0s - loss: 0.6751 - accuracy: 0.57 - ETA: 0s - loss: 0.6750 - accuracy: 0.57 - ETA: 0s - loss: 0.6745 - accuracy: 0.57 - ETA: 0s - loss: 0.6743 - accuracy: 0.58 - ETA: 0s - loss: 0.6736 - accuracy: 0.58 - ETA: 0s - loss: 0.6734 - accuracy: 0.58 - ETA: 0s - loss: 0.6730 - accuracy: 0.58 - ETA: 0s - loss: 0.6728 - accuracy: 0.58 - ETA: 0s - loss: 0.6724 - accuracy: 0.58 - ETA: 0s - loss: 0.6721 - accuracy: 0.58 - ETA: 0s - loss: 0.6717 - accuracy: 0.58 - ETA: 0s - loss: 0.6713 - accuracy: 0.58 - ETA: 0s - loss: 0.6712 - accuracy: 0.58 - ETA: 0s - loss: 0.6708 - accuracy: 0.59 - ETA: 0s - loss: 0.6707 - accuracy: 0.59 - ETA: 0s - loss: 0.6706 - accuracy: 0.59 - ETA: 0s - loss: 0.6702 - accuracy: 0.59 - ETA: 0s - loss: 0.6698 - accuracy: 0.59 - ETA: 0s - loss: 0.6696 - accuracy: 0.59 - 3s 256us/step - loss: 0.6696 - accuracy: 0.5960 - val_loss: 0.6478 - val_accuracy: 0.7017\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 109us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:04 - loss: 0.6905 - accuracy: 0.50 - ETA: 8s - loss: 0.6970 - accuracy: 0.4717 - ETA: 5s - loss: 0.6930 - accuracy: 0.53 - ETA: 4s - loss: 0.6910 - accuracy: 0.53 - ETA: 4s - loss: 0.6907 - accuracy: 0.53 - ETA: 3s - loss: 0.6887 - accuracy: 0.55 - ETA: 3s - loss: 0.6881 - accuracy: 0.56 - ETA: 3s - loss: 0.6880 - accuracy: 0.56 - ETA: 3s - loss: 0.6859 - accuracy: 0.57 - ETA: 3s - loss: 0.6848 - accuracy: 0.58 - ETA: 2s - loss: 0.6843 - accuracy: 0.58 - ETA: 2s - loss: 0.6835 - accuracy: 0.58 - ETA: 2s - loss: 0.6831 - accuracy: 0.58 - ETA: 2s - loss: 0.6826 - accuracy: 0.58 - ETA: 2s - loss: 0.6823 - accuracy: 0.58 - ETA: 2s - loss: 0.6814 - accuracy: 0.59 - ETA: 2s - loss: 0.6801 - accuracy: 0.59 - ETA: 2s - loss: 0.6792 - accuracy: 0.60 - ETA: 2s - loss: 0.6789 - accuracy: 0.60 - ETA: 2s - loss: 0.6782 - accuracy: 0.60 - ETA: 2s - loss: 0.6772 - accuracy: 0.60 - ETA: 2s - loss: 0.6764 - accuracy: 0.61 - ETA: 2s - loss: 0.6754 - accuracy: 0.61 - ETA: 1s - loss: 0.6747 - accuracy: 0.61 - ETA: 1s - loss: 0.6743 - accuracy: 0.61 - ETA: 1s - loss: 0.6744 - accuracy: 0.61 - ETA: 1s - loss: 0.6737 - accuracy: 0.61 - ETA: 1s - loss: 0.6731 - accuracy: 0.61 - ETA: 1s - loss: 0.6720 - accuracy: 0.62 - ETA: 1s - loss: 0.6707 - accuracy: 0.62 - ETA: 1s - loss: 0.6703 - accuracy: 0.62 - ETA: 1s - loss: 0.6695 - accuracy: 0.62 - ETA: 1s - loss: 0.6681 - accuracy: 0.62 - ETA: 1s - loss: 0.6673 - accuracy: 0.63 - ETA: 1s - loss: 0.6667 - accuracy: 0.63 - ETA: 1s - loss: 0.6660 - accuracy: 0.63 - ETA: 1s - loss: 0.6657 - accuracy: 0.63 - ETA: 1s - loss: 0.6654 - accuracy: 0.63 - ETA: 1s - loss: 0.6648 - accuracy: 0.63 - ETA: 1s - loss: 0.6643 - accuracy: 0.63 - ETA: 0s - loss: 0.6643 - accuracy: 0.63 - ETA: 0s - loss: 0.6639 - accuracy: 0.63 - ETA: 0s - loss: 0.6633 - accuracy: 0.63 - ETA: 0s - loss: 0.6629 - accuracy: 0.63 - ETA: 0s - loss: 0.6624 - accuracy: 0.63 - ETA: 0s - loss: 0.6620 - accuracy: 0.63 - ETA: 0s - loss: 0.6612 - accuracy: 0.63 - ETA: 0s - loss: 0.6611 - accuracy: 0.63 - ETA: 0s - loss: 0.6606 - accuracy: 0.63 - ETA: 0s - loss: 0.6603 - accuracy: 0.63 - ETA: 0s - loss: 0.6601 - accuracy: 0.63 - ETA: 0s - loss: 0.6594 - accuracy: 0.64 - ETA: 0s - loss: 0.6589 - accuracy: 0.64 - ETA: 0s - loss: 0.6586 - accuracy: 0.64 - ETA: 0s - loss: 0.6585 - accuracy: 0.64 - ETA: 0s - loss: 0.6579 - accuracy: 0.64 - ETA: 0s - loss: 0.6575 - accuracy: 0.64 - ETA: 0s - loss: 0.6570 - accuracy: 0.64 - ETA: 0s - loss: 0.6561 - accuracy: 0.64 - 3s 259us/step - loss: 0.6559 - accuracy: 0.6473 - val_loss: 0.6178 - val_accuracy: 0.7109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:10 - loss: 0.6782 - accuracy: 1.00 - ETA: 8s - loss: 0.6939 - accuracy: 0.5182 - ETA: 5s - loss: 0.6929 - accuracy: 0.52 - ETA: 4s - loss: 0.6927 - accuracy: 0.53 - ETA: 4s - loss: 0.6914 - accuracy: 0.54 - ETA: 3s - loss: 0.6898 - accuracy: 0.54 - ETA: 3s - loss: 0.6894 - accuracy: 0.55 - ETA: 3s - loss: 0.6876 - accuracy: 0.56 - ETA: 3s - loss: 0.6873 - accuracy: 0.56 - ETA: 3s - loss: 0.6861 - accuracy: 0.57 - ETA: 2s - loss: 0.6852 - accuracy: 0.57 - ETA: 2s - loss: 0.6847 - accuracy: 0.57 - ETA: 2s - loss: 0.6843 - accuracy: 0.57 - ETA: 2s - loss: 0.6837 - accuracy: 0.57 - ETA: 2s - loss: 0.6829 - accuracy: 0.58 - ETA: 2s - loss: 0.6823 - accuracy: 0.58 - ETA: 2s - loss: 0.6811 - accuracy: 0.59 - ETA: 2s - loss: 0.6802 - accuracy: 0.59 - ETA: 2s - loss: 0.6797 - accuracy: 0.59 - ETA: 2s - loss: 0.6793 - accuracy: 0.59 - ETA: 2s - loss: 0.6780 - accuracy: 0.60 - ETA: 2s - loss: 0.6774 - accuracy: 0.60 - ETA: 1s - loss: 0.6767 - accuracy: 0.60 - ETA: 1s - loss: 0.6755 - accuracy: 0.60 - ETA: 1s - loss: 0.6739 - accuracy: 0.61 - ETA: 1s - loss: 0.6734 - accuracy: 0.61 - ETA: 1s - loss: 0.6731 - accuracy: 0.61 - ETA: 1s - loss: 0.6724 - accuracy: 0.61 - ETA: 1s - loss: 0.6724 - accuracy: 0.61 - ETA: 1s - loss: 0.6719 - accuracy: 0.61 - ETA: 1s - loss: 0.6708 - accuracy: 0.61 - ETA: 1s - loss: 0.6702 - accuracy: 0.61 - ETA: 1s - loss: 0.6695 - accuracy: 0.61 - ETA: 1s - loss: 0.6694 - accuracy: 0.61 - ETA: 1s - loss: 0.6694 - accuracy: 0.61 - ETA: 1s - loss: 0.6689 - accuracy: 0.61 - ETA: 1s - loss: 0.6684 - accuracy: 0.61 - ETA: 1s - loss: 0.6678 - accuracy: 0.62 - ETA: 1s - loss: 0.6676 - accuracy: 0.62 - ETA: 1s - loss: 0.6670 - accuracy: 0.62 - ETA: 0s - loss: 0.6661 - accuracy: 0.62 - ETA: 0s - loss: 0.6658 - accuracy: 0.62 - ETA: 0s - loss: 0.6655 - accuracy: 0.62 - ETA: 0s - loss: 0.6647 - accuracy: 0.62 - ETA: 0s - loss: 0.6647 - accuracy: 0.62 - ETA: 0s - loss: 0.6641 - accuracy: 0.62 - ETA: 0s - loss: 0.6636 - accuracy: 0.62 - ETA: 0s - loss: 0.6631 - accuracy: 0.62 - ETA: 0s - loss: 0.6626 - accuracy: 0.62 - ETA: 0s - loss: 0.6623 - accuracy: 0.62 - ETA: 0s - loss: 0.6618 - accuracy: 0.62 - ETA: 0s - loss: 0.6609 - accuracy: 0.63 - ETA: 0s - loss: 0.6607 - accuracy: 0.63 - ETA: 0s - loss: 0.6604 - accuracy: 0.63 - ETA: 0s - loss: 0.6603 - accuracy: 0.63 - ETA: 0s - loss: 0.6598 - accuracy: 0.63 - ETA: 0s - loss: 0.6593 - accuracy: 0.63 - ETA: 0s - loss: 0.6589 - accuracy: 0.63 - 3s 254us/step - loss: 0.6586 - accuracy: 0.6332 - val_loss: 0.6248 - val_accuracy: 0.7124\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 106us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 5:04 - loss: 0.6996 - accuracy: 0.0000e+ - ETA: 8s - loss: 0.6946 - accuracy: 0.4591     - ETA: 5s - loss: 0.6921 - accuracy: 0.50 - ETA: 4s - loss: 0.6919 - accuracy: 0.50 - ETA: 3s - loss: 0.6903 - accuracy: 0.51 - ETA: 3s - loss: 0.6894 - accuracy: 0.51 - ETA: 3s - loss: 0.6891 - accuracy: 0.51 - ETA: 3s - loss: 0.6890 - accuracy: 0.51 - ETA: 3s - loss: 0.6888 - accuracy: 0.51 - ETA: 3s - loss: 0.6887 - accuracy: 0.52 - ETA: 2s - loss: 0.6886 - accuracy: 0.52 - ETA: 2s - loss: 0.6885 - accuracy: 0.53 - ETA: 2s - loss: 0.6880 - accuracy: 0.53 - ETA: 2s - loss: 0.6877 - accuracy: 0.53 - ETA: 2s - loss: 0.6873 - accuracy: 0.54 - ETA: 2s - loss: 0.6863 - accuracy: 0.54 - ETA: 2s - loss: 0.6854 - accuracy: 0.55 - ETA: 2s - loss: 0.6843 - accuracy: 0.55 - ETA: 2s - loss: 0.6839 - accuracy: 0.55 - ETA: 2s - loss: 0.6828 - accuracy: 0.56 - ETA: 2s - loss: 0.6821 - accuracy: 0.56 - ETA: 2s - loss: 0.6809 - accuracy: 0.57 - ETA: 2s - loss: 0.6806 - accuracy: 0.57 - ETA: 1s - loss: 0.6798 - accuracy: 0.57 - ETA: 1s - loss: 0.6791 - accuracy: 0.57 - ETA: 1s - loss: 0.6788 - accuracy: 0.57 - ETA: 1s - loss: 0.6784 - accuracy: 0.57 - ETA: 1s - loss: 0.6781 - accuracy: 0.58 - ETA: 1s - loss: 0.6774 - accuracy: 0.58 - ETA: 1s - loss: 0.6767 - accuracy: 0.58 - ETA: 1s - loss: 0.6761 - accuracy: 0.58 - ETA: 1s - loss: 0.6752 - accuracy: 0.59 - ETA: 1s - loss: 0.6743 - accuracy: 0.59 - ETA: 1s - loss: 0.6736 - accuracy: 0.59 - ETA: 1s - loss: 0.6730 - accuracy: 0.59 - ETA: 1s - loss: 0.6724 - accuracy: 0.59 - ETA: 1s - loss: 0.6715 - accuracy: 0.59 - ETA: 1s - loss: 0.6712 - accuracy: 0.60 - ETA: 1s - loss: 0.6711 - accuracy: 0.59 - ETA: 1s - loss: 0.6707 - accuracy: 0.60 - ETA: 1s - loss: 0.6708 - accuracy: 0.60 - ETA: 0s - loss: 0.6703 - accuracy: 0.60 - ETA: 0s - loss: 0.6698 - accuracy: 0.60 - ETA: 0s - loss: 0.6691 - accuracy: 0.60 - ETA: 0s - loss: 0.6688 - accuracy: 0.60 - ETA: 0s - loss: 0.6678 - accuracy: 0.60 - ETA: 0s - loss: 0.6676 - accuracy: 0.61 - ETA: 0s - loss: 0.6667 - accuracy: 0.61 - ETA: 0s - loss: 0.6662 - accuracy: 0.61 - ETA: 0s - loss: 0.6656 - accuracy: 0.61 - ETA: 0s - loss: 0.6651 - accuracy: 0.61 - ETA: 0s - loss: 0.6645 - accuracy: 0.61 - ETA: 0s - loss: 0.6641 - accuracy: 0.61 - ETA: 0s - loss: 0.6640 - accuracy: 0.61 - ETA: 0s - loss: 0.6637 - accuracy: 0.61 - ETA: 0s - loss: 0.6631 - accuracy: 0.61 - ETA: 0s - loss: 0.6624 - accuracy: 0.62 - ETA: 0s - loss: 0.6626 - accuracy: 0.61 - ETA: 0s - loss: 0.6621 - accuracy: 0.62 - ETA: 0s - loss: 0.6618 - accuracy: 0.62 - 3s 260us/step - loss: 0.6617 - accuracy: 0.6217 - val_loss: 0.6250 - val_accuracy: 0.7053\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:10 - loss: 0.6967 - accuracy: 0.50 - ETA: 8s - loss: 0.6956 - accuracy: 0.5046 - ETA: 5s - loss: 0.6957 - accuracy: 0.47 - ETA: 4s - loss: 0.6937 - accuracy: 0.52 - ETA: 4s - loss: 0.6933 - accuracy: 0.53 - ETA: 3s - loss: 0.6916 - accuracy: 0.53 - ETA: 3s - loss: 0.6910 - accuracy: 0.53 - ETA: 3s - loss: 0.6893 - accuracy: 0.55 - ETA: 3s - loss: 0.6890 - accuracy: 0.54 - ETA: 3s - loss: 0.6888 - accuracy: 0.54 - ETA: 3s - loss: 0.6878 - accuracy: 0.55 - ETA: 2s - loss: 0.6863 - accuracy: 0.56 - ETA: 2s - loss: 0.6848 - accuracy: 0.56 - ETA: 2s - loss: 0.6834 - accuracy: 0.57 - ETA: 2s - loss: 0.6831 - accuracy: 0.57 - ETA: 2s - loss: 0.6826 - accuracy: 0.57 - ETA: 2s - loss: 0.6819 - accuracy: 0.57 - ETA: 2s - loss: 0.6816 - accuracy: 0.57 - ETA: 2s - loss: 0.6808 - accuracy: 0.58 - ETA: 2s - loss: 0.6795 - accuracy: 0.58 - ETA: 2s - loss: 0.6788 - accuracy: 0.58 - ETA: 2s - loss: 0.6783 - accuracy: 0.58 - ETA: 2s - loss: 0.6781 - accuracy: 0.58 - ETA: 2s - loss: 0.6775 - accuracy: 0.59 - ETA: 1s - loss: 0.6767 - accuracy: 0.59 - ETA: 1s - loss: 0.6762 - accuracy: 0.59 - ETA: 1s - loss: 0.6750 - accuracy: 0.59 - ETA: 1s - loss: 0.6743 - accuracy: 0.60 - ETA: 1s - loss: 0.6734 - accuracy: 0.60 - ETA: 1s - loss: 0.6727 - accuracy: 0.60 - ETA: 1s - loss: 0.6721 - accuracy: 0.60 - ETA: 1s - loss: 0.6714 - accuracy: 0.60 - ETA: 1s - loss: 0.6707 - accuracy: 0.60 - ETA: 1s - loss: 0.6700 - accuracy: 0.60 - ETA: 1s - loss: 0.6695 - accuracy: 0.60 - ETA: 1s - loss: 0.6687 - accuracy: 0.61 - ETA: 1s - loss: 0.6682 - accuracy: 0.61 - ETA: 1s - loss: 0.6675 - accuracy: 0.61 - ETA: 1s - loss: 0.6669 - accuracy: 0.61 - ETA: 1s - loss: 0.6666 - accuracy: 0.61 - ETA: 1s - loss: 0.6659 - accuracy: 0.61 - ETA: 0s - loss: 0.6654 - accuracy: 0.61 - ETA: 0s - loss: 0.6650 - accuracy: 0.61 - ETA: 0s - loss: 0.6642 - accuracy: 0.62 - ETA: 0s - loss: 0.6636 - accuracy: 0.62 - ETA: 0s - loss: 0.6627 - accuracy: 0.62 - ETA: 0s - loss: 0.6623 - accuracy: 0.62 - ETA: 0s - loss: 0.6616 - accuracy: 0.62 - ETA: 0s - loss: 0.6612 - accuracy: 0.62 - ETA: 0s - loss: 0.6605 - accuracy: 0.62 - ETA: 0s - loss: 0.6597 - accuracy: 0.62 - ETA: 0s - loss: 0.6590 - accuracy: 0.62 - ETA: 0s - loss: 0.6584 - accuracy: 0.63 - ETA: 0s - loss: 0.6581 - accuracy: 0.63 - ETA: 0s - loss: 0.6577 - accuracy: 0.63 - ETA: 0s - loss: 0.6577 - accuracy: 0.63 - ETA: 0s - loss: 0.6572 - accuracy: 0.63 - ETA: 0s - loss: 0.6570 - accuracy: 0.63 - ETA: 0s - loss: 0.6566 - accuracy: 0.63 - ETA: 0s - loss: 0.6565 - accuracy: 0.63 - ETA: 0s - loss: 0.6560 - accuracy: 0.63 - 3s 264us/step - loss: 0.6559 - accuracy: 0.6341 - val_loss: 0.6193 - val_accuracy: 0.7109\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 110us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 5:07 - loss: 0.6923 - accuracy: 0.75 - ETA: 8s - loss: 0.6942 - accuracy: 0.4953 - ETA: 5s - loss: 0.6947 - accuracy: 0.49 - ETA: 4s - loss: 0.6939 - accuracy: 0.50 - ETA: 4s - loss: 0.6932 - accuracy: 0.51 - ETA: 3s - loss: 0.6924 - accuracy: 0.53 - ETA: 3s - loss: 0.6914 - accuracy: 0.54 - ETA: 3s - loss: 0.6912 - accuracy: 0.54 - ETA: 3s - loss: 0.6903 - accuracy: 0.55 - ETA: 3s - loss: 0.6898 - accuracy: 0.55 - ETA: 3s - loss: 0.6890 - accuracy: 0.56 - ETA: 3s - loss: 0.6887 - accuracy: 0.56 - ETA: 2s - loss: 0.6872 - accuracy: 0.56 - ETA: 2s - loss: 0.6867 - accuracy: 0.57 - ETA: 2s - loss: 0.6864 - accuracy: 0.57 - ETA: 2s - loss: 0.6861 - accuracy: 0.57 - ETA: 2s - loss: 0.6859 - accuracy: 0.57 - ETA: 2s - loss: 0.6858 - accuracy: 0.57 - ETA: 2s - loss: 0.6856 - accuracy: 0.57 - ETA: 2s - loss: 0.6848 - accuracy: 0.57 - ETA: 2s - loss: 0.6843 - accuracy: 0.57 - ETA: 2s - loss: 0.6841 - accuracy: 0.57 - ETA: 2s - loss: 0.6833 - accuracy: 0.57 - ETA: 1s - loss: 0.6825 - accuracy: 0.58 - ETA: 1s - loss: 0.6822 - accuracy: 0.58 - ETA: 1s - loss: 0.6817 - accuracy: 0.58 - ETA: 1s - loss: 0.6810 - accuracy: 0.58 - ETA: 1s - loss: 0.6803 - accuracy: 0.58 - ETA: 1s - loss: 0.6800 - accuracy: 0.58 - ETA: 1s - loss: 0.6791 - accuracy: 0.59 - ETA: 1s - loss: 0.6784 - accuracy: 0.59 - ETA: 1s - loss: 0.6778 - accuracy: 0.59 - ETA: 1s - loss: 0.6781 - accuracy: 0.59 - ETA: 1s - loss: 0.6773 - accuracy: 0.59 - ETA: 1s - loss: 0.6773 - accuracy: 0.59 - ETA: 1s - loss: 0.6768 - accuracy: 0.59 - ETA: 1s - loss: 0.6764 - accuracy: 0.59 - ETA: 1s - loss: 0.6761 - accuracy: 0.59 - ETA: 1s - loss: 0.6760 - accuracy: 0.59 - ETA: 1s - loss: 0.6758 - accuracy: 0.60 - ETA: 1s - loss: 0.6750 - accuracy: 0.60 - ETA: 0s - loss: 0.6744 - accuracy: 0.60 - ETA: 0s - loss: 0.6744 - accuracy: 0.60 - ETA: 0s - loss: 0.6742 - accuracy: 0.60 - ETA: 0s - loss: 0.6738 - accuracy: 0.60 - ETA: 0s - loss: 0.6732 - accuracy: 0.60 - ETA: 0s - loss: 0.6730 - accuracy: 0.60 - ETA: 0s - loss: 0.6725 - accuracy: 0.60 - ETA: 0s - loss: 0.6722 - accuracy: 0.60 - ETA: 0s - loss: 0.6713 - accuracy: 0.61 - ETA: 0s - loss: 0.6713 - accuracy: 0.60 - ETA: 0s - loss: 0.6708 - accuracy: 0.61 - ETA: 0s - loss: 0.6702 - accuracy: 0.61 - ETA: 0s - loss: 0.6693 - accuracy: 0.61 - ETA: 0s - loss: 0.6689 - accuracy: 0.61 - ETA: 0s - loss: 0.6686 - accuracy: 0.61 - ETA: 0s - loss: 0.6680 - accuracy: 0.61 - ETA: 0s - loss: 0.6675 - accuracy: 0.61 - ETA: 0s - loss: 0.6670 - accuracy: 0.61 - ETA: 0s - loss: 0.6669 - accuracy: 0.61 - 3s 260us/step - loss: 0.6670 - accuracy: 0.6172 - val_loss: 0.6341 - val_accuracy: 0.7138\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 99us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 5:04 - loss: 0.6983 - accuracy: 0.0000e+ - ETA: 8s - loss: 0.6949 - accuracy: 0.4769     - ETA: 5s - loss: 0.6936 - accuracy: 0.49 - ETA: 4s - loss: 0.6914 - accuracy: 0.51 - ETA: 4s - loss: 0.6901 - accuracy: 0.52 - ETA: 3s - loss: 0.6874 - accuracy: 0.54 - ETA: 3s - loss: 0.6868 - accuracy: 0.54 - ETA: 3s - loss: 0.6868 - accuracy: 0.53 - ETA: 3s - loss: 0.6867 - accuracy: 0.54 - ETA: 3s - loss: 0.6868 - accuracy: 0.54 - ETA: 2s - loss: 0.6864 - accuracy: 0.55 - ETA: 2s - loss: 0.6851 - accuracy: 0.55 - ETA: 2s - loss: 0.6847 - accuracy: 0.55 - ETA: 2s - loss: 0.6837 - accuracy: 0.56 - ETA: 2s - loss: 0.6832 - accuracy: 0.56 - ETA: 2s - loss: 0.6825 - accuracy: 0.56 - ETA: 2s - loss: 0.6823 - accuracy: 0.57 - ETA: 2s - loss: 0.6819 - accuracy: 0.57 - ETA: 2s - loss: 0.6817 - accuracy: 0.57 - ETA: 2s - loss: 0.6809 - accuracy: 0.57 - ETA: 2s - loss: 0.6803 - accuracy: 0.58 - ETA: 2s - loss: 0.6797 - accuracy: 0.58 - ETA: 2s - loss: 0.6797 - accuracy: 0.58 - ETA: 1s - loss: 0.6789 - accuracy: 0.58 - ETA: 1s - loss: 0.6783 - accuracy: 0.58 - ETA: 1s - loss: 0.6779 - accuracy: 0.59 - ETA: 1s - loss: 0.6770 - accuracy: 0.59 - ETA: 1s - loss: 0.6765 - accuracy: 0.59 - ETA: 1s - loss: 0.6761 - accuracy: 0.59 - ETA: 1s - loss: 0.6758 - accuracy: 0.59 - ETA: 1s - loss: 0.6752 - accuracy: 0.59 - ETA: 1s - loss: 0.6745 - accuracy: 0.60 - ETA: 1s - loss: 0.6743 - accuracy: 0.60 - ETA: 1s - loss: 0.6741 - accuracy: 0.60 - ETA: 1s - loss: 0.6738 - accuracy: 0.60 - ETA: 1s - loss: 0.6732 - accuracy: 0.60 - ETA: 1s - loss: 0.6726 - accuracy: 0.60 - ETA: 1s - loss: 0.6727 - accuracy: 0.60 - ETA: 1s - loss: 0.6723 - accuracy: 0.60 - ETA: 1s - loss: 0.6717 - accuracy: 0.60 - ETA: 1s - loss: 0.6714 - accuracy: 0.60 - ETA: 0s - loss: 0.6713 - accuracy: 0.60 - ETA: 0s - loss: 0.6713 - accuracy: 0.60 - ETA: 0s - loss: 0.6709 - accuracy: 0.60 - ETA: 0s - loss: 0.6706 - accuracy: 0.61 - ETA: 0s - loss: 0.6700 - accuracy: 0.61 - ETA: 0s - loss: 0.6695 - accuracy: 0.61 - ETA: 0s - loss: 0.6689 - accuracy: 0.61 - ETA: 0s - loss: 0.6687 - accuracy: 0.61 - ETA: 0s - loss: 0.6683 - accuracy: 0.61 - ETA: 0s - loss: 0.6678 - accuracy: 0.61 - ETA: 0s - loss: 0.6670 - accuracy: 0.61 - ETA: 0s - loss: 0.6667 - accuracy: 0.62 - ETA: 0s - loss: 0.6661 - accuracy: 0.62 - ETA: 0s - loss: 0.6659 - accuracy: 0.62 - ETA: 0s - loss: 0.6655 - accuracy: 0.62 - ETA: 0s - loss: 0.6652 - accuracy: 0.62 - ETA: 0s - loss: 0.6644 - accuracy: 0.62 - ETA: 0s - loss: 0.6642 - accuracy: 0.62 - ETA: 0s - loss: 0.6638 - accuracy: 0.62 - 3s 261us/step - loss: 0.6635 - accuracy: 0.6255 - val_loss: 0.6377 - val_accuracy: 0.7095\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:13 - loss: 0.6933 - accuracy: 0.75 - ETA: 9s - loss: 0.6939 - accuracy: 0.5189 - ETA: 6s - loss: 0.6939 - accuracy: 0.50 - ETA: 5s - loss: 0.6937 - accuracy: 0.51 - ETA: 4s - loss: 0.6934 - accuracy: 0.51 - ETA: 4s - loss: 0.6933 - accuracy: 0.51 - ETA: 3s - loss: 0.6931 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.50 - ETA: 3s - loss: 0.6926 - accuracy: 0.50 - ETA: 3s - loss: 0.6925 - accuracy: 0.51 - ETA: 3s - loss: 0.6922 - accuracy: 0.51 - ETA: 3s - loss: 0.6919 - accuracy: 0.51 - ETA: 2s - loss: 0.6915 - accuracy: 0.51 - ETA: 2s - loss: 0.6915 - accuracy: 0.51 - ETA: 2s - loss: 0.6907 - accuracy: 0.52 - ETA: 2s - loss: 0.6891 - accuracy: 0.52 - ETA: 2s - loss: 0.6882 - accuracy: 0.52 - ETA: 2s - loss: 0.6876 - accuracy: 0.52 - ETA: 2s - loss: 0.6869 - accuracy: 0.53 - ETA: 2s - loss: 0.6867 - accuracy: 0.53 - ETA: 2s - loss: 0.6865 - accuracy: 0.53 - ETA: 2s - loss: 0.6860 - accuracy: 0.53 - ETA: 2s - loss: 0.6854 - accuracy: 0.54 - ETA: 2s - loss: 0.6845 - accuracy: 0.54 - ETA: 2s - loss: 0.6837 - accuracy: 0.54 - ETA: 2s - loss: 0.6828 - accuracy: 0.54 - ETA: 1s - loss: 0.6819 - accuracy: 0.55 - ETA: 1s - loss: 0.6812 - accuracy: 0.55 - ETA: 1s - loss: 0.6809 - accuracy: 0.55 - ETA: 1s - loss: 0.6802 - accuracy: 0.55 - ETA: 1s - loss: 0.6790 - accuracy: 0.56 - ETA: 1s - loss: 0.6775 - accuracy: 0.56 - ETA: 1s - loss: 0.6770 - accuracy: 0.56 - ETA: 1s - loss: 0.6764 - accuracy: 0.56 - ETA: 1s - loss: 0.6758 - accuracy: 0.56 - ETA: 1s - loss: 0.6759 - accuracy: 0.56 - ETA: 1s - loss: 0.6750 - accuracy: 0.56 - ETA: 1s - loss: 0.6746 - accuracy: 0.56 - ETA: 1s - loss: 0.6741 - accuracy: 0.57 - ETA: 1s - loss: 0.6730 - accuracy: 0.57 - ETA: 1s - loss: 0.6726 - accuracy: 0.57 - ETA: 1s - loss: 0.6719 - accuracy: 0.57 - ETA: 1s - loss: 0.6714 - accuracy: 0.57 - ETA: 1s - loss: 0.6706 - accuracy: 0.57 - ETA: 0s - loss: 0.6699 - accuracy: 0.58 - ETA: 0s - loss: 0.6694 - accuracy: 0.58 - ETA: 0s - loss: 0.6692 - accuracy: 0.58 - ETA: 0s - loss: 0.6689 - accuracy: 0.58 - ETA: 0s - loss: 0.6678 - accuracy: 0.58 - ETA: 0s - loss: 0.6669 - accuracy: 0.58 - ETA: 0s - loss: 0.6667 - accuracy: 0.58 - ETA: 0s - loss: 0.6661 - accuracy: 0.58 - ETA: 0s - loss: 0.6658 - accuracy: 0.58 - ETA: 0s - loss: 0.6652 - accuracy: 0.58 - ETA: 0s - loss: 0.6649 - accuracy: 0.58 - ETA: 0s - loss: 0.6639 - accuracy: 0.59 - ETA: 0s - loss: 0.6639 - accuracy: 0.59 - ETA: 0s - loss: 0.6639 - accuracy: 0.59 - ETA: 0s - loss: 0.6632 - accuracy: 0.59 - ETA: 0s - loss: 0.6624 - accuracy: 0.59 - ETA: 0s - loss: 0.6622 - accuracy: 0.59 - ETA: 0s - loss: 0.6619 - accuracy: 0.59 - ETA: 0s - loss: 0.6620 - accuracy: 0.59 - 4s 279us/step - loss: 0.6616 - accuracy: 0.5958 - val_loss: 0.6108 - val_accuracy: 0.7088\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 101us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:20 - loss: 0.6979 - accuracy: 0.50 - ETA: 10s - loss: 0.6923 - accuracy: 0.5490 - ETA: 6s - loss: 0.6916 - accuracy: 0.532 - ETA: 5s - loss: 0.6915 - accuracy: 0.54 - ETA: 4s - loss: 0.6912 - accuracy: 0.53 - ETA: 4s - loss: 0.6920 - accuracy: 0.53 - ETA: 3s - loss: 0.6918 - accuracy: 0.53 - ETA: 3s - loss: 0.6920 - accuracy: 0.53 - ETA: 3s - loss: 0.6913 - accuracy: 0.53 - ETA: 3s - loss: 0.6915 - accuracy: 0.52 - ETA: 3s - loss: 0.6911 - accuracy: 0.52 - ETA: 3s - loss: 0.6907 - accuracy: 0.52 - ETA: 3s - loss: 0.6903 - accuracy: 0.52 - ETA: 2s - loss: 0.6900 - accuracy: 0.52 - ETA: 2s - loss: 0.6888 - accuracy: 0.53 - ETA: 2s - loss: 0.6886 - accuracy: 0.53 - ETA: 2s - loss: 0.6877 - accuracy: 0.53 - ETA: 2s - loss: 0.6871 - accuracy: 0.53 - ETA: 2s - loss: 0.6869 - accuracy: 0.53 - ETA: 2s - loss: 0.6866 - accuracy: 0.53 - ETA: 2s - loss: 0.6859 - accuracy: 0.54 - ETA: 2s - loss: 0.6855 - accuracy: 0.54 - ETA: 2s - loss: 0.6851 - accuracy: 0.54 - ETA: 2s - loss: 0.6842 - accuracy: 0.54 - ETA: 2s - loss: 0.6840 - accuracy: 0.54 - ETA: 2s - loss: 0.6834 - accuracy: 0.54 - ETA: 2s - loss: 0.6823 - accuracy: 0.54 - ETA: 1s - loss: 0.6816 - accuracy: 0.54 - ETA: 1s - loss: 0.6812 - accuracy: 0.54 - ETA: 1s - loss: 0.6801 - accuracy: 0.54 - ETA: 1s - loss: 0.6790 - accuracy: 0.55 - ETA: 1s - loss: 0.6779 - accuracy: 0.55 - ETA: 1s - loss: 0.6762 - accuracy: 0.55 - ETA: 1s - loss: 0.6754 - accuracy: 0.55 - ETA: 1s - loss: 0.6750 - accuracy: 0.55 - ETA: 1s - loss: 0.6738 - accuracy: 0.55 - ETA: 1s - loss: 0.6732 - accuracy: 0.55 - ETA: 1s - loss: 0.6726 - accuracy: 0.55 - ETA: 1s - loss: 0.6713 - accuracy: 0.56 - ETA: 1s - loss: 0.6702 - accuracy: 0.56 - ETA: 1s - loss: 0.6702 - accuracy: 0.56 - ETA: 1s - loss: 0.6701 - accuracy: 0.56 - ETA: 1s - loss: 0.6698 - accuracy: 0.56 - ETA: 1s - loss: 0.6694 - accuracy: 0.56 - ETA: 0s - loss: 0.6680 - accuracy: 0.56 - ETA: 0s - loss: 0.6673 - accuracy: 0.56 - ETA: 0s - loss: 0.6679 - accuracy: 0.56 - ETA: 0s - loss: 0.6674 - accuracy: 0.56 - ETA: 0s - loss: 0.6670 - accuracy: 0.56 - ETA: 0s - loss: 0.6669 - accuracy: 0.56 - ETA: 0s - loss: 0.6663 - accuracy: 0.56 - ETA: 0s - loss: 0.6660 - accuracy: 0.56 - ETA: 0s - loss: 0.6655 - accuracy: 0.57 - ETA: 0s - loss: 0.6649 - accuracy: 0.57 - ETA: 0s - loss: 0.6650 - accuracy: 0.57 - ETA: 0s - loss: 0.6651 - accuracy: 0.57 - ETA: 0s - loss: 0.6652 - accuracy: 0.57 - ETA: 0s - loss: 0.6646 - accuracy: 0.57 - ETA: 0s - loss: 0.6645 - accuracy: 0.57 - ETA: 0s - loss: 0.6643 - accuracy: 0.57 - ETA: 0s - loss: 0.6637 - accuracy: 0.57 - ETA: 0s - loss: 0.6638 - accuracy: 0.57 - ETA: 0s - loss: 0.6636 - accuracy: 0.57 - 3s 276us/step - loss: 0.6634 - accuracy: 0.5730 - val_loss: 0.6222 - val_accuracy: 0.7273\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 106us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:45 - loss: 0.7004 - accuracy: 0.25 - ETA: 11s - loss: 0.6945 - accuracy: 0.5050 - ETA: 6s - loss: 0.6942 - accuracy: 0.509 - ETA: 5s - loss: 0.6939 - accuracy: 0.50 - ETA: 4s - loss: 0.6931 - accuracy: 0.51 - ETA: 4s - loss: 0.6934 - accuracy: 0.50 - ETA: 4s - loss: 0.6932 - accuracy: 0.50 - ETA: 3s - loss: 0.6931 - accuracy: 0.50 - ETA: 3s - loss: 0.6925 - accuracy: 0.51 - ETA: 3s - loss: 0.6918 - accuracy: 0.51 - ETA: 3s - loss: 0.6917 - accuracy: 0.51 - ETA: 3s - loss: 0.6913 - accuracy: 0.51 - ETA: 3s - loss: 0.6908 - accuracy: 0.51 - ETA: 3s - loss: 0.6905 - accuracy: 0.51 - ETA: 3s - loss: 0.6904 - accuracy: 0.51 - ETA: 3s - loss: 0.6900 - accuracy: 0.51 - ETA: 3s - loss: 0.6881 - accuracy: 0.52 - ETA: 3s - loss: 0.6874 - accuracy: 0.52 - ETA: 3s - loss: 0.6873 - accuracy: 0.52 - ETA: 3s - loss: 0.6870 - accuracy: 0.52 - ETA: 2s - loss: 0.6862 - accuracy: 0.52 - ETA: 2s - loss: 0.6854 - accuracy: 0.52 - ETA: 2s - loss: 0.6843 - accuracy: 0.53 - ETA: 2s - loss: 0.6842 - accuracy: 0.53 - ETA: 2s - loss: 0.6843 - accuracy: 0.53 - ETA: 2s - loss: 0.6840 - accuracy: 0.53 - ETA: 2s - loss: 0.6837 - accuracy: 0.53 - ETA: 2s - loss: 0.6832 - accuracy: 0.53 - ETA: 2s - loss: 0.6821 - accuracy: 0.53 - ETA: 2s - loss: 0.6821 - accuracy: 0.53 - ETA: 2s - loss: 0.6813 - accuracy: 0.53 - ETA: 2s - loss: 0.6799 - accuracy: 0.54 - ETA: 2s - loss: 0.6792 - accuracy: 0.54 - ETA: 1s - loss: 0.6786 - accuracy: 0.54 - ETA: 1s - loss: 0.6782 - accuracy: 0.54 - ETA: 1s - loss: 0.6778 - accuracy: 0.54 - ETA: 1s - loss: 0.6768 - accuracy: 0.54 - ETA: 1s - loss: 0.6763 - accuracy: 0.54 - ETA: 1s - loss: 0.6755 - accuracy: 0.54 - ETA: 1s - loss: 0.6748 - accuracy: 0.54 - ETA: 1s - loss: 0.6745 - accuracy: 0.54 - ETA: 1s - loss: 0.6740 - accuracy: 0.54 - ETA: 1s - loss: 0.6741 - accuracy: 0.54 - ETA: 1s - loss: 0.6733 - accuracy: 0.55 - ETA: 1s - loss: 0.6728 - accuracy: 0.55 - ETA: 1s - loss: 0.6725 - accuracy: 0.55 - ETA: 1s - loss: 0.6722 - accuracy: 0.55 - ETA: 1s - loss: 0.6717 - accuracy: 0.55 - ETA: 1s - loss: 0.6710 - accuracy: 0.55 - ETA: 0s - loss: 0.6705 - accuracy: 0.55 - ETA: 0s - loss: 0.6699 - accuracy: 0.55 - ETA: 0s - loss: 0.6695 - accuracy: 0.55 - ETA: 0s - loss: 0.6694 - accuracy: 0.55 - ETA: 0s - loss: 0.6694 - accuracy: 0.55 - ETA: 0s - loss: 0.6693 - accuracy: 0.55 - ETA: 0s - loss: 0.6692 - accuracy: 0.55 - ETA: 0s - loss: 0.6689 - accuracy: 0.55 - ETA: 0s - loss: 0.6687 - accuracy: 0.55 - ETA: 0s - loss: 0.6681 - accuracy: 0.55 - ETA: 0s - loss: 0.6677 - accuracy: 0.55 - ETA: 0s - loss: 0.6677 - accuracy: 0.55 - ETA: 0s - loss: 0.6669 - accuracy: 0.55 - ETA: 0s - loss: 0.6667 - accuracy: 0.55 - ETA: 0s - loss: 0.6663 - accuracy: 0.55 - ETA: 0s - loss: 0.6658 - accuracy: 0.56 - ETA: 0s - loss: 0.6652 - accuracy: 0.56 - ETA: 0s - loss: 0.6652 - accuracy: 0.56 - 4s 292us/step - loss: 0.6649 - accuracy: 0.5616 - val_loss: 0.6218 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 106us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 10:49 - loss: 0.6996 - accuracy: 0.250 - ETA: 24s - loss: 0.6938 - accuracy: 0.5000  - ETA: 13s - loss: 0.6936 - accuracy: 0.517 - ETA: 9s - loss: 0.6933 - accuracy: 0.521 - ETA: 8s - loss: 0.6937 - accuracy: 0.51 - ETA: 6s - loss: 0.6936 - accuracy: 0.51 - ETA: 6s - loss: 0.6931 - accuracy: 0.51 - ETA: 5s - loss: 0.6932 - accuracy: 0.51 - ETA: 5s - loss: 0.6929 - accuracy: 0.52 - ETA: 5s - loss: 0.6924 - accuracy: 0.52 - ETA: 4s - loss: 0.6919 - accuracy: 0.53 - ETA: 4s - loss: 0.6914 - accuracy: 0.53 - ETA: 4s - loss: 0.6913 - accuracy: 0.53 - ETA: 4s - loss: 0.6912 - accuracy: 0.53 - ETA: 3s - loss: 0.6906 - accuracy: 0.54 - ETA: 3s - loss: 0.6903 - accuracy: 0.54 - ETA: 3s - loss: 0.6900 - accuracy: 0.54 - ETA: 3s - loss: 0.6890 - accuracy: 0.54 - ETA: 3s - loss: 0.6882 - accuracy: 0.54 - ETA: 3s - loss: 0.6878 - accuracy: 0.55 - ETA: 3s - loss: 0.6877 - accuracy: 0.54 - ETA: 3s - loss: 0.6873 - accuracy: 0.55 - ETA: 3s - loss: 0.6865 - accuracy: 0.55 - ETA: 3s - loss: 0.6853 - accuracy: 0.55 - ETA: 3s - loss: 0.6844 - accuracy: 0.55 - ETA: 2s - loss: 0.6838 - accuracy: 0.56 - ETA: 2s - loss: 0.6822 - accuracy: 0.56 - ETA: 2s - loss: 0.6799 - accuracy: 0.56 - ETA: 2s - loss: 0.6790 - accuracy: 0.56 - ETA: 2s - loss: 0.6779 - accuracy: 0.56 - ETA: 2s - loss: 0.6772 - accuracy: 0.57 - ETA: 2s - loss: 0.6763 - accuracy: 0.57 - ETA: 2s - loss: 0.6756 - accuracy: 0.57 - ETA: 2s - loss: 0.6747 - accuracy: 0.57 - ETA: 2s - loss: 0.6736 - accuracy: 0.57 - ETA: 2s - loss: 0.6728 - accuracy: 0.57 - ETA: 2s - loss: 0.6711 - accuracy: 0.58 - ETA: 2s - loss: 0.6701 - accuracy: 0.58 - ETA: 2s - loss: 0.6692 - accuracy: 0.58 - ETA: 1s - loss: 0.6694 - accuracy: 0.58 - ETA: 1s - loss: 0.6690 - accuracy: 0.58 - ETA: 1s - loss: 0.6678 - accuracy: 0.58 - ETA: 1s - loss: 0.6669 - accuracy: 0.58 - ETA: 1s - loss: 0.6661 - accuracy: 0.58 - ETA: 1s - loss: 0.6648 - accuracy: 0.59 - ETA: 1s - loss: 0.6640 - accuracy: 0.59 - ETA: 1s - loss: 0.6630 - accuracy: 0.59 - ETA: 1s - loss: 0.6619 - accuracy: 0.59 - ETA: 1s - loss: 0.6614 - accuracy: 0.59 - ETA: 1s - loss: 0.6595 - accuracy: 0.59 - ETA: 1s - loss: 0.6593 - accuracy: 0.59 - ETA: 1s - loss: 0.6584 - accuracy: 0.59 - ETA: 0s - loss: 0.6576 - accuracy: 0.59 - ETA: 0s - loss: 0.6563 - accuracy: 0.59 - ETA: 0s - loss: 0.6560 - accuracy: 0.59 - ETA: 0s - loss: 0.6557 - accuracy: 0.59 - ETA: 0s - loss: 0.6550 - accuracy: 0.60 - ETA: 0s - loss: 0.6543 - accuracy: 0.60 - ETA: 0s - loss: 0.6540 - accuracy: 0.60 - ETA: 0s - loss: 0.6532 - accuracy: 0.60 - ETA: 0s - loss: 0.6534 - accuracy: 0.60 - ETA: 0s - loss: 0.6533 - accuracy: 0.60 - ETA: 0s - loss: 0.6531 - accuracy: 0.60 - ETA: 0s - loss: 0.6523 - accuracy: 0.60 - ETA: 0s - loss: 0.6519 - accuracy: 0.60 - ETA: 0s - loss: 0.6513 - accuracy: 0.60 - ETA: 0s - loss: 0.6507 - accuracy: 0.60 - ETA: 0s - loss: 0.6500 - accuracy: 0.60 - ETA: 0s - loss: 0.6492 - accuracy: 0.60 - 4s 304us/step - loss: 0.6492 - accuracy: 0.6082 - val_loss: 0.5925 - val_accuracy: 0.7294\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 117us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:42 - loss: 0.6932 - accuracy: 0.50 - ETA: 10s - loss: 0.6938 - accuracy: 0.4904 - ETA: 6s - loss: 0.6939 - accuracy: 0.482 - ETA: 5s - loss: 0.6938 - accuracy: 0.49 - ETA: 4s - loss: 0.6936 - accuracy: 0.50 - ETA: 4s - loss: 0.6931 - accuracy: 0.51 - ETA: 4s - loss: 0.6929 - accuracy: 0.51 - ETA: 3s - loss: 0.6922 - accuracy: 0.52 - ETA: 3s - loss: 0.6919 - accuracy: 0.52 - ETA: 3s - loss: 0.6921 - accuracy: 0.52 - ETA: 3s - loss: 0.6913 - accuracy: 0.52 - ETA: 3s - loss: 0.6913 - accuracy: 0.52 - ETA: 3s - loss: 0.6909 - accuracy: 0.52 - ETA: 3s - loss: 0.6905 - accuracy: 0.52 - ETA: 2s - loss: 0.6907 - accuracy: 0.52 - ETA: 2s - loss: 0.6902 - accuracy: 0.52 - ETA: 2s - loss: 0.6894 - accuracy: 0.52 - ETA: 2s - loss: 0.6886 - accuracy: 0.53 - ETA: 2s - loss: 0.6879 - accuracy: 0.53 - ETA: 2s - loss: 0.6872 - accuracy: 0.53 - ETA: 2s - loss: 0.6864 - accuracy: 0.53 - ETA: 2s - loss: 0.6856 - accuracy: 0.54 - ETA: 2s - loss: 0.6844 - accuracy: 0.54 - ETA: 2s - loss: 0.6838 - accuracy: 0.54 - ETA: 2s - loss: 0.6821 - accuracy: 0.54 - ETA: 2s - loss: 0.6808 - accuracy: 0.55 - ETA: 2s - loss: 0.6804 - accuracy: 0.55 - ETA: 2s - loss: 0.6799 - accuracy: 0.55 - ETA: 1s - loss: 0.6796 - accuracy: 0.55 - ETA: 1s - loss: 0.6789 - accuracy: 0.55 - ETA: 1s - loss: 0.6787 - accuracy: 0.55 - ETA: 1s - loss: 0.6774 - accuracy: 0.55 - ETA: 1s - loss: 0.6762 - accuracy: 0.55 - ETA: 1s - loss: 0.6757 - accuracy: 0.55 - ETA: 1s - loss: 0.6741 - accuracy: 0.56 - ETA: 1s - loss: 0.6731 - accuracy: 0.56 - ETA: 1s - loss: 0.6728 - accuracy: 0.56 - ETA: 1s - loss: 0.6725 - accuracy: 0.56 - ETA: 1s - loss: 0.6724 - accuracy: 0.56 - ETA: 1s - loss: 0.6711 - accuracy: 0.56 - ETA: 1s - loss: 0.6703 - accuracy: 0.56 - ETA: 1s - loss: 0.6703 - accuracy: 0.56 - ETA: 1s - loss: 0.6696 - accuracy: 0.56 - ETA: 1s - loss: 0.6687 - accuracy: 0.57 - ETA: 1s - loss: 0.6681 - accuracy: 0.57 - ETA: 0s - loss: 0.6680 - accuracy: 0.57 - ETA: 0s - loss: 0.6673 - accuracy: 0.57 - ETA: 0s - loss: 0.6667 - accuracy: 0.57 - ETA: 0s - loss: 0.6663 - accuracy: 0.57 - ETA: 0s - loss: 0.6659 - accuracy: 0.57 - ETA: 0s - loss: 0.6658 - accuracy: 0.57 - ETA: 0s - loss: 0.6658 - accuracy: 0.57 - ETA: 0s - loss: 0.6652 - accuracy: 0.58 - ETA: 0s - loss: 0.6640 - accuracy: 0.58 - ETA: 0s - loss: 0.6637 - accuracy: 0.58 - ETA: 0s - loss: 0.6636 - accuracy: 0.58 - ETA: 0s - loss: 0.6630 - accuracy: 0.58 - ETA: 0s - loss: 0.6620 - accuracy: 0.58 - ETA: 0s - loss: 0.6618 - accuracy: 0.58 - ETA: 0s - loss: 0.6616 - accuracy: 0.58 - ETA: 0s - loss: 0.6614 - accuracy: 0.58 - ETA: 0s - loss: 0.6610 - accuracy: 0.58 - ETA: 0s - loss: 0.6602 - accuracy: 0.59 - ETA: 0s - loss: 0.6601 - accuracy: 0.59 - 4s 282us/step - loss: 0.6601 - accuracy: 0.5917 - val_loss: 0.6139 - val_accuracy: 0.7216\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 104us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:20 - loss: 0.6997 - accuracy: 0.25 - ETA: 10s - loss: 0.6940 - accuracy: 0.4657 - ETA: 6s - loss: 0.6939 - accuracy: 0.466 - ETA: 5s - loss: 0.6935 - accuracy: 0.49 - ETA: 4s - loss: 0.6933 - accuracy: 0.50 - ETA: 4s - loss: 0.6924 - accuracy: 0.51 - ETA: 3s - loss: 0.6919 - accuracy: 0.52 - ETA: 3s - loss: 0.6920 - accuracy: 0.52 - ETA: 3s - loss: 0.6916 - accuracy: 0.52 - ETA: 3s - loss: 0.6912 - accuracy: 0.53 - ETA: 3s - loss: 0.6906 - accuracy: 0.53 - ETA: 3s - loss: 0.6899 - accuracy: 0.53 - ETA: 2s - loss: 0.6881 - accuracy: 0.54 - ETA: 2s - loss: 0.6876 - accuracy: 0.53 - ETA: 2s - loss: 0.6865 - accuracy: 0.54 - ETA: 2s - loss: 0.6855 - accuracy: 0.54 - ETA: 2s - loss: 0.6846 - accuracy: 0.54 - ETA: 2s - loss: 0.6831 - accuracy: 0.55 - ETA: 2s - loss: 0.6818 - accuracy: 0.55 - ETA: 2s - loss: 0.6792 - accuracy: 0.56 - ETA: 2s - loss: 0.6782 - accuracy: 0.56 - ETA: 2s - loss: 0.6778 - accuracy: 0.56 - ETA: 2s - loss: 0.6766 - accuracy: 0.57 - ETA: 2s - loss: 0.6760 - accuracy: 0.57 - ETA: 2s - loss: 0.6757 - accuracy: 0.57 - ETA: 1s - loss: 0.6750 - accuracy: 0.57 - ETA: 1s - loss: 0.6742 - accuracy: 0.58 - ETA: 1s - loss: 0.6731 - accuracy: 0.58 - ETA: 1s - loss: 0.6725 - accuracy: 0.58 - ETA: 1s - loss: 0.6718 - accuracy: 0.58 - ETA: 1s - loss: 0.6713 - accuracy: 0.58 - ETA: 1s - loss: 0.6696 - accuracy: 0.59 - ETA: 1s - loss: 0.6683 - accuracy: 0.59 - ETA: 1s - loss: 0.6681 - accuracy: 0.59 - ETA: 1s - loss: 0.6664 - accuracy: 0.59 - ETA: 1s - loss: 0.6649 - accuracy: 0.59 - ETA: 1s - loss: 0.6640 - accuracy: 0.60 - ETA: 1s - loss: 0.6630 - accuracy: 0.60 - ETA: 1s - loss: 0.6628 - accuracy: 0.60 - ETA: 1s - loss: 0.6615 - accuracy: 0.60 - ETA: 1s - loss: 0.6613 - accuracy: 0.60 - ETA: 1s - loss: 0.6603 - accuracy: 0.60 - ETA: 1s - loss: 0.6593 - accuracy: 0.60 - ETA: 0s - loss: 0.6587 - accuracy: 0.61 - ETA: 0s - loss: 0.6579 - accuracy: 0.61 - ETA: 0s - loss: 0.6578 - accuracy: 0.61 - ETA: 0s - loss: 0.6583 - accuracy: 0.61 - ETA: 0s - loss: 0.6585 - accuracy: 0.61 - ETA: 0s - loss: 0.6580 - accuracy: 0.61 - ETA: 0s - loss: 0.6573 - accuracy: 0.61 - ETA: 0s - loss: 0.6565 - accuracy: 0.61 - ETA: 0s - loss: 0.6559 - accuracy: 0.61 - ETA: 0s - loss: 0.6555 - accuracy: 0.61 - ETA: 0s - loss: 0.6553 - accuracy: 0.61 - ETA: 0s - loss: 0.6553 - accuracy: 0.61 - ETA: 0s - loss: 0.6550 - accuracy: 0.61 - ETA: 0s - loss: 0.6547 - accuracy: 0.61 - ETA: 0s - loss: 0.6539 - accuracy: 0.61 - ETA: 0s - loss: 0.6529 - accuracy: 0.62 - ETA: 0s - loss: 0.6523 - accuracy: 0.62 - ETA: 0s - loss: 0.6523 - accuracy: 0.62 - ETA: 0s - loss: 0.6521 - accuracy: 0.62 - ETA: 0s - loss: 0.6518 - accuracy: 0.62 - 3s 276us/step - loss: 0.6515 - accuracy: 0.6225 - val_loss: 0.6088 - val_accuracy: 0.6797\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 102us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:42 - loss: 0.6904 - accuracy: 0.50 - ETA: 10s - loss: 0.6934 - accuracy: 0.5144 - ETA: 7s - loss: 0.6930 - accuracy: 0.522 - ETA: 5s - loss: 0.6930 - accuracy: 0.52 - ETA: 4s - loss: 0.6926 - accuracy: 0.53 - ETA: 4s - loss: 0.6922 - accuracy: 0.53 - ETA: 4s - loss: 0.6911 - accuracy: 0.53 - ETA: 4s - loss: 0.6907 - accuracy: 0.53 - ETA: 3s - loss: 0.6907 - accuracy: 0.53 - ETA: 3s - loss: 0.6901 - accuracy: 0.53 - ETA: 3s - loss: 0.6884 - accuracy: 0.54 - ETA: 3s - loss: 0.6883 - accuracy: 0.53 - ETA: 3s - loss: 0.6868 - accuracy: 0.54 - ETA: 3s - loss: 0.6843 - accuracy: 0.55 - ETA: 3s - loss: 0.6836 - accuracy: 0.55 - ETA: 2s - loss: 0.6818 - accuracy: 0.55 - ETA: 2s - loss: 0.6804 - accuracy: 0.55 - ETA: 2s - loss: 0.6805 - accuracy: 0.55 - ETA: 2s - loss: 0.6802 - accuracy: 0.55 - ETA: 2s - loss: 0.6786 - accuracy: 0.55 - ETA: 2s - loss: 0.6776 - accuracy: 0.55 - ETA: 2s - loss: 0.6769 - accuracy: 0.56 - ETA: 2s - loss: 0.6761 - accuracy: 0.56 - ETA: 2s - loss: 0.6756 - accuracy: 0.56 - ETA: 2s - loss: 0.6753 - accuracy: 0.56 - ETA: 2s - loss: 0.6746 - accuracy: 0.56 - ETA: 2s - loss: 0.6741 - accuracy: 0.56 - ETA: 2s - loss: 0.6737 - accuracy: 0.57 - ETA: 2s - loss: 0.6726 - accuracy: 0.57 - ETA: 1s - loss: 0.6708 - accuracy: 0.57 - ETA: 1s - loss: 0.6704 - accuracy: 0.57 - ETA: 1s - loss: 0.6689 - accuracy: 0.57 - ETA: 1s - loss: 0.6685 - accuracy: 0.57 - ETA: 1s - loss: 0.6687 - accuracy: 0.57 - ETA: 1s - loss: 0.6669 - accuracy: 0.58 - ETA: 1s - loss: 0.6669 - accuracy: 0.58 - ETA: 1s - loss: 0.6654 - accuracy: 0.58 - ETA: 1s - loss: 0.6651 - accuracy: 0.58 - ETA: 1s - loss: 0.6648 - accuracy: 0.58 - ETA: 1s - loss: 0.6648 - accuracy: 0.58 - ETA: 1s - loss: 0.6643 - accuracy: 0.59 - ETA: 1s - loss: 0.6634 - accuracy: 0.59 - ETA: 1s - loss: 0.6621 - accuracy: 0.59 - ETA: 1s - loss: 0.6615 - accuracy: 0.59 - ETA: 1s - loss: 0.6610 - accuracy: 0.59 - ETA: 1s - loss: 0.6603 - accuracy: 0.59 - ETA: 1s - loss: 0.6598 - accuracy: 0.59 - ETA: 0s - loss: 0.6591 - accuracy: 0.59 - ETA: 0s - loss: 0.6581 - accuracy: 0.60 - ETA: 0s - loss: 0.6586 - accuracy: 0.60 - ETA: 0s - loss: 0.6585 - accuracy: 0.60 - ETA: 0s - loss: 0.6581 - accuracy: 0.60 - ETA: 0s - loss: 0.6580 - accuracy: 0.60 - ETA: 0s - loss: 0.6576 - accuracy: 0.60 - ETA: 0s - loss: 0.6569 - accuracy: 0.60 - ETA: 0s - loss: 0.6569 - accuracy: 0.60 - ETA: 0s - loss: 0.6564 - accuracy: 0.60 - ETA: 0s - loss: 0.6562 - accuracy: 0.60 - ETA: 0s - loss: 0.6556 - accuracy: 0.60 - ETA: 0s - loss: 0.6553 - accuracy: 0.60 - ETA: 0s - loss: 0.6551 - accuracy: 0.60 - ETA: 0s - loss: 0.6548 - accuracy: 0.60 - ETA: 0s - loss: 0.6550 - accuracy: 0.60 - ETA: 0s - loss: 0.6540 - accuracy: 0.61 - ETA: 0s - loss: 0.6540 - accuracy: 0.61 - 4s 287us/step - loss: 0.6540 - accuracy: 0.6105 - val_loss: 0.6171 - val_accuracy: 0.7145\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 112us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 6:31 - loss: 0.6968 - accuracy: 0.25 - ETA: 10s - loss: 0.6945 - accuracy: 0.4279 - ETA: 6s - loss: 0.6942 - accuracy: 0.468 - ETA: 5s - loss: 0.6942 - accuracy: 0.46 - ETA: 4s - loss: 0.6940 - accuracy: 0.47 - ETA: 4s - loss: 0.6938 - accuracy: 0.49 - ETA: 4s - loss: 0.6936 - accuracy: 0.49 - ETA: 4s - loss: 0.6935 - accuracy: 0.49 - ETA: 4s - loss: 0.6933 - accuracy: 0.50 - ETA: 3s - loss: 0.6928 - accuracy: 0.51 - ETA: 3s - loss: 0.6923 - accuracy: 0.52 - ETA: 3s - loss: 0.6922 - accuracy: 0.52 - ETA: 3s - loss: 0.6918 - accuracy: 0.52 - ETA: 3s - loss: 0.6914 - accuracy: 0.53 - ETA: 3s - loss: 0.6909 - accuracy: 0.53 - ETA: 3s - loss: 0.6905 - accuracy: 0.53 - ETA: 2s - loss: 0.6899 - accuracy: 0.54 - ETA: 2s - loss: 0.6887 - accuracy: 0.54 - ETA: 2s - loss: 0.6883 - accuracy: 0.54 - ETA: 2s - loss: 0.6880 - accuracy: 0.55 - ETA: 2s - loss: 0.6869 - accuracy: 0.55 - ETA: 2s - loss: 0.6865 - accuracy: 0.55 - ETA: 2s - loss: 0.6857 - accuracy: 0.55 - ETA: 2s - loss: 0.6850 - accuracy: 0.56 - ETA: 2s - loss: 0.6836 - accuracy: 0.56 - ETA: 2s - loss: 0.6834 - accuracy: 0.56 - ETA: 2s - loss: 0.6824 - accuracy: 0.56 - ETA: 2s - loss: 0.6819 - accuracy: 0.56 - ETA: 2s - loss: 0.6812 - accuracy: 0.57 - ETA: 1s - loss: 0.6803 - accuracy: 0.57 - ETA: 1s - loss: 0.6797 - accuracy: 0.57 - ETA: 1s - loss: 0.6788 - accuracy: 0.57 - ETA: 1s - loss: 0.6779 - accuracy: 0.57 - ETA: 1s - loss: 0.6769 - accuracy: 0.58 - ETA: 1s - loss: 0.6756 - accuracy: 0.58 - ETA: 1s - loss: 0.6747 - accuracy: 0.58 - ETA: 1s - loss: 0.6739 - accuracy: 0.58 - ETA: 1s - loss: 0.6731 - accuracy: 0.58 - ETA: 1s - loss: 0.6721 - accuracy: 0.58 - ETA: 1s - loss: 0.6719 - accuracy: 0.58 - ETA: 1s - loss: 0.6701 - accuracy: 0.59 - ETA: 1s - loss: 0.6688 - accuracy: 0.59 - ETA: 1s - loss: 0.6685 - accuracy: 0.59 - ETA: 1s - loss: 0.6677 - accuracy: 0.59 - ETA: 1s - loss: 0.6666 - accuracy: 0.60 - ETA: 1s - loss: 0.6657 - accuracy: 0.60 - ETA: 0s - loss: 0.6647 - accuracy: 0.60 - ETA: 0s - loss: 0.6637 - accuracy: 0.60 - ETA: 0s - loss: 0.6630 - accuracy: 0.60 - ETA: 0s - loss: 0.6626 - accuracy: 0.60 - ETA: 0s - loss: 0.6615 - accuracy: 0.61 - ETA: 0s - loss: 0.6601 - accuracy: 0.61 - ETA: 0s - loss: 0.6594 - accuracy: 0.61 - ETA: 0s - loss: 0.6588 - accuracy: 0.61 - ETA: 0s - loss: 0.6584 - accuracy: 0.61 - ETA: 0s - loss: 0.6579 - accuracy: 0.61 - ETA: 0s - loss: 0.6568 - accuracy: 0.61 - ETA: 0s - loss: 0.6563 - accuracy: 0.61 - ETA: 0s - loss: 0.6558 - accuracy: 0.61 - ETA: 0s - loss: 0.6552 - accuracy: 0.61 - ETA: 0s - loss: 0.6543 - accuracy: 0.62 - ETA: 0s - loss: 0.6538 - accuracy: 0.62 - ETA: 0s - loss: 0.6537 - accuracy: 0.62 - ETA: 0s - loss: 0.6529 - accuracy: 0.62 - 4s 279us/step - loss: 0.6528 - accuracy: 0.6240 - val_loss: 0.5934 - val_accuracy: 0.7188\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 107us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:31 - loss: 0.6969 - accuracy: 0.75 - ETA: 10s - loss: 0.6930 - accuracy: 0.5337 - ETA: 6s - loss: 0.6931 - accuracy: 0.521 - ETA: 5s - loss: 0.6927 - accuracy: 0.53 - ETA: 4s - loss: 0.6923 - accuracy: 0.52 - ETA: 4s - loss: 0.6904 - accuracy: 0.54 - ETA: 4s - loss: 0.6911 - accuracy: 0.53 - ETA: 3s - loss: 0.6891 - accuracy: 0.54 - ETA: 3s - loss: 0.6888 - accuracy: 0.53 - ETA: 3s - loss: 0.6877 - accuracy: 0.54 - ETA: 3s - loss: 0.6857 - accuracy: 0.55 - ETA: 3s - loss: 0.6844 - accuracy: 0.56 - ETA: 3s - loss: 0.6824 - accuracy: 0.56 - ETA: 3s - loss: 0.6815 - accuracy: 0.56 - ETA: 3s - loss: 0.6807 - accuracy: 0.56 - ETA: 2s - loss: 0.6805 - accuracy: 0.56 - ETA: 2s - loss: 0.6793 - accuracy: 0.57 - ETA: 2s - loss: 0.6778 - accuracy: 0.57 - ETA: 2s - loss: 0.6763 - accuracy: 0.58 - ETA: 2s - loss: 0.6746 - accuracy: 0.58 - ETA: 2s - loss: 0.6748 - accuracy: 0.58 - ETA: 2s - loss: 0.6735 - accuracy: 0.58 - ETA: 2s - loss: 0.6717 - accuracy: 0.59 - ETA: 2s - loss: 0.6707 - accuracy: 0.59 - ETA: 2s - loss: 0.6696 - accuracy: 0.59 - ETA: 2s - loss: 0.6693 - accuracy: 0.59 - ETA: 2s - loss: 0.6688 - accuracy: 0.59 - ETA: 2s - loss: 0.6676 - accuracy: 0.60 - ETA: 1s - loss: 0.6663 - accuracy: 0.60 - ETA: 1s - loss: 0.6656 - accuracy: 0.60 - ETA: 1s - loss: 0.6652 - accuracy: 0.60 - ETA: 1s - loss: 0.6641 - accuracy: 0.60 - ETA: 1s - loss: 0.6634 - accuracy: 0.60 - ETA: 1s - loss: 0.6627 - accuracy: 0.60 - ETA: 1s - loss: 0.6617 - accuracy: 0.61 - ETA: 1s - loss: 0.6608 - accuracy: 0.61 - ETA: 1s - loss: 0.6605 - accuracy: 0.61 - ETA: 1s - loss: 0.6587 - accuracy: 0.61 - ETA: 1s - loss: 0.6577 - accuracy: 0.61 - ETA: 1s - loss: 0.6572 - accuracy: 0.61 - ETA: 1s - loss: 0.6558 - accuracy: 0.62 - ETA: 1s - loss: 0.6560 - accuracy: 0.62 - ETA: 1s - loss: 0.6547 - accuracy: 0.62 - ETA: 1s - loss: 0.6541 - accuracy: 0.62 - ETA: 1s - loss: 0.6535 - accuracy: 0.62 - ETA: 1s - loss: 0.6527 - accuracy: 0.62 - ETA: 0s - loss: 0.6521 - accuracy: 0.62 - ETA: 0s - loss: 0.6512 - accuracy: 0.62 - ETA: 0s - loss: 0.6498 - accuracy: 0.63 - ETA: 0s - loss: 0.6497 - accuracy: 0.62 - ETA: 0s - loss: 0.6489 - accuracy: 0.63 - ETA: 0s - loss: 0.6481 - accuracy: 0.63 - ETA: 0s - loss: 0.6482 - accuracy: 0.63 - ETA: 0s - loss: 0.6475 - accuracy: 0.63 - ETA: 0s - loss: 0.6474 - accuracy: 0.63 - ETA: 0s - loss: 0.6469 - accuracy: 0.63 - ETA: 0s - loss: 0.6462 - accuracy: 0.63 - ETA: 0s - loss: 0.6458 - accuracy: 0.63 - ETA: 0s - loss: 0.6452 - accuracy: 0.63 - ETA: 0s - loss: 0.6453 - accuracy: 0.63 - ETA: 0s - loss: 0.6450 - accuracy: 0.63 - ETA: 0s - loss: 0.6444 - accuracy: 0.63 - ETA: 0s - loss: 0.6432 - accuracy: 0.63 - ETA: 0s - loss: 0.6429 - accuracy: 0.63 - 4s 281us/step - loss: 0.6430 - accuracy: 0.6389 - val_loss: 0.5947 - val_accuracy: 0.7244\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 123us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:15 - loss: 0.6980 - accuracy: 0.25 - ETA: 10s - loss: 0.6943 - accuracy: 0.4951 - ETA: 6s - loss: 0.6920 - accuracy: 0.522 - ETA: 5s - loss: 0.6887 - accuracy: 0.53 - ETA: 4s - loss: 0.6882 - accuracy: 0.53 - ETA: 4s - loss: 0.6887 - accuracy: 0.52 - ETA: 4s - loss: 0.6875 - accuracy: 0.53 - ETA: 3s - loss: 0.6861 - accuracy: 0.53 - ETA: 3s - loss: 0.6830 - accuracy: 0.54 - ETA: 3s - loss: 0.6820 - accuracy: 0.54 - ETA: 3s - loss: 0.6810 - accuracy: 0.54 - ETA: 3s - loss: 0.6787 - accuracy: 0.54 - ETA: 3s - loss: 0.6779 - accuracy: 0.54 - ETA: 2s - loss: 0.6761 - accuracy: 0.55 - ETA: 2s - loss: 0.6763 - accuracy: 0.56 - ETA: 2s - loss: 0.6739 - accuracy: 0.56 - ETA: 2s - loss: 0.6723 - accuracy: 0.57 - ETA: 2s - loss: 0.6684 - accuracy: 0.58 - ETA: 2s - loss: 0.6692 - accuracy: 0.58 - ETA: 2s - loss: 0.6685 - accuracy: 0.58 - ETA: 2s - loss: 0.6691 - accuracy: 0.58 - ETA: 2s - loss: 0.6678 - accuracy: 0.58 - ETA: 2s - loss: 0.6664 - accuracy: 0.59 - ETA: 2s - loss: 0.6657 - accuracy: 0.59 - ETA: 2s - loss: 0.6644 - accuracy: 0.59 - ETA: 2s - loss: 0.6632 - accuracy: 0.59 - ETA: 2s - loss: 0.6615 - accuracy: 0.60 - ETA: 1s - loss: 0.6606 - accuracy: 0.60 - ETA: 1s - loss: 0.6604 - accuracy: 0.60 - ETA: 1s - loss: 0.6589 - accuracy: 0.60 - ETA: 1s - loss: 0.6572 - accuracy: 0.61 - ETA: 1s - loss: 0.6564 - accuracy: 0.61 - ETA: 1s - loss: 0.6561 - accuracy: 0.61 - ETA: 1s - loss: 0.6558 - accuracy: 0.61 - ETA: 1s - loss: 0.6547 - accuracy: 0.61 - ETA: 1s - loss: 0.6535 - accuracy: 0.61 - ETA: 1s - loss: 0.6529 - accuracy: 0.62 - ETA: 1s - loss: 0.6521 - accuracy: 0.62 - ETA: 1s - loss: 0.6515 - accuracy: 0.62 - ETA: 1s - loss: 0.6508 - accuracy: 0.62 - ETA: 1s - loss: 0.6496 - accuracy: 0.62 - ETA: 1s - loss: 0.6480 - accuracy: 0.63 - ETA: 1s - loss: 0.6468 - accuracy: 0.63 - ETA: 1s - loss: 0.6455 - accuracy: 0.63 - ETA: 1s - loss: 0.6437 - accuracy: 0.63 - ETA: 0s - loss: 0.6428 - accuracy: 0.63 - ETA: 0s - loss: 0.6421 - accuracy: 0.64 - ETA: 0s - loss: 0.6418 - accuracy: 0.64 - ETA: 0s - loss: 0.6421 - accuracy: 0.64 - ETA: 0s - loss: 0.6426 - accuracy: 0.64 - ETA: 0s - loss: 0.6422 - accuracy: 0.64 - ETA: 0s - loss: 0.6414 - accuracy: 0.64 - ETA: 0s - loss: 0.6411 - accuracy: 0.64 - ETA: 0s - loss: 0.6409 - accuracy: 0.64 - ETA: 0s - loss: 0.6404 - accuracy: 0.64 - ETA: 0s - loss: 0.6401 - accuracy: 0.64 - ETA: 0s - loss: 0.6397 - accuracy: 0.64 - ETA: 0s - loss: 0.6386 - accuracy: 0.64 - ETA: 0s - loss: 0.6384 - accuracy: 0.64 - ETA: 0s - loss: 0.6386 - accuracy: 0.64 - ETA: 0s - loss: 0.6387 - accuracy: 0.64 - ETA: 0s - loss: 0.6386 - accuracy: 0.64 - ETA: 0s - loss: 0.6388 - accuracy: 0.64 - 3s 276us/step - loss: 0.6388 - accuracy: 0.6478 - val_loss: 0.5915 - val_accuracy: 0.7230\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 100us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:21 - loss: 0.6923 - accuracy: 0.50 - ETA: 10s - loss: 0.6934 - accuracy: 0.5472 - ETA: 6s - loss: 0.6933 - accuracy: 0.570 - ETA: 5s - loss: 0.6914 - accuracy: 0.56 - ETA: 4s - loss: 0.6903 - accuracy: 0.55 - ETA: 4s - loss: 0.6889 - accuracy: 0.55 - ETA: 4s - loss: 0.6883 - accuracy: 0.54 - ETA: 3s - loss: 0.6878 - accuracy: 0.53 - ETA: 3s - loss: 0.6861 - accuracy: 0.54 - ETA: 3s - loss: 0.6836 - accuracy: 0.54 - ETA: 3s - loss: 0.6797 - accuracy: 0.55 - ETA: 3s - loss: 0.6775 - accuracy: 0.55 - ETA: 3s - loss: 0.6786 - accuracy: 0.55 - ETA: 3s - loss: 0.6781 - accuracy: 0.56 - ETA: 3s - loss: 0.6765 - accuracy: 0.57 - ETA: 2s - loss: 0.6752 - accuracy: 0.57 - ETA: 2s - loss: 0.6755 - accuracy: 0.57 - ETA: 2s - loss: 0.6749 - accuracy: 0.57 - ETA: 2s - loss: 0.6735 - accuracy: 0.57 - ETA: 2s - loss: 0.6714 - accuracy: 0.58 - ETA: 2s - loss: 0.6703 - accuracy: 0.58 - ETA: 2s - loss: 0.6704 - accuracy: 0.58 - ETA: 2s - loss: 0.6694 - accuracy: 0.58 - ETA: 2s - loss: 0.6680 - accuracy: 0.59 - ETA: 2s - loss: 0.6681 - accuracy: 0.58 - ETA: 2s - loss: 0.6673 - accuracy: 0.59 - ETA: 2s - loss: 0.6663 - accuracy: 0.59 - ETA: 1s - loss: 0.6657 - accuracy: 0.59 - ETA: 1s - loss: 0.6641 - accuracy: 0.59 - ETA: 1s - loss: 0.6628 - accuracy: 0.59 - ETA: 1s - loss: 0.6619 - accuracy: 0.59 - ETA: 1s - loss: 0.6605 - accuracy: 0.60 - ETA: 1s - loss: 0.6602 - accuracy: 0.60 - ETA: 1s - loss: 0.6598 - accuracy: 0.60 - ETA: 1s - loss: 0.6589 - accuracy: 0.60 - ETA: 1s - loss: 0.6581 - accuracy: 0.60 - ETA: 1s - loss: 0.6572 - accuracy: 0.60 - ETA: 1s - loss: 0.6562 - accuracy: 0.60 - ETA: 1s - loss: 0.6554 - accuracy: 0.60 - ETA: 1s - loss: 0.6549 - accuracy: 0.60 - ETA: 1s - loss: 0.6533 - accuracy: 0.61 - ETA: 1s - loss: 0.6526 - accuracy: 0.61 - ETA: 1s - loss: 0.6525 - accuracy: 0.61 - ETA: 0s - loss: 0.6521 - accuracy: 0.61 - ETA: 0s - loss: 0.6520 - accuracy: 0.61 - ETA: 0s - loss: 0.6516 - accuracy: 0.61 - ETA: 0s - loss: 0.6511 - accuracy: 0.61 - ETA: 0s - loss: 0.6503 - accuracy: 0.61 - ETA: 0s - loss: 0.6500 - accuracy: 0.61 - ETA: 0s - loss: 0.6491 - accuracy: 0.61 - ETA: 0s - loss: 0.6492 - accuracy: 0.61 - ETA: 0s - loss: 0.6477 - accuracy: 0.61 - ETA: 0s - loss: 0.6477 - accuracy: 0.61 - ETA: 0s - loss: 0.6477 - accuracy: 0.61 - ETA: 0s - loss: 0.6472 - accuracy: 0.61 - ETA: 0s - loss: 0.6468 - accuracy: 0.61 - ETA: 0s - loss: 0.6467 - accuracy: 0.61 - ETA: 0s - loss: 0.6456 - accuracy: 0.61 - ETA: 0s - loss: 0.6452 - accuracy: 0.62 - ETA: 0s - loss: 0.6446 - accuracy: 0.62 - ETA: 0s - loss: 0.6441 - accuracy: 0.62 - ETA: 0s - loss: 0.6444 - accuracy: 0.62 - 3s 273us/step - loss: 0.6436 - accuracy: 0.6216 - val_loss: 0.5914 - val_accuracy: 0.7166\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 6:16 - loss: 0.7003 - accuracy: 0.50 - ETA: 10s - loss: 0.6949 - accuracy: 0.4279 - ETA: 6s - loss: 0.6943 - accuracy: 0.468 - ETA: 5s - loss: 0.6918 - accuracy: 0.51 - ETA: 4s - loss: 0.6900 - accuracy: 0.52 - ETA: 4s - loss: 0.6917 - accuracy: 0.51 - ETA: 3s - loss: 0.6912 - accuracy: 0.51 - ETA: 3s - loss: 0.6906 - accuracy: 0.51 - ETA: 3s - loss: 0.6905 - accuracy: 0.51 - ETA: 3s - loss: 0.6892 - accuracy: 0.51 - ETA: 3s - loss: 0.6882 - accuracy: 0.51 - ETA: 3s - loss: 0.6880 - accuracy: 0.52 - ETA: 3s - loss: 0.6867 - accuracy: 0.53 - ETA: 2s - loss: 0.6845 - accuracy: 0.53 - ETA: 2s - loss: 0.6836 - accuracy: 0.54 - ETA: 2s - loss: 0.6816 - accuracy: 0.54 - ETA: 2s - loss: 0.6797 - accuracy: 0.55 - ETA: 2s - loss: 0.6777 - accuracy: 0.55 - ETA: 2s - loss: 0.6765 - accuracy: 0.56 - ETA: 2s - loss: 0.6757 - accuracy: 0.56 - ETA: 2s - loss: 0.6743 - accuracy: 0.56 - ETA: 2s - loss: 0.6717 - accuracy: 0.57 - ETA: 2s - loss: 0.6706 - accuracy: 0.57 - ETA: 2s - loss: 0.6691 - accuracy: 0.58 - ETA: 2s - loss: 0.6668 - accuracy: 0.58 - ETA: 2s - loss: 0.6657 - accuracy: 0.58 - ETA: 2s - loss: 0.6655 - accuracy: 0.58 - ETA: 1s - loss: 0.6646 - accuracy: 0.58 - ETA: 1s - loss: 0.6643 - accuracy: 0.58 - ETA: 1s - loss: 0.6631 - accuracy: 0.59 - ETA: 1s - loss: 0.6622 - accuracy: 0.59 - ETA: 1s - loss: 0.6618 - accuracy: 0.59 - ETA: 1s - loss: 0.6608 - accuracy: 0.59 - ETA: 1s - loss: 0.6593 - accuracy: 0.59 - ETA: 1s - loss: 0.6582 - accuracy: 0.60 - ETA: 1s - loss: 0.6577 - accuracy: 0.60 - ETA: 1s - loss: 0.6571 - accuracy: 0.60 - ETA: 1s - loss: 0.6578 - accuracy: 0.60 - ETA: 1s - loss: 0.6561 - accuracy: 0.60 - ETA: 1s - loss: 0.6559 - accuracy: 0.60 - ETA: 1s - loss: 0.6551 - accuracy: 0.60 - ETA: 1s - loss: 0.6541 - accuracy: 0.60 - ETA: 1s - loss: 0.6529 - accuracy: 0.60 - ETA: 1s - loss: 0.6530 - accuracy: 0.61 - ETA: 1s - loss: 0.6518 - accuracy: 0.61 - ETA: 0s - loss: 0.6516 - accuracy: 0.61 - ETA: 0s - loss: 0.6509 - accuracy: 0.61 - ETA: 0s - loss: 0.6498 - accuracy: 0.61 - ETA: 0s - loss: 0.6488 - accuracy: 0.61 - ETA: 0s - loss: 0.6476 - accuracy: 0.61 - ETA: 0s - loss: 0.6459 - accuracy: 0.62 - ETA: 0s - loss: 0.6447 - accuracy: 0.62 - ETA: 0s - loss: 0.6446 - accuracy: 0.62 - ETA: 0s - loss: 0.6440 - accuracy: 0.62 - ETA: 0s - loss: 0.6436 - accuracy: 0.62 - ETA: 0s - loss: 0.6426 - accuracy: 0.62 - ETA: 0s - loss: 0.6424 - accuracy: 0.62 - ETA: 0s - loss: 0.6418 - accuracy: 0.62 - ETA: 0s - loss: 0.6413 - accuracy: 0.62 - ETA: 0s - loss: 0.6406 - accuracy: 0.62 - ETA: 0s - loss: 0.6402 - accuracy: 0.63 - ETA: 0s - loss: 0.6398 - accuracy: 0.63 - ETA: 0s - loss: 0.6392 - accuracy: 0.63 - ETA: 0s - loss: 0.6388 - accuracy: 0.63 - 4s 279us/step - loss: 0.6386 - accuracy: 0.6332 - val_loss: 0.6044 - val_accuracy: 0.6882\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 103us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:08 - loss: 0.7032 - accuracy: 0.0000e+ - ETA: 10s - loss: 0.6953 - accuracy: 0.4600     - ETA: 6s - loss: 0.6939 - accuracy: 0.509 - ETA: 5s - loss: 0.6906 - accuracy: 0.53 - ETA: 4s - loss: 0.6910 - accuracy: 0.53 - ETA: 4s - loss: 0.6899 - accuracy: 0.53 - ETA: 3s - loss: 0.6882 - accuracy: 0.54 - ETA: 3s - loss: 0.6851 - accuracy: 0.56 - ETA: 3s - loss: 0.6848 - accuracy: 0.56 - ETA: 3s - loss: 0.6829 - accuracy: 0.56 - ETA: 3s - loss: 0.6808 - accuracy: 0.57 - ETA: 3s - loss: 0.6799 - accuracy: 0.57 - ETA: 3s - loss: 0.6783 - accuracy: 0.58 - ETA: 3s - loss: 0.6770 - accuracy: 0.59 - ETA: 2s - loss: 0.6761 - accuracy: 0.59 - ETA: 2s - loss: 0.6742 - accuracy: 0.59 - ETA: 2s - loss: 0.6734 - accuracy: 0.59 - ETA: 2s - loss: 0.6714 - accuracy: 0.60 - ETA: 2s - loss: 0.6682 - accuracy: 0.60 - ETA: 2s - loss: 0.6666 - accuracy: 0.61 - ETA: 2s - loss: 0.6648 - accuracy: 0.61 - ETA: 2s - loss: 0.6634 - accuracy: 0.61 - ETA: 2s - loss: 0.6619 - accuracy: 0.62 - ETA: 2s - loss: 0.6612 - accuracy: 0.62 - ETA: 2s - loss: 0.6594 - accuracy: 0.62 - ETA: 2s - loss: 0.6580 - accuracy: 0.62 - ETA: 2s - loss: 0.6559 - accuracy: 0.62 - ETA: 2s - loss: 0.6547 - accuracy: 0.62 - ETA: 2s - loss: 0.6544 - accuracy: 0.62 - ETA: 1s - loss: 0.6528 - accuracy: 0.63 - ETA: 1s - loss: 0.6519 - accuracy: 0.63 - ETA: 1s - loss: 0.6498 - accuracy: 0.63 - ETA: 1s - loss: 0.6494 - accuracy: 0.63 - ETA: 1s - loss: 0.6484 - accuracy: 0.63 - ETA: 1s - loss: 0.6475 - accuracy: 0.63 - ETA: 1s - loss: 0.6464 - accuracy: 0.64 - ETA: 1s - loss: 0.6454 - accuracy: 0.64 - ETA: 1s - loss: 0.6441 - accuracy: 0.64 - ETA: 1s - loss: 0.6438 - accuracy: 0.64 - ETA: 1s - loss: 0.6423 - accuracy: 0.64 - ETA: 1s - loss: 0.6421 - accuracy: 0.64 - ETA: 1s - loss: 0.6419 - accuracy: 0.64 - ETA: 1s - loss: 0.6409 - accuracy: 0.64 - ETA: 1s - loss: 0.6402 - accuracy: 0.64 - ETA: 1s - loss: 0.6391 - accuracy: 0.65 - ETA: 1s - loss: 0.6370 - accuracy: 0.65 - ETA: 0s - loss: 0.6366 - accuracy: 0.65 - ETA: 0s - loss: 0.6359 - accuracy: 0.65 - ETA: 0s - loss: 0.6354 - accuracy: 0.65 - ETA: 0s - loss: 0.6341 - accuracy: 0.65 - ETA: 0s - loss: 0.6338 - accuracy: 0.65 - ETA: 0s - loss: 0.6336 - accuracy: 0.65 - ETA: 0s - loss: 0.6329 - accuracy: 0.65 - ETA: 0s - loss: 0.6325 - accuracy: 0.65 - ETA: 0s - loss: 0.6329 - accuracy: 0.65 - ETA: 0s - loss: 0.6326 - accuracy: 0.65 - ETA: 0s - loss: 0.6325 - accuracy: 0.65 - ETA: 0s - loss: 0.6325 - accuracy: 0.65 - ETA: 0s - loss: 0.6321 - accuracy: 0.65 - ETA: 0s - loss: 0.6323 - accuracy: 0.65 - ETA: 0s - loss: 0.6315 - accuracy: 0.66 - ETA: 0s - loss: 0.6312 - accuracy: 0.66 - ETA: 0s - loss: 0.6306 - accuracy: 0.66 - ETA: 0s - loss: 0.6302 - accuracy: 0.66 - ETA: 0s - loss: 0.6298 - accuracy: 0.66 - 4s 284us/step - loss: 0.6298 - accuracy: 0.6614 - val_loss: 0.5840 - val_accuracy: 0.7237\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 100us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:13 - loss: 0.7007 - accuracy: 0.50 - ETA: 9s - loss: 0.6964 - accuracy: 0.4434 - ETA: 6s - loss: 0.6942 - accuracy: 0.49 - ETA: 5s - loss: 0.6912 - accuracy: 0.51 - ETA: 4s - loss: 0.6890 - accuracy: 0.51 - ETA: 4s - loss: 0.6879 - accuracy: 0.52 - ETA: 3s - loss: 0.6871 - accuracy: 0.52 - ETA: 3s - loss: 0.6870 - accuracy: 0.53 - ETA: 3s - loss: 0.6871 - accuracy: 0.53 - ETA: 3s - loss: 0.6861 - accuracy: 0.54 - ETA: 3s - loss: 0.6850 - accuracy: 0.55 - ETA: 3s - loss: 0.6833 - accuracy: 0.55 - ETA: 3s - loss: 0.6825 - accuracy: 0.56 - ETA: 3s - loss: 0.6809 - accuracy: 0.56 - ETA: 3s - loss: 0.6790 - accuracy: 0.57 - ETA: 2s - loss: 0.6782 - accuracy: 0.57 - ETA: 2s - loss: 0.6780 - accuracy: 0.57 - ETA: 2s - loss: 0.6765 - accuracy: 0.58 - ETA: 2s - loss: 0.6746 - accuracy: 0.58 - ETA: 2s - loss: 0.6705 - accuracy: 0.58 - ETA: 2s - loss: 0.6675 - accuracy: 0.59 - ETA: 2s - loss: 0.6650 - accuracy: 0.60 - ETA: 2s - loss: 0.6633 - accuracy: 0.60 - ETA: 2s - loss: 0.6623 - accuracy: 0.60 - ETA: 2s - loss: 0.6621 - accuracy: 0.60 - ETA: 2s - loss: 0.6603 - accuracy: 0.60 - ETA: 2s - loss: 0.6591 - accuracy: 0.61 - ETA: 2s - loss: 0.6583 - accuracy: 0.61 - ETA: 1s - loss: 0.6568 - accuracy: 0.61 - ETA: 1s - loss: 0.6559 - accuracy: 0.62 - ETA: 1s - loss: 0.6526 - accuracy: 0.62 - ETA: 1s - loss: 0.6516 - accuracy: 0.62 - ETA: 1s - loss: 0.6500 - accuracy: 0.63 - ETA: 1s - loss: 0.6487 - accuracy: 0.63 - ETA: 1s - loss: 0.6474 - accuracy: 0.63 - ETA: 1s - loss: 0.6470 - accuracy: 0.63 - ETA: 1s - loss: 0.6458 - accuracy: 0.63 - ETA: 1s - loss: 0.6448 - accuracy: 0.63 - ETA: 1s - loss: 0.6440 - accuracy: 0.64 - ETA: 1s - loss: 0.6438 - accuracy: 0.64 - ETA: 1s - loss: 0.6423 - accuracy: 0.64 - ETA: 1s - loss: 0.6414 - accuracy: 0.64 - ETA: 1s - loss: 0.6404 - accuracy: 0.64 - ETA: 1s - loss: 0.6410 - accuracy: 0.64 - ETA: 0s - loss: 0.6394 - accuracy: 0.64 - ETA: 0s - loss: 0.6384 - accuracy: 0.65 - ETA: 0s - loss: 0.6381 - accuracy: 0.65 - ETA: 0s - loss: 0.6382 - accuracy: 0.65 - ETA: 0s - loss: 0.6376 - accuracy: 0.65 - ETA: 0s - loss: 0.6372 - accuracy: 0.65 - ETA: 0s - loss: 0.6362 - accuracy: 0.65 - ETA: 0s - loss: 0.6349 - accuracy: 0.65 - ETA: 0s - loss: 0.6345 - accuracy: 0.65 - ETA: 0s - loss: 0.6335 - accuracy: 0.65 - ETA: 0s - loss: 0.6326 - accuracy: 0.65 - ETA: 0s - loss: 0.6314 - accuracy: 0.65 - ETA: 0s - loss: 0.6308 - accuracy: 0.65 - ETA: 0s - loss: 0.6296 - accuracy: 0.66 - ETA: 0s - loss: 0.6295 - accuracy: 0.66 - ETA: 0s - loss: 0.6298 - accuracy: 0.66 - ETA: 0s - loss: 0.6293 - accuracy: 0.66 - ETA: 0s - loss: 0.6296 - accuracy: 0.66 - ETA: 0s - loss: 0.6289 - accuracy: 0.66 - 3s 275us/step - loss: 0.6286 - accuracy: 0.6615 - val_loss: 0.5784 - val_accuracy: 0.7202\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 100us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 6:16 - loss: 0.6911 - accuracy: 0.75 - ETA: 10s - loss: 0.6916 - accuracy: 0.5650 - ETA: 6s - loss: 0.6938 - accuracy: 0.510 - ETA: 5s - loss: 0.6934 - accuracy: 0.52 - ETA: 4s - loss: 0.6924 - accuracy: 0.53 - ETA: 4s - loss: 0.6897 - accuracy: 0.53 - ETA: 3s - loss: 0.6904 - accuracy: 0.52 - ETA: 3s - loss: 0.6891 - accuracy: 0.53 - ETA: 3s - loss: 0.6885 - accuracy: 0.54 - ETA: 3s - loss: 0.6868 - accuracy: 0.55 - ETA: 3s - loss: 0.6830 - accuracy: 0.56 - ETA: 3s - loss: 0.6819 - accuracy: 0.56 - ETA: 3s - loss: 0.6809 - accuracy: 0.57 - ETA: 3s - loss: 0.6797 - accuracy: 0.57 - ETA: 2s - loss: 0.6781 - accuracy: 0.57 - ETA: 2s - loss: 0.6760 - accuracy: 0.58 - ETA: 2s - loss: 0.6749 - accuracy: 0.58 - ETA: 2s - loss: 0.6730 - accuracy: 0.59 - ETA: 2s - loss: 0.6709 - accuracy: 0.59 - ETA: 2s - loss: 0.6689 - accuracy: 0.60 - ETA: 2s - loss: 0.6675 - accuracy: 0.60 - ETA: 2s - loss: 0.6670 - accuracy: 0.60 - ETA: 2s - loss: 0.6658 - accuracy: 0.60 - ETA: 2s - loss: 0.6636 - accuracy: 0.61 - ETA: 2s - loss: 0.6625 - accuracy: 0.61 - ETA: 2s - loss: 0.6612 - accuracy: 0.61 - ETA: 2s - loss: 0.6603 - accuracy: 0.62 - ETA: 1s - loss: 0.6590 - accuracy: 0.62 - ETA: 1s - loss: 0.6576 - accuracy: 0.62 - ETA: 1s - loss: 0.6563 - accuracy: 0.62 - ETA: 1s - loss: 0.6545 - accuracy: 0.63 - ETA: 1s - loss: 0.6536 - accuracy: 0.63 - ETA: 1s - loss: 0.6541 - accuracy: 0.63 - ETA: 1s - loss: 0.6529 - accuracy: 0.63 - ETA: 1s - loss: 0.6519 - accuracy: 0.63 - ETA: 1s - loss: 0.6513 - accuracy: 0.63 - ETA: 1s - loss: 0.6496 - accuracy: 0.64 - ETA: 1s - loss: 0.6496 - accuracy: 0.64 - ETA: 1s - loss: 0.6482 - accuracy: 0.64 - ETA: 1s - loss: 0.6463 - accuracy: 0.64 - ETA: 1s - loss: 0.6465 - accuracy: 0.64 - ETA: 1s - loss: 0.6449 - accuracy: 0.64 - ETA: 1s - loss: 0.6438 - accuracy: 0.64 - ETA: 1s - loss: 0.6436 - accuracy: 0.64 - ETA: 1s - loss: 0.6418 - accuracy: 0.65 - ETA: 0s - loss: 0.6413 - accuracy: 0.65 - ETA: 0s - loss: 0.6415 - accuracy: 0.65 - ETA: 0s - loss: 0.6415 - accuracy: 0.65 - ETA: 0s - loss: 0.6409 - accuracy: 0.65 - ETA: 0s - loss: 0.6395 - accuracy: 0.65 - ETA: 0s - loss: 0.6387 - accuracy: 0.65 - ETA: 0s - loss: 0.6382 - accuracy: 0.65 - ETA: 0s - loss: 0.6381 - accuracy: 0.65 - ETA: 0s - loss: 0.6372 - accuracy: 0.65 - ETA: 0s - loss: 0.6368 - accuracy: 0.65 - ETA: 0s - loss: 0.6355 - accuracy: 0.65 - ETA: 0s - loss: 0.6349 - accuracy: 0.65 - ETA: 0s - loss: 0.6350 - accuracy: 0.65 - ETA: 0s - loss: 0.6341 - accuracy: 0.66 - ETA: 0s - loss: 0.6345 - accuracy: 0.66 - ETA: 0s - loss: 0.6337 - accuracy: 0.66 - ETA: 0s - loss: 0.6333 - accuracy: 0.66 - ETA: 0s - loss: 0.6328 - accuracy: 0.66 - 3s 276us/step - loss: 0.6323 - accuracy: 0.6624 - val_loss: 0.5790 - val_accuracy: 0.7216\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 107us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 6:16 - loss: 0.6996 - accuracy: 0.25 - ETA: 9s - loss: 0.6947 - accuracy: 0.4953 - ETA: 6s - loss: 0.6930 - accuracy: 0.54 - ETA: 5s - loss: 0.6933 - accuracy: 0.53 - ETA: 4s - loss: 0.6915 - accuracy: 0.53 - ETA: 4s - loss: 0.6899 - accuracy: 0.53 - ETA: 4s - loss: 0.6892 - accuracy: 0.54 - ETA: 3s - loss: 0.6883 - accuracy: 0.54 - ETA: 3s - loss: 0.6868 - accuracy: 0.55 - ETA: 3s - loss: 0.6853 - accuracy: 0.56 - ETA: 3s - loss: 0.6831 - accuracy: 0.57 - ETA: 3s - loss: 0.6813 - accuracy: 0.57 - ETA: 3s - loss: 0.6789 - accuracy: 0.58 - ETA: 3s - loss: 0.6784 - accuracy: 0.58 - ETA: 2s - loss: 0.6759 - accuracy: 0.59 - ETA: 2s - loss: 0.6749 - accuracy: 0.59 - ETA: 2s - loss: 0.6725 - accuracy: 0.60 - ETA: 2s - loss: 0.6704 - accuracy: 0.60 - ETA: 2s - loss: 0.6699 - accuracy: 0.60 - ETA: 2s - loss: 0.6676 - accuracy: 0.61 - ETA: 2s - loss: 0.6677 - accuracy: 0.61 - ETA: 2s - loss: 0.6656 - accuracy: 0.61 - ETA: 2s - loss: 0.6647 - accuracy: 0.61 - ETA: 2s - loss: 0.6632 - accuracy: 0.62 - ETA: 2s - loss: 0.6631 - accuracy: 0.62 - ETA: 2s - loss: 0.6617 - accuracy: 0.62 - ETA: 2s - loss: 0.6607 - accuracy: 0.62 - ETA: 1s - loss: 0.6603 - accuracy: 0.62 - ETA: 1s - loss: 0.6597 - accuracy: 0.62 - ETA: 1s - loss: 0.6579 - accuracy: 0.62 - ETA: 1s - loss: 0.6574 - accuracy: 0.62 - ETA: 1s - loss: 0.6554 - accuracy: 0.63 - ETA: 1s - loss: 0.6553 - accuracy: 0.63 - ETA: 1s - loss: 0.6532 - accuracy: 0.63 - ETA: 1s - loss: 0.6507 - accuracy: 0.63 - ETA: 1s - loss: 0.6498 - accuracy: 0.63 - ETA: 1s - loss: 0.6487 - accuracy: 0.63 - ETA: 1s - loss: 0.6462 - accuracy: 0.64 - ETA: 1s - loss: 0.6445 - accuracy: 0.64 - ETA: 1s - loss: 0.6441 - accuracy: 0.64 - ETA: 1s - loss: 0.6441 - accuracy: 0.64 - ETA: 1s - loss: 0.6439 - accuracy: 0.64 - ETA: 1s - loss: 0.6423 - accuracy: 0.64 - ETA: 1s - loss: 0.6409 - accuracy: 0.64 - ETA: 0s - loss: 0.6403 - accuracy: 0.65 - ETA: 0s - loss: 0.6391 - accuracy: 0.65 - ETA: 0s - loss: 0.6385 - accuracy: 0.65 - ETA: 0s - loss: 0.6373 - accuracy: 0.65 - ETA: 0s - loss: 0.6366 - accuracy: 0.65 - ETA: 0s - loss: 0.6364 - accuracy: 0.65 - ETA: 0s - loss: 0.6359 - accuracy: 0.65 - ETA: 0s - loss: 0.6346 - accuracy: 0.65 - ETA: 0s - loss: 0.6342 - accuracy: 0.65 - ETA: 0s - loss: 0.6349 - accuracy: 0.65 - ETA: 0s - loss: 0.6351 - accuracy: 0.65 - ETA: 0s - loss: 0.6346 - accuracy: 0.65 - ETA: 0s - loss: 0.6344 - accuracy: 0.65 - ETA: 0s - loss: 0.6334 - accuracy: 0.65 - ETA: 0s - loss: 0.6330 - accuracy: 0.65 - ETA: 0s - loss: 0.6327 - accuracy: 0.65 - ETA: 0s - loss: 0.6319 - accuracy: 0.66 - ETA: 0s - loss: 0.6321 - accuracy: 0.66 - ETA: 0s - loss: 0.6316 - accuracy: 0.66 - 3s 275us/step - loss: 0.6310 - accuracy: 0.6618 - val_loss: 0.5797 - val_accuracy: 0.7259\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 106us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:10 - loss: 0.6919 - accuracy: 0.75 - ETA: 10s - loss: 0.6945 - accuracy: 0.4952 - ETA: 6s - loss: 0.6941 - accuracy: 0.519 - ETA: 5s - loss: 0.6945 - accuracy: 0.51 - ETA: 4s - loss: 0.6942 - accuracy: 0.52 - ETA: 4s - loss: 0.6921 - accuracy: 0.53 - ETA: 4s - loss: 0.6924 - accuracy: 0.53 - ETA: 3s - loss: 0.6928 - accuracy: 0.52 - ETA: 3s - loss: 0.6925 - accuracy: 0.53 - ETA: 3s - loss: 0.6904 - accuracy: 0.54 - ETA: 3s - loss: 0.6891 - accuracy: 0.54 - ETA: 3s - loss: 0.6889 - accuracy: 0.54 - ETA: 3s - loss: 0.6888 - accuracy: 0.54 - ETA: 3s - loss: 0.6887 - accuracy: 0.54 - ETA: 3s - loss: 0.6881 - accuracy: 0.54 - ETA: 2s - loss: 0.6875 - accuracy: 0.54 - ETA: 2s - loss: 0.6860 - accuracy: 0.55 - ETA: 2s - loss: 0.6840 - accuracy: 0.55 - ETA: 2s - loss: 0.6834 - accuracy: 0.55 - ETA: 2s - loss: 0.6821 - accuracy: 0.56 - ETA: 2s - loss: 0.6816 - accuracy: 0.56 - ETA: 2s - loss: 0.6803 - accuracy: 0.56 - ETA: 2s - loss: 0.6795 - accuracy: 0.56 - ETA: 2s - loss: 0.6784 - accuracy: 0.57 - ETA: 2s - loss: 0.6777 - accuracy: 0.57 - ETA: 2s - loss: 0.6770 - accuracy: 0.57 - ETA: 2s - loss: 0.6758 - accuracy: 0.57 - ETA: 2s - loss: 0.6748 - accuracy: 0.57 - ETA: 2s - loss: 0.6731 - accuracy: 0.58 - ETA: 1s - loss: 0.6725 - accuracy: 0.58 - ETA: 1s - loss: 0.6713 - accuracy: 0.58 - ETA: 1s - loss: 0.6702 - accuracy: 0.58 - ETA: 1s - loss: 0.6689 - accuracy: 0.59 - ETA: 1s - loss: 0.6667 - accuracy: 0.59 - ETA: 1s - loss: 0.6662 - accuracy: 0.59 - ETA: 1s - loss: 0.6651 - accuracy: 0.59 - ETA: 1s - loss: 0.6639 - accuracy: 0.60 - ETA: 1s - loss: 0.6628 - accuracy: 0.60 - ETA: 1s - loss: 0.6620 - accuracy: 0.60 - ETA: 1s - loss: 0.6605 - accuracy: 0.60 - ETA: 1s - loss: 0.6594 - accuracy: 0.60 - ETA: 1s - loss: 0.6589 - accuracy: 0.60 - ETA: 1s - loss: 0.6585 - accuracy: 0.60 - ETA: 1s - loss: 0.6587 - accuracy: 0.60 - ETA: 1s - loss: 0.6577 - accuracy: 0.61 - ETA: 1s - loss: 0.6569 - accuracy: 0.61 - ETA: 1s - loss: 0.6562 - accuracy: 0.61 - ETA: 0s - loss: 0.6551 - accuracy: 0.61 - ETA: 0s - loss: 0.6535 - accuracy: 0.61 - ETA: 0s - loss: 0.6529 - accuracy: 0.61 - ETA: 0s - loss: 0.6524 - accuracy: 0.61 - ETA: 0s - loss: 0.6514 - accuracy: 0.62 - ETA: 0s - loss: 0.6512 - accuracy: 0.62 - ETA: 0s - loss: 0.6506 - accuracy: 0.62 - ETA: 0s - loss: 0.6497 - accuracy: 0.62 - ETA: 0s - loss: 0.6486 - accuracy: 0.62 - ETA: 0s - loss: 0.6483 - accuracy: 0.62 - ETA: 0s - loss: 0.6483 - accuracy: 0.62 - ETA: 0s - loss: 0.6478 - accuracy: 0.62 - ETA: 0s - loss: 0.6465 - accuracy: 0.62 - ETA: 0s - loss: 0.6464 - accuracy: 0.62 - ETA: 0s - loss: 0.6456 - accuracy: 0.62 - ETA: 0s - loss: 0.6446 - accuracy: 0.63 - ETA: 0s - loss: 0.6447 - accuracy: 0.63 - ETA: 0s - loss: 0.6439 - accuracy: 0.63 - ETA: 0s - loss: 0.6432 - accuracy: 0.63 - 4s 287us/step - loss: 0.6432 - accuracy: 0.6342 - val_loss: 0.5874 - val_accuracy: 0.7223\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 104us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 6:16 - loss: 0.6875 - accuracy: 0.75 - ETA: 10s - loss: 0.6949 - accuracy: 0.5000 - ETA: 6s - loss: 0.6941 - accuracy: 0.532 - ETA: 5s - loss: 0.6941 - accuracy: 0.52 - ETA: 4s - loss: 0.6932 - accuracy: 0.53 - ETA: 4s - loss: 0.6925 - accuracy: 0.53 - ETA: 4s - loss: 0.6923 - accuracy: 0.54 - ETA: 3s - loss: 0.6910 - accuracy: 0.55 - ETA: 3s - loss: 0.6906 - accuracy: 0.55 - ETA: 3s - loss: 0.6893 - accuracy: 0.56 - ETA: 3s - loss: 0.6869 - accuracy: 0.57 - ETA: 3s - loss: 0.6865 - accuracy: 0.56 - ETA: 3s - loss: 0.6855 - accuracy: 0.56 - ETA: 3s - loss: 0.6820 - accuracy: 0.57 - ETA: 2s - loss: 0.6803 - accuracy: 0.58 - ETA: 2s - loss: 0.6785 - accuracy: 0.58 - ETA: 2s - loss: 0.6761 - accuracy: 0.58 - ETA: 2s - loss: 0.6738 - accuracy: 0.59 - ETA: 2s - loss: 0.6721 - accuracy: 0.59 - ETA: 2s - loss: 0.6705 - accuracy: 0.59 - ETA: 2s - loss: 0.6689 - accuracy: 0.59 - ETA: 2s - loss: 0.6678 - accuracy: 0.60 - ETA: 2s - loss: 0.6664 - accuracy: 0.60 - ETA: 2s - loss: 0.6637 - accuracy: 0.60 - ETA: 2s - loss: 0.6627 - accuracy: 0.60 - ETA: 2s - loss: 0.6609 - accuracy: 0.61 - ETA: 2s - loss: 0.6600 - accuracy: 0.61 - ETA: 2s - loss: 0.6583 - accuracy: 0.61 - ETA: 1s - loss: 0.6574 - accuracy: 0.61 - ETA: 1s - loss: 0.6557 - accuracy: 0.62 - ETA: 1s - loss: 0.6545 - accuracy: 0.62 - ETA: 1s - loss: 0.6532 - accuracy: 0.62 - ETA: 1s - loss: 0.6514 - accuracy: 0.62 - ETA: 1s - loss: 0.6508 - accuracy: 0.62 - ETA: 1s - loss: 0.6492 - accuracy: 0.62 - ETA: 1s - loss: 0.6487 - accuracy: 0.63 - ETA: 1s - loss: 0.6465 - accuracy: 0.63 - ETA: 1s - loss: 0.6456 - accuracy: 0.63 - ETA: 1s - loss: 0.6456 - accuracy: 0.63 - ETA: 1s - loss: 0.6448 - accuracy: 0.63 - ETA: 1s - loss: 0.6432 - accuracy: 0.64 - ETA: 1s - loss: 0.6432 - accuracy: 0.64 - ETA: 1s - loss: 0.6419 - accuracy: 0.64 - ETA: 1s - loss: 0.6403 - accuracy: 0.64 - ETA: 1s - loss: 0.6393 - accuracy: 0.64 - ETA: 1s - loss: 0.6401 - accuracy: 0.64 - ETA: 0s - loss: 0.6394 - accuracy: 0.64 - ETA: 0s - loss: 0.6389 - accuracy: 0.64 - ETA: 0s - loss: 0.6384 - accuracy: 0.64 - ETA: 0s - loss: 0.6369 - accuracy: 0.64 - ETA: 0s - loss: 0.6370 - accuracy: 0.65 - ETA: 0s - loss: 0.6363 - accuracy: 0.65 - ETA: 0s - loss: 0.6360 - accuracy: 0.65 - ETA: 0s - loss: 0.6351 - accuracy: 0.65 - ETA: 0s - loss: 0.6339 - accuracy: 0.65 - ETA: 0s - loss: 0.6329 - accuracy: 0.65 - ETA: 0s - loss: 0.6327 - accuracy: 0.65 - ETA: 0s - loss: 0.6325 - accuracy: 0.65 - ETA: 0s - loss: 0.6321 - accuracy: 0.65 - ETA: 0s - loss: 0.6315 - accuracy: 0.65 - ETA: 0s - loss: 0.6316 - accuracy: 0.65 - ETA: 0s - loss: 0.6308 - accuracy: 0.65 - ETA: 0s - loss: 0.6308 - accuracy: 0.65 - ETA: 0s - loss: 0.6302 - accuracy: 0.65 - 4s 279us/step - loss: 0.6294 - accuracy: 0.6599 - val_loss: 0.5946 - val_accuracy: 0.6868\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 107us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:46 - loss: 0.6943 - accuracy: 0.37 - ETA: 4s - loss: 0.6934 - accuracy: 0.5000 - ETA: 2s - loss: 0.6927 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6916 - accuracy: 0.53 - ETA: 1s - loss: 0.6909 - accuracy: 0.54 - ETA: 1s - loss: 0.6902 - accuracy: 0.54 - ETA: 1s - loss: 0.6900 - accuracy: 0.54 - ETA: 1s - loss: 0.6899 - accuracy: 0.54 - ETA: 1s - loss: 0.6893 - accuracy: 0.55 - ETA: 1s - loss: 0.6886 - accuracy: 0.55 - ETA: 1s - loss: 0.6880 - accuracy: 0.56 - ETA: 1s - loss: 0.6872 - accuracy: 0.56 - ETA: 1s - loss: 0.6862 - accuracy: 0.56 - ETA: 1s - loss: 0.6850 - accuracy: 0.57 - ETA: 0s - loss: 0.6851 - accuracy: 0.57 - ETA: 0s - loss: 0.6841 - accuracy: 0.57 - ETA: 0s - loss: 0.6831 - accuracy: 0.57 - ETA: 0s - loss: 0.6826 - accuracy: 0.57 - ETA: 0s - loss: 0.6816 - accuracy: 0.58 - ETA: 0s - loss: 0.6804 - accuracy: 0.58 - ETA: 0s - loss: 0.6798 - accuracy: 0.58 - ETA: 0s - loss: 0.6785 - accuracy: 0.58 - ETA: 0s - loss: 0.6767 - accuracy: 0.58 - ETA: 0s - loss: 0.6758 - accuracy: 0.58 - ETA: 0s - loss: 0.6743 - accuracy: 0.59 - ETA: 0s - loss: 0.6737 - accuracy: 0.59 - ETA: 0s - loss: 0.6718 - accuracy: 0.59 - ETA: 0s - loss: 0.6709 - accuracy: 0.59 - ETA: 0s - loss: 0.6701 - accuracy: 0.59 - ETA: 0s - loss: 0.6696 - accuracy: 0.59 - ETA: 0s - loss: 0.6682 - accuracy: 0.59 - 2s 144us/step - loss: 0.6665 - accuracy: 0.5993 - val_loss: 0.6158 - val_accuracy: 0.7074\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:41 - loss: 0.6941 - accuracy: 0.62 - ETA: 4s - loss: 0.6935 - accuracy: 0.4575 - ETA: 2s - loss: 0.6934 - accuracy: 0.48 - ETA: 2s - loss: 0.6934 - accuracy: 0.49 - ETA: 1s - loss: 0.6933 - accuracy: 0.50 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.52 - ETA: 1s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6922 - accuracy: 0.53 - ETA: 1s - loss: 0.6916 - accuracy: 0.54 - ETA: 1s - loss: 0.6912 - accuracy: 0.54 - ETA: 1s - loss: 0.6904 - accuracy: 0.55 - ETA: 1s - loss: 0.6899 - accuracy: 0.55 - ETA: 1s - loss: 0.6893 - accuracy: 0.55 - ETA: 0s - loss: 0.6885 - accuracy: 0.56 - ETA: 0s - loss: 0.6873 - accuracy: 0.56 - ETA: 0s - loss: 0.6869 - accuracy: 0.56 - ETA: 0s - loss: 0.6861 - accuracy: 0.56 - ETA: 0s - loss: 0.6855 - accuracy: 0.57 - ETA: 0s - loss: 0.6847 - accuracy: 0.57 - ETA: 0s - loss: 0.6838 - accuracy: 0.57 - ETA: 0s - loss: 0.6823 - accuracy: 0.57 - ETA: 0s - loss: 0.6816 - accuracy: 0.57 - ETA: 0s - loss: 0.6811 - accuracy: 0.57 - ETA: 0s - loss: 0.6805 - accuracy: 0.58 - ETA: 0s - loss: 0.6794 - accuracy: 0.58 - ETA: 0s - loss: 0.6785 - accuracy: 0.58 - ETA: 0s - loss: 0.6775 - accuracy: 0.58 - ETA: 0s - loss: 0.6766 - accuracy: 0.59 - ETA: 0s - loss: 0.6755 - accuracy: 0.59 - ETA: 0s - loss: 0.6747 - accuracy: 0.59 - 2s 137us/step - loss: 0.6746 - accuracy: 0.5931 - val_loss: 0.6350 - val_accuracy: 0.7166\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:43 - loss: 0.6984 - accuracy: 0.25 - ETA: 4s - loss: 0.6936 - accuracy: 0.5168 - ETA: 2s - loss: 0.6934 - accuracy: 0.50 - ETA: 2s - loss: 0.6933 - accuracy: 0.50 - ETA: 2s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6928 - accuracy: 0.50 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6922 - accuracy: 0.52 - ETA: 1s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6913 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.52 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 1s - loss: 0.6903 - accuracy: 0.54 - ETA: 1s - loss: 0.6895 - accuracy: 0.54 - ETA: 0s - loss: 0.6888 - accuracy: 0.55 - ETA: 0s - loss: 0.6877 - accuracy: 0.55 - ETA: 0s - loss: 0.6869 - accuracy: 0.56 - ETA: 0s - loss: 0.6862 - accuracy: 0.56 - ETA: 0s - loss: 0.6860 - accuracy: 0.56 - ETA: 0s - loss: 0.6853 - accuracy: 0.56 - ETA: 0s - loss: 0.6843 - accuracy: 0.57 - ETA: 0s - loss: 0.6830 - accuracy: 0.57 - ETA: 0s - loss: 0.6819 - accuracy: 0.57 - ETA: 0s - loss: 0.6809 - accuracy: 0.57 - ETA: 0s - loss: 0.6797 - accuracy: 0.58 - ETA: 0s - loss: 0.6789 - accuracy: 0.58 - ETA: 0s - loss: 0.6782 - accuracy: 0.58 - ETA: 0s - loss: 0.6775 - accuracy: 0.58 - ETA: 0s - loss: 0.6764 - accuracy: 0.58 - ETA: 0s - loss: 0.6755 - accuracy: 0.59 - ETA: 0s - loss: 0.6743 - accuracy: 0.59 - 2s 140us/step - loss: 0.6737 - accuracy: 0.5931 - val_loss: 0.6341 - val_accuracy: 0.7124\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 2:46 - loss: 0.6939 - accuracy: 0.50 - ETA: 4s - loss: 0.6937 - accuracy: 0.4514 - ETA: 2s - loss: 0.6928 - accuracy: 0.51 - ETA: 2s - loss: 0.6922 - accuracy: 0.51 - ETA: 2s - loss: 0.6907 - accuracy: 0.52 - ETA: 1s - loss: 0.6900 - accuracy: 0.52 - ETA: 1s - loss: 0.6895 - accuracy: 0.52 - ETA: 1s - loss: 0.6888 - accuracy: 0.53 - ETA: 1s - loss: 0.6877 - accuracy: 0.53 - ETA: 1s - loss: 0.6871 - accuracy: 0.54 - ETA: 1s - loss: 0.6866 - accuracy: 0.54 - ETA: 1s - loss: 0.6866 - accuracy: 0.54 - ETA: 1s - loss: 0.6855 - accuracy: 0.55 - ETA: 0s - loss: 0.6845 - accuracy: 0.55 - ETA: 0s - loss: 0.6832 - accuracy: 0.56 - ETA: 0s - loss: 0.6825 - accuracy: 0.56 - ETA: 0s - loss: 0.6811 - accuracy: 0.56 - ETA: 0s - loss: 0.6796 - accuracy: 0.56 - ETA: 0s - loss: 0.6784 - accuracy: 0.57 - ETA: 0s - loss: 0.6773 - accuracy: 0.57 - ETA: 0s - loss: 0.6760 - accuracy: 0.57 - ETA: 0s - loss: 0.6747 - accuracy: 0.57 - ETA: 0s - loss: 0.6736 - accuracy: 0.58 - ETA: 0s - loss: 0.6726 - accuracy: 0.58 - ETA: 0s - loss: 0.6714 - accuracy: 0.58 - ETA: 0s - loss: 0.6699 - accuracy: 0.58 - ETA: 0s - loss: 0.6691 - accuracy: 0.59 - ETA: 0s - loss: 0.6688 - accuracy: 0.59 - ETA: 0s - loss: 0.6678 - accuracy: 0.59 - ETA: 0s - loss: 0.6668 - accuracy: 0.59 - 2s 136us/step - loss: 0.6658 - accuracy: 0.5937 - val_loss: 0.6177 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:43 - loss: 0.6978 - accuracy: 0.12 - ETA: 4s - loss: 0.6920 - accuracy: 0.5364 - ETA: 2s - loss: 0.6922 - accuracy: 0.51 - ETA: 2s - loss: 0.6918 - accuracy: 0.51 - ETA: 1s - loss: 0.6917 - accuracy: 0.51 - ETA: 1s - loss: 0.6911 - accuracy: 0.51 - ETA: 1s - loss: 0.6908 - accuracy: 0.52 - ETA: 1s - loss: 0.6900 - accuracy: 0.52 - ETA: 1s - loss: 0.6894 - accuracy: 0.52 - ETA: 1s - loss: 0.6881 - accuracy: 0.53 - ETA: 1s - loss: 0.6869 - accuracy: 0.54 - ETA: 1s - loss: 0.6854 - accuracy: 0.54 - ETA: 1s - loss: 0.6841 - accuracy: 0.55 - ETA: 0s - loss: 0.6829 - accuracy: 0.55 - ETA: 0s - loss: 0.6812 - accuracy: 0.56 - ETA: 0s - loss: 0.6807 - accuracy: 0.56 - ETA: 0s - loss: 0.6799 - accuracy: 0.56 - ETA: 0s - loss: 0.6787 - accuracy: 0.56 - ETA: 0s - loss: 0.6779 - accuracy: 0.57 - ETA: 0s - loss: 0.6763 - accuracy: 0.57 - ETA: 0s - loss: 0.6755 - accuracy: 0.57 - ETA: 0s - loss: 0.6738 - accuracy: 0.57 - ETA: 0s - loss: 0.6731 - accuracy: 0.57 - ETA: 0s - loss: 0.6715 - accuracy: 0.58 - ETA: 0s - loss: 0.6699 - accuracy: 0.58 - ETA: 0s - loss: 0.6687 - accuracy: 0.58 - ETA: 0s - loss: 0.6678 - accuracy: 0.58 - ETA: 0s - loss: 0.6671 - accuracy: 0.58 - ETA: 0s - loss: 0.6662 - accuracy: 0.59 - ETA: 0s - loss: 0.6655 - accuracy: 0.59 - 2s 136us/step - loss: 0.6644 - accuracy: 0.5933 - val_loss: 0.6231 - val_accuracy: 0.7109\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:41 - loss: 0.6946 - accuracy: 0.50 - ETA: 4s - loss: 0.6938 - accuracy: 0.4676 - ETA: 2s - loss: 0.6936 - accuracy: 0.47 - ETA: 2s - loss: 0.6935 - accuracy: 0.48 - ETA: 2s - loss: 0.6933 - accuracy: 0.49 - ETA: 1s - loss: 0.6932 - accuracy: 0.49 - ETA: 1s - loss: 0.6932 - accuracy: 0.49 - ETA: 1s - loss: 0.6932 - accuracy: 0.49 - ETA: 1s - loss: 0.6931 - accuracy: 0.50 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6922 - accuracy: 0.53 - ETA: 1s - loss: 0.6918 - accuracy: 0.53 - ETA: 1s - loss: 0.6914 - accuracy: 0.54 - ETA: 0s - loss: 0.6910 - accuracy: 0.54 - ETA: 0s - loss: 0.6907 - accuracy: 0.54 - ETA: 0s - loss: 0.6900 - accuracy: 0.55 - ETA: 0s - loss: 0.6896 - accuracy: 0.55 - ETA: 0s - loss: 0.6892 - accuracy: 0.55 - ETA: 0s - loss: 0.6885 - accuracy: 0.56 - ETA: 0s - loss: 0.6878 - accuracy: 0.56 - ETA: 0s - loss: 0.6874 - accuracy: 0.56 - ETA: 0s - loss: 0.6867 - accuracy: 0.56 - ETA: 0s - loss: 0.6860 - accuracy: 0.57 - ETA: 0s - loss: 0.6852 - accuracy: 0.57 - ETA: 0s - loss: 0.6846 - accuracy: 0.57 - ETA: 0s - loss: 0.6841 - accuracy: 0.57 - ETA: 0s - loss: 0.6834 - accuracy: 0.57 - ETA: 0s - loss: 0.6828 - accuracy: 0.58 - ETA: 0s - loss: 0.6820 - accuracy: 0.58 - ETA: 0s - loss: 0.6812 - accuracy: 0.58 - 2s 140us/step - loss: 0.6805 - accuracy: 0.5859 - val_loss: 0.6539 - val_accuracy: 0.6520\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:55 - loss: 0.6950 - accuracy: 0.37 - ETA: 5s - loss: 0.6939 - accuracy: 0.4688 - ETA: 3s - loss: 0.6938 - accuracy: 0.47 - ETA: 2s - loss: 0.6931 - accuracy: 0.50 - ETA: 2s - loss: 0.6924 - accuracy: 0.50 - ETA: 1s - loss: 0.6917 - accuracy: 0.50 - ETA: 1s - loss: 0.6904 - accuracy: 0.51 - ETA: 1s - loss: 0.6895 - accuracy: 0.52 - ETA: 1s - loss: 0.6885 - accuracy: 0.53 - ETA: 1s - loss: 0.6872 - accuracy: 0.53 - ETA: 1s - loss: 0.6861 - accuracy: 0.54 - ETA: 1s - loss: 0.6843 - accuracy: 0.56 - ETA: 1s - loss: 0.6833 - accuracy: 0.56 - ETA: 1s - loss: 0.6815 - accuracy: 0.57 - ETA: 0s - loss: 0.6799 - accuracy: 0.57 - ETA: 0s - loss: 0.6776 - accuracy: 0.58 - ETA: 0s - loss: 0.6757 - accuracy: 0.58 - ETA: 0s - loss: 0.6751 - accuracy: 0.58 - ETA: 0s - loss: 0.6729 - accuracy: 0.59 - ETA: 0s - loss: 0.6721 - accuracy: 0.59 - ETA: 0s - loss: 0.6716 - accuracy: 0.59 - ETA: 0s - loss: 0.6703 - accuracy: 0.60 - ETA: 0s - loss: 0.6683 - accuracy: 0.60 - ETA: 0s - loss: 0.6669 - accuracy: 0.60 - ETA: 0s - loss: 0.6657 - accuracy: 0.60 - ETA: 0s - loss: 0.6650 - accuracy: 0.60 - ETA: 0s - loss: 0.6642 - accuracy: 0.61 - ETA: 0s - loss: 0.6625 - accuracy: 0.61 - ETA: 0s - loss: 0.6610 - accuracy: 0.61 - ETA: 0s - loss: 0.6597 - accuracy: 0.61 - 2s 137us/step - loss: 0.6584 - accuracy: 0.6177 - val_loss: 0.5990 - val_accuracy: 0.7131\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:49 - loss: 0.6903 - accuracy: 0.62 - ETA: 4s - loss: 0.6926 - accuracy: 0.5481 - ETA: 2s - loss: 0.6925 - accuracy: 0.55 - ETA: 2s - loss: 0.6919 - accuracy: 0.54 - ETA: 2s - loss: 0.6913 - accuracy: 0.54 - ETA: 1s - loss: 0.6910 - accuracy: 0.54 - ETA: 1s - loss: 0.6905 - accuracy: 0.55 - ETA: 1s - loss: 0.6890 - accuracy: 0.56 - ETA: 1s - loss: 0.6870 - accuracy: 0.56 - ETA: 1s - loss: 0.6856 - accuracy: 0.57 - ETA: 1s - loss: 0.6848 - accuracy: 0.57 - ETA: 1s - loss: 0.6833 - accuracy: 0.58 - ETA: 1s - loss: 0.6821 - accuracy: 0.58 - ETA: 1s - loss: 0.6808 - accuracy: 0.58 - ETA: 0s - loss: 0.6784 - accuracy: 0.59 - ETA: 0s - loss: 0.6768 - accuracy: 0.59 - ETA: 0s - loss: 0.6748 - accuracy: 0.60 - ETA: 0s - loss: 0.6726 - accuracy: 0.60 - ETA: 0s - loss: 0.6715 - accuracy: 0.60 - ETA: 0s - loss: 0.6697 - accuracy: 0.60 - ETA: 0s - loss: 0.6672 - accuracy: 0.61 - ETA: 0s - loss: 0.6651 - accuracy: 0.61 - ETA: 0s - loss: 0.6636 - accuracy: 0.61 - ETA: 0s - loss: 0.6623 - accuracy: 0.61 - ETA: 0s - loss: 0.6606 - accuracy: 0.62 - ETA: 0s - loss: 0.6589 - accuracy: 0.62 - ETA: 0s - loss: 0.6583 - accuracy: 0.62 - ETA: 0s - loss: 0.6568 - accuracy: 0.62 - ETA: 0s - loss: 0.6550 - accuracy: 0.62 - ETA: 0s - loss: 0.6540 - accuracy: 0.62 - ETA: 0s - loss: 0.6520 - accuracy: 0.63 - 2s 140us/step - loss: 0.6525 - accuracy: 0.6314 - val_loss: 0.5977 - val_accuracy: 0.7173\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:44 - loss: 0.6885 - accuracy: 0.50 - ETA: 4s - loss: 0.6922 - accuracy: 0.5486 - ETA: 2s - loss: 0.6909 - accuracy: 0.54 - ETA: 2s - loss: 0.6914 - accuracy: 0.53 - ETA: 1s - loss: 0.6913 - accuracy: 0.53 - ETA: 1s - loss: 0.6892 - accuracy: 0.54 - ETA: 1s - loss: 0.6883 - accuracy: 0.54 - ETA: 1s - loss: 0.6880 - accuracy: 0.54 - ETA: 1s - loss: 0.6873 - accuracy: 0.55 - ETA: 1s - loss: 0.6850 - accuracy: 0.55 - ETA: 1s - loss: 0.6832 - accuracy: 0.56 - ETA: 1s - loss: 0.6816 - accuracy: 0.56 - ETA: 1s - loss: 0.6806 - accuracy: 0.56 - ETA: 0s - loss: 0.6795 - accuracy: 0.57 - ETA: 0s - loss: 0.6781 - accuracy: 0.57 - ETA: 0s - loss: 0.6772 - accuracy: 0.57 - ETA: 0s - loss: 0.6762 - accuracy: 0.58 - ETA: 0s - loss: 0.6753 - accuracy: 0.58 - ETA: 0s - loss: 0.6741 - accuracy: 0.58 - ETA: 0s - loss: 0.6725 - accuracy: 0.58 - ETA: 0s - loss: 0.6715 - accuracy: 0.59 - ETA: 0s - loss: 0.6698 - accuracy: 0.59 - ETA: 0s - loss: 0.6686 - accuracy: 0.59 - ETA: 0s - loss: 0.6671 - accuracy: 0.60 - ETA: 0s - loss: 0.6651 - accuracy: 0.60 - ETA: 0s - loss: 0.6639 - accuracy: 0.60 - ETA: 0s - loss: 0.6623 - accuracy: 0.61 - ETA: 0s - loss: 0.6607 - accuracy: 0.61 - ETA: 0s - loss: 0.6595 - accuracy: 0.61 - ETA: 0s - loss: 0.6580 - accuracy: 0.61 - ETA: 0s - loss: 0.6567 - accuracy: 0.62 - 2s 138us/step - loss: 0.6565 - accuracy: 0.6211 - val_loss: 0.6029 - val_accuracy: 0.7173\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:47 - loss: 0.6935 - accuracy: 0.75 - ETA: 4s - loss: 0.6943 - accuracy: 0.5159 - ETA: 2s - loss: 0.6934 - accuracy: 0.53 - ETA: 2s - loss: 0.6926 - accuracy: 0.53 - ETA: 1s - loss: 0.6916 - accuracy: 0.54 - ETA: 1s - loss: 0.6897 - accuracy: 0.55 - ETA: 1s - loss: 0.6882 - accuracy: 0.56 - ETA: 1s - loss: 0.6863 - accuracy: 0.57 - ETA: 1s - loss: 0.6849 - accuracy: 0.57 - ETA: 1s - loss: 0.6836 - accuracy: 0.57 - ETA: 1s - loss: 0.6828 - accuracy: 0.58 - ETA: 1s - loss: 0.6821 - accuracy: 0.58 - ETA: 1s - loss: 0.6803 - accuracy: 0.59 - ETA: 1s - loss: 0.6781 - accuracy: 0.59 - ETA: 0s - loss: 0.6764 - accuracy: 0.60 - ETA: 0s - loss: 0.6749 - accuracy: 0.60 - ETA: 0s - loss: 0.6720 - accuracy: 0.61 - ETA: 0s - loss: 0.6700 - accuracy: 0.61 - ETA: 0s - loss: 0.6687 - accuracy: 0.61 - ETA: 0s - loss: 0.6678 - accuracy: 0.61 - ETA: 0s - loss: 0.6669 - accuracy: 0.61 - ETA: 0s - loss: 0.6653 - accuracy: 0.61 - ETA: 0s - loss: 0.6630 - accuracy: 0.62 - ETA: 0s - loss: 0.6614 - accuracy: 0.62 - ETA: 0s - loss: 0.6601 - accuracy: 0.62 - ETA: 0s - loss: 0.6587 - accuracy: 0.62 - ETA: 0s - loss: 0.6578 - accuracy: 0.62 - ETA: 0s - loss: 0.6563 - accuracy: 0.62 - ETA: 0s - loss: 0.6546 - accuracy: 0.63 - ETA: 0s - loss: 0.6534 - accuracy: 0.63 - ETA: 0s - loss: 0.6527 - accuracy: 0.63 - 2s 139us/step - loss: 0.6525 - accuracy: 0.6351 - val_loss: 0.5983 - val_accuracy: 0.7209\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:47 - loss: 0.6998 - accuracy: 0.25 - ETA: 4s - loss: 0.6945 - accuracy: 0.4976 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.53 - ETA: 1s - loss: 0.6919 - accuracy: 0.54 - ETA: 1s - loss: 0.6911 - accuracy: 0.54 - ETA: 1s - loss: 0.6904 - accuracy: 0.55 - ETA: 1s - loss: 0.6891 - accuracy: 0.56 - ETA: 1s - loss: 0.6879 - accuracy: 0.56 - ETA: 1s - loss: 0.6858 - accuracy: 0.57 - ETA: 1s - loss: 0.6842 - accuracy: 0.57 - ETA: 1s - loss: 0.6826 - accuracy: 0.57 - ETA: 1s - loss: 0.6815 - accuracy: 0.58 - ETA: 0s - loss: 0.6807 - accuracy: 0.58 - ETA: 0s - loss: 0.6786 - accuracy: 0.58 - ETA: 0s - loss: 0.6772 - accuracy: 0.59 - ETA: 0s - loss: 0.6752 - accuracy: 0.59 - ETA: 0s - loss: 0.6740 - accuracy: 0.59 - ETA: 0s - loss: 0.6725 - accuracy: 0.60 - ETA: 0s - loss: 0.6702 - accuracy: 0.60 - ETA: 0s - loss: 0.6689 - accuracy: 0.60 - ETA: 0s - loss: 0.6688 - accuracy: 0.60 - ETA: 0s - loss: 0.6676 - accuracy: 0.60 - ETA: 0s - loss: 0.6667 - accuracy: 0.61 - ETA: 0s - loss: 0.6660 - accuracy: 0.61 - ETA: 0s - loss: 0.6654 - accuracy: 0.61 - ETA: 0s - loss: 0.6647 - accuracy: 0.61 - ETA: 0s - loss: 0.6635 - accuracy: 0.61 - ETA: 0s - loss: 0.6618 - accuracy: 0.61 - ETA: 0s - loss: 0.6606 - accuracy: 0.62 - 2s 144us/step - loss: 0.6597 - accuracy: 0.6222 - val_loss: 0.6081 - val_accuracy: 0.7223\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:54 - loss: 0.6967 - accuracy: 0.50 - ETA: 4s - loss: 0.6923 - accuracy: 0.5330 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6919 - accuracy: 0.50 - ETA: 2s - loss: 0.6906 - accuracy: 0.51 - ETA: 1s - loss: 0.6897 - accuracy: 0.52 - ETA: 1s - loss: 0.6890 - accuracy: 0.52 - ETA: 1s - loss: 0.6883 - accuracy: 0.52 - ETA: 1s - loss: 0.6874 - accuracy: 0.53 - ETA: 1s - loss: 0.6864 - accuracy: 0.53 - ETA: 1s - loss: 0.6854 - accuracy: 0.54 - ETA: 1s - loss: 0.6841 - accuracy: 0.55 - ETA: 1s - loss: 0.6827 - accuracy: 0.55 - ETA: 1s - loss: 0.6803 - accuracy: 0.57 - ETA: 0s - loss: 0.6775 - accuracy: 0.57 - ETA: 0s - loss: 0.6754 - accuracy: 0.58 - ETA: 0s - loss: 0.6731 - accuracy: 0.59 - ETA: 0s - loss: 0.6719 - accuracy: 0.59 - ETA: 0s - loss: 0.6699 - accuracy: 0.59 - ETA: 0s - loss: 0.6688 - accuracy: 0.59 - ETA: 0s - loss: 0.6671 - accuracy: 0.60 - ETA: 0s - loss: 0.6653 - accuracy: 0.60 - ETA: 0s - loss: 0.6634 - accuracy: 0.60 - ETA: 0s - loss: 0.6615 - accuracy: 0.61 - ETA: 0s - loss: 0.6605 - accuracy: 0.61 - ETA: 0s - loss: 0.6585 - accuracy: 0.61 - ETA: 0s - loss: 0.6571 - accuracy: 0.62 - ETA: 0s - loss: 0.6548 - accuracy: 0.62 - ETA: 0s - loss: 0.6537 - accuracy: 0.62 - ETA: 0s - loss: 0.6534 - accuracy: 0.62 - ETA: 0s - loss: 0.6525 - accuracy: 0.63 - 2s 138us/step - loss: 0.6525 - accuracy: 0.6301 - val_loss: 0.6088 - val_accuracy: 0.7202\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:46 - loss: 0.6957 - accuracy: 0.50 - ETA: 4s - loss: 0.6952 - accuracy: 0.4552 - ETA: 2s - loss: 0.6934 - accuracy: 0.49 - ETA: 2s - loss: 0.6916 - accuracy: 0.50 - ETA: 2s - loss: 0.6904 - accuracy: 0.51 - ETA: 1s - loss: 0.6895 - accuracy: 0.51 - ETA: 1s - loss: 0.6879 - accuracy: 0.52 - ETA: 1s - loss: 0.6856 - accuracy: 0.54 - ETA: 1s - loss: 0.6841 - accuracy: 0.54 - ETA: 1s - loss: 0.6827 - accuracy: 0.55 - ETA: 1s - loss: 0.6802 - accuracy: 0.56 - ETA: 1s - loss: 0.6789 - accuracy: 0.57 - ETA: 1s - loss: 0.6775 - accuracy: 0.57 - ETA: 1s - loss: 0.6759 - accuracy: 0.58 - ETA: 0s - loss: 0.6745 - accuracy: 0.59 - ETA: 0s - loss: 0.6730 - accuracy: 0.59 - ETA: 0s - loss: 0.6709 - accuracy: 0.60 - ETA: 0s - loss: 0.6695 - accuracy: 0.60 - ETA: 0s - loss: 0.6681 - accuracy: 0.61 - ETA: 0s - loss: 0.6661 - accuracy: 0.61 - ETA: 0s - loss: 0.6644 - accuracy: 0.61 - ETA: 0s - loss: 0.6621 - accuracy: 0.62 - ETA: 0s - loss: 0.6606 - accuracy: 0.62 - ETA: 0s - loss: 0.6598 - accuracy: 0.62 - ETA: 0s - loss: 0.6589 - accuracy: 0.62 - ETA: 0s - loss: 0.6576 - accuracy: 0.62 - ETA: 0s - loss: 0.6562 - accuracy: 0.62 - ETA: 0s - loss: 0.6543 - accuracy: 0.63 - ETA: 0s - loss: 0.6533 - accuracy: 0.63 - ETA: 0s - loss: 0.6522 - accuracy: 0.63 - ETA: 0s - loss: 0.6508 - accuracy: 0.63 - 2s 139us/step - loss: 0.6503 - accuracy: 0.6399 - val_loss: 0.6020 - val_accuracy: 0.6974\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 2:39 - loss: 0.6861 - accuracy: 0.75 - ETA: 4s - loss: 0.6938 - accuracy: 0.4976 - ETA: 2s - loss: 0.6907 - accuracy: 0.50 - ETA: 2s - loss: 0.6903 - accuracy: 0.51 - ETA: 1s - loss: 0.6898 - accuracy: 0.52 - ETA: 1s - loss: 0.6886 - accuracy: 0.53 - ETA: 1s - loss: 0.6861 - accuracy: 0.55 - ETA: 1s - loss: 0.6845 - accuracy: 0.56 - ETA: 1s - loss: 0.6826 - accuracy: 0.56 - ETA: 1s - loss: 0.6811 - accuracy: 0.57 - ETA: 1s - loss: 0.6796 - accuracy: 0.57 - ETA: 1s - loss: 0.6770 - accuracy: 0.58 - ETA: 1s - loss: 0.6758 - accuracy: 0.59 - ETA: 0s - loss: 0.6740 - accuracy: 0.59 - ETA: 0s - loss: 0.6718 - accuracy: 0.60 - ETA: 0s - loss: 0.6691 - accuracy: 0.60 - ETA: 0s - loss: 0.6672 - accuracy: 0.61 - ETA: 0s - loss: 0.6657 - accuracy: 0.61 - ETA: 0s - loss: 0.6636 - accuracy: 0.61 - ETA: 0s - loss: 0.6630 - accuracy: 0.61 - ETA: 0s - loss: 0.6621 - accuracy: 0.61 - ETA: 0s - loss: 0.6606 - accuracy: 0.62 - ETA: 0s - loss: 0.6578 - accuracy: 0.62 - ETA: 0s - loss: 0.6560 - accuracy: 0.62 - ETA: 0s - loss: 0.6548 - accuracy: 0.62 - ETA: 0s - loss: 0.6541 - accuracy: 0.63 - ETA: 0s - loss: 0.6520 - accuracy: 0.63 - ETA: 0s - loss: 0.6507 - accuracy: 0.63 - ETA: 0s - loss: 0.6493 - accuracy: 0.63 - ETA: 0s - loss: 0.6480 - accuracy: 0.63 - ETA: 0s - loss: 0.6475 - accuracy: 0.64 - 2s 138us/step - loss: 0.6475 - accuracy: 0.6402 - val_loss: 0.5930 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:49 - loss: 0.6990 - accuracy: 0.37 - ETA: 4s - loss: 0.6941 - accuracy: 0.5046 - ETA: 2s - loss: 0.6934 - accuracy: 0.52 - ETA: 2s - loss: 0.6906 - accuracy: 0.53 - ETA: 2s - loss: 0.6892 - accuracy: 0.53 - ETA: 1s - loss: 0.6863 - accuracy: 0.54 - ETA: 1s - loss: 0.6864 - accuracy: 0.53 - ETA: 1s - loss: 0.6860 - accuracy: 0.54 - ETA: 1s - loss: 0.6848 - accuracy: 0.55 - ETA: 1s - loss: 0.6839 - accuracy: 0.55 - ETA: 1s - loss: 0.6828 - accuracy: 0.56 - ETA: 1s - loss: 0.6806 - accuracy: 0.56 - ETA: 1s - loss: 0.6789 - accuracy: 0.57 - ETA: 1s - loss: 0.6770 - accuracy: 0.58 - ETA: 0s - loss: 0.6744 - accuracy: 0.58 - ETA: 0s - loss: 0.6719 - accuracy: 0.59 - ETA: 0s - loss: 0.6708 - accuracy: 0.59 - ETA: 0s - loss: 0.6695 - accuracy: 0.59 - ETA: 0s - loss: 0.6672 - accuracy: 0.60 - ETA: 0s - loss: 0.6657 - accuracy: 0.60 - ETA: 0s - loss: 0.6641 - accuracy: 0.61 - ETA: 0s - loss: 0.6624 - accuracy: 0.61 - ETA: 0s - loss: 0.6619 - accuracy: 0.61 - ETA: 0s - loss: 0.6605 - accuracy: 0.61 - ETA: 0s - loss: 0.6588 - accuracy: 0.61 - ETA: 0s - loss: 0.6573 - accuracy: 0.62 - ETA: 0s - loss: 0.6552 - accuracy: 0.62 - ETA: 0s - loss: 0.6546 - accuracy: 0.62 - ETA: 0s - loss: 0.6522 - accuracy: 0.62 - ETA: 0s - loss: 0.6509 - accuracy: 0.62 - ETA: 0s - loss: 0.6501 - accuracy: 0.63 - 2s 140us/step - loss: 0.6491 - accuracy: 0.6320 - val_loss: 0.5922 - val_accuracy: 0.7124\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 57us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:41 - loss: 0.6975 - accuracy: 0.25 - ETA: 4s - loss: 0.6941 - accuracy: 0.5024 - ETA: 2s - loss: 0.6933 - accuracy: 0.52 - ETA: 2s - loss: 0.6925 - accuracy: 0.54 - ETA: 2s - loss: 0.6913 - accuracy: 0.54 - ETA: 1s - loss: 0.6904 - accuracy: 0.55 - ETA: 1s - loss: 0.6892 - accuracy: 0.55 - ETA: 1s - loss: 0.6880 - accuracy: 0.55 - ETA: 1s - loss: 0.6870 - accuracy: 0.56 - ETA: 1s - loss: 0.6860 - accuracy: 0.57 - ETA: 1s - loss: 0.6845 - accuracy: 0.57 - ETA: 1s - loss: 0.6825 - accuracy: 0.58 - ETA: 1s - loss: 0.6797 - accuracy: 0.59 - ETA: 1s - loss: 0.6781 - accuracy: 0.59 - ETA: 0s - loss: 0.6754 - accuracy: 0.60 - ETA: 0s - loss: 0.6720 - accuracy: 0.60 - ETA: 0s - loss: 0.6697 - accuracy: 0.60 - ETA: 0s - loss: 0.6671 - accuracy: 0.61 - ETA: 0s - loss: 0.6656 - accuracy: 0.61 - ETA: 0s - loss: 0.6635 - accuracy: 0.61 - ETA: 0s - loss: 0.6618 - accuracy: 0.62 - ETA: 0s - loss: 0.6598 - accuracy: 0.62 - ETA: 0s - loss: 0.6579 - accuracy: 0.62 - ETA: 0s - loss: 0.6569 - accuracy: 0.62 - ETA: 0s - loss: 0.6555 - accuracy: 0.63 - ETA: 0s - loss: 0.6541 - accuracy: 0.63 - ETA: 0s - loss: 0.6522 - accuracy: 0.63 - ETA: 0s - loss: 0.6513 - accuracy: 0.63 - ETA: 0s - loss: 0.6498 - accuracy: 0.63 - ETA: 0s - loss: 0.6480 - accuracy: 0.63 - ETA: 0s - loss: 0.6469 - accuracy: 0.64 - 2s 139us/step - loss: 0.6448 - accuracy: 0.6437 - val_loss: 0.5878 - val_accuracy: 0.7031\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:43 - loss: 0.6924 - accuracy: 0.62 - ETA: 4s - loss: 0.6941 - accuracy: 0.5024 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6927 - accuracy: 0.51 - ETA: 2s - loss: 0.6915 - accuracy: 0.53 - ETA: 1s - loss: 0.6907 - accuracy: 0.53 - ETA: 1s - loss: 0.6893 - accuracy: 0.53 - ETA: 1s - loss: 0.6886 - accuracy: 0.54 - ETA: 1s - loss: 0.6882 - accuracy: 0.54 - ETA: 1s - loss: 0.6873 - accuracy: 0.55 - ETA: 1s - loss: 0.6861 - accuracy: 0.56 - ETA: 1s - loss: 0.6846 - accuracy: 0.56 - ETA: 1s - loss: 0.6834 - accuracy: 0.57 - ETA: 1s - loss: 0.6812 - accuracy: 0.58 - ETA: 1s - loss: 0.6801 - accuracy: 0.58 - ETA: 1s - loss: 0.6783 - accuracy: 0.58 - ETA: 0s - loss: 0.6769 - accuracy: 0.59 - ETA: 0s - loss: 0.6758 - accuracy: 0.59 - ETA: 0s - loss: 0.6741 - accuracy: 0.60 - ETA: 0s - loss: 0.6726 - accuracy: 0.60 - ETA: 0s - loss: 0.6698 - accuracy: 0.60 - ETA: 0s - loss: 0.6680 - accuracy: 0.61 - ETA: 0s - loss: 0.6664 - accuracy: 0.61 - ETA: 0s - loss: 0.6644 - accuracy: 0.61 - ETA: 0s - loss: 0.6620 - accuracy: 0.61 - ETA: 0s - loss: 0.6604 - accuracy: 0.62 - ETA: 0s - loss: 0.6601 - accuracy: 0.62 - ETA: 0s - loss: 0.6585 - accuracy: 0.62 - ETA: 0s - loss: 0.6571 - accuracy: 0.62 - ETA: 0s - loss: 0.6553 - accuracy: 0.62 - ETA: 0s - loss: 0.6537 - accuracy: 0.63 - ETA: 0s - loss: 0.6527 - accuracy: 0.63 - ETA: 0s - loss: 0.6518 - accuracy: 0.63 - 2s 147us/step - loss: 0.6505 - accuracy: 0.6352 - val_loss: 0.5915 - val_accuracy: 0.7195\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:46 - loss: 0.6973 - accuracy: 0.37 - ETA: 4s - loss: 0.6917 - accuracy: 0.5212 - ETA: 2s - loss: 0.6900 - accuracy: 0.53 - ETA: 2s - loss: 0.6885 - accuracy: 0.54 - ETA: 2s - loss: 0.6891 - accuracy: 0.53 - ETA: 1s - loss: 0.6887 - accuracy: 0.53 - ETA: 1s - loss: 0.6879 - accuracy: 0.53 - ETA: 1s - loss: 0.6866 - accuracy: 0.54 - ETA: 1s - loss: 0.6857 - accuracy: 0.55 - ETA: 1s - loss: 0.6841 - accuracy: 0.56 - ETA: 1s - loss: 0.6829 - accuracy: 0.56 - ETA: 1s - loss: 0.6807 - accuracy: 0.57 - ETA: 1s - loss: 0.6795 - accuracy: 0.57 - ETA: 1s - loss: 0.6772 - accuracy: 0.58 - ETA: 0s - loss: 0.6749 - accuracy: 0.58 - ETA: 0s - loss: 0.6733 - accuracy: 0.59 - ETA: 0s - loss: 0.6720 - accuracy: 0.59 - ETA: 0s - loss: 0.6697 - accuracy: 0.60 - ETA: 0s - loss: 0.6686 - accuracy: 0.60 - ETA: 0s - loss: 0.6675 - accuracy: 0.60 - ETA: 0s - loss: 0.6650 - accuracy: 0.60 - ETA: 0s - loss: 0.6640 - accuracy: 0.61 - ETA: 0s - loss: 0.6641 - accuracy: 0.61 - ETA: 0s - loss: 0.6630 - accuracy: 0.61 - ETA: 0s - loss: 0.6607 - accuracy: 0.62 - ETA: 0s - loss: 0.6592 - accuracy: 0.62 - ETA: 0s - loss: 0.6568 - accuracy: 0.62 - ETA: 0s - loss: 0.6556 - accuracy: 0.62 - ETA: 0s - loss: 0.6539 - accuracy: 0.63 - ETA: 0s - loss: 0.6533 - accuracy: 0.63 - ETA: 0s - loss: 0.6525 - accuracy: 0.63 - 2s 139us/step - loss: 0.6515 - accuracy: 0.6356 - val_loss: 0.6064 - val_accuracy: 0.6946\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:24 - loss: 0.6938 - accuracy: 0.37 - ETA: 4s - loss: 0.6938 - accuracy: 0.4811 - ETA: 2s - loss: 0.6937 - accuracy: 0.49 - ETA: 2s - loss: 0.6938 - accuracy: 0.49 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.49 - ETA: 1s - loss: 0.6938 - accuracy: 0.49 - ETA: 1s - loss: 0.6937 - accuracy: 0.49 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - 2s 133us/step - loss: 0.6933 - accuracy: 0.5133 - val_loss: 0.6920 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:22 - loss: 0.6934 - accuracy: 0.25 - ETA: 3s - loss: 0.6938 - accuracy: 0.4560 - ETA: 2s - loss: 0.6937 - accuracy: 0.48 - ETA: 2s - loss: 0.6937 - accuracy: 0.49 - ETA: 1s - loss: 0.6937 - accuracy: 0.49 - ETA: 1s - loss: 0.6938 - accuracy: 0.49 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - 2s 135us/step - loss: 0.6934 - accuracy: 0.5128 - val_loss: 0.6925 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:20 - loss: 0.6868 - accuracy: 0.50 - ETA: 3s - loss: 0.6936 - accuracy: 0.5110 - ETA: 2s - loss: 0.6934 - accuracy: 0.49 - ETA: 2s - loss: 0.6934 - accuracy: 0.49 - ETA: 1s - loss: 0.6932 - accuracy: 0.50 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6928 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.50 - ETA: 0s - loss: 0.6925 - accuracy: 0.50 - ETA: 0s - loss: 0.6926 - accuracy: 0.50 - ETA: 0s - loss: 0.6925 - accuracy: 0.50 - ETA: 0s - loss: 0.6923 - accuracy: 0.50 - ETA: 0s - loss: 0.6924 - accuracy: 0.50 - ETA: 0s - loss: 0.6923 - accuracy: 0.50 - ETA: 0s - loss: 0.6923 - accuracy: 0.50 - ETA: 0s - loss: 0.6921 - accuracy: 0.50 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - ETA: 0s - loss: 0.6918 - accuracy: 0.51 - 2s 132us/step - loss: 0.6919 - accuracy: 0.5112 - val_loss: 0.6874 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 57us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:24 - loss: 0.6939 - accuracy: 0.37 - ETA: 3s - loss: 0.6934 - accuracy: 0.4886 - ETA: 2s - loss: 0.6935 - accuracy: 0.49 - ETA: 2s - loss: 0.6937 - accuracy: 0.48 - ETA: 1s - loss: 0.6937 - accuracy: 0.48 - ETA: 1s - loss: 0.6935 - accuracy: 0.49 - ETA: 1s - loss: 0.6933 - accuracy: 0.50 - ETA: 1s - loss: 0.6933 - accuracy: 0.50 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.50 - ETA: 1s - loss: 0.6932 - accuracy: 0.50 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - 2s 135us/step - loss: 0.6924 - accuracy: 0.5136 - val_loss: 0.6900 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:30 - loss: 0.6931 - accuracy: 0.62 - ETA: 4s - loss: 0.6928 - accuracy: 0.5409 - ETA: 2s - loss: 0.6927 - accuracy: 0.53 - ETA: 2s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - 2s 136us/step - loss: 0.6919 - accuracy: 0.5165 - val_loss: 0.6875 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 58us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 2:54 - loss: 0.6993 - accuracy: 0.37 - ETA: 5s - loss: 0.6933 - accuracy: 0.5160 - ETA: 3s - loss: 0.6933 - accuracy: 0.52 - ETA: 2s - loss: 0.6932 - accuracy: 0.52 - ETA: 2s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6928 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6928 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6923 - accuracy: 0.51 - ETA: 1s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6919 - accuracy: 0.51 - ETA: 0s - loss: 0.6918 - accuracy: 0.51 - ETA: 0s - loss: 0.6915 - accuracy: 0.52 - ETA: 0s - loss: 0.6916 - accuracy: 0.51 - ETA: 0s - loss: 0.6916 - accuracy: 0.51 - ETA: 0s - loss: 0.6914 - accuracy: 0.51 - ETA: 0s - loss: 0.6914 - accuracy: 0.51 - ETA: 0s - loss: 0.6913 - accuracy: 0.51 - ETA: 0s - loss: 0.6910 - accuracy: 0.52 - ETA: 0s - loss: 0.6910 - accuracy: 0.52 - ETA: 0s - loss: 0.6909 - accuracy: 0.52 - ETA: 0s - loss: 0.6906 - accuracy: 0.52 - ETA: 0s - loss: 0.6903 - accuracy: 0.52 - ETA: 0s - loss: 0.6901 - accuracy: 0.52 - ETA: 0s - loss: 0.6901 - accuracy: 0.52 - ETA: 0s - loss: 0.6900 - accuracy: 0.52 - ETA: 0s - loss: 0.6899 - accuracy: 0.52 - 2s 136us/step - loss: 0.6899 - accuracy: 0.5272 - val_loss: 0.6841 - val_accuracy: 0.6946\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:25 - loss: 0.6963 - accuracy: 0.37 - ETA: 3s - loss: 0.6946 - accuracy: 0.4568 - ETA: 2s - loss: 0.6947 - accuracy: 0.45 - ETA: 2s - loss: 0.6946 - accuracy: 0.46 - ETA: 1s - loss: 0.6945 - accuracy: 0.48 - ETA: 1s - loss: 0.6944 - accuracy: 0.48 - ETA: 1s - loss: 0.6944 - accuracy: 0.49 - ETA: 1s - loss: 0.6944 - accuracy: 0.49 - ETA: 1s - loss: 0.6944 - accuracy: 0.49 - ETA: 1s - loss: 0.6944 - accuracy: 0.49 - ETA: 1s - loss: 0.6941 - accuracy: 0.49 - ETA: 1s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - 2s 130us/step - loss: 0.6936 - accuracy: 0.5088 - val_loss: 0.6916 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 57us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:25 - loss: 0.6928 - accuracy: 0.62 - ETA: 3s - loss: 0.6943 - accuracy: 0.5088 - ETA: 2s - loss: 0.6941 - accuracy: 0.51 - ETA: 2s - loss: 0.6941 - accuracy: 0.50 - ETA: 1s - loss: 0.6940 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.53 - ETA: 0s - loss: 0.6919 - accuracy: 0.53 - ETA: 0s - loss: 0.6915 - accuracy: 0.53 - ETA: 0s - loss: 0.6914 - accuracy: 0.53 - ETA: 0s - loss: 0.6908 - accuracy: 0.53 - ETA: 0s - loss: 0.6908 - accuracy: 0.53 - 2s 133us/step - loss: 0.6908 - accuracy: 0.5359 - val_loss: 0.6832 - val_accuracy: 0.5391\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:27 - loss: 0.6914 - accuracy: 0.50 - ETA: 3s - loss: 0.6921 - accuracy: 0.5341 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 2s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6943 - accuracy: 0.50 - ETA: 1s - loss: 0.6942 - accuracy: 0.50 - ETA: 1s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - 2s 139us/step - loss: 0.6922 - accuracy: 0.5248 - val_loss: 0.6866 - val_accuracy: 0.5597\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 80us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:25 - loss: 0.6980 - accuracy: 0.62 - ETA: 4s - loss: 0.6940 - accuracy: 0.5370 - ETA: 2s - loss: 0.6941 - accuracy: 0.52 - ETA: 2s - loss: 0.6936 - accuracy: 0.53 - ETA: 1s - loss: 0.6937 - accuracy: 0.52 - ETA: 1s - loss: 0.6939 - accuracy: 0.52 - ETA: 1s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.52 - ETA: 1s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.53 - ETA: 0s - loss: 0.6927 - accuracy: 0.53 - ETA: 0s - loss: 0.6925 - accuracy: 0.53 - ETA: 0s - loss: 0.6923 - accuracy: 0.53 - ETA: 0s - loss: 0.6921 - accuracy: 0.53 - ETA: 0s - loss: 0.6921 - accuracy: 0.53 - ETA: 0s - loss: 0.6918 - accuracy: 0.53 - ETA: 0s - loss: 0.6915 - accuracy: 0.53 - ETA: 0s - loss: 0.6913 - accuracy: 0.53 - ETA: 0s - loss: 0.6913 - accuracy: 0.53 - ETA: 0s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6910 - accuracy: 0.53 - ETA: 0s - loss: 0.6908 - accuracy: 0.53 - ETA: 0s - loss: 0.6905 - accuracy: 0.53 - ETA: 0s - loss: 0.6904 - accuracy: 0.54 - 2s 133us/step - loss: 0.6903 - accuracy: 0.5412 - val_loss: 0.6823 - val_accuracy: 0.5497\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:20 - loss: 0.6937 - accuracy: 0.75 - ETA: 3s - loss: 0.6938 - accuracy: 0.5357 - ETA: 2s - loss: 0.6939 - accuracy: 0.53 - ETA: 2s - loss: 0.6943 - accuracy: 0.51 - ETA: 1s - loss: 0.6944 - accuracy: 0.51 - ETA: 1s - loss: 0.6944 - accuracy: 0.51 - ETA: 1s - loss: 0.6943 - accuracy: 0.51 - ETA: 1s - loss: 0.6941 - accuracy: 0.52 - ETA: 1s - loss: 0.6940 - accuracy: 0.52 - ETA: 1s - loss: 0.6940 - accuracy: 0.52 - ETA: 1s - loss: 0.6939 - accuracy: 0.52 - ETA: 1s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.52 - ETA: 0s - loss: 0.6936 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.52 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - 2s 133us/step - loss: 0.6931 - accuracy: 0.5230 - val_loss: 0.6898 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:22 - loss: 0.6906 - accuracy: 0.62 - ETA: 3s - loss: 0.6940 - accuracy: 0.4866 - ETA: 2s - loss: 0.6937 - accuracy: 0.50 - ETA: 2s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6939 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - 2s 133us/step - loss: 0.6919 - accuracy: 0.5143 - val_loss: 0.6878 - val_accuracy: 0.5959\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:24 - loss: 0.6967 - accuracy: 0.37 - ETA: 3s - loss: 0.6934 - accuracy: 0.5091 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - ETA: 0s - loss: 0.6915 - accuracy: 0.52 - ETA: 0s - loss: 0.6913 - accuracy: 0.52 - ETA: 0s - loss: 0.6911 - accuracy: 0.52 - ETA: 0s - loss: 0.6909 - accuracy: 0.52 - ETA: 0s - loss: 0.6907 - accuracy: 0.52 - ETA: 0s - loss: 0.6904 - accuracy: 0.52 - ETA: 0s - loss: 0.6902 - accuracy: 0.52 - ETA: 0s - loss: 0.6901 - accuracy: 0.53 - ETA: 0s - loss: 0.6899 - accuracy: 0.53 - ETA: 0s - loss: 0.6895 - accuracy: 0.53 - ETA: 0s - loss: 0.6893 - accuracy: 0.53 - ETA: 0s - loss: 0.6891 - accuracy: 0.53 - 2s 132us/step - loss: 0.6891 - accuracy: 0.5378 - val_loss: 0.6772 - val_accuracy: 0.6087\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:24 - loss: 0.6978 - accuracy: 0.37 - ETA: 3s - loss: 0.6921 - accuracy: 0.5477 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 1s - loss: 0.6932 - accuracy: 0.52 - ETA: 1s - loss: 0.6929 - accuracy: 0.52 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6932 - accuracy: 0.52 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.53 - ETA: 0s - loss: 0.6922 - accuracy: 0.53 - ETA: 0s - loss: 0.6920 - accuracy: 0.53 - ETA: 0s - loss: 0.6916 - accuracy: 0.53 - ETA: 0s - loss: 0.6914 - accuracy: 0.53 - ETA: 0s - loss: 0.6913 - accuracy: 0.53 - ETA: 0s - loss: 0.6913 - accuracy: 0.53 - ETA: 0s - loss: 0.6910 - accuracy: 0.53 - ETA: 0s - loss: 0.6907 - accuracy: 0.53 - ETA: 0s - loss: 0.6905 - accuracy: 0.53 - ETA: 0s - loss: 0.6905 - accuracy: 0.53 - ETA: 0s - loss: 0.6903 - accuracy: 0.53 - ETA: 0s - loss: 0.6900 - accuracy: 0.53 - ETA: 0s - loss: 0.6898 - accuracy: 0.54 - 2s 137us/step - loss: 0.6897 - accuracy: 0.5411 - val_loss: 0.6795 - val_accuracy: 0.5668\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 50us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:24 - loss: 0.6909 - accuracy: 0.87 - ETA: 3s - loss: 0.6954 - accuracy: 0.4630 - ETA: 2s - loss: 0.6954 - accuracy: 0.47 - ETA: 2s - loss: 0.6955 - accuracy: 0.46 - ETA: 2s - loss: 0.6953 - accuracy: 0.47 - ETA: 1s - loss: 0.6952 - accuracy: 0.48 - ETA: 1s - loss: 0.6951 - accuracy: 0.48 - ETA: 1s - loss: 0.6951 - accuracy: 0.48 - ETA: 1s - loss: 0.6950 - accuracy: 0.49 - ETA: 1s - loss: 0.6949 - accuracy: 0.49 - ETA: 1s - loss: 0.6948 - accuracy: 0.49 - ETA: 1s - loss: 0.6946 - accuracy: 0.49 - ETA: 1s - loss: 0.6946 - accuracy: 0.50 - ETA: 1s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6944 - accuracy: 0.50 - ETA: 0s - loss: 0.6944 - accuracy: 0.50 - ETA: 0s - loss: 0.6944 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - 2s 137us/step - loss: 0.6930 - accuracy: 0.5114 - val_loss: 0.6876 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 54us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 2:27 - loss: 0.6858 - accuracy: 1.00 - ETA: 4s - loss: 0.6946 - accuracy: 0.5139 - ETA: 2s - loss: 0.6947 - accuracy: 0.49 - ETA: 2s - loss: 0.6949 - accuracy: 0.49 - ETA: 1s - loss: 0.6949 - accuracy: 0.49 - ETA: 1s - loss: 0.6946 - accuracy: 0.49 - ETA: 1s - loss: 0.6946 - accuracy: 0.49 - ETA: 1s - loss: 0.6944 - accuracy: 0.49 - ETA: 1s - loss: 0.6943 - accuracy: 0.50 - ETA: 1s - loss: 0.6943 - accuracy: 0.50 - ETA: 1s - loss: 0.6941 - accuracy: 0.50 - ETA: 1s - loss: 0.6940 - accuracy: 0.50 - ETA: 1s - loss: 0.6939 - accuracy: 0.50 - ETA: 1s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6919 - accuracy: 0.51 - ETA: 0s - loss: 0.6917 - accuracy: 0.51 - 2s 138us/step - loss: 0.6916 - accuracy: 0.5200 - val_loss: 0.6839 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:22 - loss: 0.6950 - accuracy: 0.37 - ETA: 3s - loss: 0.6946 - accuracy: 0.4977 - ETA: 2s - loss: 0.6941 - accuracy: 0.52 - ETA: 2s - loss: 0.6945 - accuracy: 0.50 - ETA: 1s - loss: 0.6943 - accuracy: 0.50 - ETA: 1s - loss: 0.6941 - accuracy: 0.51 - ETA: 1s - loss: 0.6942 - accuracy: 0.50 - ETA: 1s - loss: 0.6942 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6922 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - 2s 139us/step - loss: 0.6918 - accuracy: 0.5268 - val_loss: 0.6845 - val_accuracy: 0.5298\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:20 - loss: 0.7043 - accuracy: 0.0000e+ - ETA: 3s - loss: 0.6959 - accuracy: 0.4705     - ETA: 2s - loss: 0.6954 - accuracy: 0.49 - ETA: 2s - loss: 0.6952 - accuracy: 0.48 - ETA: 1s - loss: 0.6951 - accuracy: 0.49 - ETA: 1s - loss: 0.6954 - accuracy: 0.48 - ETA: 1s - loss: 0.6953 - accuracy: 0.48 - ETA: 1s - loss: 0.6951 - accuracy: 0.48 - ETA: 1s - loss: 0.6951 - accuracy: 0.48 - ETA: 1s - loss: 0.6950 - accuracy: 0.48 - ETA: 1s - loss: 0.6949 - accuracy: 0.49 - ETA: 1s - loss: 0.6947 - accuracy: 0.49 - ETA: 1s - loss: 0.6946 - accuracy: 0.50 - ETA: 1s - loss: 0.6945 - accuracy: 0.50 - ETA: 0s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - 2s 139us/step - loss: 0.6921 - accuracy: 0.5250 - val_loss: 0.6864 - val_accuracy: 0.5277\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 58us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:19 - loss: 0.6941 - accuracy: 0.37 - ETA: 5s - loss: 0.6933 - accuracy: 0.5368 - ETA: 3s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6921 - accuracy: 0.53 - ETA: 2s - loss: 0.6922 - accuracy: 0.52 - ETA: 2s - loss: 0.6922 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6909 - accuracy: 0.53 - ETA: 1s - loss: 0.6895 - accuracy: 0.54 - ETA: 1s - loss: 0.6890 - accuracy: 0.54 - ETA: 1s - loss: 0.6878 - accuracy: 0.55 - ETA: 1s - loss: 0.6870 - accuracy: 0.55 - ETA: 1s - loss: 0.6859 - accuracy: 0.56 - ETA: 1s - loss: 0.6854 - accuracy: 0.56 - ETA: 1s - loss: 0.6842 - accuracy: 0.56 - ETA: 1s - loss: 0.6836 - accuracy: 0.56 - ETA: 0s - loss: 0.6821 - accuracy: 0.57 - ETA: 0s - loss: 0.6812 - accuracy: 0.57 - ETA: 0s - loss: 0.6797 - accuracy: 0.57 - ETA: 0s - loss: 0.6781 - accuracy: 0.57 - ETA: 0s - loss: 0.6766 - accuracy: 0.58 - ETA: 0s - loss: 0.6752 - accuracy: 0.58 - ETA: 0s - loss: 0.6744 - accuracy: 0.58 - ETA: 0s - loss: 0.6723 - accuracy: 0.58 - ETA: 0s - loss: 0.6713 - accuracy: 0.58 - ETA: 0s - loss: 0.6708 - accuracy: 0.58 - ETA: 0s - loss: 0.6689 - accuracy: 0.59 - ETA: 0s - loss: 0.6675 - accuracy: 0.59 - ETA: 0s - loss: 0.6671 - accuracy: 0.59 - ETA: 0s - loss: 0.6663 - accuracy: 0.59 - ETA: 0s - loss: 0.6656 - accuracy: 0.59 - ETA: 0s - loss: 0.6643 - accuracy: 0.59 - ETA: 0s - loss: 0.6635 - accuracy: 0.59 - 2s 149us/step - loss: 0.6632 - accuracy: 0.5996 - val_loss: 0.6126 - val_accuracy: 0.7095\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:06 - loss: 0.6932 - accuracy: 0.87 - ETA: 5s - loss: 0.6932 - accuracy: 0.5294 - ETA: 3s - loss: 0.6928 - accuracy: 0.53 - ETA: 2s - loss: 0.6926 - accuracy: 0.52 - ETA: 2s - loss: 0.6923 - accuracy: 0.52 - ETA: 2s - loss: 0.6916 - accuracy: 0.52 - ETA: 2s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6914 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.52 - ETA: 1s - loss: 0.6902 - accuracy: 0.52 - ETA: 1s - loss: 0.6898 - accuracy: 0.53 - ETA: 1s - loss: 0.6896 - accuracy: 0.52 - ETA: 1s - loss: 0.6890 - accuracy: 0.53 - ETA: 1s - loss: 0.6880 - accuracy: 0.53 - ETA: 1s - loss: 0.6869 - accuracy: 0.54 - ETA: 0s - loss: 0.6858 - accuracy: 0.54 - ETA: 0s - loss: 0.6847 - accuracy: 0.55 - ETA: 0s - loss: 0.6829 - accuracy: 0.55 - ETA: 0s - loss: 0.6823 - accuracy: 0.55 - ETA: 0s - loss: 0.6815 - accuracy: 0.55 - ETA: 0s - loss: 0.6800 - accuracy: 0.55 - ETA: 0s - loss: 0.6782 - accuracy: 0.56 - ETA: 0s - loss: 0.6775 - accuracy: 0.56 - ETA: 0s - loss: 0.6767 - accuracy: 0.56 - ETA: 0s - loss: 0.6755 - accuracy: 0.56 - ETA: 0s - loss: 0.6741 - accuracy: 0.57 - ETA: 0s - loss: 0.6734 - accuracy: 0.57 - ETA: 0s - loss: 0.6732 - accuracy: 0.57 - ETA: 0s - loss: 0.6719 - accuracy: 0.57 - ETA: 0s - loss: 0.6706 - accuracy: 0.57 - ETA: 0s - loss: 0.6699 - accuracy: 0.58 - ETA: 0s - loss: 0.6686 - accuracy: 0.58 - 2s 144us/step - loss: 0.6681 - accuracy: 0.5856 - val_loss: 0.6206 - val_accuracy: 0.7294\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:08 - loss: 0.6956 - accuracy: 0.25 - ETA: 5s - loss: 0.6923 - accuracy: 0.5490 - ETA: 3s - loss: 0.6925 - accuracy: 0.53 - ETA: 2s - loss: 0.6927 - accuracy: 0.52 - ETA: 2s - loss: 0.6925 - accuracy: 0.52 - ETA: 2s - loss: 0.6923 - accuracy: 0.51 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6915 - accuracy: 0.52 - ETA: 1s - loss: 0.6909 - accuracy: 0.53 - ETA: 1s - loss: 0.6895 - accuracy: 0.54 - ETA: 1s - loss: 0.6881 - accuracy: 0.54 - ETA: 1s - loss: 0.6868 - accuracy: 0.54 - ETA: 1s - loss: 0.6863 - accuracy: 0.54 - ETA: 1s - loss: 0.6846 - accuracy: 0.55 - ETA: 1s - loss: 0.6835 - accuracy: 0.55 - ETA: 0s - loss: 0.6815 - accuracy: 0.55 - ETA: 0s - loss: 0.6794 - accuracy: 0.56 - ETA: 0s - loss: 0.6775 - accuracy: 0.57 - ETA: 0s - loss: 0.6763 - accuracy: 0.57 - ETA: 0s - loss: 0.6737 - accuracy: 0.57 - ETA: 0s - loss: 0.6720 - accuracy: 0.58 - ETA: 0s - loss: 0.6699 - accuracy: 0.58 - ETA: 0s - loss: 0.6685 - accuracy: 0.58 - ETA: 0s - loss: 0.6673 - accuracy: 0.58 - ETA: 0s - loss: 0.6654 - accuracy: 0.59 - ETA: 0s - loss: 0.6642 - accuracy: 0.59 - ETA: 0s - loss: 0.6632 - accuracy: 0.59 - ETA: 0s - loss: 0.6620 - accuracy: 0.59 - ETA: 0s - loss: 0.6609 - accuracy: 0.59 - ETA: 0s - loss: 0.6593 - accuracy: 0.59 - ETA: 0s - loss: 0.6594 - accuracy: 0.59 - ETA: 0s - loss: 0.6583 - accuracy: 0.59 - 2s 145us/step - loss: 0.6586 - accuracy: 0.5985 - val_loss: 0.6057 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 61us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:17 - loss: 0.6846 - accuracy: 0.87 - ETA: 5s - loss: 0.6925 - accuracy: 0.5426 - ETA: 3s - loss: 0.6901 - accuracy: 0.56 - ETA: 2s - loss: 0.6886 - accuracy: 0.55 - ETA: 2s - loss: 0.6888 - accuracy: 0.55 - ETA: 2s - loss: 0.6884 - accuracy: 0.54 - ETA: 1s - loss: 0.6873 - accuracy: 0.55 - ETA: 1s - loss: 0.6865 - accuracy: 0.55 - ETA: 1s - loss: 0.6857 - accuracy: 0.56 - ETA: 1s - loss: 0.6840 - accuracy: 0.56 - ETA: 1s - loss: 0.6835 - accuracy: 0.57 - ETA: 1s - loss: 0.6822 - accuracy: 0.57 - ETA: 1s - loss: 0.6811 - accuracy: 0.57 - ETA: 1s - loss: 0.6794 - accuracy: 0.57 - ETA: 1s - loss: 0.6782 - accuracy: 0.57 - ETA: 1s - loss: 0.6768 - accuracy: 0.57 - ETA: 0s - loss: 0.6752 - accuracy: 0.57 - ETA: 0s - loss: 0.6735 - accuracy: 0.58 - ETA: 0s - loss: 0.6719 - accuracy: 0.58 - ETA: 0s - loss: 0.6700 - accuracy: 0.58 - ETA: 0s - loss: 0.6685 - accuracy: 0.59 - ETA: 0s - loss: 0.6681 - accuracy: 0.59 - ETA: 0s - loss: 0.6677 - accuracy: 0.59 - ETA: 0s - loss: 0.6662 - accuracy: 0.59 - ETA: 0s - loss: 0.6649 - accuracy: 0.59 - ETA: 0s - loss: 0.6641 - accuracy: 0.59 - ETA: 0s - loss: 0.6632 - accuracy: 0.59 - ETA: 0s - loss: 0.6623 - accuracy: 0.60 - ETA: 0s - loss: 0.6616 - accuracy: 0.60 - ETA: 0s - loss: 0.6599 - accuracy: 0.60 - ETA: 0s - loss: 0.6588 - accuracy: 0.60 - ETA: 0s - loss: 0.6588 - accuracy: 0.60 - ETA: 0s - loss: 0.6581 - accuracy: 0.60 - ETA: 0s - loss: 0.6567 - accuracy: 0.60 - 2s 152us/step - loss: 0.6566 - accuracy: 0.6096 - val_loss: 0.6038 - val_accuracy: 0.7145\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:05 - loss: 0.6941 - accuracy: 0.25 - ETA: 6s - loss: 0.6932 - accuracy: 0.5183 - ETA: 4s - loss: 0.6930 - accuracy: 0.52 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 3s - loss: 0.6925 - accuracy: 0.51 - ETA: 2s - loss: 0.6921 - accuracy: 0.51 - ETA: 2s - loss: 0.6916 - accuracy: 0.51 - ETA: 2s - loss: 0.6912 - accuracy: 0.51 - ETA: 2s - loss: 0.6902 - accuracy: 0.52 - ETA: 2s - loss: 0.6899 - accuracy: 0.52 - ETA: 1s - loss: 0.6897 - accuracy: 0.51 - ETA: 1s - loss: 0.6898 - accuracy: 0.51 - ETA: 1s - loss: 0.6893 - accuracy: 0.51 - ETA: 1s - loss: 0.6891 - accuracy: 0.51 - ETA: 1s - loss: 0.6886 - accuracy: 0.51 - ETA: 1s - loss: 0.6881 - accuracy: 0.51 - ETA: 1s - loss: 0.6875 - accuracy: 0.52 - ETA: 1s - loss: 0.6871 - accuracy: 0.52 - ETA: 1s - loss: 0.6868 - accuracy: 0.53 - ETA: 1s - loss: 0.6856 - accuracy: 0.53 - ETA: 1s - loss: 0.6846 - accuracy: 0.53 - ETA: 1s - loss: 0.6842 - accuracy: 0.54 - ETA: 0s - loss: 0.6832 - accuracy: 0.54 - ETA: 0s - loss: 0.6828 - accuracy: 0.54 - ETA: 0s - loss: 0.6816 - accuracy: 0.55 - ETA: 0s - loss: 0.6804 - accuracy: 0.55 - ETA: 0s - loss: 0.6801 - accuracy: 0.55 - ETA: 0s - loss: 0.6799 - accuracy: 0.55 - ETA: 0s - loss: 0.6788 - accuracy: 0.55 - ETA: 0s - loss: 0.6783 - accuracy: 0.56 - ETA: 0s - loss: 0.6775 - accuracy: 0.56 - ETA: 0s - loss: 0.6765 - accuracy: 0.56 - ETA: 0s - loss: 0.6760 - accuracy: 0.56 - ETA: 0s - loss: 0.6753 - accuracy: 0.56 - ETA: 0s - loss: 0.6744 - accuracy: 0.57 - ETA: 0s - loss: 0.6731 - accuracy: 0.57 - ETA: 0s - loss: 0.6726 - accuracy: 0.57 - ETA: 0s - loss: 0.6718 - accuracy: 0.57 - ETA: 0s - loss: 0.6708 - accuracy: 0.57 - 2s 175us/step - loss: 0.6704 - accuracy: 0.5800 - val_loss: 0.6222 - val_accuracy: 0.7202\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 70us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:19 - loss: 0.6938 - accuracy: 0.75 - ETA: 7s - loss: 0.6933 - accuracy: 0.5691 - ETA: 4s - loss: 0.6932 - accuracy: 0.54 - ETA: 3s - loss: 0.6930 - accuracy: 0.55 - ETA: 2s - loss: 0.6928 - accuracy: 0.55 - ETA: 2s - loss: 0.6926 - accuracy: 0.54 - ETA: 2s - loss: 0.6924 - accuracy: 0.54 - ETA: 2s - loss: 0.6919 - accuracy: 0.55 - ETA: 2s - loss: 0.6915 - accuracy: 0.55 - ETA: 2s - loss: 0.6909 - accuracy: 0.55 - ETA: 1s - loss: 0.6902 - accuracy: 0.56 - ETA: 1s - loss: 0.6896 - accuracy: 0.56 - ETA: 1s - loss: 0.6890 - accuracy: 0.56 - ETA: 1s - loss: 0.6883 - accuracy: 0.56 - ETA: 1s - loss: 0.6880 - accuracy: 0.56 - ETA: 1s - loss: 0.6873 - accuracy: 0.56 - ETA: 1s - loss: 0.6864 - accuracy: 0.57 - ETA: 1s - loss: 0.6850 - accuracy: 0.57 - ETA: 1s - loss: 0.6846 - accuracy: 0.57 - ETA: 1s - loss: 0.6836 - accuracy: 0.57 - ETA: 1s - loss: 0.6831 - accuracy: 0.57 - ETA: 1s - loss: 0.6822 - accuracy: 0.57 - ETA: 1s - loss: 0.6814 - accuracy: 0.57 - ETA: 0s - loss: 0.6809 - accuracy: 0.57 - ETA: 0s - loss: 0.6805 - accuracy: 0.57 - ETA: 0s - loss: 0.6794 - accuracy: 0.58 - ETA: 0s - loss: 0.6783 - accuracy: 0.58 - ETA: 0s - loss: 0.6768 - accuracy: 0.58 - ETA: 0s - loss: 0.6755 - accuracy: 0.58 - ETA: 0s - loss: 0.6742 - accuracy: 0.58 - ETA: 0s - loss: 0.6729 - accuracy: 0.59 - ETA: 0s - loss: 0.6720 - accuracy: 0.59 - ETA: 0s - loss: 0.6713 - accuracy: 0.59 - ETA: 0s - loss: 0.6705 - accuracy: 0.59 - ETA: 0s - loss: 0.6696 - accuracy: 0.59 - ETA: 0s - loss: 0.6687 - accuracy: 0.59 - ETA: 0s - loss: 0.6674 - accuracy: 0.59 - ETA: 0s - loss: 0.6672 - accuracy: 0.59 - ETA: 0s - loss: 0.6664 - accuracy: 0.59 - ETA: 0s - loss: 0.6662 - accuracy: 0.60 - ETA: 0s - loss: 0.6658 - accuracy: 0.60 - ETA: 0s - loss: 0.6647 - accuracy: 0.60 - 2s 186us/step - loss: 0.6643 - accuracy: 0.6038 - val_loss: 0.6160 - val_accuracy: 0.7152\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 69us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 3:05 - loss: 0.6935 - accuracy: 0.87 - ETA: 5s - loss: 0.6910 - accuracy: 0.5711 - ETA: 3s - loss: 0.6919 - accuracy: 0.52 - ETA: 2s - loss: 0.6912 - accuracy: 0.52 - ETA: 2s - loss: 0.6904 - accuracy: 0.52 - ETA: 1s - loss: 0.6876 - accuracy: 0.53 - ETA: 1s - loss: 0.6866 - accuracy: 0.53 - ETA: 1s - loss: 0.6850 - accuracy: 0.53 - ETA: 1s - loss: 0.6847 - accuracy: 0.53 - ETA: 1s - loss: 0.6828 - accuracy: 0.53 - ETA: 1s - loss: 0.6822 - accuracy: 0.53 - ETA: 1s - loss: 0.6812 - accuracy: 0.54 - ETA: 1s - loss: 0.6810 - accuracy: 0.55 - ETA: 1s - loss: 0.6793 - accuracy: 0.56 - ETA: 0s - loss: 0.6768 - accuracy: 0.56 - ETA: 0s - loss: 0.6750 - accuracy: 0.56 - ETA: 0s - loss: 0.6738 - accuracy: 0.57 - ETA: 0s - loss: 0.6721 - accuracy: 0.58 - ETA: 0s - loss: 0.6707 - accuracy: 0.58 - ETA: 0s - loss: 0.6689 - accuracy: 0.59 - ETA: 0s - loss: 0.6674 - accuracy: 0.59 - ETA: 0s - loss: 0.6667 - accuracy: 0.60 - ETA: 0s - loss: 0.6659 - accuracy: 0.60 - ETA: 0s - loss: 0.6646 - accuracy: 0.60 - ETA: 0s - loss: 0.6632 - accuracy: 0.61 - ETA: 0s - loss: 0.6618 - accuracy: 0.61 - ETA: 0s - loss: 0.6608 - accuracy: 0.61 - ETA: 0s - loss: 0.6603 - accuracy: 0.62 - ETA: 0s - loss: 0.6593 - accuracy: 0.62 - ETA: 0s - loss: 0.6580 - accuracy: 0.62 - ETA: 0s - loss: 0.6571 - accuracy: 0.63 - 2s 140us/step - loss: 0.6567 - accuracy: 0.6315 - val_loss: 0.6189 - val_accuracy: 0.7259\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:35 - loss: 0.6927 - accuracy: 0.62 - ETA: 5s - loss: 0.6926 - accuracy: 0.5294 - ETA: 3s - loss: 0.6934 - accuracy: 0.50 - ETA: 2s - loss: 0.6907 - accuracy: 0.52 - ETA: 2s - loss: 0.6904 - accuracy: 0.51 - ETA: 2s - loss: 0.6900 - accuracy: 0.51 - ETA: 1s - loss: 0.6884 - accuracy: 0.52 - ETA: 1s - loss: 0.6878 - accuracy: 0.52 - ETA: 1s - loss: 0.6872 - accuracy: 0.53 - ETA: 1s - loss: 0.6860 - accuracy: 0.53 - ETA: 1s - loss: 0.6844 - accuracy: 0.54 - ETA: 1s - loss: 0.6816 - accuracy: 0.55 - ETA: 1s - loss: 0.6792 - accuracy: 0.56 - ETA: 1s - loss: 0.6779 - accuracy: 0.56 - ETA: 1s - loss: 0.6754 - accuracy: 0.57 - ETA: 0s - loss: 0.6730 - accuracy: 0.58 - ETA: 0s - loss: 0.6711 - accuracy: 0.58 - ETA: 0s - loss: 0.6692 - accuracy: 0.58 - ETA: 0s - loss: 0.6678 - accuracy: 0.59 - ETA: 0s - loss: 0.6676 - accuracy: 0.59 - ETA: 0s - loss: 0.6665 - accuracy: 0.59 - ETA: 0s - loss: 0.6659 - accuracy: 0.59 - ETA: 0s - loss: 0.6640 - accuracy: 0.60 - ETA: 0s - loss: 0.6619 - accuracy: 0.60 - ETA: 0s - loss: 0.6610 - accuracy: 0.60 - ETA: 0s - loss: 0.6597 - accuracy: 0.61 - ETA: 0s - loss: 0.6585 - accuracy: 0.61 - ETA: 0s - loss: 0.6572 - accuracy: 0.61 - ETA: 0s - loss: 0.6556 - accuracy: 0.62 - ETA: 0s - loss: 0.6551 - accuracy: 0.62 - ETA: 0s - loss: 0.6533 - accuracy: 0.62 - ETA: 0s - loss: 0.6515 - accuracy: 0.62 - 2s 145us/step - loss: 0.6515 - accuracy: 0.6293 - val_loss: 0.6027 - val_accuracy: 0.7131\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:09 - loss: 0.6939 - accuracy: 0.50 - ETA: 5s - loss: 0.6938 - accuracy: 0.5325 - ETA: 3s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.52 - ETA: 2s - loss: 0.6918 - accuracy: 0.53 - ETA: 1s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6904 - accuracy: 0.54 - ETA: 1s - loss: 0.6896 - accuracy: 0.54 - ETA: 1s - loss: 0.6889 - accuracy: 0.54 - ETA: 1s - loss: 0.6882 - accuracy: 0.55 - ETA: 1s - loss: 0.6872 - accuracy: 0.56 - ETA: 1s - loss: 0.6859 - accuracy: 0.56 - ETA: 1s - loss: 0.6844 - accuracy: 0.57 - ETA: 1s - loss: 0.6826 - accuracy: 0.57 - ETA: 1s - loss: 0.6808 - accuracy: 0.58 - ETA: 1s - loss: 0.6800 - accuracy: 0.58 - ETA: 0s - loss: 0.6784 - accuracy: 0.59 - ETA: 0s - loss: 0.6770 - accuracy: 0.59 - ETA: 0s - loss: 0.6760 - accuracy: 0.59 - ETA: 0s - loss: 0.6756 - accuracy: 0.59 - ETA: 0s - loss: 0.6743 - accuracy: 0.60 - ETA: 0s - loss: 0.6730 - accuracy: 0.60 - ETA: 0s - loss: 0.6710 - accuracy: 0.60 - ETA: 0s - loss: 0.6697 - accuracy: 0.60 - ETA: 0s - loss: 0.6681 - accuracy: 0.61 - ETA: 0s - loss: 0.6675 - accuracy: 0.61 - ETA: 0s - loss: 0.6660 - accuracy: 0.61 - ETA: 0s - loss: 0.6650 - accuracy: 0.61 - ETA: 0s - loss: 0.6641 - accuracy: 0.61 - ETA: 0s - loss: 0.6634 - accuracy: 0.62 - ETA: 0s - loss: 0.6625 - accuracy: 0.62 - ETA: 0s - loss: 0.6611 - accuracy: 0.62 - ETA: 0s - loss: 0.6599 - accuracy: 0.62 - ETA: 0s - loss: 0.6594 - accuracy: 0.62 - 2s 151us/step - loss: 0.6594 - accuracy: 0.6281 - val_loss: 0.6077 - val_accuracy: 0.7180\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 58us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:16 - loss: 0.7041 - accuracy: 0.12 - ETA: 5s - loss: 0.6938 - accuracy: 0.5052 - ETA: 3s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6928 - accuracy: 0.52 - ETA: 2s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6914 - accuracy: 0.53 - ETA: 1s - loss: 0.6903 - accuracy: 0.54 - ETA: 1s - loss: 0.6888 - accuracy: 0.55 - ETA: 1s - loss: 0.6875 - accuracy: 0.55 - ETA: 1s - loss: 0.6859 - accuracy: 0.56 - ETA: 1s - loss: 0.6839 - accuracy: 0.57 - ETA: 1s - loss: 0.6821 - accuracy: 0.57 - ETA: 1s - loss: 0.6788 - accuracy: 0.58 - ETA: 1s - loss: 0.6765 - accuracy: 0.58 - ETA: 1s - loss: 0.6741 - accuracy: 0.59 - ETA: 0s - loss: 0.6725 - accuracy: 0.59 - ETA: 0s - loss: 0.6696 - accuracy: 0.60 - ETA: 0s - loss: 0.6664 - accuracy: 0.60 - ETA: 0s - loss: 0.6634 - accuracy: 0.61 - ETA: 0s - loss: 0.6617 - accuracy: 0.61 - ETA: 0s - loss: 0.6598 - accuracy: 0.61 - ETA: 0s - loss: 0.6586 - accuracy: 0.61 - ETA: 0s - loss: 0.6566 - accuracy: 0.62 - ETA: 0s - loss: 0.6554 - accuracy: 0.62 - ETA: 0s - loss: 0.6534 - accuracy: 0.62 - ETA: 0s - loss: 0.6516 - accuracy: 0.62 - ETA: 0s - loss: 0.6505 - accuracy: 0.63 - ETA: 0s - loss: 0.6482 - accuracy: 0.63 - ETA: 0s - loss: 0.6476 - accuracy: 0.63 - ETA: 0s - loss: 0.6466 - accuracy: 0.63 - ETA: 0s - loss: 0.6451 - accuracy: 0.63 - ETA: 0s - loss: 0.6442 - accuracy: 0.64 - 2s 149us/step - loss: 0.6434 - accuracy: 0.6415 - val_loss: 0.5875 - val_accuracy: 0.7131\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 4:13 - loss: 0.6951 - accuracy: 0.25 - ETA: 6s - loss: 0.6944 - accuracy: 0.4951 - ETA: 3s - loss: 0.6941 - accuracy: 0.48 - ETA: 2s - loss: 0.6936 - accuracy: 0.49 - ETA: 2s - loss: 0.6928 - accuracy: 0.51 - ETA: 2s - loss: 0.6924 - accuracy: 0.51 - ETA: 2s - loss: 0.6917 - accuracy: 0.52 - ETA: 1s - loss: 0.6911 - accuracy: 0.52 - ETA: 1s - loss: 0.6893 - accuracy: 0.53 - ETA: 1s - loss: 0.6886 - accuracy: 0.53 - ETA: 1s - loss: 0.6879 - accuracy: 0.53 - ETA: 1s - loss: 0.6872 - accuracy: 0.54 - ETA: 1s - loss: 0.6853 - accuracy: 0.55 - ETA: 1s - loss: 0.6838 - accuracy: 0.55 - ETA: 1s - loss: 0.6823 - accuracy: 0.56 - ETA: 1s - loss: 0.6816 - accuracy: 0.56 - ETA: 1s - loss: 0.6806 - accuracy: 0.56 - ETA: 0s - loss: 0.6789 - accuracy: 0.57 - ETA: 0s - loss: 0.6776 - accuracy: 0.57 - ETA: 0s - loss: 0.6764 - accuracy: 0.58 - ETA: 0s - loss: 0.6750 - accuracy: 0.58 - ETA: 0s - loss: 0.6732 - accuracy: 0.58 - ETA: 0s - loss: 0.6720 - accuracy: 0.59 - ETA: 0s - loss: 0.6711 - accuracy: 0.59 - ETA: 0s - loss: 0.6692 - accuracy: 0.59 - ETA: 0s - loss: 0.6674 - accuracy: 0.60 - ETA: 0s - loss: 0.6658 - accuracy: 0.60 - ETA: 0s - loss: 0.6651 - accuracy: 0.60 - ETA: 0s - loss: 0.6643 - accuracy: 0.60 - ETA: 0s - loss: 0.6631 - accuracy: 0.61 - ETA: 0s - loss: 0.6616 - accuracy: 0.61 - ETA: 0s - loss: 0.6603 - accuracy: 0.61 - ETA: 0s - loss: 0.6589 - accuracy: 0.61 - 2s 151us/step - loss: 0.6581 - accuracy: 0.6171 - val_loss: 0.6073 - val_accuracy: 0.7173\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:02 - loss: 0.6904 - accuracy: 0.62 - ETA: 4s - loss: 0.6925 - accuracy: 0.5457 - ETA: 3s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.53 - ETA: 2s - loss: 0.6921 - accuracy: 0.54 - ETA: 1s - loss: 0.6912 - accuracy: 0.55 - ETA: 1s - loss: 0.6897 - accuracy: 0.56 - ETA: 1s - loss: 0.6896 - accuracy: 0.56 - ETA: 1s - loss: 0.6888 - accuracy: 0.56 - ETA: 1s - loss: 0.6871 - accuracy: 0.57 - ETA: 1s - loss: 0.6866 - accuracy: 0.57 - ETA: 1s - loss: 0.6851 - accuracy: 0.57 - ETA: 1s - loss: 0.6838 - accuracy: 0.58 - ETA: 1s - loss: 0.6816 - accuracy: 0.58 - ETA: 1s - loss: 0.6797 - accuracy: 0.59 - ETA: 0s - loss: 0.6779 - accuracy: 0.59 - ETA: 0s - loss: 0.6757 - accuracy: 0.60 - ETA: 0s - loss: 0.6733 - accuracy: 0.60 - ETA: 0s - loss: 0.6719 - accuracy: 0.60 - ETA: 0s - loss: 0.6708 - accuracy: 0.60 - ETA: 0s - loss: 0.6690 - accuracy: 0.61 - ETA: 0s - loss: 0.6677 - accuracy: 0.61 - ETA: 0s - loss: 0.6666 - accuracy: 0.61 - ETA: 0s - loss: 0.6658 - accuracy: 0.61 - ETA: 0s - loss: 0.6634 - accuracy: 0.61 - ETA: 0s - loss: 0.6616 - accuracy: 0.62 - ETA: 0s - loss: 0.6597 - accuracy: 0.62 - ETA: 0s - loss: 0.6586 - accuracy: 0.62 - ETA: 0s - loss: 0.6579 - accuracy: 0.62 - ETA: 0s - loss: 0.6568 - accuracy: 0.62 - ETA: 0s - loss: 0.6558 - accuracy: 0.62 - ETA: 0s - loss: 0.6545 - accuracy: 0.63 - 2s 145us/step - loss: 0.6538 - accuracy: 0.6323 - val_loss: 0.6024 - val_accuracy: 0.7230\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:08 - loss: 0.6939 - accuracy: 0.50 - ETA: 5s - loss: 0.6946 - accuracy: 0.5074 - ETA: 3s - loss: 0.6943 - accuracy: 0.50 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.53 - ETA: 2s - loss: 0.6921 - accuracy: 0.54 - ETA: 1s - loss: 0.6914 - accuracy: 0.54 - ETA: 1s - loss: 0.6901 - accuracy: 0.55 - ETA: 1s - loss: 0.6877 - accuracy: 0.56 - ETA: 1s - loss: 0.6852 - accuracy: 0.57 - ETA: 1s - loss: 0.6825 - accuracy: 0.58 - ETA: 1s - loss: 0.6817 - accuracy: 0.58 - ETA: 1s - loss: 0.6791 - accuracy: 0.59 - ETA: 1s - loss: 0.6767 - accuracy: 0.59 - ETA: 1s - loss: 0.6735 - accuracy: 0.60 - ETA: 1s - loss: 0.6721 - accuracy: 0.60 - ETA: 0s - loss: 0.6688 - accuracy: 0.61 - ETA: 0s - loss: 0.6668 - accuracy: 0.61 - ETA: 0s - loss: 0.6639 - accuracy: 0.62 - ETA: 0s - loss: 0.6624 - accuracy: 0.62 - ETA: 0s - loss: 0.6606 - accuracy: 0.62 - ETA: 0s - loss: 0.6584 - accuracy: 0.62 - ETA: 0s - loss: 0.6568 - accuracy: 0.62 - ETA: 0s - loss: 0.6555 - accuracy: 0.63 - ETA: 0s - loss: 0.6541 - accuracy: 0.63 - ETA: 0s - loss: 0.6525 - accuracy: 0.63 - ETA: 0s - loss: 0.6512 - accuracy: 0.63 - ETA: 0s - loss: 0.6498 - accuracy: 0.63 - ETA: 0s - loss: 0.6492 - accuracy: 0.63 - ETA: 0s - loss: 0.6477 - accuracy: 0.64 - ETA: 0s - loss: 0.6458 - accuracy: 0.64 - ETA: 0s - loss: 0.6441 - accuracy: 0.64 - ETA: 0s - loss: 0.6434 - accuracy: 0.64 - 2s 149us/step - loss: 0.6428 - accuracy: 0.6481 - val_loss: 0.5906 - val_accuracy: 0.7166\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 58us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:08 - loss: 0.6928 - accuracy: 0.62 - ETA: 5s - loss: 0.6940 - accuracy: 0.4926 - ETA: 3s - loss: 0.6938 - accuracy: 0.50 - ETA: 2s - loss: 0.6929 - accuracy: 0.51 - ETA: 2s - loss: 0.6922 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6899 - accuracy: 0.53 - ETA: 1s - loss: 0.6896 - accuracy: 0.54 - ETA: 1s - loss: 0.6878 - accuracy: 0.55 - ETA: 1s - loss: 0.6870 - accuracy: 0.55 - ETA: 1s - loss: 0.6844 - accuracy: 0.56 - ETA: 1s - loss: 0.6824 - accuracy: 0.56 - ETA: 1s - loss: 0.6808 - accuracy: 0.57 - ETA: 1s - loss: 0.6786 - accuracy: 0.57 - ETA: 1s - loss: 0.6770 - accuracy: 0.58 - ETA: 0s - loss: 0.6743 - accuracy: 0.59 - ETA: 0s - loss: 0.6718 - accuracy: 0.59 - ETA: 0s - loss: 0.6695 - accuracy: 0.59 - ETA: 0s - loss: 0.6675 - accuracy: 0.60 - ETA: 0s - loss: 0.6644 - accuracy: 0.60 - ETA: 0s - loss: 0.6633 - accuracy: 0.61 - ETA: 0s - loss: 0.6614 - accuracy: 0.61 - ETA: 0s - loss: 0.6596 - accuracy: 0.61 - ETA: 0s - loss: 0.6596 - accuracy: 0.61 - ETA: 0s - loss: 0.6584 - accuracy: 0.62 - ETA: 0s - loss: 0.6574 - accuracy: 0.62 - ETA: 0s - loss: 0.6569 - accuracy: 0.62 - ETA: 0s - loss: 0.6557 - accuracy: 0.62 - ETA: 0s - loss: 0.6547 - accuracy: 0.62 - ETA: 0s - loss: 0.6532 - accuracy: 0.63 - ETA: 0s - loss: 0.6522 - accuracy: 0.63 - ETA: 0s - loss: 0.6506 - accuracy: 0.63 - 2s 143us/step - loss: 0.6505 - accuracy: 0.6367 - val_loss: 0.5915 - val_accuracy: 0.7109\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:09 - loss: 0.6967 - accuracy: 0.25 - ETA: 5s - loss: 0.6934 - accuracy: 0.5525 - ETA: 3s - loss: 0.6928 - accuracy: 0.53 - ETA: 2s - loss: 0.6911 - accuracy: 0.54 - ETA: 2s - loss: 0.6905 - accuracy: 0.54 - ETA: 2s - loss: 0.6895 - accuracy: 0.55 - ETA: 1s - loss: 0.6883 - accuracy: 0.55 - ETA: 1s - loss: 0.6871 - accuracy: 0.56 - ETA: 1s - loss: 0.6854 - accuracy: 0.57 - ETA: 1s - loss: 0.6838 - accuracy: 0.57 - ETA: 1s - loss: 0.6822 - accuracy: 0.58 - ETA: 1s - loss: 0.6808 - accuracy: 0.58 - ETA: 1s - loss: 0.6790 - accuracy: 0.59 - ETA: 1s - loss: 0.6777 - accuracy: 0.59 - ETA: 1s - loss: 0.6762 - accuracy: 0.60 - ETA: 0s - loss: 0.6740 - accuracy: 0.60 - ETA: 0s - loss: 0.6713 - accuracy: 0.60 - ETA: 0s - loss: 0.6703 - accuracy: 0.60 - ETA: 0s - loss: 0.6684 - accuracy: 0.61 - ETA: 0s - loss: 0.6664 - accuracy: 0.61 - ETA: 0s - loss: 0.6657 - accuracy: 0.61 - ETA: 0s - loss: 0.6639 - accuracy: 0.61 - ETA: 0s - loss: 0.6623 - accuracy: 0.62 - ETA: 0s - loss: 0.6606 - accuracy: 0.62 - ETA: 0s - loss: 0.6592 - accuracy: 0.62 - ETA: 0s - loss: 0.6580 - accuracy: 0.62 - ETA: 0s - loss: 0.6560 - accuracy: 0.62 - ETA: 0s - loss: 0.6547 - accuracy: 0.63 - ETA: 0s - loss: 0.6534 - accuracy: 0.63 - ETA: 0s - loss: 0.6513 - accuracy: 0.63 - ETA: 0s - loss: 0.6507 - accuracy: 0.63 - ETA: 0s - loss: 0.6498 - accuracy: 0.63 - 2s 145us/step - loss: 0.6489 - accuracy: 0.6394 - val_loss: 0.5948 - val_accuracy: 0.7180\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:09 - loss: 0.6937 - accuracy: 0.75 - ETA: 5s - loss: 0.6945 - accuracy: 0.4853 - ETA: 3s - loss: 0.6937 - accuracy: 0.50 - ETA: 2s - loss: 0.6921 - accuracy: 0.53 - ETA: 2s - loss: 0.6909 - accuracy: 0.54 - ETA: 1s - loss: 0.6903 - accuracy: 0.54 - ETA: 1s - loss: 0.6890 - accuracy: 0.55 - ETA: 1s - loss: 0.6873 - accuracy: 0.56 - ETA: 1s - loss: 0.6858 - accuracy: 0.57 - ETA: 1s - loss: 0.6845 - accuracy: 0.57 - ETA: 1s - loss: 0.6826 - accuracy: 0.58 - ETA: 1s - loss: 0.6805 - accuracy: 0.58 - ETA: 1s - loss: 0.6774 - accuracy: 0.59 - ETA: 1s - loss: 0.6748 - accuracy: 0.60 - ETA: 1s - loss: 0.6721 - accuracy: 0.60 - ETA: 1s - loss: 0.6692 - accuracy: 0.61 - ETA: 0s - loss: 0.6666 - accuracy: 0.61 - ETA: 0s - loss: 0.6635 - accuracy: 0.61 - ETA: 0s - loss: 0.6609 - accuracy: 0.62 - ETA: 0s - loss: 0.6602 - accuracy: 0.62 - ETA: 0s - loss: 0.6579 - accuracy: 0.62 - ETA: 0s - loss: 0.6576 - accuracy: 0.62 - ETA: 0s - loss: 0.6560 - accuracy: 0.63 - ETA: 0s - loss: 0.6553 - accuracy: 0.63 - ETA: 0s - loss: 0.6536 - accuracy: 0.63 - ETA: 0s - loss: 0.6524 - accuracy: 0.63 - ETA: 0s - loss: 0.6502 - accuracy: 0.63 - ETA: 0s - loss: 0.6488 - accuracy: 0.64 - ETA: 0s - loss: 0.6464 - accuracy: 0.64 - ETA: 0s - loss: 0.6442 - accuracy: 0.64 - ETA: 0s - loss: 0.6434 - accuracy: 0.64 - ETA: 0s - loss: 0.6422 - accuracy: 0.64 - ETA: 0s - loss: 0.6414 - accuracy: 0.65 - 2s 149us/step - loss: 0.6404 - accuracy: 0.6509 - val_loss: 0.5927 - val_accuracy: 0.7159\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:08 - loss: 0.6924 - accuracy: 0.50 - ETA: 5s - loss: 0.6943 - accuracy: 0.4900 - ETA: 3s - loss: 0.6928 - accuracy: 0.51 - ETA: 2s - loss: 0.6924 - accuracy: 0.52 - ETA: 2s - loss: 0.6921 - accuracy: 0.53 - ETA: 2s - loss: 0.6920 - accuracy: 0.53 - ETA: 1s - loss: 0.6907 - accuracy: 0.54 - ETA: 1s - loss: 0.6888 - accuracy: 0.55 - ETA: 1s - loss: 0.6876 - accuracy: 0.56 - ETA: 1s - loss: 0.6857 - accuracy: 0.57 - ETA: 1s - loss: 0.6838 - accuracy: 0.57 - ETA: 1s - loss: 0.6816 - accuracy: 0.58 - ETA: 1s - loss: 0.6794 - accuracy: 0.58 - ETA: 1s - loss: 0.6772 - accuracy: 0.59 - ETA: 1s - loss: 0.6754 - accuracy: 0.59 - ETA: 1s - loss: 0.6736 - accuracy: 0.59 - ETA: 1s - loss: 0.6707 - accuracy: 0.60 - ETA: 0s - loss: 0.6695 - accuracy: 0.60 - ETA: 0s - loss: 0.6678 - accuracy: 0.61 - ETA: 0s - loss: 0.6656 - accuracy: 0.61 - ETA: 0s - loss: 0.6638 - accuracy: 0.61 - ETA: 0s - loss: 0.6618 - accuracy: 0.61 - ETA: 0s - loss: 0.6601 - accuracy: 0.62 - ETA: 0s - loss: 0.6588 - accuracy: 0.62 - ETA: 0s - loss: 0.6568 - accuracy: 0.62 - ETA: 0s - loss: 0.6551 - accuracy: 0.62 - ETA: 0s - loss: 0.6544 - accuracy: 0.62 - ETA: 0s - loss: 0.6532 - accuracy: 0.63 - ETA: 0s - loss: 0.6517 - accuracy: 0.63 - ETA: 0s - loss: 0.6511 - accuracy: 0.63 - ETA: 0s - loss: 0.6501 - accuracy: 0.63 - ETA: 0s - loss: 0.6491 - accuracy: 0.63 - ETA: 0s - loss: 0.6483 - accuracy: 0.63 - ETA: 0s - loss: 0.6477 - accuracy: 0.63 - ETA: 0s - loss: 0.6469 - accuracy: 0.63 - 2s 155us/step - loss: 0.6469 - accuracy: 0.6388 - val_loss: 0.6000 - val_accuracy: 0.7173\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:06 - loss: 0.7023 - accuracy: 0.25 - ETA: 4s - loss: 0.6940 - accuracy: 0.5312 - ETA: 3s - loss: 0.6933 - accuracy: 0.52 - ETA: 2s - loss: 0.6922 - accuracy: 0.53 - ETA: 2s - loss: 0.6923 - accuracy: 0.53 - ETA: 2s - loss: 0.6906 - accuracy: 0.54 - ETA: 1s - loss: 0.6895 - accuracy: 0.55 - ETA: 1s - loss: 0.6875 - accuracy: 0.56 - ETA: 1s - loss: 0.6863 - accuracy: 0.56 - ETA: 1s - loss: 0.6853 - accuracy: 0.56 - ETA: 1s - loss: 0.6840 - accuracy: 0.57 - ETA: 1s - loss: 0.6825 - accuracy: 0.57 - ETA: 1s - loss: 0.6813 - accuracy: 0.58 - ETA: 1s - loss: 0.6799 - accuracy: 0.58 - ETA: 1s - loss: 0.6774 - accuracy: 0.59 - ETA: 1s - loss: 0.6754 - accuracy: 0.59 - ETA: 0s - loss: 0.6727 - accuracy: 0.60 - ETA: 0s - loss: 0.6710 - accuracy: 0.60 - ETA: 0s - loss: 0.6700 - accuracy: 0.60 - ETA: 0s - loss: 0.6683 - accuracy: 0.60 - ETA: 0s - loss: 0.6659 - accuracy: 0.61 - ETA: 0s - loss: 0.6628 - accuracy: 0.61 - ETA: 0s - loss: 0.6610 - accuracy: 0.62 - ETA: 0s - loss: 0.6580 - accuracy: 0.62 - ETA: 0s - loss: 0.6571 - accuracy: 0.62 - ETA: 0s - loss: 0.6552 - accuracy: 0.62 - ETA: 0s - loss: 0.6533 - accuracy: 0.63 - ETA: 0s - loss: 0.6529 - accuracy: 0.63 - ETA: 0s - loss: 0.6523 - accuracy: 0.63 - ETA: 0s - loss: 0.6508 - accuracy: 0.63 - ETA: 0s - loss: 0.6501 - accuracy: 0.63 - ETA: 0s - loss: 0.6492 - accuracy: 0.63 - ETA: 0s - loss: 0.6473 - accuracy: 0.64 - 2s 148us/step - loss: 0.6472 - accuracy: 0.6406 - val_loss: 0.5973 - val_accuracy: 0.7124\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:33 - loss: 0.6939 - accuracy: 0.37 - ETA: 4s - loss: 0.6924 - accuracy: 0.5208 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6928 - accuracy: 0.51 - ETA: 1s - loss: 0.6923 - accuracy: 0.51 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6907 - accuracy: 0.54 - ETA: 1s - loss: 0.6904 - accuracy: 0.54 - ETA: 1s - loss: 0.6899 - accuracy: 0.54 - ETA: 1s - loss: 0.6895 - accuracy: 0.55 - ETA: 1s - loss: 0.6888 - accuracy: 0.55 - ETA: 0s - loss: 0.6884 - accuracy: 0.56 - ETA: 0s - loss: 0.6881 - accuracy: 0.56 - ETA: 0s - loss: 0.6872 - accuracy: 0.56 - ETA: 0s - loss: 0.6868 - accuracy: 0.57 - ETA: 0s - loss: 0.6865 - accuracy: 0.57 - ETA: 0s - loss: 0.6862 - accuracy: 0.57 - ETA: 0s - loss: 0.6857 - accuracy: 0.57 - ETA: 0s - loss: 0.6853 - accuracy: 0.57 - ETA: 0s - loss: 0.6848 - accuracy: 0.57 - ETA: 0s - loss: 0.6841 - accuracy: 0.58 - ETA: 0s - loss: 0.6839 - accuracy: 0.58 - ETA: 0s - loss: 0.6835 - accuracy: 0.58 - ETA: 0s - loss: 0.6833 - accuracy: 0.58 - ETA: 0s - loss: 0.6832 - accuracy: 0.58 - ETA: 0s - loss: 0.6831 - accuracy: 0.58 - ETA: 0s - loss: 0.6826 - accuracy: 0.58 - ETA: 0s - loss: 0.6822 - accuracy: 0.58 - 2s 134us/step - loss: 0.6822 - accuracy: 0.5863 - val_loss: 0.6661 - val_accuracy: 0.7053\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:39 - loss: 0.6932 - accuracy: 0.50 - ETA: 4s - loss: 0.6928 - accuracy: 0.5347 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.50 - ETA: 1s - loss: 0.6926 - accuracy: 0.50 - ETA: 1s - loss: 0.6921 - accuracy: 0.50 - ETA: 1s - loss: 0.6918 - accuracy: 0.50 - ETA: 1s - loss: 0.6916 - accuracy: 0.50 - ETA: 1s - loss: 0.6910 - accuracy: 0.51 - ETA: 1s - loss: 0.6904 - accuracy: 0.52 - ETA: 1s - loss: 0.6897 - accuracy: 0.52 - ETA: 1s - loss: 0.6895 - accuracy: 0.52 - ETA: 0s - loss: 0.6889 - accuracy: 0.52 - ETA: 0s - loss: 0.6886 - accuracy: 0.52 - ETA: 0s - loss: 0.6882 - accuracy: 0.53 - ETA: 0s - loss: 0.6877 - accuracy: 0.53 - ETA: 0s - loss: 0.6871 - accuracy: 0.53 - ETA: 0s - loss: 0.6870 - accuracy: 0.53 - ETA: 0s - loss: 0.6865 - accuracy: 0.53 - ETA: 0s - loss: 0.6861 - accuracy: 0.54 - ETA: 0s - loss: 0.6857 - accuracy: 0.54 - ETA: 0s - loss: 0.6854 - accuracy: 0.54 - ETA: 0s - loss: 0.6851 - accuracy: 0.54 - ETA: 0s - loss: 0.6846 - accuracy: 0.54 - ETA: 0s - loss: 0.6842 - accuracy: 0.54 - ETA: 0s - loss: 0.6838 - accuracy: 0.55 - ETA: 0s - loss: 0.6835 - accuracy: 0.55 - ETA: 0s - loss: 0.6834 - accuracy: 0.55 - ETA: 0s - loss: 0.6828 - accuracy: 0.55 - 2s 136us/step - loss: 0.6826 - accuracy: 0.5537 - val_loss: 0.6676 - val_accuracy: 0.7060\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:35 - loss: 0.6930 - accuracy: 0.75 - ETA: 4s - loss: 0.6932 - accuracy: 0.5047 - ETA: 2s - loss: 0.6918 - accuracy: 0.52 - ETA: 2s - loss: 0.6913 - accuracy: 0.52 - ETA: 1s - loss: 0.6918 - accuracy: 0.51 - ETA: 1s - loss: 0.6910 - accuracy: 0.52 - ETA: 1s - loss: 0.6909 - accuracy: 0.52 - ETA: 1s - loss: 0.6905 - accuracy: 0.52 - ETA: 1s - loss: 0.6895 - accuracy: 0.53 - ETA: 1s - loss: 0.6895 - accuracy: 0.53 - ETA: 1s - loss: 0.6890 - accuracy: 0.54 - ETA: 1s - loss: 0.6890 - accuracy: 0.54 - ETA: 1s - loss: 0.6888 - accuracy: 0.54 - ETA: 0s - loss: 0.6888 - accuracy: 0.54 - ETA: 0s - loss: 0.6883 - accuracy: 0.54 - ETA: 0s - loss: 0.6877 - accuracy: 0.55 - ETA: 0s - loss: 0.6875 - accuracy: 0.55 - ETA: 0s - loss: 0.6870 - accuracy: 0.55 - ETA: 0s - loss: 0.6862 - accuracy: 0.55 - ETA: 0s - loss: 0.6857 - accuracy: 0.56 - ETA: 0s - loss: 0.6854 - accuracy: 0.56 - ETA: 0s - loss: 0.6850 - accuracy: 0.56 - ETA: 0s - loss: 0.6848 - accuracy: 0.56 - ETA: 0s - loss: 0.6844 - accuracy: 0.56 - ETA: 0s - loss: 0.6844 - accuracy: 0.56 - ETA: 0s - loss: 0.6841 - accuracy: 0.56 - ETA: 0s - loss: 0.6837 - accuracy: 0.56 - ETA: 0s - loss: 0.6835 - accuracy: 0.56 - ETA: 0s - loss: 0.6831 - accuracy: 0.56 - ETA: 0s - loss: 0.6827 - accuracy: 0.56 - 2s 134us/step - loss: 0.6826 - accuracy: 0.5656 - val_loss: 0.6689 - val_accuracy: 0.7053\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:33 - loss: 0.6936 - accuracy: 0.62 - ETA: 4s - loss: 0.6932 - accuracy: 0.5114 - ETA: 2s - loss: 0.6935 - accuracy: 0.49 - ETA: 2s - loss: 0.6934 - accuracy: 0.48 - ETA: 1s - loss: 0.6933 - accuracy: 0.50 - ETA: 1s - loss: 0.6931 - accuracy: 0.50 - ETA: 1s - loss: 0.6930 - accuracy: 0.50 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6927 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6919 - accuracy: 0.53 - ETA: 0s - loss: 0.6917 - accuracy: 0.53 - ETA: 0s - loss: 0.6916 - accuracy: 0.53 - ETA: 0s - loss: 0.6916 - accuracy: 0.53 - ETA: 0s - loss: 0.6914 - accuracy: 0.54 - ETA: 0s - loss: 0.6911 - accuracy: 0.54 - ETA: 0s - loss: 0.6910 - accuracy: 0.55 - ETA: 0s - loss: 0.6908 - accuracy: 0.55 - ETA: 0s - loss: 0.6905 - accuracy: 0.55 - ETA: 0s - loss: 0.6904 - accuracy: 0.55 - ETA: 0s - loss: 0.6902 - accuracy: 0.55 - ETA: 0s - loss: 0.6899 - accuracy: 0.55 - ETA: 0s - loss: 0.6898 - accuracy: 0.55 - ETA: 0s - loss: 0.6896 - accuracy: 0.55 - ETA: 0s - loss: 0.6893 - accuracy: 0.55 - ETA: 0s - loss: 0.6892 - accuracy: 0.55 - ETA: 0s - loss: 0.6890 - accuracy: 0.55 - ETA: 0s - loss: 0.6888 - accuracy: 0.56 - 2s 134us/step - loss: 0.6887 - accuracy: 0.5604 - val_loss: 0.6817 - val_accuracy: 0.6726\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 54us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:33 - loss: 0.6934 - accuracy: 0.75 - ETA: 4s - loss: 0.6919 - accuracy: 0.5301 - ETA: 2s - loss: 0.6913 - accuracy: 0.52 - ETA: 2s - loss: 0.6899 - accuracy: 0.53 - ETA: 1s - loss: 0.6897 - accuracy: 0.53 - ETA: 1s - loss: 0.6904 - accuracy: 0.52 - ETA: 1s - loss: 0.6899 - accuracy: 0.52 - ETA: 1s - loss: 0.6897 - accuracy: 0.52 - ETA: 1s - loss: 0.6897 - accuracy: 0.51 - ETA: 1s - loss: 0.6898 - accuracy: 0.51 - ETA: 1s - loss: 0.6894 - accuracy: 0.51 - ETA: 1s - loss: 0.6890 - accuracy: 0.51 - ETA: 1s - loss: 0.6885 - accuracy: 0.51 - ETA: 0s - loss: 0.6880 - accuracy: 0.51 - ETA: 0s - loss: 0.6878 - accuracy: 0.51 - ETA: 0s - loss: 0.6873 - accuracy: 0.51 - ETA: 0s - loss: 0.6873 - accuracy: 0.51 - ETA: 0s - loss: 0.6876 - accuracy: 0.51 - ETA: 0s - loss: 0.6872 - accuracy: 0.51 - ETA: 0s - loss: 0.6870 - accuracy: 0.51 - ETA: 0s - loss: 0.6866 - accuracy: 0.51 - ETA: 0s - loss: 0.6866 - accuracy: 0.51 - ETA: 0s - loss: 0.6865 - accuracy: 0.51 - ETA: 0s - loss: 0.6863 - accuracy: 0.51 - ETA: 0s - loss: 0.6860 - accuracy: 0.51 - ETA: 0s - loss: 0.6856 - accuracy: 0.52 - ETA: 0s - loss: 0.6853 - accuracy: 0.52 - ETA: 0s - loss: 0.6855 - accuracy: 0.52 - ETA: 0s - loss: 0.6853 - accuracy: 0.52 - ETA: 0s - loss: 0.6848 - accuracy: 0.52 - 2s 133us/step - loss: 0.6848 - accuracy: 0.5270 - val_loss: 0.6726 - val_accuracy: 0.5874\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:33 - loss: 0.6933 - accuracy: 0.50 - ETA: 4s - loss: 0.6926 - accuracy: 0.5185 - ETA: 2s - loss: 0.6920 - accuracy: 0.51 - ETA: 2s - loss: 0.6917 - accuracy: 0.51 - ETA: 1s - loss: 0.6916 - accuracy: 0.51 - ETA: 1s - loss: 0.6915 - accuracy: 0.51 - ETA: 1s - loss: 0.6911 - accuracy: 0.52 - ETA: 1s - loss: 0.6906 - accuracy: 0.53 - ETA: 1s - loss: 0.6896 - accuracy: 0.53 - ETA: 1s - loss: 0.6894 - accuracy: 0.53 - ETA: 1s - loss: 0.6892 - accuracy: 0.54 - ETA: 1s - loss: 0.6893 - accuracy: 0.53 - ETA: 1s - loss: 0.6889 - accuracy: 0.53 - ETA: 1s - loss: 0.6884 - accuracy: 0.54 - ETA: 0s - loss: 0.6878 - accuracy: 0.54 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6870 - accuracy: 0.54 - ETA: 0s - loss: 0.6865 - accuracy: 0.54 - ETA: 0s - loss: 0.6858 - accuracy: 0.55 - ETA: 0s - loss: 0.6851 - accuracy: 0.55 - ETA: 0s - loss: 0.6848 - accuracy: 0.55 - ETA: 0s - loss: 0.6844 - accuracy: 0.55 - ETA: 0s - loss: 0.6839 - accuracy: 0.55 - ETA: 0s - loss: 0.6837 - accuracy: 0.55 - ETA: 0s - loss: 0.6833 - accuracy: 0.55 - ETA: 0s - loss: 0.6830 - accuracy: 0.55 - ETA: 0s - loss: 0.6825 - accuracy: 0.55 - ETA: 0s - loss: 0.6825 - accuracy: 0.55 - ETA: 0s - loss: 0.6823 - accuracy: 0.55 - ETA: 0s - loss: 0.6819 - accuracy: 0.56 - ETA: 0s - loss: 0.6813 - accuracy: 0.56 - 2s 138us/step - loss: 0.6812 - accuracy: 0.5613 - val_loss: 0.6667 - val_accuracy: 0.7053\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:34 - loss: 0.6945 - accuracy: 0.25 - ETA: 4s - loss: 0.6936 - accuracy: 0.5023 - ETA: 2s - loss: 0.6915 - accuracy: 0.55 - ETA: 2s - loss: 0.6896 - accuracy: 0.55 - ETA: 1s - loss: 0.6896 - accuracy: 0.55 - ETA: 1s - loss: 0.6897 - accuracy: 0.55 - ETA: 1s - loss: 0.6887 - accuracy: 0.55 - ETA: 1s - loss: 0.6887 - accuracy: 0.55 - ETA: 1s - loss: 0.6880 - accuracy: 0.56 - ETA: 1s - loss: 0.6864 - accuracy: 0.57 - ETA: 1s - loss: 0.6856 - accuracy: 0.58 - ETA: 1s - loss: 0.6847 - accuracy: 0.58 - ETA: 1s - loss: 0.6840 - accuracy: 0.58 - ETA: 0s - loss: 0.6832 - accuracy: 0.58 - ETA: 0s - loss: 0.6825 - accuracy: 0.59 - ETA: 0s - loss: 0.6816 - accuracy: 0.59 - ETA: 0s - loss: 0.6806 - accuracy: 0.59 - ETA: 0s - loss: 0.6804 - accuracy: 0.59 - ETA: 0s - loss: 0.6795 - accuracy: 0.60 - ETA: 0s - loss: 0.6787 - accuracy: 0.60 - ETA: 0s - loss: 0.6781 - accuracy: 0.60 - ETA: 0s - loss: 0.6774 - accuracy: 0.60 - ETA: 0s - loss: 0.6770 - accuracy: 0.60 - ETA: 0s - loss: 0.6764 - accuracy: 0.61 - ETA: 0s - loss: 0.6755 - accuracy: 0.61 - ETA: 0s - loss: 0.6751 - accuracy: 0.61 - ETA: 0s - loss: 0.6746 - accuracy: 0.61 - ETA: 0s - loss: 0.6739 - accuracy: 0.61 - ETA: 0s - loss: 0.6735 - accuracy: 0.61 - ETA: 0s - loss: 0.6728 - accuracy: 0.62 - 2s 135us/step - loss: 0.6725 - accuracy: 0.6207 - val_loss: 0.6470 - val_accuracy: 0.7053\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:35 - loss: 0.6938 - accuracy: 0.50 - ETA: 4s - loss: 0.6905 - accuracy: 0.5216 - ETA: 2s - loss: 0.6898 - accuracy: 0.53 - ETA: 2s - loss: 0.6892 - accuracy: 0.53 - ETA: 1s - loss: 0.6904 - accuracy: 0.52 - ETA: 1s - loss: 0.6889 - accuracy: 0.54 - ETA: 1s - loss: 0.6879 - accuracy: 0.54 - ETA: 1s - loss: 0.6866 - accuracy: 0.55 - ETA: 1s - loss: 0.6856 - accuracy: 0.56 - ETA: 1s - loss: 0.6845 - accuracy: 0.56 - ETA: 1s - loss: 0.6840 - accuracy: 0.57 - ETA: 1s - loss: 0.6832 - accuracy: 0.57 - ETA: 1s - loss: 0.6826 - accuracy: 0.57 - ETA: 0s - loss: 0.6822 - accuracy: 0.57 - ETA: 0s - loss: 0.6814 - accuracy: 0.57 - ETA: 0s - loss: 0.6807 - accuracy: 0.57 - ETA: 0s - loss: 0.6798 - accuracy: 0.58 - ETA: 0s - loss: 0.6783 - accuracy: 0.58 - ETA: 0s - loss: 0.6777 - accuracy: 0.59 - ETA: 0s - loss: 0.6765 - accuracy: 0.59 - ETA: 0s - loss: 0.6753 - accuracy: 0.59 - ETA: 0s - loss: 0.6740 - accuracy: 0.59 - ETA: 0s - loss: 0.6728 - accuracy: 0.60 - ETA: 0s - loss: 0.6723 - accuracy: 0.60 - ETA: 0s - loss: 0.6712 - accuracy: 0.60 - ETA: 0s - loss: 0.6705 - accuracy: 0.60 - ETA: 0s - loss: 0.6696 - accuracy: 0.60 - ETA: 0s - loss: 0.6687 - accuracy: 0.60 - ETA: 0s - loss: 0.6686 - accuracy: 0.60 - ETA: 0s - loss: 0.6680 - accuracy: 0.60 - ETA: 0s - loss: 0.6674 - accuracy: 0.61 - 2s 138us/step - loss: 0.6672 - accuracy: 0.6110 - val_loss: 0.6367 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:35 - loss: 0.6878 - accuracy: 0.75 - ETA: 4s - loss: 0.6902 - accuracy: 0.5705 - ETA: 2s - loss: 0.6883 - accuracy: 0.57 - ETA: 2s - loss: 0.6906 - accuracy: 0.54 - ETA: 1s - loss: 0.6897 - accuracy: 0.54 - ETA: 1s - loss: 0.6903 - accuracy: 0.53 - ETA: 1s - loss: 0.6900 - accuracy: 0.54 - ETA: 1s - loss: 0.6894 - accuracy: 0.54 - ETA: 1s - loss: 0.6889 - accuracy: 0.55 - ETA: 1s - loss: 0.6886 - accuracy: 0.55 - ETA: 1s - loss: 0.6882 - accuracy: 0.56 - ETA: 1s - loss: 0.6873 - accuracy: 0.56 - ETA: 1s - loss: 0.6868 - accuracy: 0.56 - ETA: 0s - loss: 0.6860 - accuracy: 0.57 - ETA: 0s - loss: 0.6855 - accuracy: 0.57 - ETA: 0s - loss: 0.6846 - accuracy: 0.57 - ETA: 0s - loss: 0.6837 - accuracy: 0.58 - ETA: 0s - loss: 0.6828 - accuracy: 0.58 - ETA: 0s - loss: 0.6822 - accuracy: 0.58 - ETA: 0s - loss: 0.6817 - accuracy: 0.58 - ETA: 0s - loss: 0.6809 - accuracy: 0.58 - ETA: 0s - loss: 0.6802 - accuracy: 0.59 - ETA: 0s - loss: 0.6795 - accuracy: 0.59 - ETA: 0s - loss: 0.6789 - accuracy: 0.59 - ETA: 0s - loss: 0.6785 - accuracy: 0.59 - ETA: 0s - loss: 0.6780 - accuracy: 0.59 - ETA: 0s - loss: 0.6774 - accuracy: 0.59 - ETA: 0s - loss: 0.6771 - accuracy: 0.59 - ETA: 0s - loss: 0.6767 - accuracy: 0.59 - ETA: 0s - loss: 0.6761 - accuracy: 0.59 - ETA: 0s - loss: 0.6758 - accuracy: 0.59 - 2s 137us/step - loss: 0.6757 - accuracy: 0.5997 - val_loss: 0.6522 - val_accuracy: 0.7109\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:36 - loss: 0.6949 - accuracy: 0.62 - ETA: 4s - loss: 0.6920 - accuracy: 0.5000 - ETA: 2s - loss: 0.6906 - accuracy: 0.52 - ETA: 2s - loss: 0.6893 - accuracy: 0.53 - ETA: 2s - loss: 0.6875 - accuracy: 0.54 - ETA: 1s - loss: 0.6882 - accuracy: 0.54 - ETA: 1s - loss: 0.6880 - accuracy: 0.54 - ETA: 1s - loss: 0.6877 - accuracy: 0.54 - ETA: 1s - loss: 0.6865 - accuracy: 0.56 - ETA: 1s - loss: 0.6853 - accuracy: 0.57 - ETA: 1s - loss: 0.6842 - accuracy: 0.57 - ETA: 1s - loss: 0.6835 - accuracy: 0.57 - ETA: 1s - loss: 0.6825 - accuracy: 0.58 - ETA: 0s - loss: 0.6820 - accuracy: 0.58 - ETA: 0s - loss: 0.6808 - accuracy: 0.58 - ETA: 0s - loss: 0.6796 - accuracy: 0.59 - ETA: 0s - loss: 0.6790 - accuracy: 0.59 - ETA: 0s - loss: 0.6785 - accuracy: 0.59 - ETA: 0s - loss: 0.6782 - accuracy: 0.59 - ETA: 0s - loss: 0.6774 - accuracy: 0.59 - ETA: 0s - loss: 0.6767 - accuracy: 0.59 - ETA: 0s - loss: 0.6761 - accuracy: 0.59 - ETA: 0s - loss: 0.6759 - accuracy: 0.59 - ETA: 0s - loss: 0.6752 - accuracy: 0.59 - ETA: 0s - loss: 0.6746 - accuracy: 0.60 - ETA: 0s - loss: 0.6740 - accuracy: 0.60 - ETA: 0s - loss: 0.6730 - accuracy: 0.60 - ETA: 0s - loss: 0.6730 - accuracy: 0.60 - ETA: 0s - loss: 0.6726 - accuracy: 0.60 - ETA: 0s - loss: 0.6721 - accuracy: 0.60 - ETA: 0s - loss: 0.6714 - accuracy: 0.60 - 2s 137us/step - loss: 0.6712 - accuracy: 0.6077 - val_loss: 0.6470 - val_accuracy: 0.7145\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 54us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:35 - loss: 0.6980 - accuracy: 0.37 - ETA: 4s - loss: 0.6932 - accuracy: 0.5162 - ETA: 2s - loss: 0.6928 - accuracy: 0.51 - ETA: 2s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 1s - loss: 0.6902 - accuracy: 0.53 - ETA: 1s - loss: 0.6896 - accuracy: 0.54 - ETA: 1s - loss: 0.6889 - accuracy: 0.55 - ETA: 1s - loss: 0.6877 - accuracy: 0.55 - ETA: 1s - loss: 0.6871 - accuracy: 0.55 - ETA: 1s - loss: 0.6868 - accuracy: 0.55 - ETA: 1s - loss: 0.6853 - accuracy: 0.56 - ETA: 0s - loss: 0.6847 - accuracy: 0.56 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6829 - accuracy: 0.57 - ETA: 0s - loss: 0.6824 - accuracy: 0.57 - ETA: 0s - loss: 0.6815 - accuracy: 0.57 - ETA: 0s - loss: 0.6809 - accuracy: 0.57 - ETA: 0s - loss: 0.6804 - accuracy: 0.57 - ETA: 0s - loss: 0.6799 - accuracy: 0.58 - ETA: 0s - loss: 0.6791 - accuracy: 0.58 - ETA: 0s - loss: 0.6780 - accuracy: 0.58 - ETA: 0s - loss: 0.6772 - accuracy: 0.59 - ETA: 0s - loss: 0.6761 - accuracy: 0.59 - ETA: 0s - loss: 0.6757 - accuracy: 0.59 - ETA: 0s - loss: 0.6749 - accuracy: 0.59 - ETA: 0s - loss: 0.6746 - accuracy: 0.59 - ETA: 0s - loss: 0.6739 - accuracy: 0.59 - ETA: 0s - loss: 0.6735 - accuracy: 0.59 - 2s 136us/step - loss: 0.6727 - accuracy: 0.6005 - val_loss: 0.6466 - val_accuracy: 0.6967\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:33 - loss: 0.6988 - accuracy: 0.37 - ETA: 4s - loss: 0.6943 - accuracy: 0.5024 - ETA: 2s - loss: 0.6915 - accuracy: 0.52 - ETA: 2s - loss: 0.6894 - accuracy: 0.53 - ETA: 1s - loss: 0.6867 - accuracy: 0.54 - ETA: 1s - loss: 0.6855 - accuracy: 0.55 - ETA: 1s - loss: 0.6844 - accuracy: 0.55 - ETA: 1s - loss: 0.6834 - accuracy: 0.56 - ETA: 1s - loss: 0.6818 - accuracy: 0.57 - ETA: 1s - loss: 0.6810 - accuracy: 0.58 - ETA: 1s - loss: 0.6806 - accuracy: 0.58 - ETA: 1s - loss: 0.6789 - accuracy: 0.58 - ETA: 1s - loss: 0.6775 - accuracy: 0.58 - ETA: 0s - loss: 0.6765 - accuracy: 0.59 - ETA: 0s - loss: 0.6750 - accuracy: 0.59 - ETA: 0s - loss: 0.6737 - accuracy: 0.59 - ETA: 0s - loss: 0.6727 - accuracy: 0.60 - ETA: 0s - loss: 0.6720 - accuracy: 0.60 - ETA: 0s - loss: 0.6708 - accuracy: 0.60 - ETA: 0s - loss: 0.6700 - accuracy: 0.60 - ETA: 0s - loss: 0.6686 - accuracy: 0.60 - ETA: 0s - loss: 0.6680 - accuracy: 0.60 - ETA: 0s - loss: 0.6666 - accuracy: 0.61 - ETA: 0s - loss: 0.6656 - accuracy: 0.61 - ETA: 0s - loss: 0.6650 - accuracy: 0.61 - ETA: 0s - loss: 0.6645 - accuracy: 0.61 - ETA: 0s - loss: 0.6638 - accuracy: 0.61 - ETA: 0s - loss: 0.6635 - accuracy: 0.62 - ETA: 0s - loss: 0.6624 - accuracy: 0.62 - ETA: 0s - loss: 0.6619 - accuracy: 0.62 - 2s 135us/step - loss: 0.6614 - accuracy: 0.6235 - val_loss: 0.6371 - val_accuracy: 0.7060\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 54us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:36 - loss: 0.6936 - accuracy: 0.50 - ETA: 4s - loss: 0.6914 - accuracy: 0.5096 - ETA: 2s - loss: 0.6914 - accuracy: 0.52 - ETA: 2s - loss: 0.6902 - accuracy: 0.52 - ETA: 1s - loss: 0.6892 - accuracy: 0.52 - ETA: 1s - loss: 0.6881 - accuracy: 0.53 - ETA: 1s - loss: 0.6858 - accuracy: 0.54 - ETA: 1s - loss: 0.6849 - accuracy: 0.55 - ETA: 1s - loss: 0.6833 - accuracy: 0.56 - ETA: 1s - loss: 0.6819 - accuracy: 0.56 - ETA: 1s - loss: 0.6805 - accuracy: 0.57 - ETA: 1s - loss: 0.6800 - accuracy: 0.57 - ETA: 1s - loss: 0.6779 - accuracy: 0.58 - ETA: 1s - loss: 0.6766 - accuracy: 0.59 - ETA: 0s - loss: 0.6755 - accuracy: 0.59 - ETA: 0s - loss: 0.6726 - accuracy: 0.60 - ETA: 0s - loss: 0.6710 - accuracy: 0.60 - ETA: 0s - loss: 0.6698 - accuracy: 0.60 - ETA: 0s - loss: 0.6686 - accuracy: 0.61 - ETA: 0s - loss: 0.6683 - accuracy: 0.61 - ETA: 0s - loss: 0.6669 - accuracy: 0.61 - ETA: 0s - loss: 0.6660 - accuracy: 0.61 - ETA: 0s - loss: 0.6647 - accuracy: 0.61 - ETA: 0s - loss: 0.6630 - accuracy: 0.62 - ETA: 0s - loss: 0.6619 - accuracy: 0.62 - ETA: 0s - loss: 0.6613 - accuracy: 0.62 - ETA: 0s - loss: 0.6600 - accuracy: 0.62 - ETA: 0s - loss: 0.6599 - accuracy: 0.62 - ETA: 0s - loss: 0.6584 - accuracy: 0.63 - ETA: 0s - loss: 0.6570 - accuracy: 0.63 - ETA: 0s - loss: 0.6562 - accuracy: 0.63 - 2s 138us/step - loss: 0.6561 - accuracy: 0.6338 - val_loss: 0.6179 - val_accuracy: 0.7003\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 2:38 - loss: 0.6998 - accuracy: 0.50 - ETA: 4s - loss: 0.6917 - accuracy: 0.5231 - ETA: 2s - loss: 0.6911 - accuracy: 0.52 - ETA: 2s - loss: 0.6909 - accuracy: 0.52 - ETA: 1s - loss: 0.6891 - accuracy: 0.54 - ETA: 1s - loss: 0.6875 - accuracy: 0.55 - ETA: 1s - loss: 0.6861 - accuracy: 0.56 - ETA: 1s - loss: 0.6839 - accuracy: 0.57 - ETA: 1s - loss: 0.6822 - accuracy: 0.57 - ETA: 1s - loss: 0.6808 - accuracy: 0.58 - ETA: 1s - loss: 0.6789 - accuracy: 0.58 - ETA: 1s - loss: 0.6764 - accuracy: 0.59 - ETA: 1s - loss: 0.6755 - accuracy: 0.59 - ETA: 1s - loss: 0.6751 - accuracy: 0.60 - ETA: 0s - loss: 0.6742 - accuracy: 0.60 - ETA: 0s - loss: 0.6730 - accuracy: 0.60 - ETA: 0s - loss: 0.6717 - accuracy: 0.61 - ETA: 0s - loss: 0.6706 - accuracy: 0.61 - ETA: 0s - loss: 0.6694 - accuracy: 0.61 - ETA: 0s - loss: 0.6681 - accuracy: 0.61 - ETA: 0s - loss: 0.6667 - accuracy: 0.61 - ETA: 0s - loss: 0.6654 - accuracy: 0.62 - ETA: 0s - loss: 0.6637 - accuracy: 0.62 - ETA: 0s - loss: 0.6627 - accuracy: 0.62 - ETA: 0s - loss: 0.6612 - accuracy: 0.62 - ETA: 0s - loss: 0.6598 - accuracy: 0.62 - ETA: 0s - loss: 0.6586 - accuracy: 0.63 - ETA: 0s - loss: 0.6570 - accuracy: 0.63 - ETA: 0s - loss: 0.6559 - accuracy: 0.63 - ETA: 0s - loss: 0.6553 - accuracy: 0.63 - ETA: 0s - loss: 0.6540 - accuracy: 0.63 - 2s 139us/step - loss: 0.6536 - accuracy: 0.6392 - val_loss: 0.6148 - val_accuracy: 0.7109\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:31 - loss: 0.6955 - accuracy: 0.75 - ETA: 4s - loss: 0.6937 - accuracy: 0.5139 - ETA: 2s - loss: 0.6909 - accuracy: 0.51 - ETA: 2s - loss: 0.6887 - accuracy: 0.52 - ETA: 1s - loss: 0.6873 - accuracy: 0.54 - ETA: 1s - loss: 0.6849 - accuracy: 0.55 - ETA: 1s - loss: 0.6827 - accuracy: 0.56 - ETA: 1s - loss: 0.6816 - accuracy: 0.56 - ETA: 1s - loss: 0.6792 - accuracy: 0.57 - ETA: 1s - loss: 0.6782 - accuracy: 0.58 - ETA: 1s - loss: 0.6755 - accuracy: 0.59 - ETA: 1s - loss: 0.6745 - accuracy: 0.59 - ETA: 1s - loss: 0.6724 - accuracy: 0.60 - ETA: 0s - loss: 0.6703 - accuracy: 0.60 - ETA: 0s - loss: 0.6690 - accuracy: 0.60 - ETA: 0s - loss: 0.6678 - accuracy: 0.61 - ETA: 0s - loss: 0.6660 - accuracy: 0.61 - ETA: 0s - loss: 0.6648 - accuracy: 0.61 - ETA: 0s - loss: 0.6637 - accuracy: 0.62 - ETA: 0s - loss: 0.6629 - accuracy: 0.62 - ETA: 0s - loss: 0.6619 - accuracy: 0.62 - ETA: 0s - loss: 0.6614 - accuracy: 0.62 - ETA: 0s - loss: 0.6605 - accuracy: 0.62 - ETA: 0s - loss: 0.6593 - accuracy: 0.63 - ETA: 0s - loss: 0.6586 - accuracy: 0.63 - ETA: 0s - loss: 0.6578 - accuracy: 0.63 - ETA: 0s - loss: 0.6567 - accuracy: 0.63 - ETA: 0s - loss: 0.6557 - accuracy: 0.63 - ETA: 0s - loss: 0.6546 - accuracy: 0.63 - ETA: 0s - loss: 0.6539 - accuracy: 0.63 - ETA: 0s - loss: 0.6528 - accuracy: 0.63 - 2s 138us/step - loss: 0.6524 - accuracy: 0.6402 - val_loss: 0.6134 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:35 - loss: 0.6882 - accuracy: 0.75 - ETA: 4s - loss: 0.6940 - accuracy: 0.5168 - ETA: 2s - loss: 0.6918 - accuracy: 0.52 - ETA: 2s - loss: 0.6868 - accuracy: 0.54 - ETA: 1s - loss: 0.6855 - accuracy: 0.54 - ETA: 1s - loss: 0.6845 - accuracy: 0.55 - ETA: 1s - loss: 0.6841 - accuracy: 0.55 - ETA: 1s - loss: 0.6829 - accuracy: 0.55 - ETA: 1s - loss: 0.6812 - accuracy: 0.56 - ETA: 1s - loss: 0.6805 - accuracy: 0.56 - ETA: 1s - loss: 0.6792 - accuracy: 0.57 - ETA: 1s - loss: 0.6777 - accuracy: 0.58 - ETA: 1s - loss: 0.6756 - accuracy: 0.58 - ETA: 0s - loss: 0.6745 - accuracy: 0.59 - ETA: 0s - loss: 0.6730 - accuracy: 0.59 - ETA: 0s - loss: 0.6719 - accuracy: 0.60 - ETA: 0s - loss: 0.6714 - accuracy: 0.60 - ETA: 0s - loss: 0.6704 - accuracy: 0.60 - ETA: 0s - loss: 0.6695 - accuracy: 0.60 - ETA: 0s - loss: 0.6689 - accuracy: 0.60 - ETA: 0s - loss: 0.6676 - accuracy: 0.61 - ETA: 0s - loss: 0.6664 - accuracy: 0.61 - ETA: 0s - loss: 0.6652 - accuracy: 0.61 - ETA: 0s - loss: 0.6640 - accuracy: 0.62 - ETA: 0s - loss: 0.6628 - accuracy: 0.62 - ETA: 0s - loss: 0.6622 - accuracy: 0.62 - ETA: 0s - loss: 0.6605 - accuracy: 0.62 - ETA: 0s - loss: 0.6599 - accuracy: 0.62 - ETA: 0s - loss: 0.6595 - accuracy: 0.62 - ETA: 0s - loss: 0.6586 - accuracy: 0.63 - ETA: 0s - loss: 0.6577 - accuracy: 0.63 - 2s 139us/step - loss: 0.6571 - accuracy: 0.6328 - val_loss: 0.6222 - val_accuracy: 0.7159\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:38 - loss: 0.6982 - accuracy: 0.37 - ETA: 4s - loss: 0.6927 - accuracy: 0.5144 - ETA: 2s - loss: 0.6909 - accuracy: 0.50 - ETA: 2s - loss: 0.6889 - accuracy: 0.51 - ETA: 1s - loss: 0.6879 - accuracy: 0.53 - ETA: 1s - loss: 0.6856 - accuracy: 0.54 - ETA: 1s - loss: 0.6844 - accuracy: 0.55 - ETA: 1s - loss: 0.6832 - accuracy: 0.55 - ETA: 1s - loss: 0.6811 - accuracy: 0.56 - ETA: 1s - loss: 0.6798 - accuracy: 0.57 - ETA: 1s - loss: 0.6774 - accuracy: 0.58 - ETA: 1s - loss: 0.6761 - accuracy: 0.58 - ETA: 1s - loss: 0.6734 - accuracy: 0.59 - ETA: 1s - loss: 0.6721 - accuracy: 0.59 - ETA: 0s - loss: 0.6709 - accuracy: 0.59 - ETA: 0s - loss: 0.6699 - accuracy: 0.60 - ETA: 0s - loss: 0.6681 - accuracy: 0.60 - ETA: 0s - loss: 0.6671 - accuracy: 0.61 - ETA: 0s - loss: 0.6658 - accuracy: 0.61 - ETA: 0s - loss: 0.6639 - accuracy: 0.61 - ETA: 0s - loss: 0.6626 - accuracy: 0.61 - ETA: 0s - loss: 0.6625 - accuracy: 0.61 - ETA: 0s - loss: 0.6617 - accuracy: 0.62 - ETA: 0s - loss: 0.6609 - accuracy: 0.62 - ETA: 0s - loss: 0.6592 - accuracy: 0.62 - ETA: 0s - loss: 0.6576 - accuracy: 0.62 - ETA: 0s - loss: 0.6562 - accuracy: 0.63 - ETA: 0s - loss: 0.6557 - accuracy: 0.63 - ETA: 0s - loss: 0.6546 - accuracy: 0.63 - ETA: 0s - loss: 0.6542 - accuracy: 0.63 - ETA: 0s - loss: 0.6536 - accuracy: 0.63 - ETA: 0s - loss: 0.6526 - accuracy: 0.63 - 2s 142us/step - loss: 0.6526 - accuracy: 0.6352 - val_loss: 0.6178 - val_accuracy: 0.7145\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 62us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:35 - loss: 0.6934 - accuracy: 0.75 - ETA: 4s - loss: 0.6920 - accuracy: 0.5165 - ETA: 2s - loss: 0.6922 - accuracy: 0.52 - ETA: 2s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6893 - accuracy: 0.55 - ETA: 1s - loss: 0.6876 - accuracy: 0.56 - ETA: 1s - loss: 0.6866 - accuracy: 0.56 - ETA: 1s - loss: 0.6839 - accuracy: 0.57 - ETA: 1s - loss: 0.6824 - accuracy: 0.58 - ETA: 1s - loss: 0.6810 - accuracy: 0.58 - ETA: 1s - loss: 0.6794 - accuracy: 0.59 - ETA: 1s - loss: 0.6778 - accuracy: 0.59 - ETA: 1s - loss: 0.6773 - accuracy: 0.59 - ETA: 0s - loss: 0.6757 - accuracy: 0.60 - ETA: 0s - loss: 0.6748 - accuracy: 0.60 - ETA: 0s - loss: 0.6737 - accuracy: 0.60 - ETA: 0s - loss: 0.6718 - accuracy: 0.60 - ETA: 0s - loss: 0.6706 - accuracy: 0.61 - ETA: 0s - loss: 0.6696 - accuracy: 0.61 - ETA: 0s - loss: 0.6683 - accuracy: 0.61 - ETA: 0s - loss: 0.6670 - accuracy: 0.61 - ETA: 0s - loss: 0.6660 - accuracy: 0.62 - ETA: 0s - loss: 0.6650 - accuracy: 0.62 - ETA: 0s - loss: 0.6639 - accuracy: 0.62 - ETA: 0s - loss: 0.6631 - accuracy: 0.62 - ETA: 0s - loss: 0.6625 - accuracy: 0.62 - ETA: 0s - loss: 0.6615 - accuracy: 0.62 - ETA: 0s - loss: 0.6607 - accuracy: 0.62 - ETA: 0s - loss: 0.6597 - accuracy: 0.63 - ETA: 0s - loss: 0.6587 - accuracy: 0.63 - 2s 135us/step - loss: 0.6584 - accuracy: 0.6333 - val_loss: 0.6284 - val_accuracy: 0.7067\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 64us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:06 - loss: 0.6929 - accuracy: 0.50 - ETA: 6s - loss: 0.6934 - accuracy: 0.5385 - ETA: 4s - loss: 0.6930 - accuracy: 0.54 - ETA: 3s - loss: 0.6924 - accuracy: 0.53 - ETA: 2s - loss: 0.6921 - accuracy: 0.53 - ETA: 2s - loss: 0.6920 - accuracy: 0.53 - ETA: 2s - loss: 0.6906 - accuracy: 0.54 - ETA: 2s - loss: 0.6899 - accuracy: 0.54 - ETA: 1s - loss: 0.6885 - accuracy: 0.55 - ETA: 1s - loss: 0.6867 - accuracy: 0.55 - ETA: 1s - loss: 0.6839 - accuracy: 0.55 - ETA: 1s - loss: 0.6823 - accuracy: 0.55 - ETA: 1s - loss: 0.6800 - accuracy: 0.56 - ETA: 1s - loss: 0.6794 - accuracy: 0.56 - ETA: 1s - loss: 0.6755 - accuracy: 0.56 - ETA: 1s - loss: 0.6744 - accuracy: 0.56 - ETA: 1s - loss: 0.6719 - accuracy: 0.56 - ETA: 1s - loss: 0.6695 - accuracy: 0.57 - ETA: 1s - loss: 0.6695 - accuracy: 0.57 - ETA: 1s - loss: 0.6685 - accuracy: 0.57 - ETA: 1s - loss: 0.6680 - accuracy: 0.57 - ETA: 1s - loss: 0.6675 - accuracy: 0.57 - ETA: 0s - loss: 0.6651 - accuracy: 0.58 - ETA: 0s - loss: 0.6634 - accuracy: 0.58 - ETA: 0s - loss: 0.6615 - accuracy: 0.58 - ETA: 0s - loss: 0.6594 - accuracy: 0.59 - ETA: 0s - loss: 0.6592 - accuracy: 0.59 - ETA: 0s - loss: 0.6578 - accuracy: 0.59 - ETA: 0s - loss: 0.6567 - accuracy: 0.59 - ETA: 0s - loss: 0.6555 - accuracy: 0.59 - ETA: 0s - loss: 0.6542 - accuracy: 0.60 - ETA: 0s - loss: 0.6530 - accuracy: 0.60 - ETA: 0s - loss: 0.6523 - accuracy: 0.60 - ETA: 0s - loss: 0.6521 - accuracy: 0.60 - ETA: 0s - loss: 0.6512 - accuracy: 0.60 - ETA: 0s - loss: 0.6500 - accuracy: 0.61 - ETA: 0s - loss: 0.6493 - accuracy: 0.61 - ETA: 0s - loss: 0.6498 - accuracy: 0.61 - 2s 171us/step - loss: 0.6494 - accuracy: 0.6128 - val_loss: 0.6019 - val_accuracy: 0.7152\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 67us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:09 - loss: 0.6932 - accuracy: 0.50 - ETA: 5s - loss: 0.6932 - accuracy: 0.5264 - ETA: 3s - loss: 0.6927 - accuracy: 0.50 - ETA: 2s - loss: 0.6900 - accuracy: 0.51 - ETA: 2s - loss: 0.6893 - accuracy: 0.51 - ETA: 2s - loss: 0.6870 - accuracy: 0.52 - ETA: 1s - loss: 0.6849 - accuracy: 0.52 - ETA: 1s - loss: 0.6830 - accuracy: 0.53 - ETA: 1s - loss: 0.6818 - accuracy: 0.54 - ETA: 1s - loss: 0.6788 - accuracy: 0.55 - ETA: 1s - loss: 0.6780 - accuracy: 0.55 - ETA: 1s - loss: 0.6765 - accuracy: 0.56 - ETA: 1s - loss: 0.6734 - accuracy: 0.56 - ETA: 1s - loss: 0.6712 - accuracy: 0.57 - ETA: 1s - loss: 0.6698 - accuracy: 0.57 - ETA: 1s - loss: 0.6671 - accuracy: 0.58 - ETA: 0s - loss: 0.6654 - accuracy: 0.58 - ETA: 0s - loss: 0.6650 - accuracy: 0.58 - ETA: 0s - loss: 0.6637 - accuracy: 0.59 - ETA: 0s - loss: 0.6630 - accuracy: 0.59 - ETA: 0s - loss: 0.6611 - accuracy: 0.59 - ETA: 0s - loss: 0.6589 - accuracy: 0.60 - ETA: 0s - loss: 0.6578 - accuracy: 0.60 - ETA: 0s - loss: 0.6566 - accuracy: 0.60 - ETA: 0s - loss: 0.6555 - accuracy: 0.60 - ETA: 0s - loss: 0.6542 - accuracy: 0.60 - ETA: 0s - loss: 0.6534 - accuracy: 0.60 - ETA: 0s - loss: 0.6531 - accuracy: 0.60 - ETA: 0s - loss: 0.6518 - accuracy: 0.61 - ETA: 0s - loss: 0.6514 - accuracy: 0.61 - ETA: 0s - loss: 0.6504 - accuracy: 0.61 - ETA: 0s - loss: 0.6494 - accuracy: 0.61 - ETA: 0s - loss: 0.6495 - accuracy: 0.61 - 2s 149us/step - loss: 0.6492 - accuracy: 0.6137 - val_loss: 0.6000 - val_accuracy: 0.7237\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 54us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:09 - loss: 0.6951 - accuracy: 0.37 - ETA: 5s - loss: 0.6936 - accuracy: 0.5306 - ETA: 3s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6928 - accuracy: 0.51 - ETA: 2s - loss: 0.6913 - accuracy: 0.51 - ETA: 1s - loss: 0.6901 - accuracy: 0.52 - ETA: 1s - loss: 0.6895 - accuracy: 0.51 - ETA: 1s - loss: 0.6884 - accuracy: 0.51 - ETA: 1s - loss: 0.6863 - accuracy: 0.52 - ETA: 1s - loss: 0.6858 - accuracy: 0.52 - ETA: 1s - loss: 0.6845 - accuracy: 0.53 - ETA: 1s - loss: 0.6833 - accuracy: 0.54 - ETA: 1s - loss: 0.6818 - accuracy: 0.54 - ETA: 1s - loss: 0.6806 - accuracy: 0.55 - ETA: 1s - loss: 0.6782 - accuracy: 0.55 - ETA: 0s - loss: 0.6763 - accuracy: 0.56 - ETA: 0s - loss: 0.6751 - accuracy: 0.56 - ETA: 0s - loss: 0.6741 - accuracy: 0.56 - ETA: 0s - loss: 0.6732 - accuracy: 0.57 - ETA: 0s - loss: 0.6723 - accuracy: 0.57 - ETA: 0s - loss: 0.6710 - accuracy: 0.57 - ETA: 0s - loss: 0.6702 - accuracy: 0.57 - ETA: 0s - loss: 0.6686 - accuracy: 0.58 - ETA: 0s - loss: 0.6677 - accuracy: 0.58 - ETA: 0s - loss: 0.6668 - accuracy: 0.58 - ETA: 0s - loss: 0.6661 - accuracy: 0.58 - ETA: 0s - loss: 0.6647 - accuracy: 0.59 - ETA: 0s - loss: 0.6640 - accuracy: 0.59 - ETA: 0s - loss: 0.6635 - accuracy: 0.59 - ETA: 0s - loss: 0.6626 - accuracy: 0.59 - ETA: 0s - loss: 0.6616 - accuracy: 0.59 - 2s 144us/step - loss: 0.6616 - accuracy: 0.5962 - val_loss: 0.6110 - val_accuracy: 0.7259\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 60us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:24 - loss: 0.6940 - accuracy: 0.37 - ETA: 5s - loss: 0.6927 - accuracy: 0.4880 - ETA: 3s - loss: 0.6916 - accuracy: 0.50 - ETA: 2s - loss: 0.6910 - accuracy: 0.51 - ETA: 2s - loss: 0.6903 - accuracy: 0.51 - ETA: 2s - loss: 0.6899 - accuracy: 0.51 - ETA: 1s - loss: 0.6894 - accuracy: 0.51 - ETA: 1s - loss: 0.6881 - accuracy: 0.52 - ETA: 1s - loss: 0.6882 - accuracy: 0.51 - ETA: 1s - loss: 0.6878 - accuracy: 0.52 - ETA: 1s - loss: 0.6868 - accuracy: 0.52 - ETA: 1s - loss: 0.6865 - accuracy: 0.52 - ETA: 1s - loss: 0.6858 - accuracy: 0.53 - ETA: 1s - loss: 0.6850 - accuracy: 0.53 - ETA: 1s - loss: 0.6837 - accuracy: 0.53 - ETA: 1s - loss: 0.6819 - accuracy: 0.54 - ETA: 1s - loss: 0.6807 - accuracy: 0.54 - ETA: 0s - loss: 0.6798 - accuracy: 0.55 - ETA: 0s - loss: 0.6794 - accuracy: 0.55 - ETA: 0s - loss: 0.6779 - accuracy: 0.55 - ETA: 0s - loss: 0.6777 - accuracy: 0.55 - ETA: 0s - loss: 0.6775 - accuracy: 0.56 - ETA: 0s - loss: 0.6766 - accuracy: 0.56 - ETA: 0s - loss: 0.6757 - accuracy: 0.56 - ETA: 0s - loss: 0.6749 - accuracy: 0.57 - ETA: 0s - loss: 0.6748 - accuracy: 0.57 - ETA: 0s - loss: 0.6741 - accuracy: 0.57 - ETA: 0s - loss: 0.6739 - accuracy: 0.58 - ETA: 0s - loss: 0.6731 - accuracy: 0.58 - ETA: 0s - loss: 0.6725 - accuracy: 0.58 - ETA: 0s - loss: 0.6717 - accuracy: 0.58 - ETA: 0s - loss: 0.6712 - accuracy: 0.58 - ETA: 0s - loss: 0.6708 - accuracy: 0.58 - ETA: 0s - loss: 0.6703 - accuracy: 0.59 - 2s 154us/step - loss: 0.6699 - accuracy: 0.5912 - val_loss: 0.6341 - val_accuracy: 0.7003\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 57us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 3:40 - loss: 0.6924 - accuracy: 0.37 - ETA: 5s - loss: 0.6933 - accuracy: 0.5179 - ETA: 3s - loss: 0.6930 - accuracy: 0.53 - ETA: 2s - loss: 0.6930 - accuracy: 0.52 - ETA: 2s - loss: 0.6927 - accuracy: 0.52 - ETA: 2s - loss: 0.6917 - accuracy: 0.54 - ETA: 1s - loss: 0.6909 - accuracy: 0.54 - ETA: 1s - loss: 0.6897 - accuracy: 0.55 - ETA: 1s - loss: 0.6885 - accuracy: 0.55 - ETA: 1s - loss: 0.6872 - accuracy: 0.55 - ETA: 1s - loss: 0.6851 - accuracy: 0.55 - ETA: 1s - loss: 0.6843 - accuracy: 0.55 - ETA: 1s - loss: 0.6823 - accuracy: 0.56 - ETA: 1s - loss: 0.6808 - accuracy: 0.56 - ETA: 1s - loss: 0.6802 - accuracy: 0.57 - ETA: 1s - loss: 0.6790 - accuracy: 0.57 - ETA: 1s - loss: 0.6771 - accuracy: 0.57 - ETA: 1s - loss: 0.6758 - accuracy: 0.57 - ETA: 1s - loss: 0.6737 - accuracy: 0.58 - ETA: 0s - loss: 0.6708 - accuracy: 0.58 - ETA: 0s - loss: 0.6695 - accuracy: 0.59 - ETA: 0s - loss: 0.6684 - accuracy: 0.59 - ETA: 0s - loss: 0.6663 - accuracy: 0.59 - ETA: 0s - loss: 0.6652 - accuracy: 0.59 - ETA: 0s - loss: 0.6641 - accuracy: 0.59 - ETA: 0s - loss: 0.6624 - accuracy: 0.60 - ETA: 0s - loss: 0.6611 - accuracy: 0.60 - ETA: 0s - loss: 0.6601 - accuracy: 0.60 - ETA: 0s - loss: 0.6591 - accuracy: 0.60 - ETA: 0s - loss: 0.6572 - accuracy: 0.60 - ETA: 0s - loss: 0.6560 - accuracy: 0.60 - ETA: 0s - loss: 0.6548 - accuracy: 0.60 - ETA: 0s - loss: 0.6541 - accuracy: 0.61 - ETA: 0s - loss: 0.6532 - accuracy: 0.61 - 2s 154us/step - loss: 0.6528 - accuracy: 0.6120 - val_loss: 0.6020 - val_accuracy: 0.7152\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 58us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:06 - loss: 0.6978 - accuracy: 0.12 - ETA: 4s - loss: 0.6940 - accuracy: 0.4835 - ETA: 3s - loss: 0.6931 - accuracy: 0.50 - ETA: 2s - loss: 0.6925 - accuracy: 0.51 - ETA: 2s - loss: 0.6913 - accuracy: 0.52 - ETA: 1s - loss: 0.6908 - accuracy: 0.52 - ETA: 1s - loss: 0.6895 - accuracy: 0.53 - ETA: 1s - loss: 0.6886 - accuracy: 0.53 - ETA: 1s - loss: 0.6873 - accuracy: 0.53 - ETA: 1s - loss: 0.6855 - accuracy: 0.54 - ETA: 1s - loss: 0.6839 - accuracy: 0.54 - ETA: 1s - loss: 0.6825 - accuracy: 0.55 - ETA: 1s - loss: 0.6810 - accuracy: 0.55 - ETA: 1s - loss: 0.6799 - accuracy: 0.55 - ETA: 1s - loss: 0.6784 - accuracy: 0.56 - ETA: 0s - loss: 0.6767 - accuracy: 0.56 - ETA: 0s - loss: 0.6755 - accuracy: 0.57 - ETA: 0s - loss: 0.6735 - accuracy: 0.57 - ETA: 0s - loss: 0.6729 - accuracy: 0.57 - ETA: 0s - loss: 0.6712 - accuracy: 0.58 - ETA: 0s - loss: 0.6703 - accuracy: 0.58 - ETA: 0s - loss: 0.6683 - accuracy: 0.58 - ETA: 0s - loss: 0.6671 - accuracy: 0.58 - ETA: 0s - loss: 0.6661 - accuracy: 0.59 - ETA: 0s - loss: 0.6646 - accuracy: 0.59 - ETA: 0s - loss: 0.6635 - accuracy: 0.59 - ETA: 0s - loss: 0.6621 - accuracy: 0.59 - ETA: 0s - loss: 0.6607 - accuracy: 0.60 - ETA: 0s - loss: 0.6588 - accuracy: 0.60 - ETA: 0s - loss: 0.6575 - accuracy: 0.60 - ETA: 0s - loss: 0.6561 - accuracy: 0.61 - ETA: 0s - loss: 0.6549 - accuracy: 0.61 - 2s 145us/step - loss: 0.6534 - accuracy: 0.6139 - val_loss: 0.6039 - val_accuracy: 0.7166\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:13 - loss: 0.6965 - accuracy: 0.50 - ETA: 5s - loss: 0.6930 - accuracy: 0.5441 - ETA: 3s - loss: 0.6932 - accuracy: 0.52 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 2s - loss: 0.6917 - accuracy: 0.53 - ETA: 2s - loss: 0.6903 - accuracy: 0.53 - ETA: 1s - loss: 0.6889 - accuracy: 0.53 - ETA: 1s - loss: 0.6869 - accuracy: 0.54 - ETA: 1s - loss: 0.6852 - accuracy: 0.55 - ETA: 1s - loss: 0.6832 - accuracy: 0.55 - ETA: 1s - loss: 0.6816 - accuracy: 0.56 - ETA: 1s - loss: 0.6797 - accuracy: 0.56 - ETA: 1s - loss: 0.6780 - accuracy: 0.57 - ETA: 1s - loss: 0.6748 - accuracy: 0.58 - ETA: 1s - loss: 0.6723 - accuracy: 0.58 - ETA: 1s - loss: 0.6709 - accuracy: 0.59 - ETA: 0s - loss: 0.6684 - accuracy: 0.59 - ETA: 0s - loss: 0.6679 - accuracy: 0.59 - ETA: 0s - loss: 0.6664 - accuracy: 0.59 - ETA: 0s - loss: 0.6651 - accuracy: 0.60 - ETA: 0s - loss: 0.6631 - accuracy: 0.60 - ETA: 0s - loss: 0.6630 - accuracy: 0.60 - ETA: 0s - loss: 0.6621 - accuracy: 0.61 - ETA: 0s - loss: 0.6609 - accuracy: 0.61 - ETA: 0s - loss: 0.6589 - accuracy: 0.61 - ETA: 0s - loss: 0.6584 - accuracy: 0.61 - ETA: 0s - loss: 0.6577 - accuracy: 0.62 - ETA: 0s - loss: 0.6563 - accuracy: 0.62 - ETA: 0s - loss: 0.6546 - accuracy: 0.62 - ETA: 0s - loss: 0.6529 - accuracy: 0.62 - ETA: 0s - loss: 0.6520 - accuracy: 0.63 - ETA: 0s - loss: 0.6513 - accuracy: 0.63 - ETA: 0s - loss: 0.6503 - accuracy: 0.63 - 2s 149us/step - loss: 0.6494 - accuracy: 0.6362 - val_loss: 0.5991 - val_accuracy: 0.7195\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:11 - loss: 0.6972 - accuracy: 0.50 - ETA: 4s - loss: 0.6945 - accuracy: 0.5000 - ETA: 3s - loss: 0.6930 - accuracy: 0.52 - ETA: 2s - loss: 0.6920 - accuracy: 0.51 - ETA: 2s - loss: 0.6911 - accuracy: 0.52 - ETA: 2s - loss: 0.6898 - accuracy: 0.52 - ETA: 1s - loss: 0.6889 - accuracy: 0.53 - ETA: 1s - loss: 0.6870 - accuracy: 0.53 - ETA: 1s - loss: 0.6843 - accuracy: 0.54 - ETA: 1s - loss: 0.6826 - accuracy: 0.55 - ETA: 1s - loss: 0.6820 - accuracy: 0.55 - ETA: 1s - loss: 0.6803 - accuracy: 0.56 - ETA: 1s - loss: 0.6776 - accuracy: 0.56 - ETA: 1s - loss: 0.6752 - accuracy: 0.57 - ETA: 1s - loss: 0.6733 - accuracy: 0.57 - ETA: 0s - loss: 0.6693 - accuracy: 0.58 - ETA: 0s - loss: 0.6655 - accuracy: 0.58 - ETA: 0s - loss: 0.6645 - accuracy: 0.59 - ETA: 0s - loss: 0.6633 - accuracy: 0.59 - ETA: 0s - loss: 0.6605 - accuracy: 0.59 - ETA: 0s - loss: 0.6590 - accuracy: 0.60 - ETA: 0s - loss: 0.6584 - accuracy: 0.60 - ETA: 0s - loss: 0.6569 - accuracy: 0.60 - ETA: 0s - loss: 0.6553 - accuracy: 0.61 - ETA: 0s - loss: 0.6544 - accuracy: 0.61 - ETA: 0s - loss: 0.6535 - accuracy: 0.61 - ETA: 0s - loss: 0.6518 - accuracy: 0.61 - ETA: 0s - loss: 0.6505 - accuracy: 0.62 - ETA: 0s - loss: 0.6495 - accuracy: 0.62 - ETA: 0s - loss: 0.6482 - accuracy: 0.62 - ETA: 0s - loss: 0.6460 - accuracy: 0.62 - ETA: 0s - loss: 0.6451 - accuracy: 0.63 - ETA: 0s - loss: 0.6438 - accuracy: 0.63 - 2s 147us/step - loss: 0.6436 - accuracy: 0.6318 - val_loss: 0.5844 - val_accuracy: 0.7223\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:13 - loss: 0.6926 - accuracy: 0.50 - ETA: 5s - loss: 0.6943 - accuracy: 0.4975 - ETA: 3s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6908 - accuracy: 0.53 - ETA: 2s - loss: 0.6909 - accuracy: 0.52 - ETA: 1s - loss: 0.6891 - accuracy: 0.53 - ETA: 1s - loss: 0.6863 - accuracy: 0.54 - ETA: 1s - loss: 0.6842 - accuracy: 0.55 - ETA: 1s - loss: 0.6804 - accuracy: 0.56 - ETA: 1s - loss: 0.6779 - accuracy: 0.57 - ETA: 1s - loss: 0.6758 - accuracy: 0.57 - ETA: 1s - loss: 0.6725 - accuracy: 0.58 - ETA: 1s - loss: 0.6686 - accuracy: 0.58 - ETA: 1s - loss: 0.6663 - accuracy: 0.59 - ETA: 1s - loss: 0.6657 - accuracy: 0.59 - ETA: 0s - loss: 0.6640 - accuracy: 0.59 - ETA: 0s - loss: 0.6620 - accuracy: 0.60 - ETA: 0s - loss: 0.6600 - accuracy: 0.60 - ETA: 0s - loss: 0.6558 - accuracy: 0.61 - ETA: 0s - loss: 0.6543 - accuracy: 0.61 - ETA: 0s - loss: 0.6532 - accuracy: 0.61 - ETA: 0s - loss: 0.6518 - accuracy: 0.61 - ETA: 0s - loss: 0.6505 - accuracy: 0.61 - ETA: 0s - loss: 0.6492 - accuracy: 0.62 - ETA: 0s - loss: 0.6484 - accuracy: 0.62 - ETA: 0s - loss: 0.6466 - accuracy: 0.62 - ETA: 0s - loss: 0.6449 - accuracy: 0.63 - ETA: 0s - loss: 0.6433 - accuracy: 0.63 - ETA: 0s - loss: 0.6431 - accuracy: 0.63 - ETA: 0s - loss: 0.6411 - accuracy: 0.63 - ETA: 0s - loss: 0.6409 - accuracy: 0.63 - ETA: 0s - loss: 0.6403 - accuracy: 0.63 - ETA: 0s - loss: 0.6397 - accuracy: 0.63 - 2s 147us/step - loss: 0.6397 - accuracy: 0.6382 - val_loss: 0.5888 - val_accuracy: 0.7244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:06 - loss: 0.6950 - accuracy: 0.37 - ETA: 4s - loss: 0.6935 - accuracy: 0.5361 - ETA: 3s - loss: 0.6926 - accuracy: 0.53 - ETA: 2s - loss: 0.6917 - accuracy: 0.52 - ETA: 2s - loss: 0.6901 - accuracy: 0.53 - ETA: 1s - loss: 0.6877 - accuracy: 0.55 - ETA: 1s - loss: 0.6866 - accuracy: 0.55 - ETA: 1s - loss: 0.6849 - accuracy: 0.56 - ETA: 1s - loss: 0.6822 - accuracy: 0.56 - ETA: 1s - loss: 0.6792 - accuracy: 0.57 - ETA: 1s - loss: 0.6747 - accuracy: 0.58 - ETA: 1s - loss: 0.6720 - accuracy: 0.58 - ETA: 1s - loss: 0.6698 - accuracy: 0.59 - ETA: 1s - loss: 0.6683 - accuracy: 0.59 - ETA: 1s - loss: 0.6663 - accuracy: 0.59 - ETA: 1s - loss: 0.6648 - accuracy: 0.60 - ETA: 0s - loss: 0.6637 - accuracy: 0.60 - ETA: 0s - loss: 0.6618 - accuracy: 0.60 - ETA: 0s - loss: 0.6606 - accuracy: 0.60 - ETA: 0s - loss: 0.6598 - accuracy: 0.60 - ETA: 0s - loss: 0.6572 - accuracy: 0.61 - ETA: 0s - loss: 0.6552 - accuracy: 0.61 - ETA: 0s - loss: 0.6529 - accuracy: 0.62 - ETA: 0s - loss: 0.6504 - accuracy: 0.62 - ETA: 0s - loss: 0.6495 - accuracy: 0.62 - ETA: 0s - loss: 0.6478 - accuracy: 0.62 - ETA: 0s - loss: 0.6468 - accuracy: 0.63 - ETA: 0s - loss: 0.6458 - accuracy: 0.63 - ETA: 0s - loss: 0.6455 - accuracy: 0.63 - ETA: 0s - loss: 0.6444 - accuracy: 0.63 - ETA: 0s - loss: 0.6427 - accuracy: 0.63 - ETA: 0s - loss: 0.6411 - accuracy: 0.63 - ETA: 0s - loss: 0.6401 - accuracy: 0.64 - 2s 147us/step - loss: 0.6400 - accuracy: 0.6428 - val_loss: 0.5842 - val_accuracy: 0.7202\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:18 - loss: 0.6931 - accuracy: 0.62 - ETA: 5s - loss: 0.6936 - accuracy: 0.5587 - ETA: 3s - loss: 0.6932 - accuracy: 0.54 - ETA: 2s - loss: 0.6927 - accuracy: 0.55 - ETA: 2s - loss: 0.6917 - accuracy: 0.56 - ETA: 2s - loss: 0.6901 - accuracy: 0.57 - ETA: 1s - loss: 0.6892 - accuracy: 0.57 - ETA: 1s - loss: 0.6880 - accuracy: 0.57 - ETA: 1s - loss: 0.6866 - accuracy: 0.57 - ETA: 1s - loss: 0.6834 - accuracy: 0.57 - ETA: 1s - loss: 0.6794 - accuracy: 0.58 - ETA: 1s - loss: 0.6788 - accuracy: 0.58 - ETA: 1s - loss: 0.6763 - accuracy: 0.58 - ETA: 1s - loss: 0.6741 - accuracy: 0.58 - ETA: 1s - loss: 0.6713 - accuracy: 0.59 - ETA: 0s - loss: 0.6695 - accuracy: 0.59 - ETA: 0s - loss: 0.6670 - accuracy: 0.59 - ETA: 0s - loss: 0.6647 - accuracy: 0.60 - ETA: 0s - loss: 0.6618 - accuracy: 0.60 - ETA: 0s - loss: 0.6602 - accuracy: 0.60 - ETA: 0s - loss: 0.6571 - accuracy: 0.61 - ETA: 0s - loss: 0.6558 - accuracy: 0.61 - ETA: 0s - loss: 0.6539 - accuracy: 0.61 - ETA: 0s - loss: 0.6532 - accuracy: 0.61 - ETA: 0s - loss: 0.6521 - accuracy: 0.62 - ETA: 0s - loss: 0.6502 - accuracy: 0.62 - ETA: 0s - loss: 0.6487 - accuracy: 0.62 - ETA: 0s - loss: 0.6468 - accuracy: 0.62 - ETA: 0s - loss: 0.6451 - accuracy: 0.63 - ETA: 0s - loss: 0.6438 - accuracy: 0.63 - ETA: 0s - loss: 0.6430 - accuracy: 0.63 - ETA: 0s - loss: 0.6435 - accuracy: 0.63 - 2s 146us/step - loss: 0.6429 - accuracy: 0.6342 - val_loss: 0.5892 - val_accuracy: 0.7173\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:05 - loss: 0.6947 - accuracy: 0.50 - ETA: 5s - loss: 0.6946 - accuracy: 0.5123 - ETA: 3s - loss: 0.6941 - accuracy: 0.50 - ETA: 2s - loss: 0.6929 - accuracy: 0.52 - ETA: 2s - loss: 0.6917 - accuracy: 0.53 - ETA: 1s - loss: 0.6896 - accuracy: 0.55 - ETA: 1s - loss: 0.6885 - accuracy: 0.55 - ETA: 1s - loss: 0.6873 - accuracy: 0.56 - ETA: 1s - loss: 0.6859 - accuracy: 0.56 - ETA: 1s - loss: 0.6841 - accuracy: 0.57 - ETA: 1s - loss: 0.6815 - accuracy: 0.57 - ETA: 1s - loss: 0.6789 - accuracy: 0.58 - ETA: 1s - loss: 0.6769 - accuracy: 0.58 - ETA: 1s - loss: 0.6744 - accuracy: 0.59 - ETA: 1s - loss: 0.6726 - accuracy: 0.59 - ETA: 0s - loss: 0.6693 - accuracy: 0.60 - ETA: 0s - loss: 0.6663 - accuracy: 0.60 - ETA: 0s - loss: 0.6639 - accuracy: 0.60 - ETA: 0s - loss: 0.6619 - accuracy: 0.61 - ETA: 0s - loss: 0.6599 - accuracy: 0.61 - ETA: 0s - loss: 0.6583 - accuracy: 0.61 - ETA: 0s - loss: 0.6559 - accuracy: 0.62 - ETA: 0s - loss: 0.6552 - accuracy: 0.62 - ETA: 0s - loss: 0.6546 - accuracy: 0.62 - ETA: 0s - loss: 0.6535 - accuracy: 0.63 - ETA: 0s - loss: 0.6526 - accuracy: 0.63 - ETA: 0s - loss: 0.6524 - accuracy: 0.63 - ETA: 0s - loss: 0.6511 - accuracy: 0.63 - ETA: 0s - loss: 0.6505 - accuracy: 0.63 - ETA: 0s - loss: 0.6498 - accuracy: 0.63 - ETA: 0s - loss: 0.6484 - accuracy: 0.63 - ETA: 0s - loss: 0.6466 - accuracy: 0.64 - 2s 143us/step - loss: 0.6465 - accuracy: 0.6403 - val_loss: 0.5984 - val_accuracy: 0.6868\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:06 - loss: 0.6938 - accuracy: 0.50 - ETA: 5s - loss: 0.6923 - accuracy: 0.5275 - ETA: 4s - loss: 0.6913 - accuracy: 0.54 - ETA: 3s - loss: 0.6908 - accuracy: 0.53 - ETA: 2s - loss: 0.6890 - accuracy: 0.53 - ETA: 2s - loss: 0.6865 - accuracy: 0.53 - ETA: 2s - loss: 0.6855 - accuracy: 0.54 - ETA: 1s - loss: 0.6816 - accuracy: 0.56 - ETA: 1s - loss: 0.6786 - accuracy: 0.57 - ETA: 1s - loss: 0.6778 - accuracy: 0.57 - ETA: 1s - loss: 0.6755 - accuracy: 0.58 - ETA: 1s - loss: 0.6722 - accuracy: 0.59 - ETA: 1s - loss: 0.6695 - accuracy: 0.60 - ETA: 1s - loss: 0.6676 - accuracy: 0.60 - ETA: 1s - loss: 0.6677 - accuracy: 0.60 - ETA: 1s - loss: 0.6662 - accuracy: 0.60 - ETA: 0s - loss: 0.6637 - accuracy: 0.61 - ETA: 0s - loss: 0.6610 - accuracy: 0.61 - ETA: 0s - loss: 0.6586 - accuracy: 0.61 - ETA: 0s - loss: 0.6562 - accuracy: 0.62 - ETA: 0s - loss: 0.6556 - accuracy: 0.62 - ETA: 0s - loss: 0.6530 - accuracy: 0.62 - ETA: 0s - loss: 0.6514 - accuracy: 0.62 - ETA: 0s - loss: 0.6493 - accuracy: 0.63 - ETA: 0s - loss: 0.6482 - accuracy: 0.63 - ETA: 0s - loss: 0.6462 - accuracy: 0.63 - ETA: 0s - loss: 0.6437 - accuracy: 0.64 - ETA: 0s - loss: 0.6424 - accuracy: 0.64 - ETA: 0s - loss: 0.6423 - accuracy: 0.64 - ETA: 0s - loss: 0.6406 - accuracy: 0.64 - ETA: 0s - loss: 0.6393 - accuracy: 0.64 - ETA: 0s - loss: 0.6395 - accuracy: 0.64 - ETA: 0s - loss: 0.6388 - accuracy: 0.64 - 2s 147us/step - loss: 0.6387 - accuracy: 0.6488 - val_loss: 0.5890 - val_accuracy: 0.7337\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:09 - loss: 0.6913 - accuracy: 0.50 - ETA: 5s - loss: 0.6914 - accuracy: 0.5425 - ETA: 3s - loss: 0.6919 - accuracy: 0.53 - ETA: 2s - loss: 0.6913 - accuracy: 0.53 - ETA: 2s - loss: 0.6894 - accuracy: 0.54 - ETA: 2s - loss: 0.6888 - accuracy: 0.54 - ETA: 1s - loss: 0.6847 - accuracy: 0.56 - ETA: 1s - loss: 0.6811 - accuracy: 0.57 - ETA: 1s - loss: 0.6791 - accuracy: 0.58 - ETA: 1s - loss: 0.6767 - accuracy: 0.58 - ETA: 1s - loss: 0.6729 - accuracy: 0.59 - ETA: 1s - loss: 0.6709 - accuracy: 0.59 - ETA: 1s - loss: 0.6678 - accuracy: 0.59 - ETA: 1s - loss: 0.6660 - accuracy: 0.60 - ETA: 1s - loss: 0.6631 - accuracy: 0.60 - ETA: 0s - loss: 0.6615 - accuracy: 0.60 - ETA: 0s - loss: 0.6589 - accuracy: 0.61 - ETA: 0s - loss: 0.6556 - accuracy: 0.61 - ETA: 0s - loss: 0.6558 - accuracy: 0.61 - ETA: 0s - loss: 0.6546 - accuracy: 0.61 - ETA: 0s - loss: 0.6532 - accuracy: 0.62 - ETA: 0s - loss: 0.6509 - accuracy: 0.62 - ETA: 0s - loss: 0.6504 - accuracy: 0.62 - ETA: 0s - loss: 0.6483 - accuracy: 0.62 - ETA: 0s - loss: 0.6457 - accuracy: 0.63 - ETA: 0s - loss: 0.6437 - accuracy: 0.63 - ETA: 0s - loss: 0.6418 - accuracy: 0.63 - ETA: 0s - loss: 0.6409 - accuracy: 0.63 - ETA: 0s - loss: 0.6404 - accuracy: 0.63 - ETA: 0s - loss: 0.6395 - accuracy: 0.64 - ETA: 0s - loss: 0.6386 - accuracy: 0.64 - ETA: 0s - loss: 0.6366 - accuracy: 0.64 - 2s 143us/step - loss: 0.6360 - accuracy: 0.6470 - val_loss: 0.5783 - val_accuracy: 0.7230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 57us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:08 - loss: 0.6974 - accuracy: 0.37 - ETA: 5s - loss: 0.6929 - accuracy: 0.5074 - ETA: 3s - loss: 0.6892 - accuracy: 0.54 - ETA: 2s - loss: 0.6894 - accuracy: 0.53 - ETA: 2s - loss: 0.6894 - accuracy: 0.53 - ETA: 2s - loss: 0.6883 - accuracy: 0.54 - ETA: 1s - loss: 0.6864 - accuracy: 0.54 - ETA: 1s - loss: 0.6838 - accuracy: 0.55 - ETA: 1s - loss: 0.6800 - accuracy: 0.56 - ETA: 1s - loss: 0.6762 - accuracy: 0.57 - ETA: 1s - loss: 0.6726 - accuracy: 0.58 - ETA: 1s - loss: 0.6701 - accuracy: 0.58 - ETA: 1s - loss: 0.6659 - accuracy: 0.59 - ETA: 1s - loss: 0.6632 - accuracy: 0.59 - ETA: 1s - loss: 0.6604 - accuracy: 0.60 - ETA: 1s - loss: 0.6597 - accuracy: 0.60 - ETA: 1s - loss: 0.6577 - accuracy: 0.61 - ETA: 0s - loss: 0.6566 - accuracy: 0.61 - ETA: 0s - loss: 0.6550 - accuracy: 0.61 - ETA: 0s - loss: 0.6525 - accuracy: 0.61 - ETA: 0s - loss: 0.6507 - accuracy: 0.62 - ETA: 0s - loss: 0.6496 - accuracy: 0.62 - ETA: 0s - loss: 0.6468 - accuracy: 0.63 - ETA: 0s - loss: 0.6467 - accuracy: 0.63 - ETA: 0s - loss: 0.6445 - accuracy: 0.63 - ETA: 0s - loss: 0.6441 - accuracy: 0.63 - ETA: 0s - loss: 0.6429 - accuracy: 0.64 - ETA: 0s - loss: 0.6415 - accuracy: 0.64 - ETA: 0s - loss: 0.6396 - accuracy: 0.64 - ETA: 0s - loss: 0.6385 - accuracy: 0.64 - ETA: 0s - loss: 0.6369 - accuracy: 0.64 - ETA: 0s - loss: 0.6358 - accuracy: 0.65 - ETA: 0s - loss: 0.6359 - accuracy: 0.65 - 2s 150us/step - loss: 0.6354 - accuracy: 0.6512 - val_loss: 0.5834 - val_accuracy: 0.7230\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:05 - loss: 0.6970 - accuracy: 0.37 - ETA: 5s - loss: 0.6935 - accuracy: 0.5539 - ETA: 3s - loss: 0.6922 - accuracy: 0.53 - ETA: 2s - loss: 0.6858 - accuracy: 0.55 - ETA: 2s - loss: 0.6845 - accuracy: 0.56 - ETA: 1s - loss: 0.6852 - accuracy: 0.55 - ETA: 1s - loss: 0.6843 - accuracy: 0.56 - ETA: 1s - loss: 0.6819 - accuracy: 0.57 - ETA: 1s - loss: 0.6760 - accuracy: 0.59 - ETA: 1s - loss: 0.6723 - accuracy: 0.59 - ETA: 1s - loss: 0.6704 - accuracy: 0.60 - ETA: 1s - loss: 0.6679 - accuracy: 0.60 - ETA: 1s - loss: 0.6651 - accuracy: 0.61 - ETA: 1s - loss: 0.6631 - accuracy: 0.62 - ETA: 1s - loss: 0.6612 - accuracy: 0.62 - ETA: 1s - loss: 0.6567 - accuracy: 0.62 - ETA: 0s - loss: 0.6549 - accuracy: 0.62 - ETA: 0s - loss: 0.6544 - accuracy: 0.63 - ETA: 0s - loss: 0.6530 - accuracy: 0.63 - ETA: 0s - loss: 0.6522 - accuracy: 0.63 - ETA: 0s - loss: 0.6506 - accuracy: 0.63 - ETA: 0s - loss: 0.6483 - accuracy: 0.63 - ETA: 0s - loss: 0.6466 - accuracy: 0.64 - ETA: 0s - loss: 0.6445 - accuracy: 0.64 - ETA: 0s - loss: 0.6439 - accuracy: 0.64 - ETA: 0s - loss: 0.6436 - accuracy: 0.64 - ETA: 0s - loss: 0.6425 - accuracy: 0.64 - ETA: 0s - loss: 0.6408 - accuracy: 0.64 - ETA: 0s - loss: 0.6384 - accuracy: 0.65 - ETA: 0s - loss: 0.6372 - accuracy: 0.65 - ETA: 0s - loss: 0.6368 - accuracy: 0.65 - ETA: 0s - loss: 0.6361 - accuracy: 0.65 - ETA: 0s - loss: 0.6343 - accuracy: 0.65 - ETA: 0s - loss: 0.6342 - accuracy: 0.65 - 2s 151us/step - loss: 0.6341 - accuracy: 0.6553 - val_loss: 0.5854 - val_accuracy: 0.7145\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 74us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:09 - loss: 0.6953 - accuracy: 0.50 - ETA: 5s - loss: 0.6943 - accuracy: 0.5000 - ETA: 3s - loss: 0.6934 - accuracy: 0.52 - ETA: 2s - loss: 0.6919 - accuracy: 0.53 - ETA: 2s - loss: 0.6907 - accuracy: 0.53 - ETA: 2s - loss: 0.6886 - accuracy: 0.54 - ETA: 1s - loss: 0.6854 - accuracy: 0.56 - ETA: 1s - loss: 0.6838 - accuracy: 0.56 - ETA: 1s - loss: 0.6807 - accuracy: 0.57 - ETA: 1s - loss: 0.6781 - accuracy: 0.58 - ETA: 1s - loss: 0.6763 - accuracy: 0.58 - ETA: 1s - loss: 0.6747 - accuracy: 0.59 - ETA: 1s - loss: 0.6738 - accuracy: 0.59 - ETA: 1s - loss: 0.6713 - accuracy: 0.60 - ETA: 1s - loss: 0.6690 - accuracy: 0.60 - ETA: 0s - loss: 0.6667 - accuracy: 0.60 - ETA: 0s - loss: 0.6652 - accuracy: 0.61 - ETA: 0s - loss: 0.6638 - accuracy: 0.61 - ETA: 0s - loss: 0.6621 - accuracy: 0.61 - ETA: 0s - loss: 0.6598 - accuracy: 0.62 - ETA: 0s - loss: 0.6568 - accuracy: 0.62 - ETA: 0s - loss: 0.6545 - accuracy: 0.62 - ETA: 0s - loss: 0.6523 - accuracy: 0.63 - ETA: 0s - loss: 0.6502 - accuracy: 0.63 - ETA: 0s - loss: 0.6483 - accuracy: 0.63 - ETA: 0s - loss: 0.6459 - accuracy: 0.63 - ETA: 0s - loss: 0.6436 - accuracy: 0.64 - ETA: 0s - loss: 0.6424 - accuracy: 0.64 - ETA: 0s - loss: 0.6423 - accuracy: 0.64 - ETA: 0s - loss: 0.6414 - accuracy: 0.64 - ETA: 0s - loss: 0.6403 - accuracy: 0.64 - ETA: 0s - loss: 0.6382 - accuracy: 0.64 - 2s 145us/step - loss: 0.6378 - accuracy: 0.6480 - val_loss: 0.5881 - val_accuracy: 0.7145\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:05 - loss: 0.6980 - accuracy: 0.37 - ETA: 4s - loss: 0.6933 - accuracy: 0.5457 - ETA: 3s - loss: 0.6926 - accuracy: 0.53 - ETA: 2s - loss: 0.6921 - accuracy: 0.53 - ETA: 2s - loss: 0.6892 - accuracy: 0.55 - ETA: 1s - loss: 0.6871 - accuracy: 0.56 - ETA: 1s - loss: 0.6829 - accuracy: 0.56 - ETA: 1s - loss: 0.6786 - accuracy: 0.58 - ETA: 1s - loss: 0.6752 - accuracy: 0.58 - ETA: 1s - loss: 0.6741 - accuracy: 0.59 - ETA: 1s - loss: 0.6690 - accuracy: 0.60 - ETA: 1s - loss: 0.6650 - accuracy: 0.61 - ETA: 1s - loss: 0.6635 - accuracy: 0.61 - ETA: 1s - loss: 0.6614 - accuracy: 0.61 - ETA: 1s - loss: 0.6598 - accuracy: 0.61 - ETA: 0s - loss: 0.6584 - accuracy: 0.62 - ETA: 0s - loss: 0.6562 - accuracy: 0.62 - ETA: 0s - loss: 0.6531 - accuracy: 0.63 - ETA: 0s - loss: 0.6505 - accuracy: 0.63 - ETA: 0s - loss: 0.6479 - accuracy: 0.63 - ETA: 0s - loss: 0.6475 - accuracy: 0.63 - ETA: 0s - loss: 0.6456 - accuracy: 0.64 - ETA: 0s - loss: 0.6435 - accuracy: 0.64 - ETA: 0s - loss: 0.6432 - accuracy: 0.64 - ETA: 0s - loss: 0.6428 - accuracy: 0.64 - ETA: 0s - loss: 0.6422 - accuracy: 0.64 - ETA: 0s - loss: 0.6413 - accuracy: 0.64 - ETA: 0s - loss: 0.6414 - accuracy: 0.64 - ETA: 0s - loss: 0.6401 - accuracy: 0.65 - ETA: 0s - loss: 0.6388 - accuracy: 0.65 - ETA: 0s - loss: 0.6376 - accuracy: 0.65 - ETA: 0s - loss: 0.6365 - accuracy: 0.65 - 2s 146us/step - loss: 0.6358 - accuracy: 0.6574 - val_loss: 0.5878 - val_accuracy: 0.7060\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:43 - loss: 0.6930 - accuracy: 0.75 - ETA: 4s - loss: 0.6934 - accuracy: 0.5000 - ETA: 2s - loss: 0.6934 - accuracy: 0.49 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6929 - accuracy: 0.53 - ETA: 1s - loss: 0.6926 - accuracy: 0.54 - ETA: 1s - loss: 0.6924 - accuracy: 0.54 - ETA: 1s - loss: 0.6920 - accuracy: 0.56 - ETA: 1s - loss: 0.6917 - accuracy: 0.56 - ETA: 1s - loss: 0.6913 - accuracy: 0.56 - ETA: 1s - loss: 0.6906 - accuracy: 0.57 - ETA: 1s - loss: 0.6900 - accuracy: 0.57 - ETA: 1s - loss: 0.6894 - accuracy: 0.57 - ETA: 1s - loss: 0.6889 - accuracy: 0.58 - ETA: 0s - loss: 0.6881 - accuracy: 0.58 - ETA: 0s - loss: 0.6871 - accuracy: 0.59 - ETA: 0s - loss: 0.6864 - accuracy: 0.59 - ETA: 0s - loss: 0.6860 - accuracy: 0.59 - ETA: 0s - loss: 0.6851 - accuracy: 0.60 - ETA: 0s - loss: 0.6846 - accuracy: 0.60 - ETA: 0s - loss: 0.6836 - accuracy: 0.60 - ETA: 0s - loss: 0.6827 - accuracy: 0.60 - ETA: 0s - loss: 0.6820 - accuracy: 0.61 - ETA: 0s - loss: 0.6809 - accuracy: 0.61 - ETA: 0s - loss: 0.6799 - accuracy: 0.61 - ETA: 0s - loss: 0.6788 - accuracy: 0.61 - ETA: 0s - loss: 0.6777 - accuracy: 0.61 - ETA: 0s - loss: 0.6773 - accuracy: 0.61 - ETA: 0s - loss: 0.6769 - accuracy: 0.61 - ETA: 0s - loss: 0.6764 - accuracy: 0.61 - ETA: 0s - loss: 0.6758 - accuracy: 0.62 - ETA: 0s - loss: 0.6751 - accuracy: 0.62 - 2s 142us/step - loss: 0.6746 - accuracy: 0.6236 - val_loss: 0.6405 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:41 - loss: 0.6937 - accuracy: 0.87 - ETA: 4s - loss: 0.6931 - accuracy: 0.5448 - ETA: 2s - loss: 0.6932 - accuracy: 0.52 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.53 - ETA: 1s - loss: 0.6929 - accuracy: 0.52 - ETA: 1s - loss: 0.6927 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6924 - accuracy: 0.53 - ETA: 1s - loss: 0.6924 - accuracy: 0.53 - ETA: 1s - loss: 0.6921 - accuracy: 0.54 - ETA: 0s - loss: 0.6920 - accuracy: 0.54 - ETA: 0s - loss: 0.6919 - accuracy: 0.54 - ETA: 0s - loss: 0.6916 - accuracy: 0.54 - ETA: 0s - loss: 0.6913 - accuracy: 0.54 - ETA: 0s - loss: 0.6912 - accuracy: 0.54 - ETA: 0s - loss: 0.6909 - accuracy: 0.54 - ETA: 0s - loss: 0.6907 - accuracy: 0.54 - ETA: 0s - loss: 0.6905 - accuracy: 0.54 - ETA: 0s - loss: 0.6900 - accuracy: 0.55 - ETA: 0s - loss: 0.6897 - accuracy: 0.55 - ETA: 0s - loss: 0.6894 - accuracy: 0.55 - ETA: 0s - loss: 0.6892 - accuracy: 0.55 - ETA: 0s - loss: 0.6888 - accuracy: 0.55 - ETA: 0s - loss: 0.6884 - accuracy: 0.55 - ETA: 0s - loss: 0.6883 - accuracy: 0.55 - ETA: 0s - loss: 0.6878 - accuracy: 0.55 - ETA: 0s - loss: 0.6876 - accuracy: 0.56 - 2s 140us/step - loss: 0.6875 - accuracy: 0.5597 - val_loss: 0.6709 - val_accuracy: 0.7131\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 64us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:41 - loss: 0.6936 - accuracy: 0.62 - ETA: 4s - loss: 0.6934 - accuracy: 0.5192 - ETA: 2s - loss: 0.6931 - accuracy: 0.53 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 2s - loss: 0.6929 - accuracy: 0.53 - ETA: 1s - loss: 0.6925 - accuracy: 0.54 - ETA: 1s - loss: 0.6924 - accuracy: 0.54 - ETA: 1s - loss: 0.6923 - accuracy: 0.53 - ETA: 1s - loss: 0.6921 - accuracy: 0.54 - ETA: 1s - loss: 0.6918 - accuracy: 0.54 - ETA: 1s - loss: 0.6916 - accuracy: 0.54 - ETA: 1s - loss: 0.6915 - accuracy: 0.54 - ETA: 1s - loss: 0.6912 - accuracy: 0.55 - ETA: 1s - loss: 0.6906 - accuracy: 0.55 - ETA: 0s - loss: 0.6900 - accuracy: 0.56 - ETA: 0s - loss: 0.6896 - accuracy: 0.56 - ETA: 0s - loss: 0.6891 - accuracy: 0.56 - ETA: 0s - loss: 0.6886 - accuracy: 0.57 - ETA: 0s - loss: 0.6882 - accuracy: 0.57 - ETA: 0s - loss: 0.6878 - accuracy: 0.57 - ETA: 0s - loss: 0.6875 - accuracy: 0.57 - ETA: 0s - loss: 0.6872 - accuracy: 0.57 - ETA: 0s - loss: 0.6867 - accuracy: 0.57 - ETA: 0s - loss: 0.6867 - accuracy: 0.57 - ETA: 0s - loss: 0.6865 - accuracy: 0.57 - ETA: 0s - loss: 0.6859 - accuracy: 0.57 - ETA: 0s - loss: 0.6850 - accuracy: 0.57 - ETA: 0s - loss: 0.6847 - accuracy: 0.57 - ETA: 0s - loss: 0.6842 - accuracy: 0.57 - ETA: 0s - loss: 0.6838 - accuracy: 0.57 - ETA: 0s - loss: 0.6835 - accuracy: 0.57 - 2s 140us/step - loss: 0.6834 - accuracy: 0.5753 - val_loss: 0.6646 - val_accuracy: 0.7067\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:50 - loss: 0.6930 - accuracy: 0.50 - ETA: 4s - loss: 0.6928 - accuracy: 0.5301 - ETA: 2s - loss: 0.6928 - accuracy: 0.52 - ETA: 2s - loss: 0.6918 - accuracy: 0.52 - ETA: 2s - loss: 0.6915 - accuracy: 0.52 - ETA: 1s - loss: 0.6903 - accuracy: 0.53 - ETA: 1s - loss: 0.6888 - accuracy: 0.54 - ETA: 1s - loss: 0.6882 - accuracy: 0.54 - ETA: 1s - loss: 0.6882 - accuracy: 0.54 - ETA: 1s - loss: 0.6876 - accuracy: 0.54 - ETA: 1s - loss: 0.6871 - accuracy: 0.54 - ETA: 1s - loss: 0.6865 - accuracy: 0.54 - ETA: 1s - loss: 0.6854 - accuracy: 0.55 - ETA: 1s - loss: 0.6836 - accuracy: 0.55 - ETA: 0s - loss: 0.6828 - accuracy: 0.56 - ETA: 0s - loss: 0.6823 - accuracy: 0.55 - ETA: 0s - loss: 0.6811 - accuracy: 0.56 - ETA: 0s - loss: 0.6809 - accuracy: 0.56 - ETA: 0s - loss: 0.6794 - accuracy: 0.56 - ETA: 0s - loss: 0.6787 - accuracy: 0.56 - ETA: 0s - loss: 0.6775 - accuracy: 0.57 - ETA: 0s - loss: 0.6760 - accuracy: 0.57 - ETA: 0s - loss: 0.6755 - accuracy: 0.57 - ETA: 0s - loss: 0.6749 - accuracy: 0.57 - ETA: 0s - loss: 0.6737 - accuracy: 0.57 - ETA: 0s - loss: 0.6721 - accuracy: 0.58 - ETA: 0s - loss: 0.6717 - accuracy: 0.58 - ETA: 0s - loss: 0.6700 - accuracy: 0.58 - ETA: 0s - loss: 0.6689 - accuracy: 0.58 - ETA: 0s - loss: 0.6678 - accuracy: 0.58 - ETA: 0s - loss: 0.6671 - accuracy: 0.59 - ETA: 0s - loss: 0.6659 - accuracy: 0.59 - 2s 142us/step - loss: 0.6660 - accuracy: 0.5937 - val_loss: 0.6213 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:43 - loss: 0.6961 - accuracy: 0.12 - ETA: 4s - loss: 0.6938 - accuracy: 0.4560 - ETA: 2s - loss: 0.6936 - accuracy: 0.47 - ETA: 2s - loss: 0.6933 - accuracy: 0.49 - ETA: 2s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6923 - accuracy: 0.51 - ETA: 1s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6909 - accuracy: 0.53 - ETA: 1s - loss: 0.6902 - accuracy: 0.53 - ETA: 1s - loss: 0.6900 - accuracy: 0.53 - ETA: 1s - loss: 0.6896 - accuracy: 0.53 - ETA: 1s - loss: 0.6890 - accuracy: 0.53 - ETA: 1s - loss: 0.6883 - accuracy: 0.54 - ETA: 1s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6870 - accuracy: 0.55 - ETA: 0s - loss: 0.6867 - accuracy: 0.55 - ETA: 0s - loss: 0.6864 - accuracy: 0.55 - ETA: 0s - loss: 0.6860 - accuracy: 0.55 - ETA: 0s - loss: 0.6853 - accuracy: 0.55 - ETA: 0s - loss: 0.6850 - accuracy: 0.55 - ETA: 0s - loss: 0.6845 - accuracy: 0.56 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6820 - accuracy: 0.56 - ETA: 0s - loss: 0.6808 - accuracy: 0.57 - ETA: 0s - loss: 0.6797 - accuracy: 0.57 - ETA: 0s - loss: 0.6783 - accuracy: 0.57 - ETA: 0s - loss: 0.6776 - accuracy: 0.57 - ETA: 0s - loss: 0.6764 - accuracy: 0.58 - ETA: 0s - loss: 0.6754 - accuracy: 0.58 - ETA: 0s - loss: 0.6745 - accuracy: 0.58 - ETA: 0s - loss: 0.6739 - accuracy: 0.58 - ETA: 0s - loss: 0.6731 - accuracy: 0.58 - 2s 150us/step - loss: 0.6729 - accuracy: 0.5901 - val_loss: 0.6323 - val_accuracy: 0.7081\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 2:47 - loss: 0.6920 - accuracy: 0.87 - ETA: 4s - loss: 0.6935 - accuracy: 0.5118 - ETA: 3s - loss: 0.6924 - accuracy: 0.52 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6918 - accuracy: 0.53 - ETA: 1s - loss: 0.6914 - accuracy: 0.53 - ETA: 1s - loss: 0.6907 - accuracy: 0.54 - ETA: 1s - loss: 0.6899 - accuracy: 0.54 - ETA: 1s - loss: 0.6895 - accuracy: 0.54 - ETA: 1s - loss: 0.6888 - accuracy: 0.55 - ETA: 1s - loss: 0.6885 - accuracy: 0.55 - ETA: 0s - loss: 0.6877 - accuracy: 0.56 - ETA: 0s - loss: 0.6867 - accuracy: 0.56 - ETA: 0s - loss: 0.6855 - accuracy: 0.56 - ETA: 0s - loss: 0.6845 - accuracy: 0.57 - ETA: 0s - loss: 0.6835 - accuracy: 0.57 - ETA: 0s - loss: 0.6820 - accuracy: 0.57 - ETA: 0s - loss: 0.6807 - accuracy: 0.58 - ETA: 0s - loss: 0.6797 - accuracy: 0.58 - ETA: 0s - loss: 0.6786 - accuracy: 0.58 - ETA: 0s - loss: 0.6773 - accuracy: 0.58 - ETA: 0s - loss: 0.6763 - accuracy: 0.58 - ETA: 0s - loss: 0.6751 - accuracy: 0.58 - ETA: 0s - loss: 0.6739 - accuracy: 0.59 - ETA: 0s - loss: 0.6734 - accuracy: 0.59 - ETA: 0s - loss: 0.6727 - accuracy: 0.59 - ETA: 0s - loss: 0.6714 - accuracy: 0.59 - ETA: 0s - loss: 0.6704 - accuracy: 0.59 - 2s 135us/step - loss: 0.6697 - accuracy: 0.5959 - val_loss: 0.6269 - val_accuracy: 0.7095\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:44 - loss: 0.6946 - accuracy: 0.50 - ETA: 4s - loss: 0.6943 - accuracy: 0.4769 - ETA: 2s - loss: 0.6936 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6918 - accuracy: 0.53 - ETA: 1s - loss: 0.6911 - accuracy: 0.53 - ETA: 1s - loss: 0.6911 - accuracy: 0.53 - ETA: 1s - loss: 0.6903 - accuracy: 0.54 - ETA: 1s - loss: 0.6890 - accuracy: 0.55 - ETA: 1s - loss: 0.6879 - accuracy: 0.55 - ETA: 1s - loss: 0.6874 - accuracy: 0.56 - ETA: 1s - loss: 0.6862 - accuracy: 0.56 - ETA: 1s - loss: 0.6851 - accuracy: 0.57 - ETA: 1s - loss: 0.6836 - accuracy: 0.57 - ETA: 0s - loss: 0.6829 - accuracy: 0.57 - ETA: 0s - loss: 0.6814 - accuracy: 0.58 - ETA: 0s - loss: 0.6805 - accuracy: 0.58 - ETA: 0s - loss: 0.6797 - accuracy: 0.58 - ETA: 0s - loss: 0.6787 - accuracy: 0.58 - ETA: 0s - loss: 0.6776 - accuracy: 0.58 - ETA: 0s - loss: 0.6761 - accuracy: 0.59 - ETA: 0s - loss: 0.6751 - accuracy: 0.59 - ETA: 0s - loss: 0.6738 - accuracy: 0.59 - ETA: 0s - loss: 0.6730 - accuracy: 0.59 - ETA: 0s - loss: 0.6714 - accuracy: 0.59 - ETA: 0s - loss: 0.6697 - accuracy: 0.59 - ETA: 0s - loss: 0.6684 - accuracy: 0.60 - ETA: 0s - loss: 0.6669 - accuracy: 0.60 - ETA: 0s - loss: 0.6653 - accuracy: 0.60 - ETA: 0s - loss: 0.6648 - accuracy: 0.60 - ETA: 0s - loss: 0.6643 - accuracy: 0.60 - ETA: 0s - loss: 0.6633 - accuracy: 0.60 - 2s 143us/step - loss: 0.6625 - accuracy: 0.6101 - val_loss: 0.6107 - val_accuracy: 0.7223\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:41 - loss: 0.6914 - accuracy: 0.50 - ETA: 4s - loss: 0.6945 - accuracy: 0.4976 - ETA: 2s - loss: 0.6934 - accuracy: 0.52 - ETA: 2s - loss: 0.6932 - accuracy: 0.53 - ETA: 1s - loss: 0.6928 - accuracy: 0.54 - ETA: 1s - loss: 0.6919 - accuracy: 0.55 - ETA: 1s - loss: 0.6914 - accuracy: 0.55 - ETA: 1s - loss: 0.6906 - accuracy: 0.55 - ETA: 1s - loss: 0.6900 - accuracy: 0.55 - ETA: 1s - loss: 0.6895 - accuracy: 0.56 - ETA: 1s - loss: 0.6888 - accuracy: 0.56 - ETA: 1s - loss: 0.6872 - accuracy: 0.57 - ETA: 1s - loss: 0.6861 - accuracy: 0.57 - ETA: 1s - loss: 0.6848 - accuracy: 0.57 - ETA: 0s - loss: 0.6829 - accuracy: 0.58 - ETA: 0s - loss: 0.6813 - accuracy: 0.58 - ETA: 0s - loss: 0.6804 - accuracy: 0.58 - ETA: 0s - loss: 0.6796 - accuracy: 0.58 - ETA: 0s - loss: 0.6782 - accuracy: 0.59 - ETA: 0s - loss: 0.6774 - accuracy: 0.59 - ETA: 0s - loss: 0.6758 - accuracy: 0.59 - ETA: 0s - loss: 0.6742 - accuracy: 0.59 - ETA: 0s - loss: 0.6728 - accuracy: 0.59 - ETA: 0s - loss: 0.6713 - accuracy: 0.59 - ETA: 0s - loss: 0.6697 - accuracy: 0.60 - ETA: 0s - loss: 0.6681 - accuracy: 0.60 - ETA: 0s - loss: 0.6664 - accuracy: 0.60 - ETA: 0s - loss: 0.6656 - accuracy: 0.60 - ETA: 0s - loss: 0.6640 - accuracy: 0.60 - ETA: 0s - loss: 0.6633 - accuracy: 0.60 - ETA: 0s - loss: 0.6615 - accuracy: 0.61 - 2s 140us/step - loss: 0.6610 - accuracy: 0.6139 - val_loss: 0.6039 - val_accuracy: 0.7188\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:43 - loss: 0.6922 - accuracy: 0.50 - ETA: 4s - loss: 0.6933 - accuracy: 0.5231 - ETA: 2s - loss: 0.6933 - accuracy: 0.50 - ETA: 2s - loss: 0.6929 - accuracy: 0.51 - ETA: 2s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6913 - accuracy: 0.52 - ETA: 1s - loss: 0.6911 - accuracy: 0.52 - ETA: 1s - loss: 0.6906 - accuracy: 0.53 - ETA: 1s - loss: 0.6899 - accuracy: 0.54 - ETA: 1s - loss: 0.6887 - accuracy: 0.54 - ETA: 1s - loss: 0.6879 - accuracy: 0.55 - ETA: 1s - loss: 0.6868 - accuracy: 0.55 - ETA: 1s - loss: 0.6853 - accuracy: 0.56 - ETA: 1s - loss: 0.6838 - accuracy: 0.56 - ETA: 0s - loss: 0.6828 - accuracy: 0.57 - ETA: 0s - loss: 0.6817 - accuracy: 0.57 - ETA: 0s - loss: 0.6808 - accuracy: 0.57 - ETA: 0s - loss: 0.6797 - accuracy: 0.57 - ETA: 0s - loss: 0.6785 - accuracy: 0.58 - ETA: 0s - loss: 0.6772 - accuracy: 0.58 - ETA: 0s - loss: 0.6759 - accuracy: 0.58 - ETA: 0s - loss: 0.6753 - accuracy: 0.58 - ETA: 0s - loss: 0.6744 - accuracy: 0.58 - ETA: 0s - loss: 0.6733 - accuracy: 0.58 - ETA: 0s - loss: 0.6720 - accuracy: 0.59 - ETA: 0s - loss: 0.6709 - accuracy: 0.59 - ETA: 0s - loss: 0.6696 - accuracy: 0.59 - ETA: 0s - loss: 0.6689 - accuracy: 0.59 - ETA: 0s - loss: 0.6673 - accuracy: 0.59 - ETA: 0s - loss: 0.6670 - accuracy: 0.59 - ETA: 0s - loss: 0.6663 - accuracy: 0.60 - ETA: 0s - loss: 0.6651 - accuracy: 0.60 - 2s 144us/step - loss: 0.6640 - accuracy: 0.6039 - val_loss: 0.6118 - val_accuracy: 0.7188\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:03 - loss: 0.6938 - accuracy: 0.50 - ETA: 4s - loss: 0.6942 - accuracy: 0.4953 - ETA: 3s - loss: 0.6936 - accuracy: 0.50 - ETA: 2s - loss: 0.6935 - accuracy: 0.49 - ETA: 2s - loss: 0.6931 - accuracy: 0.50 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6922 - accuracy: 0.53 - ETA: 1s - loss: 0.6912 - accuracy: 0.54 - ETA: 1s - loss: 0.6899 - accuracy: 0.55 - ETA: 1s - loss: 0.6891 - accuracy: 0.55 - ETA: 1s - loss: 0.6877 - accuracy: 0.56 - ETA: 1s - loss: 0.6859 - accuracy: 0.57 - ETA: 1s - loss: 0.6846 - accuracy: 0.58 - ETA: 1s - loss: 0.6831 - accuracy: 0.58 - ETA: 0s - loss: 0.6823 - accuracy: 0.59 - ETA: 0s - loss: 0.6810 - accuracy: 0.59 - ETA: 0s - loss: 0.6794 - accuracy: 0.59 - ETA: 0s - loss: 0.6778 - accuracy: 0.60 - ETA: 0s - loss: 0.6759 - accuracy: 0.60 - ETA: 0s - loss: 0.6741 - accuracy: 0.60 - ETA: 0s - loss: 0.6724 - accuracy: 0.61 - ETA: 0s - loss: 0.6705 - accuracy: 0.61 - ETA: 0s - loss: 0.6692 - accuracy: 0.61 - ETA: 0s - loss: 0.6675 - accuracy: 0.61 - ETA: 0s - loss: 0.6657 - accuracy: 0.61 - ETA: 0s - loss: 0.6633 - accuracy: 0.62 - ETA: 0s - loss: 0.6605 - accuracy: 0.62 - ETA: 0s - loss: 0.6594 - accuracy: 0.62 - ETA: 0s - loss: 0.6585 - accuracy: 0.62 - ETA: 0s - loss: 0.6572 - accuracy: 0.62 - ETA: 0s - loss: 0.6563 - accuracy: 0.62 - 2s 142us/step - loss: 0.6549 - accuracy: 0.6317 - val_loss: 0.6007 - val_accuracy: 0.7216\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 62us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:44 - loss: 0.6885 - accuracy: 0.87 - ETA: 4s - loss: 0.6923 - accuracy: 0.5425 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6921 - accuracy: 0.53 - ETA: 2s - loss: 0.6911 - accuracy: 0.54 - ETA: 1s - loss: 0.6906 - accuracy: 0.54 - ETA: 1s - loss: 0.6903 - accuracy: 0.54 - ETA: 1s - loss: 0.6898 - accuracy: 0.55 - ETA: 1s - loss: 0.6892 - accuracy: 0.55 - ETA: 1s - loss: 0.6881 - accuracy: 0.56 - ETA: 1s - loss: 0.6869 - accuracy: 0.56 - ETA: 1s - loss: 0.6855 - accuracy: 0.57 - ETA: 1s - loss: 0.6837 - accuracy: 0.58 - ETA: 1s - loss: 0.6820 - accuracy: 0.58 - ETA: 0s - loss: 0.6809 - accuracy: 0.58 - ETA: 0s - loss: 0.6793 - accuracy: 0.59 - ETA: 0s - loss: 0.6771 - accuracy: 0.59 - ETA: 0s - loss: 0.6750 - accuracy: 0.60 - ETA: 0s - loss: 0.6728 - accuracy: 0.60 - ETA: 0s - loss: 0.6712 - accuracy: 0.61 - ETA: 0s - loss: 0.6688 - accuracy: 0.61 - ETA: 0s - loss: 0.6671 - accuracy: 0.61 - ETA: 0s - loss: 0.6656 - accuracy: 0.61 - ETA: 0s - loss: 0.6628 - accuracy: 0.62 - ETA: 0s - loss: 0.6612 - accuracy: 0.62 - ETA: 0s - loss: 0.6590 - accuracy: 0.62 - ETA: 0s - loss: 0.6581 - accuracy: 0.62 - ETA: 0s - loss: 0.6569 - accuracy: 0.63 - ETA: 0s - loss: 0.6559 - accuracy: 0.63 - ETA: 0s - loss: 0.6540 - accuracy: 0.63 - ETA: 0s - loss: 0.6519 - accuracy: 0.63 - 2s 138us/step - loss: 0.6523 - accuracy: 0.6343 - val_loss: 0.5977 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:44 - loss: 0.6972 - accuracy: 0.50 - ETA: 4s - loss: 0.6926 - accuracy: 0.5307 - ETA: 2s - loss: 0.6917 - accuracy: 0.54 - ETA: 2s - loss: 0.6908 - accuracy: 0.54 - ETA: 2s - loss: 0.6907 - accuracy: 0.53 - ETA: 1s - loss: 0.6899 - accuracy: 0.53 - ETA: 1s - loss: 0.6891 - accuracy: 0.54 - ETA: 1s - loss: 0.6889 - accuracy: 0.54 - ETA: 1s - loss: 0.6882 - accuracy: 0.54 - ETA: 1s - loss: 0.6867 - accuracy: 0.55 - ETA: 1s - loss: 0.6859 - accuracy: 0.55 - ETA: 1s - loss: 0.6851 - accuracy: 0.56 - ETA: 1s - loss: 0.6840 - accuracy: 0.56 - ETA: 1s - loss: 0.6824 - accuracy: 0.57 - ETA: 1s - loss: 0.6804 - accuracy: 0.58 - ETA: 0s - loss: 0.6790 - accuracy: 0.58 - ETA: 0s - loss: 0.6769 - accuracy: 0.59 - ETA: 0s - loss: 0.6751 - accuracy: 0.59 - ETA: 0s - loss: 0.6736 - accuracy: 0.59 - ETA: 0s - loss: 0.6720 - accuracy: 0.59 - ETA: 0s - loss: 0.6705 - accuracy: 0.60 - ETA: 0s - loss: 0.6684 - accuracy: 0.60 - ETA: 0s - loss: 0.6662 - accuracy: 0.60 - ETA: 0s - loss: 0.6644 - accuracy: 0.61 - ETA: 0s - loss: 0.6633 - accuracy: 0.61 - ETA: 0s - loss: 0.6615 - accuracy: 0.61 - ETA: 0s - loss: 0.6607 - accuracy: 0.61 - ETA: 0s - loss: 0.6592 - accuracy: 0.62 - ETA: 0s - loss: 0.6562 - accuracy: 0.62 - ETA: 0s - loss: 0.6555 - accuracy: 0.62 - ETA: 0s - loss: 0.6537 - accuracy: 0.62 - 2s 141us/step - loss: 0.6520 - accuracy: 0.6307 - val_loss: 0.6015 - val_accuracy: 0.7166\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:44 - loss: 0.6955 - accuracy: 0.50 - ETA: 4s - loss: 0.6942 - accuracy: 0.4882 - ETA: 2s - loss: 0.6937 - accuracy: 0.50 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6919 - accuracy: 0.53 - ETA: 1s - loss: 0.6900 - accuracy: 0.54 - ETA: 1s - loss: 0.6890 - accuracy: 0.54 - ETA: 1s - loss: 0.6879 - accuracy: 0.55 - ETA: 1s - loss: 0.6865 - accuracy: 0.56 - ETA: 1s - loss: 0.6851 - accuracy: 0.56 - ETA: 1s - loss: 0.6841 - accuracy: 0.56 - ETA: 1s - loss: 0.6828 - accuracy: 0.56 - ETA: 1s - loss: 0.6819 - accuracy: 0.57 - ETA: 1s - loss: 0.6797 - accuracy: 0.57 - ETA: 0s - loss: 0.6787 - accuracy: 0.58 - ETA: 0s - loss: 0.6770 - accuracy: 0.58 - ETA: 0s - loss: 0.6758 - accuracy: 0.58 - ETA: 0s - loss: 0.6739 - accuracy: 0.59 - ETA: 0s - loss: 0.6726 - accuracy: 0.59 - ETA: 0s - loss: 0.6715 - accuracy: 0.59 - ETA: 0s - loss: 0.6699 - accuracy: 0.60 - ETA: 0s - loss: 0.6688 - accuracy: 0.60 - ETA: 0s - loss: 0.6672 - accuracy: 0.60 - ETA: 0s - loss: 0.6658 - accuracy: 0.60 - ETA: 0s - loss: 0.6643 - accuracy: 0.61 - ETA: 0s - loss: 0.6634 - accuracy: 0.61 - ETA: 0s - loss: 0.6616 - accuracy: 0.61 - ETA: 0s - loss: 0.6605 - accuracy: 0.61 - ETA: 0s - loss: 0.6590 - accuracy: 0.61 - ETA: 0s - loss: 0.6582 - accuracy: 0.62 - ETA: 0s - loss: 0.6568 - accuracy: 0.62 - 2s 143us/step - loss: 0.6564 - accuracy: 0.6230 - val_loss: 0.5987 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:46 - loss: 0.6936 - accuracy: 0.50 - ETA: 4s - loss: 0.6945 - accuracy: 0.5046 - ETA: 2s - loss: 0.6933 - accuracy: 0.52 - ETA: 2s - loss: 0.6926 - accuracy: 0.53 - ETA: 2s - loss: 0.6918 - accuracy: 0.54 - ETA: 1s - loss: 0.6898 - accuracy: 0.55 - ETA: 1s - loss: 0.6889 - accuracy: 0.56 - ETA: 1s - loss: 0.6870 - accuracy: 0.56 - ETA: 1s - loss: 0.6860 - accuracy: 0.57 - ETA: 1s - loss: 0.6837 - accuracy: 0.58 - ETA: 1s - loss: 0.6812 - accuracy: 0.58 - ETA: 1s - loss: 0.6786 - accuracy: 0.59 - ETA: 1s - loss: 0.6766 - accuracy: 0.59 - ETA: 1s - loss: 0.6741 - accuracy: 0.60 - ETA: 0s - loss: 0.6716 - accuracy: 0.60 - ETA: 0s - loss: 0.6694 - accuracy: 0.61 - ETA: 0s - loss: 0.6673 - accuracy: 0.61 - ETA: 0s - loss: 0.6661 - accuracy: 0.61 - ETA: 0s - loss: 0.6638 - accuracy: 0.62 - ETA: 0s - loss: 0.6612 - accuracy: 0.62 - ETA: 0s - loss: 0.6602 - accuracy: 0.62 - ETA: 0s - loss: 0.6584 - accuracy: 0.62 - ETA: 0s - loss: 0.6566 - accuracy: 0.63 - ETA: 0s - loss: 0.6553 - accuracy: 0.63 - ETA: 0s - loss: 0.6545 - accuracy: 0.63 - ETA: 0s - loss: 0.6538 - accuracy: 0.63 - ETA: 0s - loss: 0.6528 - accuracy: 0.63 - ETA: 0s - loss: 0.6508 - accuracy: 0.64 - ETA: 0s - loss: 0.6494 - accuracy: 0.64 - ETA: 0s - loss: 0.6479 - accuracy: 0.64 - ETA: 0s - loss: 0.6462 - accuracy: 0.64 - 2s 140us/step - loss: 0.6447 - accuracy: 0.6495 - val_loss: 0.5820 - val_accuracy: 0.7173\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 54us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:49 - loss: 0.6915 - accuracy: 0.50 - ETA: 4s - loss: 0.6934 - accuracy: 0.5000 - ETA: 2s - loss: 0.6931 - accuracy: 0.51 - ETA: 2s - loss: 0.6921 - accuracy: 0.51 - ETA: 2s - loss: 0.6877 - accuracy: 0.53 - ETA: 1s - loss: 0.6882 - accuracy: 0.53 - ETA: 1s - loss: 0.6877 - accuracy: 0.53 - ETA: 1s - loss: 0.6856 - accuracy: 0.54 - ETA: 1s - loss: 0.6853 - accuracy: 0.54 - ETA: 1s - loss: 0.6838 - accuracy: 0.55 - ETA: 1s - loss: 0.6828 - accuracy: 0.56 - ETA: 1s - loss: 0.6818 - accuracy: 0.56 - ETA: 1s - loss: 0.6801 - accuracy: 0.57 - ETA: 1s - loss: 0.6787 - accuracy: 0.58 - ETA: 1s - loss: 0.6772 - accuracy: 0.58 - ETA: 0s - loss: 0.6752 - accuracy: 0.58 - ETA: 0s - loss: 0.6731 - accuracy: 0.59 - ETA: 0s - loss: 0.6719 - accuracy: 0.59 - ETA: 0s - loss: 0.6703 - accuracy: 0.59 - ETA: 0s - loss: 0.6688 - accuracy: 0.60 - ETA: 0s - loss: 0.6666 - accuracy: 0.60 - ETA: 0s - loss: 0.6647 - accuracy: 0.60 - ETA: 0s - loss: 0.6627 - accuracy: 0.61 - ETA: 0s - loss: 0.6608 - accuracy: 0.61 - ETA: 0s - loss: 0.6597 - accuracy: 0.61 - ETA: 0s - loss: 0.6567 - accuracy: 0.62 - ETA: 0s - loss: 0.6553 - accuracy: 0.62 - ETA: 0s - loss: 0.6533 - accuracy: 0.62 - ETA: 0s - loss: 0.6510 - accuracy: 0.63 - ETA: 0s - loss: 0.6502 - accuracy: 0.63 - ETA: 0s - loss: 0.6488 - accuracy: 0.63 - 2s 140us/step - loss: 0.6476 - accuracy: 0.6357 - val_loss: 0.5876 - val_accuracy: 0.7230\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 2:43 - loss: 0.6938 - accuracy: 0.37 - ETA: 4s - loss: 0.6936 - accuracy: 0.5069 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6915 - accuracy: 0.52 - ETA: 2s - loss: 0.6912 - accuracy: 0.52 - ETA: 1s - loss: 0.6901 - accuracy: 0.52 - ETA: 1s - loss: 0.6890 - accuracy: 0.52 - ETA: 1s - loss: 0.6878 - accuracy: 0.53 - ETA: 1s - loss: 0.6860 - accuracy: 0.54 - ETA: 1s - loss: 0.6853 - accuracy: 0.54 - ETA: 1s - loss: 0.6829 - accuracy: 0.55 - ETA: 1s - loss: 0.6819 - accuracy: 0.55 - ETA: 1s - loss: 0.6805 - accuracy: 0.56 - ETA: 1s - loss: 0.6791 - accuracy: 0.56 - ETA: 0s - loss: 0.6771 - accuracy: 0.57 - ETA: 0s - loss: 0.6751 - accuracy: 0.57 - ETA: 0s - loss: 0.6735 - accuracy: 0.58 - ETA: 0s - loss: 0.6710 - accuracy: 0.58 - ETA: 0s - loss: 0.6684 - accuracy: 0.59 - ETA: 0s - loss: 0.6670 - accuracy: 0.59 - ETA: 0s - loss: 0.6656 - accuracy: 0.60 - ETA: 0s - loss: 0.6634 - accuracy: 0.60 - ETA: 0s - loss: 0.6621 - accuracy: 0.60 - ETA: 0s - loss: 0.6610 - accuracy: 0.61 - ETA: 0s - loss: 0.6601 - accuracy: 0.61 - ETA: 0s - loss: 0.6590 - accuracy: 0.61 - ETA: 0s - loss: 0.6583 - accuracy: 0.61 - ETA: 0s - loss: 0.6564 - accuracy: 0.62 - ETA: 0s - loss: 0.6554 - accuracy: 0.62 - ETA: 0s - loss: 0.6541 - accuracy: 0.62 - ETA: 0s - loss: 0.6536 - accuracy: 0.62 - 2s 138us/step - loss: 0.6534 - accuracy: 0.6282 - val_loss: 0.6034 - val_accuracy: 0.7216\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:43 - loss: 0.6931 - accuracy: 0.62 - ETA: 4s - loss: 0.6932 - accuracy: 0.5071 - ETA: 2s - loss: 0.6924 - accuracy: 0.52 - ETA: 2s - loss: 0.6921 - accuracy: 0.52 - ETA: 2s - loss: 0.6908 - accuracy: 0.54 - ETA: 1s - loss: 0.6898 - accuracy: 0.54 - ETA: 1s - loss: 0.6885 - accuracy: 0.55 - ETA: 1s - loss: 0.6877 - accuracy: 0.56 - ETA: 1s - loss: 0.6869 - accuracy: 0.57 - ETA: 1s - loss: 0.6856 - accuracy: 0.57 - ETA: 1s - loss: 0.6838 - accuracy: 0.58 - ETA: 1s - loss: 0.6823 - accuracy: 0.58 - ETA: 1s - loss: 0.6813 - accuracy: 0.58 - ETA: 1s - loss: 0.6803 - accuracy: 0.58 - ETA: 1s - loss: 0.6794 - accuracy: 0.58 - ETA: 0s - loss: 0.6769 - accuracy: 0.59 - ETA: 0s - loss: 0.6757 - accuracy: 0.59 - ETA: 0s - loss: 0.6734 - accuracy: 0.59 - ETA: 0s - loss: 0.6715 - accuracy: 0.60 - ETA: 0s - loss: 0.6692 - accuracy: 0.60 - ETA: 0s - loss: 0.6672 - accuracy: 0.60 - ETA: 0s - loss: 0.6665 - accuracy: 0.60 - ETA: 0s - loss: 0.6654 - accuracy: 0.60 - ETA: 0s - loss: 0.6640 - accuracy: 0.61 - ETA: 0s - loss: 0.6618 - accuracy: 0.61 - ETA: 0s - loss: 0.6602 - accuracy: 0.61 - ETA: 0s - loss: 0.6582 - accuracy: 0.62 - ETA: 0s - loss: 0.6566 - accuracy: 0.62 - ETA: 0s - loss: 0.6540 - accuracy: 0.62 - ETA: 0s - loss: 0.6522 - accuracy: 0.62 - ETA: 0s - loss: 0.6512 - accuracy: 0.62 - ETA: 0s - loss: 0.6506 - accuracy: 0.63 - 2s 144us/step - loss: 0.6499 - accuracy: 0.6327 - val_loss: 0.5952 - val_accuracy: 0.7166\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:00 - loss: 0.6895 - accuracy: 0.75 - ETA: 4s - loss: 0.6943 - accuracy: 0.5312 - ETA: 3s - loss: 0.6936 - accuracy: 0.53 - ETA: 2s - loss: 0.6923 - accuracy: 0.55 - ETA: 2s - loss: 0.6918 - accuracy: 0.55 - ETA: 1s - loss: 0.6906 - accuracy: 0.56 - ETA: 1s - loss: 0.6889 - accuracy: 0.57 - ETA: 1s - loss: 0.6876 - accuracy: 0.57 - ETA: 1s - loss: 0.6868 - accuracy: 0.58 - ETA: 1s - loss: 0.6852 - accuracy: 0.58 - ETA: 1s - loss: 0.6824 - accuracy: 0.59 - ETA: 1s - loss: 0.6804 - accuracy: 0.60 - ETA: 1s - loss: 0.6787 - accuracy: 0.60 - ETA: 1s - loss: 0.6763 - accuracy: 0.60 - ETA: 0s - loss: 0.6742 - accuracy: 0.61 - ETA: 0s - loss: 0.6725 - accuracy: 0.61 - ETA: 0s - loss: 0.6704 - accuracy: 0.62 - ETA: 0s - loss: 0.6682 - accuracy: 0.62 - ETA: 0s - loss: 0.6664 - accuracy: 0.62 - ETA: 0s - loss: 0.6651 - accuracy: 0.62 - ETA: 0s - loss: 0.6636 - accuracy: 0.63 - ETA: 0s - loss: 0.6611 - accuracy: 0.63 - ETA: 0s - loss: 0.6594 - accuracy: 0.63 - ETA: 0s - loss: 0.6576 - accuracy: 0.63 - ETA: 0s - loss: 0.6560 - accuracy: 0.63 - ETA: 0s - loss: 0.6544 - accuracy: 0.64 - ETA: 0s - loss: 0.6535 - accuracy: 0.64 - ETA: 0s - loss: 0.6519 - accuracy: 0.64 - ETA: 0s - loss: 0.6500 - accuracy: 0.64 - ETA: 0s - loss: 0.6479 - accuracy: 0.64 - ETA: 0s - loss: 0.6473 - accuracy: 0.64 - ETA: 0s - loss: 0.6461 - accuracy: 0.64 - 2s 143us/step - loss: 0.6459 - accuracy: 0.6496 - val_loss: 0.5966 - val_accuracy: 0.6903\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:03 - loss: 0.6927 - accuracy: 0.75 - ETA: 4s - loss: 0.6938 - accuracy: 0.5046 - ETA: 2s - loss: 0.6935 - accuracy: 0.52 - ETA: 2s - loss: 0.6933 - accuracy: 0.53 - ETA: 2s - loss: 0.6931 - accuracy: 0.53 - ETA: 1s - loss: 0.6932 - accuracy: 0.52 - ETA: 1s - loss: 0.6926 - accuracy: 0.53 - ETA: 1s - loss: 0.6928 - accuracy: 0.53 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - 2s 133us/step - loss: 0.6926 - accuracy: 0.5170 - val_loss: 0.6906 - val_accuracy: 0.5369\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:19 - loss: 0.6955 - accuracy: 0.37 - ETA: 3s - loss: 0.6945 - accuracy: 0.4491 - ETA: 2s - loss: 0.6943 - accuracy: 0.46 - ETA: 2s - loss: 0.6943 - accuracy: 0.47 - ETA: 1s - loss: 0.6941 - accuracy: 0.48 - ETA: 1s - loss: 0.6939 - accuracy: 0.49 - ETA: 1s - loss: 0.6941 - accuracy: 0.49 - ETA: 1s - loss: 0.6939 - accuracy: 0.49 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - 2s 137us/step - loss: 0.6931 - accuracy: 0.5106 - val_loss: 0.6911 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:31 - loss: 0.6938 - accuracy: 0.50 - ETA: 4s - loss: 0.6952 - accuracy: 0.4306 - ETA: 3s - loss: 0.6951 - accuracy: 0.43 - ETA: 2s - loss: 0.6945 - accuracy: 0.45 - ETA: 2s - loss: 0.6941 - accuracy: 0.48 - ETA: 1s - loss: 0.6935 - accuracy: 0.49 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6934 - accuracy: 0.50 - ETA: 1s - loss: 0.6933 - accuracy: 0.50 - ETA: 1s - loss: 0.6932 - accuracy: 0.50 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6922 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - 2s 134us/step - loss: 0.6919 - accuracy: 0.5254 - val_loss: 0.6874 - val_accuracy: 0.5618\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:22 - loss: 0.6942 - accuracy: 0.75 - ETA: 3s - loss: 0.6935 - accuracy: 0.5509 - ETA: 2s - loss: 0.6933 - accuracy: 0.54 - ETA: 2s - loss: 0.6932 - accuracy: 0.54 - ETA: 1s - loss: 0.6931 - accuracy: 0.54 - ETA: 1s - loss: 0.6930 - accuracy: 0.53 - ETA: 1s - loss: 0.6929 - accuracy: 0.53 - ETA: 1s - loss: 0.6929 - accuracy: 0.53 - ETA: 1s - loss: 0.6929 - accuracy: 0.53 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6933 - accuracy: 0.52 - ETA: 1s - loss: 0.6933 - accuracy: 0.52 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - 2s 135us/step - loss: 0.6926 - accuracy: 0.5254 - val_loss: 0.6905 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:31 - loss: 0.6922 - accuracy: 0.62 - ETA: 3s - loss: 0.6932 - accuracy: 0.5379 - ETA: 2s - loss: 0.6935 - accuracy: 0.52 - ETA: 2s - loss: 0.6935 - accuracy: 0.52 - ETA: 1s - loss: 0.6935 - accuracy: 0.52 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.52 - ETA: 1s - loss: 0.6933 - accuracy: 0.52 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - 2s 134us/step - loss: 0.6935 - accuracy: 0.5141 - val_loss: 0.6926 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:35 - loss: 0.6934 - accuracy: 0.50 - ETA: 4s - loss: 0.6939 - accuracy: 0.4821 - ETA: 2s - loss: 0.6939 - accuracy: 0.48 - ETA: 2s - loss: 0.6939 - accuracy: 0.48 - ETA: 1s - loss: 0.6939 - accuracy: 0.48 - ETA: 1s - loss: 0.6939 - accuracy: 0.48 - ETA: 1s - loss: 0.6938 - accuracy: 0.48 - ETA: 1s - loss: 0.6938 - accuracy: 0.49 - ETA: 1s - loss: 0.6937 - accuracy: 0.49 - ETA: 1s - loss: 0.6938 - accuracy: 0.49 - ETA: 1s - loss: 0.6938 - accuracy: 0.49 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.49 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.49 - ETA: 0s - loss: 0.6938 - accuracy: 0.49 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - 2s 138us/step - loss: 0.6936 - accuracy: 0.5064 - val_loss: 0.6934 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:24 - loss: 0.6991 - accuracy: 0.25 - ETA: 3s - loss: 0.6939 - accuracy: 0.5134 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 2s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.52 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.52 - ETA: 1s - loss: 0.6932 - accuracy: 0.52 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.53 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.53 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.53 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - ETA: 0s - loss: 0.6916 - accuracy: 0.53 - ETA: 0s - loss: 0.6913 - accuracy: 0.53 - ETA: 0s - loss: 0.6912 - accuracy: 0.53 - ETA: 0s - loss: 0.6907 - accuracy: 0.53 - ETA: 0s - loss: 0.6906 - accuracy: 0.53 - ETA: 0s - loss: 0.6905 - accuracy: 0.53 - ETA: 0s - loss: 0.6906 - accuracy: 0.53 - ETA: 0s - loss: 0.6904 - accuracy: 0.53 - 2s 133us/step - loss: 0.6902 - accuracy: 0.5358 - val_loss: 0.6823 - val_accuracy: 0.5824\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 60us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 2:27 - loss: 0.6927 - accuracy: 0.75 - ETA: 4s - loss: 0.6945 - accuracy: 0.5024 - ETA: 2s - loss: 0.6944 - accuracy: 0.50 - ETA: 2s - loss: 0.6943 - accuracy: 0.50 - ETA: 1s - loss: 0.6942 - accuracy: 0.50 - ETA: 1s - loss: 0.6943 - accuracy: 0.50 - ETA: 1s - loss: 0.6940 - accuracy: 0.51 - ETA: 1s - loss: 0.6941 - accuracy: 0.51 - ETA: 1s - loss: 0.6942 - accuracy: 0.51 - ETA: 1s - loss: 0.6941 - accuracy: 0.51 - ETA: 1s - loss: 0.6940 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.53 - 2s 134us/step - loss: 0.6929 - accuracy: 0.5309 - val_loss: 0.6895 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:22 - loss: 0.6947 - accuracy: 0.62 - ETA: 3s - loss: 0.6940 - accuracy: 0.5000 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6929 - accuracy: 0.53 - ETA: 1s - loss: 0.6933 - accuracy: 0.52 - ETA: 1s - loss: 0.6932 - accuracy: 0.52 - ETA: 1s - loss: 0.6935 - accuracy: 0.52 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.52 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - 2s 137us/step - loss: 0.6925 - accuracy: 0.5149 - val_loss: 0.6873 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:24 - loss: 0.6956 - accuracy: 0.25 - ETA: 3s - loss: 0.6941 - accuracy: 0.4803 - ETA: 2s - loss: 0.6941 - accuracy: 0.49 - ETA: 2s - loss: 0.6939 - accuracy: 0.49 - ETA: 1s - loss: 0.6940 - accuracy: 0.49 - ETA: 1s - loss: 0.6940 - accuracy: 0.50 - ETA: 1s - loss: 0.6941 - accuracy: 0.49 - ETA: 1s - loss: 0.6941 - accuracy: 0.49 - ETA: 1s - loss: 0.6939 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - 2s 136us/step - loss: 0.6930 - accuracy: 0.5130 - val_loss: 0.6897 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:24 - loss: 0.6926 - accuracy: 0.87 - ETA: 3s - loss: 0.6942 - accuracy: 0.4977 - ETA: 2s - loss: 0.6943 - accuracy: 0.47 - ETA: 2s - loss: 0.6941 - accuracy: 0.48 - ETA: 1s - loss: 0.6940 - accuracy: 0.49 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6939 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - 2s 137us/step - loss: 0.6925 - accuracy: 0.5139 - val_loss: 0.6882 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 58us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:27 - loss: 0.6936 - accuracy: 0.75 - ETA: 4s - loss: 0.6943 - accuracy: 0.4884 - ETA: 2s - loss: 0.6941 - accuracy: 0.49 - ETA: 2s - loss: 0.6943 - accuracy: 0.48 - ETA: 1s - loss: 0.6941 - accuracy: 0.50 - ETA: 1s - loss: 0.6942 - accuracy: 0.49 - ETA: 1s - loss: 0.6941 - accuracy: 0.50 - ETA: 1s - loss: 0.6939 - accuracy: 0.50 - ETA: 1s - loss: 0.6940 - accuracy: 0.50 - ETA: 1s - loss: 0.6940 - accuracy: 0.50 - ETA: 1s - loss: 0.6940 - accuracy: 0.51 - ETA: 1s - loss: 0.6940 - accuracy: 0.51 - ETA: 1s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - 2s 135us/step - loss: 0.6926 - accuracy: 0.5165 - val_loss: 0.6906 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:24 - loss: 0.6976 - accuracy: 0.37 - ETA: 3s - loss: 0.6961 - accuracy: 0.4907 - ETA: 2s - loss: 0.6958 - accuracy: 0.49 - ETA: 2s - loss: 0.6954 - accuracy: 0.49 - ETA: 1s - loss: 0.6954 - accuracy: 0.49 - ETA: 1s - loss: 0.6952 - accuracy: 0.49 - ETA: 1s - loss: 0.6951 - accuracy: 0.50 - ETA: 1s - loss: 0.6948 - accuracy: 0.50 - ETA: 1s - loss: 0.6943 - accuracy: 0.51 - ETA: 1s - loss: 0.6943 - accuracy: 0.51 - ETA: 1s - loss: 0.6942 - accuracy: 0.51 - ETA: 1s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6918 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6922 - accuracy: 0.52 - ETA: 0s - loss: 0.6919 - accuracy: 0.52 - ETA: 0s - loss: 0.6918 - accuracy: 0.52 - ETA: 0s - loss: 0.6918 - accuracy: 0.53 - ETA: 0s - loss: 0.6916 - accuracy: 0.53 - ETA: 0s - loss: 0.6913 - accuracy: 0.53 - ETA: 0s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6910 - accuracy: 0.53 - ETA: 0s - loss: 0.6908 - accuracy: 0.54 - ETA: 0s - loss: 0.6905 - accuracy: 0.54 - 2s 133us/step - loss: 0.6904 - accuracy: 0.5428 - val_loss: 0.6803 - val_accuracy: 0.5639\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:25 - loss: 0.6941 - accuracy: 0.50 - ETA: 3s - loss: 0.6945 - accuracy: 0.5273 - ETA: 2s - loss: 0.6942 - accuracy: 0.51 - ETA: 2s - loss: 0.6944 - accuracy: 0.51 - ETA: 1s - loss: 0.6942 - accuracy: 0.51 - ETA: 1s - loss: 0.6942 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.52 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6922 - accuracy: 0.52 - ETA: 0s - loss: 0.6922 - accuracy: 0.52 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - ETA: 0s - loss: 0.6919 - accuracy: 0.52 - ETA: 0s - loss: 0.6916 - accuracy: 0.52 - ETA: 0s - loss: 0.6915 - accuracy: 0.52 - ETA: 0s - loss: 0.6914 - accuracy: 0.52 - ETA: 0s - loss: 0.6913 - accuracy: 0.52 - ETA: 0s - loss: 0.6911 - accuracy: 0.52 - ETA: 0s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6907 - accuracy: 0.53 - ETA: 0s - loss: 0.6907 - accuracy: 0.53 - 2s 135us/step - loss: 0.6906 - accuracy: 0.5351 - val_loss: 0.6829 - val_accuracy: 0.6875\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:22 - loss: 0.6942 - accuracy: 0.62 - ETA: 3s - loss: 0.6948 - accuracy: 0.5409 - ETA: 2s - loss: 0.6945 - accuracy: 0.53 - ETA: 2s - loss: 0.6945 - accuracy: 0.52 - ETA: 1s - loss: 0.6946 - accuracy: 0.52 - ETA: 1s - loss: 0.6945 - accuracy: 0.51 - ETA: 1s - loss: 0.6945 - accuracy: 0.51 - ETA: 1s - loss: 0.6945 - accuracy: 0.51 - ETA: 1s - loss: 0.6944 - accuracy: 0.51 - ETA: 1s - loss: 0.6942 - accuracy: 0.52 - ETA: 1s - loss: 0.6941 - accuracy: 0.52 - ETA: 1s - loss: 0.6940 - accuracy: 0.52 - ETA: 1s - loss: 0.6938 - accuracy: 0.52 - ETA: 0s - loss: 0.6936 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6922 - accuracy: 0.53 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6918 - accuracy: 0.53 - ETA: 0s - loss: 0.6916 - accuracy: 0.53 - ETA: 0s - loss: 0.6915 - accuracy: 0.53 - ETA: 0s - loss: 0.6913 - accuracy: 0.53 - ETA: 0s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6909 - accuracy: 0.53 - 2s 135us/step - loss: 0.6907 - accuracy: 0.5374 - val_loss: 0.6817 - val_accuracy: 0.6051\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:22 - loss: 0.7004 - accuracy: 0.25 - ETA: 3s - loss: 0.6948 - accuracy: 0.5023 - ETA: 2s - loss: 0.6948 - accuracy: 0.50 - ETA: 2s - loss: 0.6948 - accuracy: 0.50 - ETA: 1s - loss: 0.6948 - accuracy: 0.49 - ETA: 1s - loss: 0.6946 - accuracy: 0.50 - ETA: 1s - loss: 0.6943 - accuracy: 0.51 - ETA: 1s - loss: 0.6941 - accuracy: 0.51 - ETA: 1s - loss: 0.6939 - accuracy: 0.52 - ETA: 1s - loss: 0.6940 - accuracy: 0.51 - ETA: 1s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6917 - accuracy: 0.52 - ETA: 0s - loss: 0.6915 - accuracy: 0.52 - ETA: 0s - loss: 0.6912 - accuracy: 0.52 - ETA: 0s - loss: 0.6911 - accuracy: 0.52 - ETA: 0s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6909 - accuracy: 0.53 - ETA: 0s - loss: 0.6907 - accuracy: 0.53 - ETA: 0s - loss: 0.6904 - accuracy: 0.53 - ETA: 0s - loss: 0.6902 - accuracy: 0.53 - ETA: 0s - loss: 0.6898 - accuracy: 0.53 - 2s 138us/step - loss: 0.6895 - accuracy: 0.5414 - val_loss: 0.6784 - val_accuracy: 0.5568\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:27 - loss: 0.6969 - accuracy: 0.50 - ETA: 4s - loss: 0.6955 - accuracy: 0.5023 - ETA: 2s - loss: 0.6955 - accuracy: 0.50 - ETA: 2s - loss: 0.6953 - accuracy: 0.48 - ETA: 1s - loss: 0.6952 - accuracy: 0.49 - ETA: 1s - loss: 0.6948 - accuracy: 0.50 - ETA: 1s - loss: 0.6948 - accuracy: 0.50 - ETA: 1s - loss: 0.6948 - accuracy: 0.50 - ETA: 1s - loss: 0.6949 - accuracy: 0.50 - ETA: 1s - loss: 0.6950 - accuracy: 0.49 - ETA: 1s - loss: 0.6950 - accuracy: 0.50 - ETA: 1s - loss: 0.6948 - accuracy: 0.50 - ETA: 1s - loss: 0.6947 - accuracy: 0.50 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - 2s 137us/step - loss: 0.6927 - accuracy: 0.5246 - val_loss: 0.6875 - val_accuracy: 0.5511\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 2:24 - loss: 0.6927 - accuracy: 0.62 - ETA: 4s - loss: 0.6961 - accuracy: 0.4444 - ETA: 2s - loss: 0.6955 - accuracy: 0.45 - ETA: 2s - loss: 0.6953 - accuracy: 0.47 - ETA: 1s - loss: 0.6952 - accuracy: 0.48 - ETA: 1s - loss: 0.6952 - accuracy: 0.49 - ETA: 1s - loss: 0.6950 - accuracy: 0.49 - ETA: 1s - loss: 0.6949 - accuracy: 0.50 - ETA: 1s - loss: 0.6949 - accuracy: 0.50 - ETA: 1s - loss: 0.6949 - accuracy: 0.50 - ETA: 1s - loss: 0.6949 - accuracy: 0.50 - ETA: 1s - loss: 0.6950 - accuracy: 0.50 - ETA: 1s - loss: 0.6949 - accuracy: 0.50 - ETA: 0s - loss: 0.6949 - accuracy: 0.50 - ETA: 0s - loss: 0.6949 - accuracy: 0.50 - ETA: 0s - loss: 0.6948 - accuracy: 0.50 - ETA: 0s - loss: 0.6949 - accuracy: 0.49 - ETA: 0s - loss: 0.6948 - accuracy: 0.50 - ETA: 0s - loss: 0.6948 - accuracy: 0.50 - ETA: 0s - loss: 0.6948 - accuracy: 0.50 - ETA: 0s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.52 - ETA: 0s - loss: 0.6942 - accuracy: 0.52 - ETA: 0s - loss: 0.6941 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - 2s 134us/step - loss: 0.6938 - accuracy: 0.5249 - val_loss: 0.6910 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:03 - loss: 0.6900 - accuracy: 0.62 - ETA: 4s - loss: 0.6941 - accuracy: 0.4760 - ETA: 3s - loss: 0.6928 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.50 - ETA: 2s - loss: 0.6918 - accuracy: 0.50 - ETA: 1s - loss: 0.6915 - accuracy: 0.50 - ETA: 1s - loss: 0.6903 - accuracy: 0.51 - ETA: 1s - loss: 0.6895 - accuracy: 0.52 - ETA: 1s - loss: 0.6882 - accuracy: 0.52 - ETA: 1s - loss: 0.6875 - accuracy: 0.53 - ETA: 1s - loss: 0.6854 - accuracy: 0.53 - ETA: 1s - loss: 0.6843 - accuracy: 0.54 - ETA: 1s - loss: 0.6836 - accuracy: 0.55 - ETA: 1s - loss: 0.6823 - accuracy: 0.55 - ETA: 0s - loss: 0.6802 - accuracy: 0.55 - ETA: 0s - loss: 0.6782 - accuracy: 0.56 - ETA: 0s - loss: 0.6769 - accuracy: 0.56 - ETA: 0s - loss: 0.6759 - accuracy: 0.57 - ETA: 0s - loss: 0.6740 - accuracy: 0.57 - ETA: 0s - loss: 0.6726 - accuracy: 0.57 - ETA: 0s - loss: 0.6713 - accuracy: 0.57 - ETA: 0s - loss: 0.6689 - accuracy: 0.58 - ETA: 0s - loss: 0.6670 - accuracy: 0.58 - ETA: 0s - loss: 0.6658 - accuracy: 0.58 - ETA: 0s - loss: 0.6645 - accuracy: 0.59 - ETA: 0s - loss: 0.6641 - accuracy: 0.59 - ETA: 0s - loss: 0.6635 - accuracy: 0.59 - ETA: 0s - loss: 0.6626 - accuracy: 0.59 - ETA: 0s - loss: 0.6619 - accuracy: 0.59 - ETA: 0s - loss: 0.6610 - accuracy: 0.60 - ETA: 0s - loss: 0.6608 - accuracy: 0.60 - 2s 140us/step - loss: 0.6606 - accuracy: 0.6031 - val_loss: 0.6158 - val_accuracy: 0.7173\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:16 - loss: 0.6918 - accuracy: 0.87 - ETA: 5s - loss: 0.6937 - accuracy: 0.5230 - ETA: 3s - loss: 0.6934 - accuracy: 0.52 - ETA: 2s - loss: 0.6929 - accuracy: 0.53 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 2s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6921 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6911 - accuracy: 0.53 - ETA: 1s - loss: 0.6903 - accuracy: 0.53 - ETA: 1s - loss: 0.6896 - accuracy: 0.53 - ETA: 1s - loss: 0.6887 - accuracy: 0.54 - ETA: 1s - loss: 0.6875 - accuracy: 0.54 - ETA: 1s - loss: 0.6868 - accuracy: 0.55 - ETA: 1s - loss: 0.6858 - accuracy: 0.55 - ETA: 0s - loss: 0.6842 - accuracy: 0.55 - ETA: 0s - loss: 0.6829 - accuracy: 0.56 - ETA: 0s - loss: 0.6819 - accuracy: 0.56 - ETA: 0s - loss: 0.6812 - accuracy: 0.56 - ETA: 0s - loss: 0.6797 - accuracy: 0.56 - ETA: 0s - loss: 0.6786 - accuracy: 0.56 - ETA: 0s - loss: 0.6773 - accuracy: 0.56 - ETA: 0s - loss: 0.6762 - accuracy: 0.57 - ETA: 0s - loss: 0.6752 - accuracy: 0.57 - ETA: 0s - loss: 0.6742 - accuracy: 0.57 - ETA: 0s - loss: 0.6733 - accuracy: 0.57 - ETA: 0s - loss: 0.6725 - accuracy: 0.57 - ETA: 0s - loss: 0.6713 - accuracy: 0.57 - ETA: 0s - loss: 0.6705 - accuracy: 0.57 - ETA: 0s - loss: 0.6698 - accuracy: 0.57 - ETA: 0s - loss: 0.6690 - accuracy: 0.57 - 2s 145us/step - loss: 0.6691 - accuracy: 0.5771 - val_loss: 0.6312 - val_accuracy: 0.7081\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:03 - loss: 0.6923 - accuracy: 0.62 - ETA: 4s - loss: 0.6935 - accuracy: 0.4907 - ETA: 3s - loss: 0.6934 - accuracy: 0.49 - ETA: 2s - loss: 0.6931 - accuracy: 0.49 - ETA: 2s - loss: 0.6922 - accuracy: 0.51 - ETA: 1s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.52 - ETA: 1s - loss: 0.6902 - accuracy: 0.53 - ETA: 1s - loss: 0.6895 - accuracy: 0.53 - ETA: 1s - loss: 0.6885 - accuracy: 0.53 - ETA: 1s - loss: 0.6874 - accuracy: 0.54 - ETA: 1s - loss: 0.6859 - accuracy: 0.55 - ETA: 1s - loss: 0.6847 - accuracy: 0.55 - ETA: 1s - loss: 0.6836 - accuracy: 0.55 - ETA: 1s - loss: 0.6818 - accuracy: 0.56 - ETA: 0s - loss: 0.6806 - accuracy: 0.56 - ETA: 0s - loss: 0.6798 - accuracy: 0.56 - ETA: 0s - loss: 0.6786 - accuracy: 0.56 - ETA: 0s - loss: 0.6769 - accuracy: 0.56 - ETA: 0s - loss: 0.6758 - accuracy: 0.56 - ETA: 0s - loss: 0.6740 - accuracy: 0.57 - ETA: 0s - loss: 0.6713 - accuracy: 0.57 - ETA: 0s - loss: 0.6716 - accuracy: 0.57 - ETA: 0s - loss: 0.6702 - accuracy: 0.57 - ETA: 0s - loss: 0.6688 - accuracy: 0.58 - ETA: 0s - loss: 0.6669 - accuracy: 0.58 - ETA: 0s - loss: 0.6654 - accuracy: 0.58 - ETA: 0s - loss: 0.6648 - accuracy: 0.58 - ETA: 0s - loss: 0.6635 - accuracy: 0.58 - ETA: 0s - loss: 0.6621 - accuracy: 0.59 - ETA: 0s - loss: 0.6615 - accuracy: 0.59 - ETA: 0s - loss: 0.6602 - accuracy: 0.59 - 2s 143us/step - loss: 0.6600 - accuracy: 0.5964 - val_loss: 0.6033 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 60us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:03 - loss: 0.6948 - accuracy: 0.50 - ETA: 5s - loss: 0.6938 - accuracy: 0.4850 - ETA: 3s - loss: 0.6933 - accuracy: 0.50 - ETA: 2s - loss: 0.6928 - accuracy: 0.53 - ETA: 2s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6915 - accuracy: 0.54 - ETA: 1s - loss: 0.6905 - accuracy: 0.54 - ETA: 1s - loss: 0.6891 - accuracy: 0.55 - ETA: 1s - loss: 0.6880 - accuracy: 0.55 - ETA: 1s - loss: 0.6877 - accuracy: 0.55 - ETA: 1s - loss: 0.6866 - accuracy: 0.56 - ETA: 1s - loss: 0.6853 - accuracy: 0.56 - ETA: 1s - loss: 0.6846 - accuracy: 0.56 - ETA: 1s - loss: 0.6827 - accuracy: 0.56 - ETA: 1s - loss: 0.6814 - accuracy: 0.57 - ETA: 0s - loss: 0.6798 - accuracy: 0.57 - ETA: 0s - loss: 0.6787 - accuracy: 0.57 - ETA: 0s - loss: 0.6772 - accuracy: 0.58 - ETA: 0s - loss: 0.6753 - accuracy: 0.58 - ETA: 0s - loss: 0.6744 - accuracy: 0.58 - ETA: 0s - loss: 0.6727 - accuracy: 0.58 - ETA: 0s - loss: 0.6711 - accuracy: 0.58 - ETA: 0s - loss: 0.6696 - accuracy: 0.59 - ETA: 0s - loss: 0.6674 - accuracy: 0.59 - ETA: 0s - loss: 0.6659 - accuracy: 0.59 - ETA: 0s - loss: 0.6642 - accuracy: 0.59 - ETA: 0s - loss: 0.6634 - accuracy: 0.60 - ETA: 0s - loss: 0.6620 - accuracy: 0.60 - ETA: 0s - loss: 0.6607 - accuracy: 0.60 - ETA: 0s - loss: 0.6589 - accuracy: 0.60 - ETA: 0s - loss: 0.6570 - accuracy: 0.60 - ETA: 0s - loss: 0.6563 - accuracy: 0.60 - 2s 143us/step - loss: 0.6563 - accuracy: 0.6096 - val_loss: 0.6031 - val_accuracy: 0.7180\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 54us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:02 - loss: 0.6938 - accuracy: 0.37 - ETA: 4s - loss: 0.6930 - accuracy: 0.5324 - ETA: 2s - loss: 0.6928 - accuracy: 0.52 - ETA: 2s - loss: 0.6926 - accuracy: 0.52 - ETA: 2s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6922 - accuracy: 0.52 - ETA: 1s - loss: 0.6915 - accuracy: 0.53 - ETA: 1s - loss: 0.6915 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.52 - ETA: 1s - loss: 0.6908 - accuracy: 0.52 - ETA: 1s - loss: 0.6902 - accuracy: 0.52 - ETA: 1s - loss: 0.6902 - accuracy: 0.52 - ETA: 1s - loss: 0.6903 - accuracy: 0.51 - ETA: 1s - loss: 0.6900 - accuracy: 0.51 - ETA: 0s - loss: 0.6894 - accuracy: 0.51 - ETA: 0s - loss: 0.6891 - accuracy: 0.51 - ETA: 0s - loss: 0.6889 - accuracy: 0.51 - ETA: 0s - loss: 0.6885 - accuracy: 0.51 - ETA: 0s - loss: 0.6882 - accuracy: 0.51 - ETA: 0s - loss: 0.6879 - accuracy: 0.51 - ETA: 0s - loss: 0.6874 - accuracy: 0.52 - ETA: 0s - loss: 0.6869 - accuracy: 0.52 - ETA: 0s - loss: 0.6867 - accuracy: 0.52 - ETA: 0s - loss: 0.6860 - accuracy: 0.52 - ETA: 0s - loss: 0.6857 - accuracy: 0.52 - ETA: 0s - loss: 0.6856 - accuracy: 0.52 - ETA: 0s - loss: 0.6851 - accuracy: 0.52 - ETA: 0s - loss: 0.6847 - accuracy: 0.52 - ETA: 0s - loss: 0.6839 - accuracy: 0.53 - ETA: 0s - loss: 0.6834 - accuracy: 0.53 - ETA: 0s - loss: 0.6832 - accuracy: 0.53 - ETA: 0s - loss: 0.6826 - accuracy: 0.53 - 2s 143us/step - loss: 0.6826 - accuracy: 0.5364 - val_loss: 0.6584 - val_accuracy: 0.6854\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:05 - loss: 0.6882 - accuracy: 0.87 - ETA: 5s - loss: 0.6930 - accuracy: 0.5025 - ETA: 3s - loss: 0.6929 - accuracy: 0.51 - ETA: 2s - loss: 0.6927 - accuracy: 0.51 - ETA: 2s - loss: 0.6924 - accuracy: 0.51 - ETA: 1s - loss: 0.6917 - accuracy: 0.52 - ETA: 1s - loss: 0.6907 - accuracy: 0.52 - ETA: 1s - loss: 0.6898 - accuracy: 0.53 - ETA: 1s - loss: 0.6890 - accuracy: 0.53 - ETA: 1s - loss: 0.6873 - accuracy: 0.54 - ETA: 1s - loss: 0.6867 - accuracy: 0.55 - ETA: 1s - loss: 0.6859 - accuracy: 0.55 - ETA: 1s - loss: 0.6845 - accuracy: 0.56 - ETA: 1s - loss: 0.6829 - accuracy: 0.56 - ETA: 1s - loss: 0.6820 - accuracy: 0.56 - ETA: 0s - loss: 0.6808 - accuracy: 0.56 - ETA: 0s - loss: 0.6799 - accuracy: 0.56 - ETA: 0s - loss: 0.6786 - accuracy: 0.57 - ETA: 0s - loss: 0.6775 - accuracy: 0.57 - ETA: 0s - loss: 0.6769 - accuracy: 0.57 - ETA: 0s - loss: 0.6746 - accuracy: 0.57 - ETA: 0s - loss: 0.6726 - accuracy: 0.58 - ETA: 0s - loss: 0.6717 - accuracy: 0.58 - ETA: 0s - loss: 0.6703 - accuracy: 0.58 - ETA: 0s - loss: 0.6689 - accuracy: 0.58 - ETA: 0s - loss: 0.6674 - accuracy: 0.58 - ETA: 0s - loss: 0.6657 - accuracy: 0.59 - ETA: 0s - loss: 0.6652 - accuracy: 0.59 - ETA: 0s - loss: 0.6644 - accuracy: 0.59 - ETA: 0s - loss: 0.6629 - accuracy: 0.59 - ETA: 0s - loss: 0.6623 - accuracy: 0.59 - ETA: 0s - loss: 0.6616 - accuracy: 0.59 - 2s 143us/step - loss: 0.6611 - accuracy: 0.5973 - val_loss: 0.6158 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 50us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:25 - loss: 0.6862 - accuracy: 0.87 - ETA: 5s - loss: 0.6947 - accuracy: 0.5051 - ETA: 3s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6918 - accuracy: 0.52 - ETA: 2s - loss: 0.6920 - accuracy: 0.51 - ETA: 2s - loss: 0.6907 - accuracy: 0.52 - ETA: 1s - loss: 0.6904 - accuracy: 0.52 - ETA: 1s - loss: 0.6893 - accuracy: 0.52 - ETA: 1s - loss: 0.6882 - accuracy: 0.53 - ETA: 1s - loss: 0.6870 - accuracy: 0.54 - ETA: 1s - loss: 0.6856 - accuracy: 0.54 - ETA: 1s - loss: 0.6837 - accuracy: 0.56 - ETA: 1s - loss: 0.6827 - accuracy: 0.56 - ETA: 1s - loss: 0.6804 - accuracy: 0.57 - ETA: 1s - loss: 0.6787 - accuracy: 0.57 - ETA: 0s - loss: 0.6773 - accuracy: 0.58 - ETA: 0s - loss: 0.6746 - accuracy: 0.58 - ETA: 0s - loss: 0.6730 - accuracy: 0.58 - ETA: 0s - loss: 0.6720 - accuracy: 0.59 - ETA: 0s - loss: 0.6709 - accuracy: 0.59 - ETA: 0s - loss: 0.6686 - accuracy: 0.59 - ETA: 0s - loss: 0.6669 - accuracy: 0.60 - ETA: 0s - loss: 0.6655 - accuracy: 0.60 - ETA: 0s - loss: 0.6639 - accuracy: 0.60 - ETA: 0s - loss: 0.6625 - accuracy: 0.60 - ETA: 0s - loss: 0.6599 - accuracy: 0.61 - ETA: 0s - loss: 0.6594 - accuracy: 0.61 - ETA: 0s - loss: 0.6572 - accuracy: 0.61 - ETA: 0s - loss: 0.6555 - accuracy: 0.62 - ETA: 0s - loss: 0.6547 - accuracy: 0.62 - ETA: 0s - loss: 0.6542 - accuracy: 0.62 - ETA: 0s - loss: 0.6525 - accuracy: 0.62 - 2s 146us/step - loss: 0.6516 - accuracy: 0.6285 - val_loss: 0.5968 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:05 - loss: 0.6970 - accuracy: 0.50 - ETA: 4s - loss: 0.6935 - accuracy: 0.5330 - ETA: 3s - loss: 0.6928 - accuracy: 0.53 - ETA: 2s - loss: 0.6913 - accuracy: 0.54 - ETA: 2s - loss: 0.6892 - accuracy: 0.54 - ETA: 1s - loss: 0.6873 - accuracy: 0.54 - ETA: 1s - loss: 0.6870 - accuracy: 0.54 - ETA: 1s - loss: 0.6870 - accuracy: 0.54 - ETA: 1s - loss: 0.6855 - accuracy: 0.55 - ETA: 1s - loss: 0.6831 - accuracy: 0.56 - ETA: 1s - loss: 0.6815 - accuracy: 0.57 - ETA: 1s - loss: 0.6808 - accuracy: 0.57 - ETA: 1s - loss: 0.6791 - accuracy: 0.57 - ETA: 1s - loss: 0.6771 - accuracy: 0.58 - ETA: 0s - loss: 0.6762 - accuracy: 0.58 - ETA: 0s - loss: 0.6748 - accuracy: 0.58 - ETA: 0s - loss: 0.6728 - accuracy: 0.59 - ETA: 0s - loss: 0.6702 - accuracy: 0.59 - ETA: 0s - loss: 0.6682 - accuracy: 0.60 - ETA: 0s - loss: 0.6661 - accuracy: 0.60 - ETA: 0s - loss: 0.6647 - accuracy: 0.60 - ETA: 0s - loss: 0.6633 - accuracy: 0.61 - ETA: 0s - loss: 0.6620 - accuracy: 0.61 - ETA: 0s - loss: 0.6603 - accuracy: 0.61 - ETA: 0s - loss: 0.6587 - accuracy: 0.61 - ETA: 0s - loss: 0.6571 - accuracy: 0.62 - ETA: 0s - loss: 0.6544 - accuracy: 0.62 - ETA: 0s - loss: 0.6536 - accuracy: 0.62 - ETA: 0s - loss: 0.6528 - accuracy: 0.62 - ETA: 0s - loss: 0.6519 - accuracy: 0.62 - ETA: 0s - loss: 0.6500 - accuracy: 0.63 - 2s 142us/step - loss: 0.6491 - accuracy: 0.6324 - val_loss: 0.5957 - val_accuracy: 0.7173\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:28 - loss: 0.6916 - accuracy: 0.62 - ETA: 5s - loss: 0.6940 - accuracy: 0.5080 - ETA: 3s - loss: 0.6937 - accuracy: 0.51 - ETA: 3s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6932 - accuracy: 0.52 - ETA: 2s - loss: 0.6923 - accuracy: 0.53 - ETA: 2s - loss: 0.6910 - accuracy: 0.55 - ETA: 1s - loss: 0.6894 - accuracy: 0.56 - ETA: 1s - loss: 0.6879 - accuracy: 0.56 - ETA: 1s - loss: 0.6851 - accuracy: 0.57 - ETA: 1s - loss: 0.6835 - accuracy: 0.57 - ETA: 1s - loss: 0.6816 - accuracy: 0.57 - ETA: 1s - loss: 0.6796 - accuracy: 0.58 - ETA: 1s - loss: 0.6774 - accuracy: 0.58 - ETA: 1s - loss: 0.6745 - accuracy: 0.59 - ETA: 0s - loss: 0.6730 - accuracy: 0.59 - ETA: 0s - loss: 0.6702 - accuracy: 0.59 - ETA: 0s - loss: 0.6678 - accuracy: 0.60 - ETA: 0s - loss: 0.6660 - accuracy: 0.60 - ETA: 0s - loss: 0.6646 - accuracy: 0.60 - ETA: 0s - loss: 0.6638 - accuracy: 0.60 - ETA: 0s - loss: 0.6627 - accuracy: 0.61 - ETA: 0s - loss: 0.6613 - accuracy: 0.61 - ETA: 0s - loss: 0.6594 - accuracy: 0.61 - ETA: 0s - loss: 0.6585 - accuracy: 0.61 - ETA: 0s - loss: 0.6564 - accuracy: 0.61 - ETA: 0s - loss: 0.6565 - accuracy: 0.62 - ETA: 0s - loss: 0.6545 - accuracy: 0.62 - ETA: 0s - loss: 0.6536 - accuracy: 0.62 - ETA: 0s - loss: 0.6528 - accuracy: 0.62 - ETA: 0s - loss: 0.6510 - accuracy: 0.62 - ETA: 0s - loss: 0.6496 - accuracy: 0.63 - 2s 144us/step - loss: 0.6492 - accuracy: 0.6321 - val_loss: 0.5914 - val_accuracy: 0.7230\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 3:11 - loss: 0.6940 - accuracy: 0.50 - ETA: 5s - loss: 0.6937 - accuracy: 0.5130 - ETA: 3s - loss: 0.6933 - accuracy: 0.53 - ETA: 2s - loss: 0.6930 - accuracy: 0.53 - ETA: 2s - loss: 0.6927 - accuracy: 0.54 - ETA: 2s - loss: 0.6920 - accuracy: 0.54 - ETA: 1s - loss: 0.6906 - accuracy: 0.55 - ETA: 1s - loss: 0.6895 - accuracy: 0.56 - ETA: 1s - loss: 0.6876 - accuracy: 0.57 - ETA: 1s - loss: 0.6857 - accuracy: 0.57 - ETA: 1s - loss: 0.6839 - accuracy: 0.57 - ETA: 1s - loss: 0.6813 - accuracy: 0.58 - ETA: 1s - loss: 0.6800 - accuracy: 0.58 - ETA: 1s - loss: 0.6792 - accuracy: 0.58 - ETA: 1s - loss: 0.6780 - accuracy: 0.58 - ETA: 0s - loss: 0.6761 - accuracy: 0.59 - ETA: 0s - loss: 0.6749 - accuracy: 0.59 - ETA: 0s - loss: 0.6721 - accuracy: 0.60 - ETA: 0s - loss: 0.6709 - accuracy: 0.60 - ETA: 0s - loss: 0.6692 - accuracy: 0.60 - ETA: 0s - loss: 0.6682 - accuracy: 0.60 - ETA: 0s - loss: 0.6669 - accuracy: 0.60 - ETA: 0s - loss: 0.6658 - accuracy: 0.61 - ETA: 0s - loss: 0.6649 - accuracy: 0.61 - ETA: 0s - loss: 0.6638 - accuracy: 0.61 - ETA: 0s - loss: 0.6627 - accuracy: 0.61 - ETA: 0s - loss: 0.6610 - accuracy: 0.62 - ETA: 0s - loss: 0.6592 - accuracy: 0.62 - ETA: 0s - loss: 0.6574 - accuracy: 0.62 - ETA: 0s - loss: 0.6554 - accuracy: 0.62 - ETA: 0s - loss: 0.6537 - accuracy: 0.63 - ETA: 0s - loss: 0.6534 - accuracy: 0.63 - 2s 145us/step - loss: 0.6529 - accuracy: 0.6340 - val_loss: 0.6027 - val_accuracy: 0.7145\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:06 - loss: 0.6939 - accuracy: 0.50 - ETA: 4s - loss: 0.6935 - accuracy: 0.5168 - ETA: 3s - loss: 0.6927 - accuracy: 0.52 - ETA: 2s - loss: 0.6925 - accuracy: 0.52 - ETA: 2s - loss: 0.6915 - accuracy: 0.53 - ETA: 1s - loss: 0.6905 - accuracy: 0.54 - ETA: 1s - loss: 0.6889 - accuracy: 0.55 - ETA: 1s - loss: 0.6882 - accuracy: 0.55 - ETA: 1s - loss: 0.6868 - accuracy: 0.56 - ETA: 1s - loss: 0.6847 - accuracy: 0.56 - ETA: 1s - loss: 0.6834 - accuracy: 0.57 - ETA: 1s - loss: 0.6813 - accuracy: 0.58 - ETA: 1s - loss: 0.6795 - accuracy: 0.58 - ETA: 1s - loss: 0.6777 - accuracy: 0.58 - ETA: 0s - loss: 0.6755 - accuracy: 0.59 - ETA: 0s - loss: 0.6741 - accuracy: 0.59 - ETA: 0s - loss: 0.6735 - accuracy: 0.59 - ETA: 0s - loss: 0.6721 - accuracy: 0.59 - ETA: 0s - loss: 0.6700 - accuracy: 0.60 - ETA: 0s - loss: 0.6682 - accuracy: 0.60 - ETA: 0s - loss: 0.6670 - accuracy: 0.60 - ETA: 0s - loss: 0.6643 - accuracy: 0.60 - ETA: 0s - loss: 0.6628 - accuracy: 0.61 - ETA: 0s - loss: 0.6618 - accuracy: 0.61 - ETA: 0s - loss: 0.6606 - accuracy: 0.61 - ETA: 0s - loss: 0.6593 - accuracy: 0.61 - ETA: 0s - loss: 0.6575 - accuracy: 0.62 - ETA: 0s - loss: 0.6560 - accuracy: 0.62 - ETA: 0s - loss: 0.6544 - accuracy: 0.62 - ETA: 0s - loss: 0.6533 - accuracy: 0.62 - ETA: 0s - loss: 0.6522 - accuracy: 0.62 - ETA: 0s - loss: 0.6516 - accuracy: 0.62 - 2s 143us/step - loss: 0.6513 - accuracy: 0.6269 - val_loss: 0.6008 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:06 - loss: 0.6914 - accuracy: 0.87 - ETA: 5s - loss: 0.6937 - accuracy: 0.5221 - ETA: 3s - loss: 0.6922 - accuracy: 0.52 - ETA: 2s - loss: 0.6917 - accuracy: 0.52 - ETA: 2s - loss: 0.6911 - accuracy: 0.52 - ETA: 2s - loss: 0.6912 - accuracy: 0.52 - ETA: 2s - loss: 0.6906 - accuracy: 0.52 - ETA: 2s - loss: 0.6893 - accuracy: 0.53 - ETA: 1s - loss: 0.6886 - accuracy: 0.53 - ETA: 1s - loss: 0.6876 - accuracy: 0.54 - ETA: 1s - loss: 0.6865 - accuracy: 0.54 - ETA: 1s - loss: 0.6842 - accuracy: 0.55 - ETA: 1s - loss: 0.6828 - accuracy: 0.55 - ETA: 1s - loss: 0.6813 - accuracy: 0.56 - ETA: 1s - loss: 0.6791 - accuracy: 0.56 - ETA: 1s - loss: 0.6772 - accuracy: 0.57 - ETA: 1s - loss: 0.6760 - accuracy: 0.57 - ETA: 1s - loss: 0.6745 - accuracy: 0.57 - ETA: 1s - loss: 0.6737 - accuracy: 0.58 - ETA: 0s - loss: 0.6727 - accuracy: 0.58 - ETA: 0s - loss: 0.6716 - accuracy: 0.58 - ETA: 0s - loss: 0.6716 - accuracy: 0.58 - ETA: 0s - loss: 0.6699 - accuracy: 0.59 - ETA: 0s - loss: 0.6685 - accuracy: 0.59 - ETA: 0s - loss: 0.6668 - accuracy: 0.59 - ETA: 0s - loss: 0.6655 - accuracy: 0.59 - ETA: 0s - loss: 0.6641 - accuracy: 0.60 - ETA: 0s - loss: 0.6624 - accuracy: 0.60 - ETA: 0s - loss: 0.6614 - accuracy: 0.60 - ETA: 0s - loss: 0.6607 - accuracy: 0.60 - ETA: 0s - loss: 0.6588 - accuracy: 0.61 - ETA: 0s - loss: 0.6580 - accuracy: 0.61 - ETA: 0s - loss: 0.6566 - accuracy: 0.61 - ETA: 0s - loss: 0.6560 - accuracy: 0.61 - ETA: 0s - loss: 0.6549 - accuracy: 0.62 - 2s 157us/step - loss: 0.6543 - accuracy: 0.6234 - val_loss: 0.6111 - val_accuracy: 0.7152\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 58us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:08 - loss: 0.6934 - accuracy: 0.37 - ETA: 5s - loss: 0.6943 - accuracy: 0.4975 - ETA: 3s - loss: 0.6934 - accuracy: 0.51 - ETA: 2s - loss: 0.6890 - accuracy: 0.53 - ETA: 2s - loss: 0.6906 - accuracy: 0.53 - ETA: 2s - loss: 0.6907 - accuracy: 0.53 - ETA: 1s - loss: 0.6895 - accuracy: 0.54 - ETA: 1s - loss: 0.6884 - accuracy: 0.55 - ETA: 1s - loss: 0.6863 - accuracy: 0.56 - ETA: 1s - loss: 0.6836 - accuracy: 0.56 - ETA: 1s - loss: 0.6832 - accuracy: 0.56 - ETA: 1s - loss: 0.6815 - accuracy: 0.57 - ETA: 1s - loss: 0.6793 - accuracy: 0.58 - ETA: 1s - loss: 0.6766 - accuracy: 0.58 - ETA: 1s - loss: 0.6749 - accuracy: 0.59 - ETA: 1s - loss: 0.6739 - accuracy: 0.59 - ETA: 0s - loss: 0.6714 - accuracy: 0.59 - ETA: 0s - loss: 0.6692 - accuracy: 0.60 - ETA: 0s - loss: 0.6671 - accuracy: 0.60 - ETA: 0s - loss: 0.6657 - accuracy: 0.61 - ETA: 0s - loss: 0.6642 - accuracy: 0.61 - ETA: 0s - loss: 0.6610 - accuracy: 0.62 - ETA: 0s - loss: 0.6577 - accuracy: 0.62 - ETA: 0s - loss: 0.6555 - accuracy: 0.62 - ETA: 0s - loss: 0.6530 - accuracy: 0.63 - ETA: 0s - loss: 0.6525 - accuracy: 0.63 - ETA: 0s - loss: 0.6510 - accuracy: 0.63 - ETA: 0s - loss: 0.6498 - accuracy: 0.63 - ETA: 0s - loss: 0.6490 - accuracy: 0.63 - ETA: 0s - loss: 0.6467 - accuracy: 0.64 - ETA: 0s - loss: 0.6454 - accuracy: 0.64 - ETA: 0s - loss: 0.6453 - accuracy: 0.64 - ETA: 0s - loss: 0.6436 - accuracy: 0.64 - 2s 148us/step - loss: 0.6432 - accuracy: 0.6477 - val_loss: 0.5914 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:09 - loss: 0.6922 - accuracy: 0.50 - ETA: 5s - loss: 0.6940 - accuracy: 0.5000 - ETA: 3s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6917 - accuracy: 0.55 - ETA: 2s - loss: 0.6902 - accuracy: 0.55 - ETA: 1s - loss: 0.6883 - accuracy: 0.56 - ETA: 1s - loss: 0.6863 - accuracy: 0.57 - ETA: 1s - loss: 0.6834 - accuracy: 0.58 - ETA: 1s - loss: 0.6827 - accuracy: 0.58 - ETA: 1s - loss: 0.6794 - accuracy: 0.59 - ETA: 1s - loss: 0.6760 - accuracy: 0.60 - ETA: 1s - loss: 0.6740 - accuracy: 0.60 - ETA: 1s - loss: 0.6712 - accuracy: 0.60 - ETA: 1s - loss: 0.6710 - accuracy: 0.60 - ETA: 1s - loss: 0.6687 - accuracy: 0.61 - ETA: 0s - loss: 0.6671 - accuracy: 0.61 - ETA: 0s - loss: 0.6651 - accuracy: 0.61 - ETA: 0s - loss: 0.6627 - accuracy: 0.62 - ETA: 0s - loss: 0.6611 - accuracy: 0.62 - ETA: 0s - loss: 0.6578 - accuracy: 0.63 - ETA: 0s - loss: 0.6564 - accuracy: 0.63 - ETA: 0s - loss: 0.6540 - accuracy: 0.63 - ETA: 0s - loss: 0.6528 - accuracy: 0.63 - ETA: 0s - loss: 0.6509 - accuracy: 0.63 - ETA: 0s - loss: 0.6498 - accuracy: 0.64 - ETA: 0s - loss: 0.6485 - accuracy: 0.64 - ETA: 0s - loss: 0.6481 - accuracy: 0.64 - ETA: 0s - loss: 0.6461 - accuracy: 0.64 - ETA: 0s - loss: 0.6455 - accuracy: 0.64 - ETA: 0s - loss: 0.6433 - accuracy: 0.65 - ETA: 0s - loss: 0.6414 - accuracy: 0.65 - ETA: 0s - loss: 0.6404 - accuracy: 0.65 - 2s 145us/step - loss: 0.6396 - accuracy: 0.6551 - val_loss: 0.5837 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:08 - loss: 0.6994 - accuracy: 0.37 - ETA: 5s - loss: 0.6950 - accuracy: 0.4755 - ETA: 3s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6923 - accuracy: 0.53 - ETA: 2s - loss: 0.6910 - accuracy: 0.55 - ETA: 2s - loss: 0.6902 - accuracy: 0.55 - ETA: 1s - loss: 0.6882 - accuracy: 0.56 - ETA: 1s - loss: 0.6872 - accuracy: 0.56 - ETA: 1s - loss: 0.6852 - accuracy: 0.57 - ETA: 1s - loss: 0.6832 - accuracy: 0.58 - ETA: 1s - loss: 0.6806 - accuracy: 0.59 - ETA: 1s - loss: 0.6792 - accuracy: 0.59 - ETA: 1s - loss: 0.6793 - accuracy: 0.59 - ETA: 1s - loss: 0.6775 - accuracy: 0.59 - ETA: 1s - loss: 0.6758 - accuracy: 0.59 - ETA: 1s - loss: 0.6729 - accuracy: 0.60 - ETA: 1s - loss: 0.6693 - accuracy: 0.61 - ETA: 1s - loss: 0.6674 - accuracy: 0.61 - ETA: 1s - loss: 0.6651 - accuracy: 0.61 - ETA: 0s - loss: 0.6630 - accuracy: 0.61 - ETA: 0s - loss: 0.6613 - accuracy: 0.61 - ETA: 0s - loss: 0.6599 - accuracy: 0.62 - ETA: 0s - loss: 0.6573 - accuracy: 0.62 - ETA: 0s - loss: 0.6557 - accuracy: 0.62 - ETA: 0s - loss: 0.6543 - accuracy: 0.62 - ETA: 0s - loss: 0.6538 - accuracy: 0.63 - ETA: 0s - loss: 0.6533 - accuracy: 0.63 - ETA: 0s - loss: 0.6519 - accuracy: 0.63 - ETA: 0s - loss: 0.6502 - accuracy: 0.63 - ETA: 0s - loss: 0.6497 - accuracy: 0.63 - ETA: 0s - loss: 0.6489 - accuracy: 0.63 - ETA: 0s - loss: 0.6478 - accuracy: 0.63 - ETA: 0s - loss: 0.6464 - accuracy: 0.64 - ETA: 0s - loss: 0.6450 - accuracy: 0.64 - ETA: 0s - loss: 0.6437 - accuracy: 0.64 - ETA: 0s - loss: 0.6425 - accuracy: 0.64 - 2s 159us/step - loss: 0.6425 - accuracy: 0.6467 - val_loss: 0.5892 - val_accuracy: 0.7230\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:03 - loss: 0.6997 - accuracy: 0.25 - ETA: 5s - loss: 0.6941 - accuracy: 0.5435 - ETA: 3s - loss: 0.6924 - accuracy: 0.56 - ETA: 2s - loss: 0.6916 - accuracy: 0.56 - ETA: 2s - loss: 0.6897 - accuracy: 0.58 - ETA: 1s - loss: 0.6885 - accuracy: 0.58 - ETA: 1s - loss: 0.6856 - accuracy: 0.59 - ETA: 1s - loss: 0.6835 - accuracy: 0.59 - ETA: 1s - loss: 0.6813 - accuracy: 0.60 - ETA: 1s - loss: 0.6783 - accuracy: 0.60 - ETA: 1s - loss: 0.6768 - accuracy: 0.61 - ETA: 1s - loss: 0.6747 - accuracy: 0.61 - ETA: 1s - loss: 0.6731 - accuracy: 0.61 - ETA: 1s - loss: 0.6706 - accuracy: 0.61 - ETA: 1s - loss: 0.6693 - accuracy: 0.61 - ETA: 1s - loss: 0.6681 - accuracy: 0.62 - ETA: 1s - loss: 0.6664 - accuracy: 0.62 - ETA: 0s - loss: 0.6645 - accuracy: 0.62 - ETA: 0s - loss: 0.6629 - accuracy: 0.62 - ETA: 0s - loss: 0.6613 - accuracy: 0.62 - ETA: 0s - loss: 0.6593 - accuracy: 0.62 - ETA: 0s - loss: 0.6579 - accuracy: 0.63 - ETA: 0s - loss: 0.6571 - accuracy: 0.63 - ETA: 0s - loss: 0.6552 - accuracy: 0.63 - ETA: 0s - loss: 0.6534 - accuracy: 0.63 - ETA: 0s - loss: 0.6513 - accuracy: 0.64 - ETA: 0s - loss: 0.6497 - accuracy: 0.64 - ETA: 0s - loss: 0.6488 - accuracy: 0.64 - ETA: 0s - loss: 0.6466 - accuracy: 0.64 - ETA: 0s - loss: 0.6452 - accuracy: 0.64 - ETA: 0s - loss: 0.6435 - accuracy: 0.65 - ETA: 0s - loss: 0.6408 - accuracy: 0.65 - ETA: 0s - loss: 0.6405 - accuracy: 0.65 - 2s 148us/step - loss: 0.6397 - accuracy: 0.6545 - val_loss: 0.5849 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:08 - loss: 0.6900 - accuracy: 0.62 - ETA: 5s - loss: 0.6943 - accuracy: 0.4800 - ETA: 3s - loss: 0.6934 - accuracy: 0.49 - ETA: 2s - loss: 0.6920 - accuracy: 0.52 - ETA: 2s - loss: 0.6915 - accuracy: 0.53 - ETA: 2s - loss: 0.6913 - accuracy: 0.53 - ETA: 1s - loss: 0.6899 - accuracy: 0.54 - ETA: 1s - loss: 0.6874 - accuracy: 0.56 - ETA: 1s - loss: 0.6855 - accuracy: 0.57 - ETA: 1s - loss: 0.6840 - accuracy: 0.57 - ETA: 1s - loss: 0.6806 - accuracy: 0.58 - ETA: 1s - loss: 0.6787 - accuracy: 0.59 - ETA: 1s - loss: 0.6760 - accuracy: 0.59 - ETA: 1s - loss: 0.6730 - accuracy: 0.60 - ETA: 1s - loss: 0.6726 - accuracy: 0.60 - ETA: 1s - loss: 0.6699 - accuracy: 0.60 - ETA: 0s - loss: 0.6688 - accuracy: 0.60 - ETA: 0s - loss: 0.6669 - accuracy: 0.61 - ETA: 0s - loss: 0.6641 - accuracy: 0.61 - ETA: 0s - loss: 0.6619 - accuracy: 0.61 - ETA: 0s - loss: 0.6604 - accuracy: 0.62 - ETA: 0s - loss: 0.6598 - accuracy: 0.62 - ETA: 0s - loss: 0.6581 - accuracy: 0.62 - ETA: 0s - loss: 0.6570 - accuracy: 0.62 - ETA: 0s - loss: 0.6552 - accuracy: 0.62 - ETA: 0s - loss: 0.6532 - accuracy: 0.63 - ETA: 0s - loss: 0.6519 - accuracy: 0.63 - ETA: 0s - loss: 0.6499 - accuracy: 0.63 - ETA: 0s - loss: 0.6480 - accuracy: 0.63 - ETA: 0s - loss: 0.6472 - accuracy: 0.63 - ETA: 0s - loss: 0.6455 - accuracy: 0.63 - ETA: 0s - loss: 0.6441 - accuracy: 0.64 - ETA: 0s - loss: 0.6428 - accuracy: 0.64 - 2s 149us/step - loss: 0.6424 - accuracy: 0.6433 - val_loss: 0.5909 - val_accuracy: 0.7180\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:06 - loss: 0.6904 - accuracy: 0.62 - ETA: 5s - loss: 0.6954 - accuracy: 0.4818 - ETA: 3s - loss: 0.6945 - accuracy: 0.50 - ETA: 2s - loss: 0.6939 - accuracy: 0.50 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 2s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6899 - accuracy: 0.53 - ETA: 1s - loss: 0.6887 - accuracy: 0.53 - ETA: 1s - loss: 0.6869 - accuracy: 0.54 - ETA: 1s - loss: 0.6852 - accuracy: 0.55 - ETA: 1s - loss: 0.6826 - accuracy: 0.56 - ETA: 1s - loss: 0.6805 - accuracy: 0.57 - ETA: 1s - loss: 0.6796 - accuracy: 0.57 - ETA: 1s - loss: 0.6773 - accuracy: 0.58 - ETA: 1s - loss: 0.6739 - accuracy: 0.58 - ETA: 1s - loss: 0.6711 - accuracy: 0.59 - ETA: 1s - loss: 0.6700 - accuracy: 0.59 - ETA: 0s - loss: 0.6686 - accuracy: 0.59 - ETA: 0s - loss: 0.6656 - accuracy: 0.60 - ETA: 0s - loss: 0.6644 - accuracy: 0.60 - ETA: 0s - loss: 0.6626 - accuracy: 0.60 - ETA: 0s - loss: 0.6611 - accuracy: 0.60 - ETA: 0s - loss: 0.6582 - accuracy: 0.61 - ETA: 0s - loss: 0.6571 - accuracy: 0.61 - ETA: 0s - loss: 0.6554 - accuracy: 0.62 - ETA: 0s - loss: 0.6539 - accuracy: 0.62 - ETA: 0s - loss: 0.6520 - accuracy: 0.62 - ETA: 0s - loss: 0.6507 - accuracy: 0.62 - ETA: 0s - loss: 0.6485 - accuracy: 0.63 - ETA: 0s - loss: 0.6476 - accuracy: 0.63 - ETA: 0s - loss: 0.6459 - accuracy: 0.63 - ETA: 0s - loss: 0.6462 - accuracy: 0.63 - ETA: 0s - loss: 0.6449 - accuracy: 0.63 - ETA: 0s - loss: 0.6438 - accuracy: 0.63 - 2s 151us/step - loss: 0.6440 - accuracy: 0.6389 - val_loss: 0.5962 - val_accuracy: 0.7266\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 57us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 2:36 - loss: 0.6938 - accuracy: 0.37 - ETA: 4s - loss: 0.6942 - accuracy: 0.4784 - ETA: 2s - loss: 0.6918 - accuracy: 0.50 - ETA: 2s - loss: 0.6913 - accuracy: 0.51 - ETA: 2s - loss: 0.6909 - accuracy: 0.52 - ETA: 1s - loss: 0.6901 - accuracy: 0.52 - ETA: 1s - loss: 0.6898 - accuracy: 0.53 - ETA: 1s - loss: 0.6891 - accuracy: 0.53 - ETA: 1s - loss: 0.6892 - accuracy: 0.53 - ETA: 1s - loss: 0.6884 - accuracy: 0.54 - ETA: 1s - loss: 0.6875 - accuracy: 0.54 - ETA: 1s - loss: 0.6874 - accuracy: 0.54 - ETA: 1s - loss: 0.6868 - accuracy: 0.54 - ETA: 1s - loss: 0.6861 - accuracy: 0.55 - ETA: 1s - loss: 0.6855 - accuracy: 0.55 - ETA: 1s - loss: 0.6846 - accuracy: 0.55 - ETA: 1s - loss: 0.6842 - accuracy: 0.55 - ETA: 1s - loss: 0.6833 - accuracy: 0.55 - ETA: 1s - loss: 0.6830 - accuracy: 0.55 - ETA: 0s - loss: 0.6825 - accuracy: 0.55 - ETA: 0s - loss: 0.6817 - accuracy: 0.56 - ETA: 0s - loss: 0.6808 - accuracy: 0.56 - ETA: 0s - loss: 0.6809 - accuracy: 0.56 - ETA: 0s - loss: 0.6801 - accuracy: 0.57 - ETA: 0s - loss: 0.6797 - accuracy: 0.57 - ETA: 0s - loss: 0.6793 - accuracy: 0.57 - ETA: 0s - loss: 0.6790 - accuracy: 0.57 - ETA: 0s - loss: 0.6782 - accuracy: 0.57 - ETA: 0s - loss: 0.6781 - accuracy: 0.58 - ETA: 0s - loss: 0.6780 - accuracy: 0.58 - ETA: 0s - loss: 0.6773 - accuracy: 0.58 - ETA: 0s - loss: 0.6769 - accuracy: 0.58 - ETA: 0s - loss: 0.6763 - accuracy: 0.58 - ETA: 0s - loss: 0.6758 - accuracy: 0.58 - 2s 151us/step - loss: 0.6756 - accuracy: 0.5913 - val_loss: 0.6551 - val_accuracy: 0.6548\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 59us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:54 - loss: 0.6947 - accuracy: 0.62 - ETA: 4s - loss: 0.6914 - accuracy: 0.5472 - ETA: 3s - loss: 0.6918 - accuracy: 0.54 - ETA: 2s - loss: 0.6907 - accuracy: 0.53 - ETA: 2s - loss: 0.6904 - accuracy: 0.54 - ETA: 1s - loss: 0.6895 - accuracy: 0.54 - ETA: 1s - loss: 0.6886 - accuracy: 0.55 - ETA: 1s - loss: 0.6881 - accuracy: 0.55 - ETA: 1s - loss: 0.6874 - accuracy: 0.56 - ETA: 1s - loss: 0.6864 - accuracy: 0.56 - ETA: 1s - loss: 0.6858 - accuracy: 0.56 - ETA: 1s - loss: 0.6853 - accuracy: 0.57 - ETA: 1s - loss: 0.6844 - accuracy: 0.57 - ETA: 0s - loss: 0.6841 - accuracy: 0.57 - ETA: 0s - loss: 0.6841 - accuracy: 0.57 - ETA: 0s - loss: 0.6835 - accuracy: 0.58 - ETA: 0s - loss: 0.6829 - accuracy: 0.58 - ETA: 0s - loss: 0.6823 - accuracy: 0.58 - ETA: 0s - loss: 0.6815 - accuracy: 0.58 - ETA: 0s - loss: 0.6806 - accuracy: 0.59 - ETA: 0s - loss: 0.6801 - accuracy: 0.59 - ETA: 0s - loss: 0.6802 - accuracy: 0.58 - ETA: 0s - loss: 0.6797 - accuracy: 0.58 - ETA: 0s - loss: 0.6792 - accuracy: 0.59 - ETA: 0s - loss: 0.6787 - accuracy: 0.59 - ETA: 0s - loss: 0.6785 - accuracy: 0.59 - ETA: 0s - loss: 0.6778 - accuracy: 0.59 - ETA: 0s - loss: 0.6774 - accuracy: 0.59 - ETA: 0s - loss: 0.6769 - accuracy: 0.59 - ETA: 0s - loss: 0.6766 - accuracy: 0.59 - 2s 136us/step - loss: 0.6762 - accuracy: 0.5963 - val_loss: 0.6571 - val_accuracy: 0.7024\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:36 - loss: 0.6915 - accuracy: 0.37 - ETA: 4s - loss: 0.6926 - accuracy: 0.5250 - ETA: 2s - loss: 0.6911 - accuracy: 0.51 - ETA: 2s - loss: 0.6917 - accuracy: 0.50 - ETA: 1s - loss: 0.6910 - accuracy: 0.51 - ETA: 1s - loss: 0.6905 - accuracy: 0.51 - ETA: 1s - loss: 0.6898 - accuracy: 0.51 - ETA: 1s - loss: 0.6903 - accuracy: 0.51 - ETA: 1s - loss: 0.6902 - accuracy: 0.51 - ETA: 1s - loss: 0.6904 - accuracy: 0.51 - ETA: 1s - loss: 0.6901 - accuracy: 0.51 - ETA: 1s - loss: 0.6898 - accuracy: 0.51 - ETA: 1s - loss: 0.6895 - accuracy: 0.52 - ETA: 1s - loss: 0.6893 - accuracy: 0.52 - ETA: 0s - loss: 0.6888 - accuracy: 0.52 - ETA: 0s - loss: 0.6880 - accuracy: 0.52 - ETA: 0s - loss: 0.6876 - accuracy: 0.52 - ETA: 0s - loss: 0.6869 - accuracy: 0.52 - ETA: 0s - loss: 0.6862 - accuracy: 0.52 - ETA: 0s - loss: 0.6858 - accuracy: 0.53 - ETA: 0s - loss: 0.6852 - accuracy: 0.53 - ETA: 0s - loss: 0.6848 - accuracy: 0.53 - ETA: 0s - loss: 0.6846 - accuracy: 0.53 - ETA: 0s - loss: 0.6844 - accuracy: 0.53 - ETA: 0s - loss: 0.6839 - accuracy: 0.53 - ETA: 0s - loss: 0.6836 - accuracy: 0.54 - ETA: 0s - loss: 0.6834 - accuracy: 0.54 - ETA: 0s - loss: 0.6831 - accuracy: 0.54 - ETA: 0s - loss: 0.6829 - accuracy: 0.54 - ETA: 0s - loss: 0.6826 - accuracy: 0.55 - ETA: 0s - loss: 0.6824 - accuracy: 0.55 - 2s 138us/step - loss: 0.6824 - accuracy: 0.5521 - val_loss: 0.6679 - val_accuracy: 0.6484\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:33 - loss: 0.6951 - accuracy: 0.37 - ETA: 4s - loss: 0.6928 - accuracy: 0.5185 - ETA: 2s - loss: 0.6936 - accuracy: 0.49 - ETA: 2s - loss: 0.6928 - accuracy: 0.50 - ETA: 1s - loss: 0.6924 - accuracy: 0.50 - ETA: 1s - loss: 0.6916 - accuracy: 0.49 - ETA: 1s - loss: 0.6913 - accuracy: 0.49 - ETA: 1s - loss: 0.6911 - accuracy: 0.49 - ETA: 1s - loss: 0.6908 - accuracy: 0.49 - ETA: 1s - loss: 0.6903 - accuracy: 0.51 - ETA: 1s - loss: 0.6897 - accuracy: 0.51 - ETA: 1s - loss: 0.6893 - accuracy: 0.52 - ETA: 1s - loss: 0.6886 - accuracy: 0.53 - ETA: 0s - loss: 0.6883 - accuracy: 0.53 - ETA: 0s - loss: 0.6880 - accuracy: 0.54 - ETA: 0s - loss: 0.6873 - accuracy: 0.54 - ETA: 0s - loss: 0.6870 - accuracy: 0.55 - ETA: 0s - loss: 0.6869 - accuracy: 0.55 - ETA: 0s - loss: 0.6865 - accuracy: 0.55 - ETA: 0s - loss: 0.6861 - accuracy: 0.56 - ETA: 0s - loss: 0.6857 - accuracy: 0.56 - ETA: 0s - loss: 0.6854 - accuracy: 0.56 - ETA: 0s - loss: 0.6849 - accuracy: 0.57 - ETA: 0s - loss: 0.6845 - accuracy: 0.57 - ETA: 0s - loss: 0.6839 - accuracy: 0.57 - ETA: 0s - loss: 0.6834 - accuracy: 0.57 - ETA: 0s - loss: 0.6830 - accuracy: 0.58 - ETA: 0s - loss: 0.6825 - accuracy: 0.58 - ETA: 0s - loss: 0.6819 - accuracy: 0.58 - ETA: 0s - loss: 0.6815 - accuracy: 0.58 - 2s 136us/step - loss: 0.6811 - accuracy: 0.5908 - val_loss: 0.6662 - val_accuracy: 0.6577\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 58us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:36 - loss: 0.6947 - accuracy: 0.25 - ETA: 4s - loss: 0.6937 - accuracy: 0.4929 - ETA: 2s - loss: 0.6918 - accuracy: 0.52 - ETA: 2s - loss: 0.6917 - accuracy: 0.51 - ETA: 1s - loss: 0.6917 - accuracy: 0.50 - ETA: 1s - loss: 0.6916 - accuracy: 0.50 - ETA: 1s - loss: 0.6903 - accuracy: 0.51 - ETA: 1s - loss: 0.6899 - accuracy: 0.51 - ETA: 1s - loss: 0.6900 - accuracy: 0.51 - ETA: 1s - loss: 0.6897 - accuracy: 0.51 - ETA: 1s - loss: 0.6890 - accuracy: 0.51 - ETA: 1s - loss: 0.6881 - accuracy: 0.51 - ETA: 1s - loss: 0.6876 - accuracy: 0.52 - ETA: 0s - loss: 0.6867 - accuracy: 0.52 - ETA: 0s - loss: 0.6866 - accuracy: 0.52 - ETA: 0s - loss: 0.6860 - accuracy: 0.52 - ETA: 0s - loss: 0.6857 - accuracy: 0.52 - ETA: 0s - loss: 0.6856 - accuracy: 0.52 - ETA: 0s - loss: 0.6855 - accuracy: 0.52 - ETA: 0s - loss: 0.6849 - accuracy: 0.52 - ETA: 0s - loss: 0.6842 - accuracy: 0.53 - ETA: 0s - loss: 0.6840 - accuracy: 0.53 - ETA: 0s - loss: 0.6838 - accuracy: 0.53 - ETA: 0s - loss: 0.6831 - accuracy: 0.53 - ETA: 0s - loss: 0.6828 - accuracy: 0.53 - ETA: 0s - loss: 0.6825 - accuracy: 0.53 - ETA: 0s - loss: 0.6822 - accuracy: 0.54 - ETA: 0s - loss: 0.6817 - accuracy: 0.54 - ETA: 0s - loss: 0.6811 - accuracy: 0.54 - ETA: 0s - loss: 0.6806 - accuracy: 0.54 - ETA: 0s - loss: 0.6803 - accuracy: 0.54 - 2s 137us/step - loss: 0.6803 - accuracy: 0.5492 - val_loss: 0.6597 - val_accuracy: 0.6946\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:35 - loss: 0.6961 - accuracy: 0.50 - ETA: 4s - loss: 0.6938 - accuracy: 0.4717 - ETA: 2s - loss: 0.6932 - accuracy: 0.49 - ETA: 2s - loss: 0.6925 - accuracy: 0.50 - ETA: 1s - loss: 0.6916 - accuracy: 0.50 - ETA: 1s - loss: 0.6904 - accuracy: 0.51 - ETA: 1s - loss: 0.6893 - accuracy: 0.51 - ETA: 1s - loss: 0.6888 - accuracy: 0.51 - ETA: 1s - loss: 0.6887 - accuracy: 0.51 - ETA: 1s - loss: 0.6886 - accuracy: 0.51 - ETA: 1s - loss: 0.6876 - accuracy: 0.51 - ETA: 1s - loss: 0.6869 - accuracy: 0.52 - ETA: 1s - loss: 0.6863 - accuracy: 0.52 - ETA: 1s - loss: 0.6860 - accuracy: 0.52 - ETA: 0s - loss: 0.6852 - accuracy: 0.53 - ETA: 0s - loss: 0.6845 - accuracy: 0.53 - ETA: 0s - loss: 0.6844 - accuracy: 0.53 - ETA: 0s - loss: 0.6836 - accuracy: 0.54 - ETA: 0s - loss: 0.6830 - accuracy: 0.54 - ETA: 0s - loss: 0.6827 - accuracy: 0.54 - ETA: 0s - loss: 0.6824 - accuracy: 0.54 - ETA: 0s - loss: 0.6821 - accuracy: 0.54 - ETA: 0s - loss: 0.6816 - accuracy: 0.54 - ETA: 0s - loss: 0.6811 - accuracy: 0.55 - ETA: 0s - loss: 0.6810 - accuracy: 0.55 - ETA: 0s - loss: 0.6807 - accuracy: 0.55 - ETA: 0s - loss: 0.6801 - accuracy: 0.55 - ETA: 0s - loss: 0.6797 - accuracy: 0.55 - ETA: 0s - loss: 0.6792 - accuracy: 0.55 - ETA: 0s - loss: 0.6788 - accuracy: 0.56 - ETA: 0s - loss: 0.6783 - accuracy: 0.56 - 2s 137us/step - loss: 0.6782 - accuracy: 0.5621 - val_loss: 0.6591 - val_accuracy: 0.7003\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:37 - loss: 0.7027 - accuracy: 0.37 - ETA: 3s - loss: 0.6940 - accuracy: 0.4591 - ETA: 2s - loss: 0.6928 - accuracy: 0.49 - ETA: 2s - loss: 0.6923 - accuracy: 0.50 - ETA: 1s - loss: 0.6912 - accuracy: 0.51 - ETA: 1s - loss: 0.6893 - accuracy: 0.53 - ETA: 1s - loss: 0.6870 - accuracy: 0.54 - ETA: 1s - loss: 0.6862 - accuracy: 0.54 - ETA: 1s - loss: 0.6863 - accuracy: 0.54 - ETA: 1s - loss: 0.6859 - accuracy: 0.54 - ETA: 1s - loss: 0.6855 - accuracy: 0.54 - ETA: 1s - loss: 0.6847 - accuracy: 0.55 - ETA: 1s - loss: 0.6843 - accuracy: 0.55 - ETA: 0s - loss: 0.6834 - accuracy: 0.55 - ETA: 0s - loss: 0.6823 - accuracy: 0.56 - ETA: 0s - loss: 0.6811 - accuracy: 0.56 - ETA: 0s - loss: 0.6802 - accuracy: 0.57 - ETA: 0s - loss: 0.6789 - accuracy: 0.57 - ETA: 0s - loss: 0.6783 - accuracy: 0.57 - ETA: 0s - loss: 0.6776 - accuracy: 0.57 - ETA: 0s - loss: 0.6772 - accuracy: 0.58 - ETA: 0s - loss: 0.6767 - accuracy: 0.58 - ETA: 0s - loss: 0.6756 - accuracy: 0.58 - ETA: 0s - loss: 0.6748 - accuracy: 0.58 - ETA: 0s - loss: 0.6741 - accuracy: 0.59 - ETA: 0s - loss: 0.6733 - accuracy: 0.59 - ETA: 0s - loss: 0.6727 - accuracy: 0.59 - ETA: 0s - loss: 0.6716 - accuracy: 0.59 - ETA: 0s - loss: 0.6712 - accuracy: 0.59 - ETA: 0s - loss: 0.6706 - accuracy: 0.59 - 2s 135us/step - loss: 0.6702 - accuracy: 0.6010 - val_loss: 0.6412 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:36 - loss: 0.6929 - accuracy: 0.50 - ETA: 4s - loss: 0.6914 - accuracy: 0.5114 - ETA: 2s - loss: 0.6915 - accuracy: 0.51 - ETA: 2s - loss: 0.6905 - accuracy: 0.53 - ETA: 1s - loss: 0.6899 - accuracy: 0.54 - ETA: 1s - loss: 0.6885 - accuracy: 0.55 - ETA: 1s - loss: 0.6877 - accuracy: 0.55 - ETA: 1s - loss: 0.6870 - accuracy: 0.56 - ETA: 1s - loss: 0.6858 - accuracy: 0.56 - ETA: 1s - loss: 0.6850 - accuracy: 0.57 - ETA: 1s - loss: 0.6837 - accuracy: 0.57 - ETA: 1s - loss: 0.6831 - accuracy: 0.57 - ETA: 1s - loss: 0.6822 - accuracy: 0.58 - ETA: 0s - loss: 0.6809 - accuracy: 0.58 - ETA: 0s - loss: 0.6794 - accuracy: 0.59 - ETA: 0s - loss: 0.6781 - accuracy: 0.59 - ETA: 0s - loss: 0.6769 - accuracy: 0.59 - ETA: 0s - loss: 0.6766 - accuracy: 0.59 - ETA: 0s - loss: 0.6754 - accuracy: 0.60 - ETA: 0s - loss: 0.6744 - accuracy: 0.60 - ETA: 0s - loss: 0.6737 - accuracy: 0.60 - ETA: 0s - loss: 0.6724 - accuracy: 0.60 - ETA: 0s - loss: 0.6713 - accuracy: 0.61 - ETA: 0s - loss: 0.6702 - accuracy: 0.61 - ETA: 0s - loss: 0.6695 - accuracy: 0.61 - ETA: 0s - loss: 0.6687 - accuracy: 0.61 - ETA: 0s - loss: 0.6680 - accuracy: 0.61 - ETA: 0s - loss: 0.6676 - accuracy: 0.61 - ETA: 0s - loss: 0.6666 - accuracy: 0.61 - ETA: 0s - loss: 0.6654 - accuracy: 0.62 - ETA: 0s - loss: 0.6646 - accuracy: 0.62 - 2s 138us/step - loss: 0.6646 - accuracy: 0.6211 - val_loss: 0.6335 - val_accuracy: 0.7145\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 54us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:36 - loss: 0.6954 - accuracy: 0.62 - ETA: 4s - loss: 0.6905 - accuracy: 0.5394 - ETA: 2s - loss: 0.6900 - accuracy: 0.52 - ETA: 2s - loss: 0.6900 - accuracy: 0.53 - ETA: 1s - loss: 0.6897 - accuracy: 0.53 - ETA: 1s - loss: 0.6888 - accuracy: 0.54 - ETA: 1s - loss: 0.6872 - accuracy: 0.56 - ETA: 1s - loss: 0.6860 - accuracy: 0.56 - ETA: 1s - loss: 0.6851 - accuracy: 0.56 - ETA: 1s - loss: 0.6843 - accuracy: 0.56 - ETA: 1s - loss: 0.6835 - accuracy: 0.57 - ETA: 1s - loss: 0.6826 - accuracy: 0.57 - ETA: 1s - loss: 0.6816 - accuracy: 0.58 - ETA: 0s - loss: 0.6807 - accuracy: 0.58 - ETA: 0s - loss: 0.6800 - accuracy: 0.58 - ETA: 0s - loss: 0.6792 - accuracy: 0.58 - ETA: 0s - loss: 0.6786 - accuracy: 0.58 - ETA: 0s - loss: 0.6783 - accuracy: 0.58 - ETA: 0s - loss: 0.6774 - accuracy: 0.59 - ETA: 0s - loss: 0.6762 - accuracy: 0.59 - ETA: 0s - loss: 0.6757 - accuracy: 0.59 - ETA: 0s - loss: 0.6744 - accuracy: 0.59 - ETA: 0s - loss: 0.6738 - accuracy: 0.59 - ETA: 0s - loss: 0.6729 - accuracy: 0.60 - ETA: 0s - loss: 0.6727 - accuracy: 0.60 - ETA: 0s - loss: 0.6716 - accuracy: 0.60 - ETA: 0s - loss: 0.6708 - accuracy: 0.60 - ETA: 0s - loss: 0.6702 - accuracy: 0.60 - ETA: 0s - loss: 0.6698 - accuracy: 0.60 - ETA: 0s - loss: 0.6692 - accuracy: 0.60 - 2s 135us/step - loss: 0.6686 - accuracy: 0.6072 - val_loss: 0.6409 - val_accuracy: 0.7102\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:36 - loss: 0.6959 - accuracy: 0.62 - ETA: 4s - loss: 0.6927 - accuracy: 0.5139 - ETA: 2s - loss: 0.6897 - accuracy: 0.55 - ETA: 2s - loss: 0.6878 - accuracy: 0.56 - ETA: 2s - loss: 0.6858 - accuracy: 0.56 - ETA: 2s - loss: 0.6840 - accuracy: 0.56 - ETA: 1s - loss: 0.6822 - accuracy: 0.56 - ETA: 1s - loss: 0.6809 - accuracy: 0.57 - ETA: 1s - loss: 0.6795 - accuracy: 0.57 - ETA: 1s - loss: 0.6787 - accuracy: 0.58 - ETA: 1s - loss: 0.6771 - accuracy: 0.58 - ETA: 1s - loss: 0.6768 - accuracy: 0.59 - ETA: 1s - loss: 0.6761 - accuracy: 0.59 - ETA: 1s - loss: 0.6759 - accuracy: 0.59 - ETA: 0s - loss: 0.6754 - accuracy: 0.59 - ETA: 0s - loss: 0.6748 - accuracy: 0.59 - ETA: 0s - loss: 0.6738 - accuracy: 0.60 - ETA: 0s - loss: 0.6729 - accuracy: 0.60 - ETA: 0s - loss: 0.6719 - accuracy: 0.60 - ETA: 0s - loss: 0.6715 - accuracy: 0.60 - ETA: 0s - loss: 0.6703 - accuracy: 0.61 - ETA: 0s - loss: 0.6690 - accuracy: 0.61 - ETA: 0s - loss: 0.6690 - accuracy: 0.61 - ETA: 0s - loss: 0.6685 - accuracy: 0.61 - ETA: 0s - loss: 0.6673 - accuracy: 0.61 - ETA: 0s - loss: 0.6666 - accuracy: 0.61 - ETA: 0s - loss: 0.6661 - accuracy: 0.61 - ETA: 0s - loss: 0.6653 - accuracy: 0.62 - ETA: 0s - loss: 0.6644 - accuracy: 0.62 - ETA: 0s - loss: 0.6632 - accuracy: 0.62 - ETA: 0s - loss: 0.6625 - accuracy: 0.62 - 2s 137us/step - loss: 0.6625 - accuracy: 0.6249 - val_loss: 0.6335 - val_accuracy: 0.7159\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 2:42 - loss: 0.6971 - accuracy: 0.37 - ETA: 4s - loss: 0.6931 - accuracy: 0.5189 - ETA: 3s - loss: 0.6929 - accuracy: 0.52 - ETA: 2s - loss: 0.6922 - accuracy: 0.53 - ETA: 2s - loss: 0.6919 - accuracy: 0.53 - ETA: 1s - loss: 0.6907 - accuracy: 0.54 - ETA: 1s - loss: 0.6900 - accuracy: 0.55 - ETA: 1s - loss: 0.6890 - accuracy: 0.56 - ETA: 1s - loss: 0.6882 - accuracy: 0.56 - ETA: 1s - loss: 0.6867 - accuracy: 0.57 - ETA: 1s - loss: 0.6856 - accuracy: 0.57 - ETA: 1s - loss: 0.6853 - accuracy: 0.57 - ETA: 1s - loss: 0.6839 - accuracy: 0.58 - ETA: 1s - loss: 0.6838 - accuracy: 0.58 - ETA: 0s - loss: 0.6828 - accuracy: 0.58 - ETA: 0s - loss: 0.6822 - accuracy: 0.58 - ETA: 0s - loss: 0.6814 - accuracy: 0.58 - ETA: 0s - loss: 0.6803 - accuracy: 0.58 - ETA: 0s - loss: 0.6801 - accuracy: 0.58 - ETA: 0s - loss: 0.6794 - accuracy: 0.59 - ETA: 0s - loss: 0.6791 - accuracy: 0.59 - ETA: 0s - loss: 0.6785 - accuracy: 0.59 - ETA: 0s - loss: 0.6782 - accuracy: 0.59 - ETA: 0s - loss: 0.6773 - accuracy: 0.59 - ETA: 0s - loss: 0.6766 - accuracy: 0.59 - ETA: 0s - loss: 0.6762 - accuracy: 0.59 - ETA: 0s - loss: 0.6761 - accuracy: 0.59 - ETA: 0s - loss: 0.6757 - accuracy: 0.59 - ETA: 0s - loss: 0.6749 - accuracy: 0.59 - ETA: 0s - loss: 0.6743 - accuracy: 0.60 - ETA: 0s - loss: 0.6737 - accuracy: 0.60 - 2s 140us/step - loss: 0.6733 - accuracy: 0.6027 - val_loss: 0.6484 - val_accuracy: 0.7131\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:50 - loss: 0.6963 - accuracy: 0.50 - ETA: 4s - loss: 0.6933 - accuracy: 0.5192 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6927 - accuracy: 0.52 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6918 - accuracy: 0.53 - ETA: 1s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 1s - loss: 0.6904 - accuracy: 0.54 - ETA: 1s - loss: 0.6900 - accuracy: 0.54 - ETA: 1s - loss: 0.6896 - accuracy: 0.54 - ETA: 0s - loss: 0.6894 - accuracy: 0.54 - ETA: 0s - loss: 0.6890 - accuracy: 0.54 - ETA: 0s - loss: 0.6886 - accuracy: 0.55 - ETA: 0s - loss: 0.6877 - accuracy: 0.55 - ETA: 0s - loss: 0.6871 - accuracy: 0.56 - ETA: 0s - loss: 0.6864 - accuracy: 0.56 - ETA: 0s - loss: 0.6860 - accuracy: 0.56 - ETA: 0s - loss: 0.6857 - accuracy: 0.56 - ETA: 0s - loss: 0.6853 - accuracy: 0.56 - ETA: 0s - loss: 0.6849 - accuracy: 0.56 - ETA: 0s - loss: 0.6843 - accuracy: 0.57 - ETA: 0s - loss: 0.6843 - accuracy: 0.57 - ETA: 0s - loss: 0.6837 - accuracy: 0.57 - ETA: 0s - loss: 0.6833 - accuracy: 0.57 - ETA: 0s - loss: 0.6827 - accuracy: 0.57 - ETA: 0s - loss: 0.6825 - accuracy: 0.57 - ETA: 0s - loss: 0.6821 - accuracy: 0.57 - 2s 136us/step - loss: 0.6819 - accuracy: 0.5766 - val_loss: 0.6656 - val_accuracy: 0.6847\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 57us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:35 - loss: 0.7066 - accuracy: 0.25 - ETA: 4s - loss: 0.6947 - accuracy: 0.4531 - ETA: 2s - loss: 0.6925 - accuracy: 0.51 - ETA: 2s - loss: 0.6920 - accuracy: 0.51 - ETA: 2s - loss: 0.6906 - accuracy: 0.53 - ETA: 1s - loss: 0.6898 - accuracy: 0.53 - ETA: 1s - loss: 0.6887 - accuracy: 0.55 - ETA: 1s - loss: 0.6870 - accuracy: 0.56 - ETA: 1s - loss: 0.6854 - accuracy: 0.57 - ETA: 1s - loss: 0.6835 - accuracy: 0.58 - ETA: 1s - loss: 0.6827 - accuracy: 0.58 - ETA: 1s - loss: 0.6807 - accuracy: 0.59 - ETA: 1s - loss: 0.6793 - accuracy: 0.59 - ETA: 0s - loss: 0.6785 - accuracy: 0.59 - ETA: 0s - loss: 0.6774 - accuracy: 0.60 - ETA: 0s - loss: 0.6759 - accuracy: 0.60 - ETA: 0s - loss: 0.6746 - accuracy: 0.61 - ETA: 0s - loss: 0.6724 - accuracy: 0.61 - ETA: 0s - loss: 0.6708 - accuracy: 0.62 - ETA: 0s - loss: 0.6701 - accuracy: 0.62 - ETA: 0s - loss: 0.6689 - accuracy: 0.62 - ETA: 0s - loss: 0.6682 - accuracy: 0.62 - ETA: 0s - loss: 0.6672 - accuracy: 0.62 - ETA: 0s - loss: 0.6662 - accuracy: 0.62 - ETA: 0s - loss: 0.6647 - accuracy: 0.62 - ETA: 0s - loss: 0.6642 - accuracy: 0.63 - ETA: 0s - loss: 0.6630 - accuracy: 0.63 - ETA: 0s - loss: 0.6617 - accuracy: 0.63 - ETA: 0s - loss: 0.6607 - accuracy: 0.63 - ETA: 0s - loss: 0.6603 - accuracy: 0.63 - ETA: 0s - loss: 0.6595 - accuracy: 0.63 - 2s 139us/step - loss: 0.6596 - accuracy: 0.6358 - val_loss: 0.6224 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:36 - loss: 0.6958 - accuracy: 0.37 - ETA: 4s - loss: 0.6943 - accuracy: 0.5000 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6908 - accuracy: 0.54 - ETA: 1s - loss: 0.6895 - accuracy: 0.55 - ETA: 1s - loss: 0.6877 - accuracy: 0.56 - ETA: 1s - loss: 0.6867 - accuracy: 0.55 - ETA: 1s - loss: 0.6853 - accuracy: 0.56 - ETA: 1s - loss: 0.6849 - accuracy: 0.56 - ETA: 1s - loss: 0.6833 - accuracy: 0.56 - ETA: 1s - loss: 0.6819 - accuracy: 0.57 - ETA: 1s - loss: 0.6808 - accuracy: 0.57 - ETA: 0s - loss: 0.6791 - accuracy: 0.58 - ETA: 0s - loss: 0.6786 - accuracy: 0.58 - ETA: 0s - loss: 0.6777 - accuracy: 0.58 - ETA: 0s - loss: 0.6767 - accuracy: 0.58 - ETA: 0s - loss: 0.6754 - accuracy: 0.59 - ETA: 0s - loss: 0.6740 - accuracy: 0.59 - ETA: 0s - loss: 0.6734 - accuracy: 0.59 - ETA: 0s - loss: 0.6729 - accuracy: 0.59 - ETA: 0s - loss: 0.6717 - accuracy: 0.60 - ETA: 0s - loss: 0.6703 - accuracy: 0.60 - ETA: 0s - loss: 0.6692 - accuracy: 0.60 - ETA: 0s - loss: 0.6680 - accuracy: 0.60 - ETA: 0s - loss: 0.6674 - accuracy: 0.60 - ETA: 0s - loss: 0.6670 - accuracy: 0.60 - ETA: 0s - loss: 0.6661 - accuracy: 0.61 - ETA: 0s - loss: 0.6653 - accuracy: 0.61 - ETA: 0s - loss: 0.6648 - accuracy: 0.61 - 2s 134us/step - loss: 0.6644 - accuracy: 0.6166 - val_loss: 0.6292 - val_accuracy: 0.7124\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:36 - loss: 0.6882 - accuracy: 0.75 - ETA: 4s - loss: 0.6918 - accuracy: 0.5477 - ETA: 2s - loss: 0.6903 - accuracy: 0.56 - ETA: 2s - loss: 0.6876 - accuracy: 0.57 - ETA: 1s - loss: 0.6854 - accuracy: 0.57 - ETA: 1s - loss: 0.6844 - accuracy: 0.57 - ETA: 1s - loss: 0.6836 - accuracy: 0.57 - ETA: 1s - loss: 0.6828 - accuracy: 0.58 - ETA: 1s - loss: 0.6821 - accuracy: 0.58 - ETA: 1s - loss: 0.6817 - accuracy: 0.58 - ETA: 1s - loss: 0.6801 - accuracy: 0.58 - ETA: 1s - loss: 0.6787 - accuracy: 0.58 - ETA: 1s - loss: 0.6766 - accuracy: 0.59 - ETA: 0s - loss: 0.6759 - accuracy: 0.59 - ETA: 0s - loss: 0.6753 - accuracy: 0.59 - ETA: 0s - loss: 0.6739 - accuracy: 0.60 - ETA: 0s - loss: 0.6729 - accuracy: 0.60 - ETA: 0s - loss: 0.6714 - accuracy: 0.60 - ETA: 0s - loss: 0.6700 - accuracy: 0.60 - ETA: 0s - loss: 0.6689 - accuracy: 0.61 - ETA: 0s - loss: 0.6678 - accuracy: 0.61 - ETA: 0s - loss: 0.6668 - accuracy: 0.61 - ETA: 0s - loss: 0.6656 - accuracy: 0.61 - ETA: 0s - loss: 0.6654 - accuracy: 0.61 - ETA: 0s - loss: 0.6643 - accuracy: 0.62 - ETA: 0s - loss: 0.6636 - accuracy: 0.62 - ETA: 0s - loss: 0.6623 - accuracy: 0.62 - ETA: 0s - loss: 0.6616 - accuracy: 0.62 - ETA: 0s - loss: 0.6611 - accuracy: 0.62 - ETA: 0s - loss: 0.6603 - accuracy: 0.62 - ETA: 0s - loss: 0.6596 - accuracy: 0.62 - 2s 138us/step - loss: 0.6594 - accuracy: 0.6297 - val_loss: 0.6256 - val_accuracy: 0.7081\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:35 - loss: 0.6876 - accuracy: 0.87 - ETA: 4s - loss: 0.6933 - accuracy: 0.5278 - ETA: 2s - loss: 0.6929 - accuracy: 0.52 - ETA: 2s - loss: 0.6920 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.54 - ETA: 1s - loss: 0.6887 - accuracy: 0.56 - ETA: 1s - loss: 0.6880 - accuracy: 0.56 - ETA: 1s - loss: 0.6867 - accuracy: 0.57 - ETA: 1s - loss: 0.6858 - accuracy: 0.57 - ETA: 1s - loss: 0.6843 - accuracy: 0.58 - ETA: 1s - loss: 0.6835 - accuracy: 0.58 - ETA: 1s - loss: 0.6827 - accuracy: 0.58 - ETA: 1s - loss: 0.6814 - accuracy: 0.59 - ETA: 1s - loss: 0.6796 - accuracy: 0.59 - ETA: 0s - loss: 0.6788 - accuracy: 0.59 - ETA: 0s - loss: 0.6780 - accuracy: 0.60 - ETA: 0s - loss: 0.6770 - accuracy: 0.60 - ETA: 0s - loss: 0.6764 - accuracy: 0.60 - ETA: 0s - loss: 0.6752 - accuracy: 0.60 - ETA: 0s - loss: 0.6736 - accuracy: 0.60 - ETA: 0s - loss: 0.6725 - accuracy: 0.61 - ETA: 0s - loss: 0.6717 - accuracy: 0.61 - ETA: 0s - loss: 0.6708 - accuracy: 0.61 - ETA: 0s - loss: 0.6696 - accuracy: 0.61 - ETA: 0s - loss: 0.6690 - accuracy: 0.61 - ETA: 0s - loss: 0.6685 - accuracy: 0.61 - ETA: 0s - loss: 0.6672 - accuracy: 0.62 - ETA: 0s - loss: 0.6664 - accuracy: 0.62 - ETA: 0s - loss: 0.6655 - accuracy: 0.62 - ETA: 0s - loss: 0.6648 - accuracy: 0.62 - ETA: 0s - loss: 0.6640 - accuracy: 0.62 - 2s 139us/step - loss: 0.6634 - accuracy: 0.6288 - val_loss: 0.6332 - val_accuracy: 0.7038\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 59us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:31 - loss: 0.6830 - accuracy: 0.75 - ETA: 4s - loss: 0.6927 - accuracy: 0.5278 - ETA: 2s - loss: 0.6888 - accuracy: 0.54 - ETA: 2s - loss: 0.6892 - accuracy: 0.54 - ETA: 1s - loss: 0.6874 - accuracy: 0.55 - ETA: 1s - loss: 0.6859 - accuracy: 0.56 - ETA: 1s - loss: 0.6834 - accuracy: 0.58 - ETA: 1s - loss: 0.6809 - accuracy: 0.59 - ETA: 1s - loss: 0.6785 - accuracy: 0.60 - ETA: 1s - loss: 0.6757 - accuracy: 0.61 - ETA: 1s - loss: 0.6737 - accuracy: 0.61 - ETA: 1s - loss: 0.6718 - accuracy: 0.61 - ETA: 1s - loss: 0.6693 - accuracy: 0.62 - ETA: 0s - loss: 0.6675 - accuracy: 0.62 - ETA: 0s - loss: 0.6657 - accuracy: 0.62 - ETA: 0s - loss: 0.6649 - accuracy: 0.63 - ETA: 0s - loss: 0.6640 - accuracy: 0.63 - ETA: 0s - loss: 0.6630 - accuracy: 0.63 - ETA: 0s - loss: 0.6617 - accuracy: 0.63 - ETA: 0s - loss: 0.6595 - accuracy: 0.64 - ETA: 0s - loss: 0.6592 - accuracy: 0.64 - ETA: 0s - loss: 0.6584 - accuracy: 0.64 - ETA: 0s - loss: 0.6576 - accuracy: 0.64 - ETA: 0s - loss: 0.6564 - accuracy: 0.64 - ETA: 0s - loss: 0.6545 - accuracy: 0.64 - ETA: 0s - loss: 0.6538 - accuracy: 0.64 - ETA: 0s - loss: 0.6524 - accuracy: 0.64 - ETA: 0s - loss: 0.6511 - accuracy: 0.65 - ETA: 0s - loss: 0.6504 - accuracy: 0.65 - ETA: 0s - loss: 0.6497 - accuracy: 0.65 - 2s 135us/step - loss: 0.6492 - accuracy: 0.6529 - val_loss: 0.6087 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:33 - loss: 0.6948 - accuracy: 0.50 - ETA: 4s - loss: 0.6924 - accuracy: 0.5264 - ETA: 2s - loss: 0.6914 - accuracy: 0.52 - ETA: 2s - loss: 0.6900 - accuracy: 0.52 - ETA: 1s - loss: 0.6895 - accuracy: 0.53 - ETA: 1s - loss: 0.6890 - accuracy: 0.54 - ETA: 1s - loss: 0.6876 - accuracy: 0.54 - ETA: 1s - loss: 0.6862 - accuracy: 0.55 - ETA: 1s - loss: 0.6851 - accuracy: 0.56 - ETA: 1s - loss: 0.6841 - accuracy: 0.56 - ETA: 1s - loss: 0.6835 - accuracy: 0.57 - ETA: 1s - loss: 0.6825 - accuracy: 0.57 - ETA: 1s - loss: 0.6808 - accuracy: 0.58 - ETA: 0s - loss: 0.6797 - accuracy: 0.58 - ETA: 0s - loss: 0.6779 - accuracy: 0.58 - ETA: 0s - loss: 0.6771 - accuracy: 0.58 - ETA: 0s - loss: 0.6765 - accuracy: 0.59 - ETA: 0s - loss: 0.6755 - accuracy: 0.59 - ETA: 0s - loss: 0.6742 - accuracy: 0.59 - ETA: 0s - loss: 0.6734 - accuracy: 0.60 - ETA: 0s - loss: 0.6720 - accuracy: 0.60 - ETA: 0s - loss: 0.6710 - accuracy: 0.60 - ETA: 0s - loss: 0.6704 - accuracy: 0.60 - ETA: 0s - loss: 0.6696 - accuracy: 0.61 - ETA: 0s - loss: 0.6684 - accuracy: 0.61 - ETA: 0s - loss: 0.6677 - accuracy: 0.61 - ETA: 0s - loss: 0.6667 - accuracy: 0.61 - ETA: 0s - loss: 0.6660 - accuracy: 0.61 - ETA: 0s - loss: 0.6648 - accuracy: 0.61 - ETA: 0s - loss: 0.6637 - accuracy: 0.62 - ETA: 0s - loss: 0.6633 - accuracy: 0.62 - 2s 137us/step - loss: 0.6632 - accuracy: 0.6221 - val_loss: 0.6334 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:04 - loss: 0.6944 - accuracy: 0.37 - ETA: 4s - loss: 0.6937 - accuracy: 0.4567 - ETA: 3s - loss: 0.6937 - accuracy: 0.48 - ETA: 2s - loss: 0.6930 - accuracy: 0.50 - ETA: 2s - loss: 0.6924 - accuracy: 0.50 - ETA: 1s - loss: 0.6923 - accuracy: 0.51 - ETA: 1s - loss: 0.6904 - accuracy: 0.52 - ETA: 1s - loss: 0.6898 - accuracy: 0.52 - ETA: 1s - loss: 0.6871 - accuracy: 0.53 - ETA: 1s - loss: 0.6846 - accuracy: 0.53 - ETA: 1s - loss: 0.6822 - accuracy: 0.54 - ETA: 1s - loss: 0.6793 - accuracy: 0.54 - ETA: 1s - loss: 0.6778 - accuracy: 0.55 - ETA: 1s - loss: 0.6768 - accuracy: 0.55 - ETA: 1s - loss: 0.6756 - accuracy: 0.55 - ETA: 0s - loss: 0.6733 - accuracy: 0.56 - ETA: 0s - loss: 0.6724 - accuracy: 0.56 - ETA: 0s - loss: 0.6700 - accuracy: 0.56 - ETA: 0s - loss: 0.6688 - accuracy: 0.57 - ETA: 0s - loss: 0.6692 - accuracy: 0.57 - ETA: 0s - loss: 0.6675 - accuracy: 0.57 - ETA: 0s - loss: 0.6666 - accuracy: 0.58 - ETA: 0s - loss: 0.6645 - accuracy: 0.58 - ETA: 0s - loss: 0.6621 - accuracy: 0.58 - ETA: 0s - loss: 0.6610 - accuracy: 0.59 - ETA: 0s - loss: 0.6599 - accuracy: 0.59 - ETA: 0s - loss: 0.6588 - accuracy: 0.59 - ETA: 0s - loss: 0.6582 - accuracy: 0.59 - ETA: 0s - loss: 0.6578 - accuracy: 0.59 - ETA: 0s - loss: 0.6570 - accuracy: 0.59 - ETA: 0s - loss: 0.6565 - accuracy: 0.59 - ETA: 0s - loss: 0.6561 - accuracy: 0.60 - ETA: 0s - loss: 0.6553 - accuracy: 0.60 - 2s 147us/step - loss: 0.6551 - accuracy: 0.6021 - val_loss: 0.6005 - val_accuracy: 0.7166\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 67us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:17 - loss: 0.6937 - accuracy: 0.50 - ETA: 5s - loss: 0.6936 - accuracy: 0.5074 - ETA: 3s - loss: 0.6934 - accuracy: 0.52 - ETA: 2s - loss: 0.6929 - accuracy: 0.52 - ETA: 2s - loss: 0.6925 - accuracy: 0.53 - ETA: 2s - loss: 0.6918 - accuracy: 0.54 - ETA: 1s - loss: 0.6910 - accuracy: 0.55 - ETA: 1s - loss: 0.6895 - accuracy: 0.56 - ETA: 1s - loss: 0.6887 - accuracy: 0.56 - ETA: 1s - loss: 0.6876 - accuracy: 0.56 - ETA: 1s - loss: 0.6857 - accuracy: 0.56 - ETA: 1s - loss: 0.6844 - accuracy: 0.56 - ETA: 1s - loss: 0.6830 - accuracy: 0.57 - ETA: 1s - loss: 0.6821 - accuracy: 0.57 - ETA: 1s - loss: 0.6805 - accuracy: 0.57 - ETA: 1s - loss: 0.6797 - accuracy: 0.58 - ETA: 1s - loss: 0.6782 - accuracy: 0.58 - ETA: 0s - loss: 0.6770 - accuracy: 0.58 - ETA: 0s - loss: 0.6750 - accuracy: 0.58 - ETA: 0s - loss: 0.6729 - accuracy: 0.59 - ETA: 0s - loss: 0.6706 - accuracy: 0.59 - ETA: 0s - loss: 0.6689 - accuracy: 0.60 - ETA: 0s - loss: 0.6676 - accuracy: 0.60 - ETA: 0s - loss: 0.6662 - accuracy: 0.60 - ETA: 0s - loss: 0.6661 - accuracy: 0.60 - ETA: 0s - loss: 0.6657 - accuracy: 0.60 - ETA: 0s - loss: 0.6641 - accuracy: 0.60 - ETA: 0s - loss: 0.6618 - accuracy: 0.61 - ETA: 0s - loss: 0.6598 - accuracy: 0.61 - ETA: 0s - loss: 0.6588 - accuracy: 0.61 - ETA: 0s - loss: 0.6575 - accuracy: 0.61 - ETA: 0s - loss: 0.6559 - accuracy: 0.61 - ETA: 0s - loss: 0.6545 - accuracy: 0.62 - 2s 149us/step - loss: 0.6542 - accuracy: 0.6225 - val_loss: 0.6061 - val_accuracy: 0.7166\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 57us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 3:08 - loss: 0.6926 - accuracy: 0.75 - ETA: 4s - loss: 0.6928 - accuracy: 0.5409 - ETA: 3s - loss: 0.6906 - accuracy: 0.54 - ETA: 2s - loss: 0.6895 - accuracy: 0.54 - ETA: 2s - loss: 0.6892 - accuracy: 0.53 - ETA: 1s - loss: 0.6876 - accuracy: 0.52 - ETA: 1s - loss: 0.6851 - accuracy: 0.53 - ETA: 1s - loss: 0.6836 - accuracy: 0.54 - ETA: 1s - loss: 0.6815 - accuracy: 0.54 - ETA: 1s - loss: 0.6810 - accuracy: 0.55 - ETA: 1s - loss: 0.6801 - accuracy: 0.56 - ETA: 1s - loss: 0.6795 - accuracy: 0.57 - ETA: 1s - loss: 0.6785 - accuracy: 0.57 - ETA: 1s - loss: 0.6766 - accuracy: 0.58 - ETA: 1s - loss: 0.6745 - accuracy: 0.58 - ETA: 0s - loss: 0.6735 - accuracy: 0.59 - ETA: 0s - loss: 0.6735 - accuracy: 0.59 - ETA: 0s - loss: 0.6726 - accuracy: 0.59 - ETA: 0s - loss: 0.6719 - accuracy: 0.60 - ETA: 0s - loss: 0.6718 - accuracy: 0.60 - ETA: 0s - loss: 0.6706 - accuracy: 0.61 - ETA: 0s - loss: 0.6697 - accuracy: 0.61 - ETA: 0s - loss: 0.6685 - accuracy: 0.61 - ETA: 0s - loss: 0.6660 - accuracy: 0.61 - ETA: 0s - loss: 0.6653 - accuracy: 0.61 - ETA: 0s - loss: 0.6644 - accuracy: 0.62 - ETA: 0s - loss: 0.6637 - accuracy: 0.62 - ETA: 0s - loss: 0.6632 - accuracy: 0.62 - ETA: 0s - loss: 0.6625 - accuracy: 0.62 - ETA: 0s - loss: 0.6623 - accuracy: 0.62 - ETA: 0s - loss: 0.6619 - accuracy: 0.62 - ETA: 0s - loss: 0.6615 - accuracy: 0.62 - 2s 144us/step - loss: 0.6610 - accuracy: 0.6285 - val_loss: 0.6214 - val_accuracy: 0.7251\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:13 - loss: 0.6936 - accuracy: 0.50 - ETA: 5s - loss: 0.6936 - accuracy: 0.5075 - ETA: 3s - loss: 0.6937 - accuracy: 0.50 - ETA: 2s - loss: 0.6932 - accuracy: 0.50 - ETA: 2s - loss: 0.6923 - accuracy: 0.51 - ETA: 1s - loss: 0.6916 - accuracy: 0.51 - ETA: 1s - loss: 0.6915 - accuracy: 0.51 - ETA: 1s - loss: 0.6909 - accuracy: 0.51 - ETA: 1s - loss: 0.6898 - accuracy: 0.52 - ETA: 1s - loss: 0.6878 - accuracy: 0.53 - ETA: 1s - loss: 0.6855 - accuracy: 0.53 - ETA: 1s - loss: 0.6849 - accuracy: 0.53 - ETA: 1s - loss: 0.6841 - accuracy: 0.54 - ETA: 1s - loss: 0.6818 - accuracy: 0.54 - ETA: 0s - loss: 0.6806 - accuracy: 0.55 - ETA: 0s - loss: 0.6794 - accuracy: 0.56 - ETA: 0s - loss: 0.6781 - accuracy: 0.56 - ETA: 0s - loss: 0.6754 - accuracy: 0.57 - ETA: 0s - loss: 0.6754 - accuracy: 0.57 - ETA: 0s - loss: 0.6743 - accuracy: 0.57 - ETA: 0s - loss: 0.6727 - accuracy: 0.57 - ETA: 0s - loss: 0.6712 - accuracy: 0.58 - ETA: 0s - loss: 0.6696 - accuracy: 0.58 - ETA: 0s - loss: 0.6669 - accuracy: 0.59 - ETA: 0s - loss: 0.6657 - accuracy: 0.59 - ETA: 0s - loss: 0.6645 - accuracy: 0.59 - ETA: 0s - loss: 0.6640 - accuracy: 0.59 - ETA: 0s - loss: 0.6643 - accuracy: 0.59 - ETA: 0s - loss: 0.6635 - accuracy: 0.59 - ETA: 0s - loss: 0.6629 - accuracy: 0.59 - ETA: 0s - loss: 0.6616 - accuracy: 0.60 - 2s 141us/step - loss: 0.6601 - accuracy: 0.6028 - val_loss: 0.6060 - val_accuracy: 0.7195\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:08 - loss: 0.6950 - accuracy: 0.25 - ETA: 4s - loss: 0.6940 - accuracy: 0.4495 - ETA: 3s - loss: 0.6937 - accuracy: 0.47 - ETA: 2s - loss: 0.6927 - accuracy: 0.50 - ETA: 2s - loss: 0.6919 - accuracy: 0.50 - ETA: 1s - loss: 0.6913 - accuracy: 0.51 - ETA: 1s - loss: 0.6899 - accuracy: 0.51 - ETA: 1s - loss: 0.6873 - accuracy: 0.52 - ETA: 1s - loss: 0.6856 - accuracy: 0.53 - ETA: 1s - loss: 0.6823 - accuracy: 0.54 - ETA: 1s - loss: 0.6807 - accuracy: 0.54 - ETA: 1s - loss: 0.6796 - accuracy: 0.54 - ETA: 1s - loss: 0.6763 - accuracy: 0.55 - ETA: 1s - loss: 0.6749 - accuracy: 0.55 - ETA: 1s - loss: 0.6730 - accuracy: 0.55 - ETA: 0s - loss: 0.6701 - accuracy: 0.56 - ETA: 0s - loss: 0.6681 - accuracy: 0.56 - ETA: 0s - loss: 0.6664 - accuracy: 0.57 - ETA: 0s - loss: 0.6653 - accuracy: 0.57 - ETA: 0s - loss: 0.6642 - accuracy: 0.57 - ETA: 0s - loss: 0.6642 - accuracy: 0.57 - ETA: 0s - loss: 0.6635 - accuracy: 0.58 - ETA: 0s - loss: 0.6628 - accuracy: 0.58 - ETA: 0s - loss: 0.6628 - accuracy: 0.58 - ETA: 0s - loss: 0.6620 - accuracy: 0.58 - ETA: 0s - loss: 0.6614 - accuracy: 0.58 - ETA: 0s - loss: 0.6606 - accuracy: 0.58 - ETA: 0s - loss: 0.6600 - accuracy: 0.58 - ETA: 0s - loss: 0.6588 - accuracy: 0.58 - ETA: 0s - loss: 0.6578 - accuracy: 0.59 - ETA: 0s - loss: 0.6570 - accuracy: 0.59 - ETA: 0s - loss: 0.6565 - accuracy: 0.59 - 2s 143us/step - loss: 0.6563 - accuracy: 0.5961 - val_loss: 0.6008 - val_accuracy: 0.7216\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:09 - loss: 0.6944 - accuracy: 0.37 - ETA: 5s - loss: 0.6931 - accuracy: 0.5319 - ETA: 3s - loss: 0.6936 - accuracy: 0.50 - ETA: 2s - loss: 0.6931 - accuracy: 0.51 - ETA: 2s - loss: 0.6921 - accuracy: 0.53 - ETA: 2s - loss: 0.6917 - accuracy: 0.52 - ETA: 1s - loss: 0.6904 - accuracy: 0.53 - ETA: 1s - loss: 0.6890 - accuracy: 0.53 - ETA: 1s - loss: 0.6879 - accuracy: 0.53 - ETA: 1s - loss: 0.6871 - accuracy: 0.53 - ETA: 1s - loss: 0.6861 - accuracy: 0.53 - ETA: 1s - loss: 0.6845 - accuracy: 0.54 - ETA: 1s - loss: 0.6832 - accuracy: 0.54 - ETA: 1s - loss: 0.6804 - accuracy: 0.55 - ETA: 1s - loss: 0.6785 - accuracy: 0.55 - ETA: 1s - loss: 0.6764 - accuracy: 0.56 - ETA: 0s - loss: 0.6756 - accuracy: 0.56 - ETA: 0s - loss: 0.6752 - accuracy: 0.56 - ETA: 0s - loss: 0.6737 - accuracy: 0.56 - ETA: 0s - loss: 0.6729 - accuracy: 0.57 - ETA: 0s - loss: 0.6710 - accuracy: 0.57 - ETA: 0s - loss: 0.6696 - accuracy: 0.57 - ETA: 0s - loss: 0.6674 - accuracy: 0.57 - ETA: 0s - loss: 0.6655 - accuracy: 0.58 - ETA: 0s - loss: 0.6646 - accuracy: 0.58 - ETA: 0s - loss: 0.6642 - accuracy: 0.58 - ETA: 0s - loss: 0.6630 - accuracy: 0.58 - ETA: 0s - loss: 0.6626 - accuracy: 0.58 - ETA: 0s - loss: 0.6615 - accuracy: 0.59 - ETA: 0s - loss: 0.6598 - accuracy: 0.59 - ETA: 0s - loss: 0.6589 - accuracy: 0.59 - ETA: 0s - loss: 0.6586 - accuracy: 0.59 - 2s 145us/step - loss: 0.6574 - accuracy: 0.5973 - val_loss: 0.6115 - val_accuracy: 0.7188\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:11 - loss: 0.6927 - accuracy: 0.50 - ETA: 5s - loss: 0.6929 - accuracy: 0.5221 - ETA: 3s - loss: 0.6926 - accuracy: 0.52 - ETA: 2s - loss: 0.6929 - accuracy: 0.51 - ETA: 2s - loss: 0.6908 - accuracy: 0.52 - ETA: 2s - loss: 0.6899 - accuracy: 0.53 - ETA: 1s - loss: 0.6889 - accuracy: 0.53 - ETA: 1s - loss: 0.6868 - accuracy: 0.54 - ETA: 1s - loss: 0.6853 - accuracy: 0.54 - ETA: 1s - loss: 0.6847 - accuracy: 0.54 - ETA: 1s - loss: 0.6822 - accuracy: 0.55 - ETA: 1s - loss: 0.6801 - accuracy: 0.56 - ETA: 1s - loss: 0.6784 - accuracy: 0.56 - ETA: 1s - loss: 0.6768 - accuracy: 0.56 - ETA: 1s - loss: 0.6748 - accuracy: 0.57 - ETA: 0s - loss: 0.6735 - accuracy: 0.57 - ETA: 0s - loss: 0.6710 - accuracy: 0.57 - ETA: 0s - loss: 0.6695 - accuracy: 0.58 - ETA: 0s - loss: 0.6686 - accuracy: 0.58 - ETA: 0s - loss: 0.6682 - accuracy: 0.58 - ETA: 0s - loss: 0.6668 - accuracy: 0.59 - ETA: 0s - loss: 0.6655 - accuracy: 0.59 - ETA: 0s - loss: 0.6627 - accuracy: 0.60 - ETA: 0s - loss: 0.6613 - accuracy: 0.60 - ETA: 0s - loss: 0.6606 - accuracy: 0.60 - ETA: 0s - loss: 0.6599 - accuracy: 0.60 - ETA: 0s - loss: 0.6591 - accuracy: 0.60 - ETA: 0s - loss: 0.6582 - accuracy: 0.60 - ETA: 0s - loss: 0.6563 - accuracy: 0.61 - ETA: 0s - loss: 0.6558 - accuracy: 0.61 - ETA: 0s - loss: 0.6546 - accuracy: 0.61 - ETA: 0s - loss: 0.6539 - accuracy: 0.61 - ETA: 0s - loss: 0.6526 - accuracy: 0.62 - 2s 150us/step - loss: 0.6523 - accuracy: 0.6223 - val_loss: 0.6042 - val_accuracy: 0.7180\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:14 - loss: 0.6960 - accuracy: 0.25 - ETA: 5s - loss: 0.6937 - accuracy: 0.5275 - ETA: 3s - loss: 0.6941 - accuracy: 0.51 - ETA: 2s - loss: 0.6937 - accuracy: 0.52 - ETA: 2s - loss: 0.6924 - accuracy: 0.54 - ETA: 2s - loss: 0.6908 - accuracy: 0.54 - ETA: 1s - loss: 0.6880 - accuracy: 0.54 - ETA: 1s - loss: 0.6855 - accuracy: 0.55 - ETA: 1s - loss: 0.6843 - accuracy: 0.56 - ETA: 1s - loss: 0.6825 - accuracy: 0.56 - ETA: 1s - loss: 0.6807 - accuracy: 0.57 - ETA: 1s - loss: 0.6785 - accuracy: 0.57 - ETA: 1s - loss: 0.6767 - accuracy: 0.58 - ETA: 1s - loss: 0.6744 - accuracy: 0.58 - ETA: 1s - loss: 0.6725 - accuracy: 0.58 - ETA: 1s - loss: 0.6701 - accuracy: 0.59 - ETA: 0s - loss: 0.6675 - accuracy: 0.59 - ETA: 0s - loss: 0.6648 - accuracy: 0.59 - ETA: 0s - loss: 0.6637 - accuracy: 0.60 - ETA: 0s - loss: 0.6621 - accuracy: 0.60 - ETA: 0s - loss: 0.6610 - accuracy: 0.60 - ETA: 0s - loss: 0.6600 - accuracy: 0.61 - ETA: 0s - loss: 0.6581 - accuracy: 0.61 - ETA: 0s - loss: 0.6565 - accuracy: 0.61 - ETA: 0s - loss: 0.6550 - accuracy: 0.61 - ETA: 0s - loss: 0.6536 - accuracy: 0.62 - ETA: 0s - loss: 0.6515 - accuracy: 0.62 - ETA: 0s - loss: 0.6511 - accuracy: 0.62 - ETA: 0s - loss: 0.6501 - accuracy: 0.62 - ETA: 0s - loss: 0.6490 - accuracy: 0.62 - ETA: 0s - loss: 0.6473 - accuracy: 0.62 - ETA: 0s - loss: 0.6461 - accuracy: 0.63 - ETA: 0s - loss: 0.6453 - accuracy: 0.63 - 2s 149us/step - loss: 0.6438 - accuracy: 0.6331 - val_loss: 0.5840 - val_accuracy: 0.7180\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:17 - loss: 0.6835 - accuracy: 0.87 - ETA: 5s - loss: 0.6925 - accuracy: 0.5575 - ETA: 3s - loss: 0.6918 - accuracy: 0.54 - ETA: 2s - loss: 0.6911 - accuracy: 0.55 - ETA: 2s - loss: 0.6906 - accuracy: 0.56 - ETA: 1s - loss: 0.6895 - accuracy: 0.56 - ETA: 1s - loss: 0.6875 - accuracy: 0.56 - ETA: 1s - loss: 0.6867 - accuracy: 0.57 - ETA: 1s - loss: 0.6845 - accuracy: 0.58 - ETA: 1s - loss: 0.6833 - accuracy: 0.58 - ETA: 1s - loss: 0.6813 - accuracy: 0.59 - ETA: 1s - loss: 0.6789 - accuracy: 0.59 - ETA: 1s - loss: 0.6765 - accuracy: 0.60 - ETA: 1s - loss: 0.6731 - accuracy: 0.60 - ETA: 1s - loss: 0.6707 - accuracy: 0.61 - ETA: 0s - loss: 0.6695 - accuracy: 0.61 - ETA: 0s - loss: 0.6670 - accuracy: 0.61 - ETA: 0s - loss: 0.6642 - accuracy: 0.62 - ETA: 0s - loss: 0.6622 - accuracy: 0.62 - ETA: 0s - loss: 0.6610 - accuracy: 0.62 - ETA: 0s - loss: 0.6593 - accuracy: 0.62 - ETA: 0s - loss: 0.6576 - accuracy: 0.62 - ETA: 0s - loss: 0.6563 - accuracy: 0.62 - ETA: 0s - loss: 0.6558 - accuracy: 0.62 - ETA: 0s - loss: 0.6546 - accuracy: 0.63 - ETA: 0s - loss: 0.6541 - accuracy: 0.63 - ETA: 0s - loss: 0.6529 - accuracy: 0.63 - ETA: 0s - loss: 0.6507 - accuracy: 0.63 - ETA: 0s - loss: 0.6490 - accuracy: 0.63 - ETA: 0s - loss: 0.6481 - accuracy: 0.63 - ETA: 0s - loss: 0.6470 - accuracy: 0.64 - ETA: 0s - loss: 0.6462 - accuracy: 0.64 - ETA: 0s - loss: 0.6457 - accuracy: 0.64 - 2s 149us/step - loss: 0.6455 - accuracy: 0.6443 - val_loss: 0.5963 - val_accuracy: 0.7202\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:08 - loss: 0.6907 - accuracy: 0.50 - ETA: 4s - loss: 0.6942 - accuracy: 0.5144 - ETA: 3s - loss: 0.6927 - accuracy: 0.53 - ETA: 2s - loss: 0.6916 - accuracy: 0.52 - ETA: 2s - loss: 0.6894 - accuracy: 0.54 - ETA: 1s - loss: 0.6877 - accuracy: 0.54 - ETA: 1s - loss: 0.6861 - accuracy: 0.54 - ETA: 1s - loss: 0.6835 - accuracy: 0.55 - ETA: 1s - loss: 0.6807 - accuracy: 0.56 - ETA: 1s - loss: 0.6796 - accuracy: 0.56 - ETA: 1s - loss: 0.6773 - accuracy: 0.57 - ETA: 1s - loss: 0.6749 - accuracy: 0.57 - ETA: 1s - loss: 0.6724 - accuracy: 0.58 - ETA: 1s - loss: 0.6693 - accuracy: 0.58 - ETA: 1s - loss: 0.6657 - accuracy: 0.59 - ETA: 0s - loss: 0.6631 - accuracy: 0.59 - ETA: 0s - loss: 0.6615 - accuracy: 0.60 - ETA: 0s - loss: 0.6590 - accuracy: 0.60 - ETA: 0s - loss: 0.6575 - accuracy: 0.60 - ETA: 0s - loss: 0.6559 - accuracy: 0.61 - ETA: 0s - loss: 0.6550 - accuracy: 0.61 - ETA: 0s - loss: 0.6538 - accuracy: 0.61 - ETA: 0s - loss: 0.6522 - accuracy: 0.61 - ETA: 0s - loss: 0.6507 - accuracy: 0.61 - ETA: 0s - loss: 0.6493 - accuracy: 0.62 - ETA: 0s - loss: 0.6480 - accuracy: 0.62 - ETA: 0s - loss: 0.6472 - accuracy: 0.62 - ETA: 0s - loss: 0.6460 - accuracy: 0.62 - ETA: 0s - loss: 0.6449 - accuracy: 0.62 - ETA: 0s - loss: 0.6438 - accuracy: 0.63 - ETA: 0s - loss: 0.6439 - accuracy: 0.63 - ETA: 0s - loss: 0.6426 - accuracy: 0.63 - ETA: 0s - loss: 0.6414 - accuracy: 0.63 - 2s 147us/step - loss: 0.6414 - accuracy: 0.6352 - val_loss: 0.5915 - val_accuracy: 0.7138\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 74us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:09 - loss: 0.6938 - accuracy: 0.62 - ETA: 5s - loss: 0.6934 - accuracy: 0.5264 - ETA: 3s - loss: 0.6937 - accuracy: 0.52 - ETA: 2s - loss: 0.6932 - accuracy: 0.53 - ETA: 2s - loss: 0.6921 - accuracy: 0.53 - ETA: 1s - loss: 0.6911 - accuracy: 0.54 - ETA: 1s - loss: 0.6901 - accuracy: 0.54 - ETA: 1s - loss: 0.6865 - accuracy: 0.55 - ETA: 1s - loss: 0.6847 - accuracy: 0.56 - ETA: 1s - loss: 0.6833 - accuracy: 0.56 - ETA: 1s - loss: 0.6811 - accuracy: 0.57 - ETA: 1s - loss: 0.6784 - accuracy: 0.57 - ETA: 1s - loss: 0.6758 - accuracy: 0.58 - ETA: 1s - loss: 0.6737 - accuracy: 0.58 - ETA: 1s - loss: 0.6694 - accuracy: 0.59 - ETA: 0s - loss: 0.6672 - accuracy: 0.59 - ETA: 0s - loss: 0.6641 - accuracy: 0.60 - ETA: 0s - loss: 0.6631 - accuracy: 0.60 - ETA: 0s - loss: 0.6615 - accuracy: 0.60 - ETA: 0s - loss: 0.6595 - accuracy: 0.61 - ETA: 0s - loss: 0.6572 - accuracy: 0.61 - ETA: 0s - loss: 0.6562 - accuracy: 0.61 - ETA: 0s - loss: 0.6544 - accuracy: 0.61 - ETA: 0s - loss: 0.6523 - accuracy: 0.62 - ETA: 0s - loss: 0.6515 - accuracy: 0.62 - ETA: 0s - loss: 0.6507 - accuracy: 0.62 - ETA: 0s - loss: 0.6496 - accuracy: 0.62 - ETA: 0s - loss: 0.6477 - accuracy: 0.62 - ETA: 0s - loss: 0.6470 - accuracy: 0.63 - ETA: 0s - loss: 0.6461 - accuracy: 0.63 - ETA: 0s - loss: 0.6451 - accuracy: 0.63 - 2s 142us/step - loss: 0.6437 - accuracy: 0.6343 - val_loss: 0.5856 - val_accuracy: 0.7209\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:08 - loss: 0.6953 - accuracy: 0.25 - ETA: 5s - loss: 0.6944 - accuracy: 0.5025 - ETA: 3s - loss: 0.6943 - accuracy: 0.50 - ETA: 2s - loss: 0.6940 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 2s - loss: 0.6934 - accuracy: 0.50 - ETA: 1s - loss: 0.6932 - accuracy: 0.50 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6921 - accuracy: 0.51 - ETA: 1s - loss: 0.6915 - accuracy: 0.52 - ETA: 1s - loss: 0.6907 - accuracy: 0.53 - ETA: 1s - loss: 0.6896 - accuracy: 0.53 - ETA: 1s - loss: 0.6882 - accuracy: 0.54 - ETA: 1s - loss: 0.6874 - accuracy: 0.54 - ETA: 1s - loss: 0.6865 - accuracy: 0.54 - ETA: 1s - loss: 0.6852 - accuracy: 0.54 - ETA: 0s - loss: 0.6834 - accuracy: 0.54 - ETA: 0s - loss: 0.6821 - accuracy: 0.55 - ETA: 0s - loss: 0.6797 - accuracy: 0.55 - ETA: 0s - loss: 0.6784 - accuracy: 0.56 - ETA: 0s - loss: 0.6773 - accuracy: 0.56 - ETA: 0s - loss: 0.6755 - accuracy: 0.56 - ETA: 0s - loss: 0.6740 - accuracy: 0.56 - ETA: 0s - loss: 0.6736 - accuracy: 0.57 - ETA: 0s - loss: 0.6723 - accuracy: 0.57 - ETA: 0s - loss: 0.6709 - accuracy: 0.57 - ETA: 0s - loss: 0.6696 - accuracy: 0.57 - ETA: 0s - loss: 0.6688 - accuracy: 0.57 - ETA: 0s - loss: 0.6667 - accuracy: 0.58 - ETA: 0s - loss: 0.6658 - accuracy: 0.58 - ETA: 0s - loss: 0.6652 - accuracy: 0.58 - ETA: 0s - loss: 0.6643 - accuracy: 0.58 - ETA: 0s - loss: 0.6635 - accuracy: 0.58 - 2s 147us/step - loss: 0.6636 - accuracy: 0.5872 - val_loss: 0.6174 - val_accuracy: 0.7145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 77us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:08 - loss: 0.6944 - accuracy: 0.25 - ETA: 5s - loss: 0.6957 - accuracy: 0.4632 - ETA: 3s - loss: 0.6947 - accuracy: 0.48 - ETA: 2s - loss: 0.6942 - accuracy: 0.49 - ETA: 2s - loss: 0.6928 - accuracy: 0.51 - ETA: 1s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6887 - accuracy: 0.53 - ETA: 1s - loss: 0.6859 - accuracy: 0.55 - ETA: 1s - loss: 0.6821 - accuracy: 0.56 - ETA: 1s - loss: 0.6785 - accuracy: 0.57 - ETA: 1s - loss: 0.6751 - accuracy: 0.57 - ETA: 1s - loss: 0.6732 - accuracy: 0.58 - ETA: 1s - loss: 0.6711 - accuracy: 0.58 - ETA: 1s - loss: 0.6668 - accuracy: 0.59 - ETA: 0s - loss: 0.6640 - accuracy: 0.60 - ETA: 0s - loss: 0.6605 - accuracy: 0.60 - ETA: 0s - loss: 0.6594 - accuracy: 0.60 - ETA: 0s - loss: 0.6581 - accuracy: 0.60 - ETA: 0s - loss: 0.6553 - accuracy: 0.61 - ETA: 0s - loss: 0.6518 - accuracy: 0.61 - ETA: 0s - loss: 0.6497 - accuracy: 0.62 - ETA: 0s - loss: 0.6495 - accuracy: 0.62 - ETA: 0s - loss: 0.6475 - accuracy: 0.62 - ETA: 0s - loss: 0.6469 - accuracy: 0.62 - ETA: 0s - loss: 0.6456 - accuracy: 0.62 - ETA: 0s - loss: 0.6449 - accuracy: 0.63 - ETA: 0s - loss: 0.6439 - accuracy: 0.63 - ETA: 0s - loss: 0.6423 - accuracy: 0.63 - ETA: 0s - loss: 0.6419 - accuracy: 0.63 - ETA: 0s - loss: 0.6411 - accuracy: 0.63 - ETA: 0s - loss: 0.6406 - accuracy: 0.63 - 2s 142us/step - loss: 0.6397 - accuracy: 0.6397 - val_loss: 0.5829 - val_accuracy: 0.7244\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:03 - loss: 0.6980 - accuracy: 0.37 - ETA: 4s - loss: 0.6925 - accuracy: 0.5392 - ETA: 3s - loss: 0.6930 - accuracy: 0.52 - ETA: 2s - loss: 0.6903 - accuracy: 0.54 - ETA: 2s - loss: 0.6882 - accuracy: 0.56 - ETA: 2s - loss: 0.6863 - accuracy: 0.56 - ETA: 1s - loss: 0.6822 - accuracy: 0.57 - ETA: 1s - loss: 0.6814 - accuracy: 0.58 - ETA: 1s - loss: 0.6795 - accuracy: 0.58 - ETA: 1s - loss: 0.6770 - accuracy: 0.59 - ETA: 1s - loss: 0.6746 - accuracy: 0.59 - ETA: 1s - loss: 0.6719 - accuracy: 0.60 - ETA: 1s - loss: 0.6697 - accuracy: 0.60 - ETA: 1s - loss: 0.6659 - accuracy: 0.61 - ETA: 1s - loss: 0.6622 - accuracy: 0.61 - ETA: 0s - loss: 0.6605 - accuracy: 0.62 - ETA: 0s - loss: 0.6593 - accuracy: 0.62 - ETA: 0s - loss: 0.6584 - accuracy: 0.62 - ETA: 0s - loss: 0.6576 - accuracy: 0.62 - ETA: 0s - loss: 0.6555 - accuracy: 0.63 - ETA: 0s - loss: 0.6531 - accuracy: 0.63 - ETA: 0s - loss: 0.6519 - accuracy: 0.63 - ETA: 0s - loss: 0.6498 - accuracy: 0.63 - ETA: 0s - loss: 0.6495 - accuracy: 0.64 - ETA: 0s - loss: 0.6474 - accuracy: 0.64 - ETA: 0s - loss: 0.6456 - accuracy: 0.64 - ETA: 0s - loss: 0.6453 - accuracy: 0.64 - ETA: 0s - loss: 0.6433 - accuracy: 0.64 - ETA: 0s - loss: 0.6425 - accuracy: 0.64 - ETA: 0s - loss: 0.6417 - accuracy: 0.65 - ETA: 0s - loss: 0.6412 - accuracy: 0.65 - ETA: 0s - loss: 0.6397 - accuracy: 0.65 - ETA: 0s - loss: 0.6391 - accuracy: 0.65 - 2s 147us/step - loss: 0.6387 - accuracy: 0.6537 - val_loss: 0.5835 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:06 - loss: 0.6950 - accuracy: 0.50 - ETA: 5s - loss: 0.6944 - accuracy: 0.4951 - ETA: 3s - loss: 0.6942 - accuracy: 0.51 - ETA: 2s - loss: 0.6938 - accuracy: 0.51 - ETA: 2s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6892 - accuracy: 0.53 - ETA: 1s - loss: 0.6866 - accuracy: 0.55 - ETA: 1s - loss: 0.6847 - accuracy: 0.55 - ETA: 1s - loss: 0.6829 - accuracy: 0.56 - ETA: 1s - loss: 0.6808 - accuracy: 0.57 - ETA: 1s - loss: 0.6777 - accuracy: 0.57 - ETA: 1s - loss: 0.6763 - accuracy: 0.58 - ETA: 1s - loss: 0.6730 - accuracy: 0.58 - ETA: 1s - loss: 0.6701 - accuracy: 0.59 - ETA: 1s - loss: 0.6657 - accuracy: 0.59 - ETA: 0s - loss: 0.6628 - accuracy: 0.60 - ETA: 0s - loss: 0.6620 - accuracy: 0.60 - ETA: 0s - loss: 0.6597 - accuracy: 0.61 - ETA: 0s - loss: 0.6580 - accuracy: 0.61 - ETA: 0s - loss: 0.6562 - accuracy: 0.61 - ETA: 0s - loss: 0.6543 - accuracy: 0.61 - ETA: 0s - loss: 0.6528 - accuracy: 0.62 - ETA: 0s - loss: 0.6517 - accuracy: 0.62 - ETA: 0s - loss: 0.6508 - accuracy: 0.62 - ETA: 0s - loss: 0.6477 - accuracy: 0.62 - ETA: 0s - loss: 0.6471 - accuracy: 0.63 - ETA: 0s - loss: 0.6462 - accuracy: 0.63 - ETA: 0s - loss: 0.6449 - accuracy: 0.63 - ETA: 0s - loss: 0.6429 - accuracy: 0.63 - ETA: 0s - loss: 0.6402 - accuracy: 0.64 - ETA: 0s - loss: 0.6379 - accuracy: 0.64 - ETA: 0s - loss: 0.6366 - accuracy: 0.64 - 2s 146us/step - loss: 0.6358 - accuracy: 0.6460 - val_loss: 0.5790 - val_accuracy: 0.7195\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:06 - loss: 0.6926 - accuracy: 0.62 - ETA: 5s - loss: 0.6936 - accuracy: 0.5123 - ETA: 3s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6906 - accuracy: 0.53 - ETA: 2s - loss: 0.6868 - accuracy: 0.54 - ETA: 2s - loss: 0.6865 - accuracy: 0.54 - ETA: 1s - loss: 0.6841 - accuracy: 0.55 - ETA: 1s - loss: 0.6817 - accuracy: 0.56 - ETA: 1s - loss: 0.6771 - accuracy: 0.57 - ETA: 1s - loss: 0.6732 - accuracy: 0.58 - ETA: 1s - loss: 0.6714 - accuracy: 0.58 - ETA: 1s - loss: 0.6716 - accuracy: 0.58 - ETA: 1s - loss: 0.6702 - accuracy: 0.58 - ETA: 1s - loss: 0.6664 - accuracy: 0.59 - ETA: 1s - loss: 0.6617 - accuracy: 0.60 - ETA: 1s - loss: 0.6598 - accuracy: 0.60 - ETA: 0s - loss: 0.6581 - accuracy: 0.61 - ETA: 0s - loss: 0.6553 - accuracy: 0.61 - ETA: 0s - loss: 0.6544 - accuracy: 0.61 - ETA: 0s - loss: 0.6526 - accuracy: 0.62 - ETA: 0s - loss: 0.6516 - accuracy: 0.62 - ETA: 0s - loss: 0.6516 - accuracy: 0.62 - ETA: 0s - loss: 0.6497 - accuracy: 0.62 - ETA: 0s - loss: 0.6476 - accuracy: 0.63 - ETA: 0s - loss: 0.6463 - accuracy: 0.63 - ETA: 0s - loss: 0.6444 - accuracy: 0.63 - ETA: 0s - loss: 0.6433 - accuracy: 0.63 - ETA: 0s - loss: 0.6410 - accuracy: 0.64 - ETA: 0s - loss: 0.6389 - accuracy: 0.64 - ETA: 0s - loss: 0.6383 - accuracy: 0.64 - ETA: 0s - loss: 0.6375 - accuracy: 0.64 - ETA: 0s - loss: 0.6369 - accuracy: 0.64 - ETA: 0s - loss: 0.6356 - accuracy: 0.64 - 2s 147us/step - loss: 0.6350 - accuracy: 0.6490 - val_loss: 0.5810 - val_accuracy: 0.7280\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:28 - loss: 0.6965 - accuracy: 0.50 - ETA: 5s - loss: 0.6932 - accuracy: 0.5346 - ETA: 3s - loss: 0.6913 - accuracy: 0.54 - ETA: 2s - loss: 0.6892 - accuracy: 0.55 - ETA: 2s - loss: 0.6881 - accuracy: 0.54 - ETA: 2s - loss: 0.6870 - accuracy: 0.54 - ETA: 1s - loss: 0.6840 - accuracy: 0.56 - ETA: 1s - loss: 0.6796 - accuracy: 0.57 - ETA: 1s - loss: 0.6758 - accuracy: 0.58 - ETA: 1s - loss: 0.6730 - accuracy: 0.58 - ETA: 1s - loss: 0.6692 - accuracy: 0.59 - ETA: 1s - loss: 0.6652 - accuracy: 0.60 - ETA: 1s - loss: 0.6634 - accuracy: 0.60 - ETA: 1s - loss: 0.6583 - accuracy: 0.61 - ETA: 1s - loss: 0.6560 - accuracy: 0.61 - ETA: 1s - loss: 0.6558 - accuracy: 0.61 - ETA: 0s - loss: 0.6541 - accuracy: 0.62 - ETA: 0s - loss: 0.6532 - accuracy: 0.62 - ETA: 0s - loss: 0.6498 - accuracy: 0.63 - ETA: 0s - loss: 0.6471 - accuracy: 0.63 - ETA: 0s - loss: 0.6453 - accuracy: 0.63 - ETA: 0s - loss: 0.6453 - accuracy: 0.63 - ETA: 0s - loss: 0.6437 - accuracy: 0.63 - ETA: 0s - loss: 0.6419 - accuracy: 0.64 - ETA: 0s - loss: 0.6394 - accuracy: 0.64 - ETA: 0s - loss: 0.6384 - accuracy: 0.64 - ETA: 0s - loss: 0.6381 - accuracy: 0.64 - ETA: 0s - loss: 0.6367 - accuracy: 0.64 - ETA: 0s - loss: 0.6363 - accuracy: 0.65 - ETA: 0s - loss: 0.6356 - accuracy: 0.65 - ETA: 0s - loss: 0.6344 - accuracy: 0.65 - ETA: 0s - loss: 0.6341 - accuracy: 0.65 - ETA: 0s - loss: 0.6331 - accuracy: 0.65 - 2s 151us/step - loss: 0.6325 - accuracy: 0.6563 - val_loss: 0.5833 - val_accuracy: 0.7251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:05 - loss: 0.6946 - accuracy: 0.37 - ETA: 5s - loss: 0.6950 - accuracy: 0.5075 - ETA: 3s - loss: 0.6942 - accuracy: 0.52 - ETA: 2s - loss: 0.6932 - accuracy: 0.52 - ETA: 2s - loss: 0.6916 - accuracy: 0.54 - ETA: 2s - loss: 0.6888 - accuracy: 0.56 - ETA: 1s - loss: 0.6848 - accuracy: 0.57 - ETA: 1s - loss: 0.6819 - accuracy: 0.58 - ETA: 1s - loss: 0.6797 - accuracy: 0.58 - ETA: 1s - loss: 0.6787 - accuracy: 0.58 - ETA: 1s - loss: 0.6721 - accuracy: 0.59 - ETA: 1s - loss: 0.6685 - accuracy: 0.60 - ETA: 1s - loss: 0.6663 - accuracy: 0.60 - ETA: 1s - loss: 0.6637 - accuracy: 0.61 - ETA: 1s - loss: 0.6617 - accuracy: 0.61 - ETA: 1s - loss: 0.6595 - accuracy: 0.62 - ETA: 1s - loss: 0.6580 - accuracy: 0.62 - ETA: 1s - loss: 0.6565 - accuracy: 0.62 - ETA: 1s - loss: 0.6560 - accuracy: 0.62 - ETA: 0s - loss: 0.6520 - accuracy: 0.63 - ETA: 0s - loss: 0.6496 - accuracy: 0.63 - ETA: 0s - loss: 0.6460 - accuracy: 0.64 - ETA: 0s - loss: 0.6449 - accuracy: 0.64 - ETA: 0s - loss: 0.6446 - accuracy: 0.64 - ETA: 0s - loss: 0.6430 - accuracy: 0.64 - ETA: 0s - loss: 0.6412 - accuracy: 0.64 - ETA: 0s - loss: 0.6410 - accuracy: 0.64 - ETA: 0s - loss: 0.6397 - accuracy: 0.65 - ETA: 0s - loss: 0.6390 - accuracy: 0.65 - ETA: 0s - loss: 0.6385 - accuracy: 0.65 - ETA: 0s - loss: 0.6366 - accuracy: 0.65 - ETA: 0s - loss: 0.6350 - accuracy: 0.65 - ETA: 0s - loss: 0.6347 - accuracy: 0.65 - ETA: 0s - loss: 0.6336 - accuracy: 0.65 - ETA: 0s - loss: 0.6331 - accuracy: 0.65 - 2s 156us/step - loss: 0.6329 - accuracy: 0.6591 - val_loss: 0.5875 - val_accuracy: 0.7195\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:09 - loss: 0.6922 - accuracy: 0.62 - ETA: 5s - loss: 0.6932 - accuracy: 0.5399 - ETA: 3s - loss: 0.6926 - accuracy: 0.53 - ETA: 2s - loss: 0.6910 - accuracy: 0.54 - ETA: 2s - loss: 0.6910 - accuracy: 0.54 - ETA: 2s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6904 - accuracy: 0.53 - ETA: 1s - loss: 0.6903 - accuracy: 0.53 - ETA: 1s - loss: 0.6903 - accuracy: 0.53 - ETA: 1s - loss: 0.6898 - accuracy: 0.53 - ETA: 1s - loss: 0.6893 - accuracy: 0.54 - ETA: 1s - loss: 0.6885 - accuracy: 0.54 - ETA: 1s - loss: 0.6878 - accuracy: 0.55 - ETA: 1s - loss: 0.6876 - accuracy: 0.55 - ETA: 1s - loss: 0.6870 - accuracy: 0.55 - ETA: 0s - loss: 0.6860 - accuracy: 0.56 - ETA: 0s - loss: 0.6848 - accuracy: 0.56 - ETA: 0s - loss: 0.6840 - accuracy: 0.56 - ETA: 0s - loss: 0.6829 - accuracy: 0.57 - ETA: 0s - loss: 0.6823 - accuracy: 0.57 - ETA: 0s - loss: 0.6810 - accuracy: 0.57 - ETA: 0s - loss: 0.6801 - accuracy: 0.57 - ETA: 0s - loss: 0.6793 - accuracy: 0.57 - ETA: 0s - loss: 0.6782 - accuracy: 0.58 - ETA: 0s - loss: 0.6772 - accuracy: 0.58 - ETA: 0s - loss: 0.6761 - accuracy: 0.58 - ETA: 0s - loss: 0.6757 - accuracy: 0.58 - ETA: 0s - loss: 0.6742 - accuracy: 0.58 - ETA: 0s - loss: 0.6733 - accuracy: 0.58 - ETA: 0s - loss: 0.6727 - accuracy: 0.58 - ETA: 0s - loss: 0.6722 - accuracy: 0.59 - ETA: 0s - loss: 0.6712 - accuracy: 0.59 - 2s 145us/step - loss: 0.6707 - accuracy: 0.5927 - val_loss: 0.6284 - val_accuracy: 0.7195\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 57us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:06 - loss: 0.6949 - accuracy: 0.37 - ETA: 5s - loss: 0.6937 - accuracy: 0.5128 - ETA: 3s - loss: 0.6934 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6905 - accuracy: 0.53 - ETA: 1s - loss: 0.6899 - accuracy: 0.53 - ETA: 1s - loss: 0.6896 - accuracy: 0.53 - ETA: 1s - loss: 0.6893 - accuracy: 0.53 - ETA: 1s - loss: 0.6884 - accuracy: 0.54 - ETA: 1s - loss: 0.6871 - accuracy: 0.54 - ETA: 1s - loss: 0.6865 - accuracy: 0.54 - ETA: 1s - loss: 0.6853 - accuracy: 0.54 - ETA: 0s - loss: 0.6850 - accuracy: 0.54 - ETA: 0s - loss: 0.6839 - accuracy: 0.55 - ETA: 0s - loss: 0.6830 - accuracy: 0.55 - ETA: 0s - loss: 0.6818 - accuracy: 0.55 - ETA: 0s - loss: 0.6804 - accuracy: 0.56 - ETA: 0s - loss: 0.6793 - accuracy: 0.56 - ETA: 0s - loss: 0.6788 - accuracy: 0.56 - ETA: 0s - loss: 0.6782 - accuracy: 0.56 - ETA: 0s - loss: 0.6775 - accuracy: 0.56 - ETA: 0s - loss: 0.6763 - accuracy: 0.56 - ETA: 0s - loss: 0.6754 - accuracy: 0.57 - ETA: 0s - loss: 0.6745 - accuracy: 0.57 - ETA: 0s - loss: 0.6732 - accuracy: 0.57 - ETA: 0s - loss: 0.6722 - accuracy: 0.57 - ETA: 0s - loss: 0.6718 - accuracy: 0.57 - ETA: 0s - loss: 0.6706 - accuracy: 0.57 - ETA: 0s - loss: 0.6703 - accuracy: 0.58 - 2s 153us/step - loss: 0.6697 - accuracy: 0.5816 - val_loss: 0.6257 - val_accuracy: 0.7102\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 61us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:57 - loss: 0.6932 - accuracy: 0.50 - ETA: 4s - loss: 0.6939 - accuracy: 0.4722 - ETA: 2s - loss: 0.6934 - accuracy: 0.49 - ETA: 2s - loss: 0.6934 - accuracy: 0.48 - ETA: 2s - loss: 0.6933 - accuracy: 0.50 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6929 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.53 - ETA: 1s - loss: 0.6920 - accuracy: 0.54 - ETA: 1s - loss: 0.6913 - accuracy: 0.54 - ETA: 1s - loss: 0.6909 - accuracy: 0.55 - ETA: 1s - loss: 0.6907 - accuracy: 0.55 - ETA: 1s - loss: 0.6901 - accuracy: 0.55 - ETA: 1s - loss: 0.6894 - accuracy: 0.56 - ETA: 1s - loss: 0.6889 - accuracy: 0.57 - ETA: 0s - loss: 0.6882 - accuracy: 0.57 - ETA: 0s - loss: 0.6873 - accuracy: 0.58 - ETA: 0s - loss: 0.6865 - accuracy: 0.58 - ETA: 0s - loss: 0.6857 - accuracy: 0.59 - ETA: 0s - loss: 0.6846 - accuracy: 0.59 - ETA: 0s - loss: 0.6832 - accuracy: 0.59 - ETA: 0s - loss: 0.6824 - accuracy: 0.60 - ETA: 0s - loss: 0.6814 - accuracy: 0.60 - ETA: 0s - loss: 0.6806 - accuracy: 0.60 - ETA: 0s - loss: 0.6796 - accuracy: 0.60 - ETA: 0s - loss: 0.6790 - accuracy: 0.60 - ETA: 0s - loss: 0.6776 - accuracy: 0.61 - ETA: 0s - loss: 0.6769 - accuracy: 0.61 - ETA: 0s - loss: 0.6759 - accuracy: 0.61 - ETA: 0s - loss: 0.6754 - accuracy: 0.61 - ETA: 0s - loss: 0.6739 - accuracy: 0.61 - ETA: 0s - loss: 0.6730 - accuracy: 0.61 - 2s 143us/step - loss: 0.6730 - accuracy: 0.6182 - val_loss: 0.6364 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 54us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:48 - loss: 0.6825 - accuracy: 0.75 - ETA: 4s - loss: 0.6937 - accuracy: 0.5172 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6922 - accuracy: 0.50 - ETA: 2s - loss: 0.6915 - accuracy: 0.51 - ETA: 1s - loss: 0.6912 - accuracy: 0.50 - ETA: 1s - loss: 0.6909 - accuracy: 0.52 - ETA: 1s - loss: 0.6905 - accuracy: 0.53 - ETA: 1s - loss: 0.6897 - accuracy: 0.54 - ETA: 1s - loss: 0.6892 - accuracy: 0.55 - ETA: 1s - loss: 0.6882 - accuracy: 0.56 - ETA: 1s - loss: 0.6876 - accuracy: 0.57 - ETA: 1s - loss: 0.6867 - accuracy: 0.57 - ETA: 1s - loss: 0.6861 - accuracy: 0.58 - ETA: 1s - loss: 0.6852 - accuracy: 0.59 - ETA: 0s - loss: 0.6842 - accuracy: 0.59 - ETA: 0s - loss: 0.6824 - accuracy: 0.59 - ETA: 0s - loss: 0.6817 - accuracy: 0.60 - ETA: 0s - loss: 0.6809 - accuracy: 0.60 - ETA: 0s - loss: 0.6797 - accuracy: 0.60 - ETA: 0s - loss: 0.6791 - accuracy: 0.60 - ETA: 0s - loss: 0.6783 - accuracy: 0.60 - ETA: 0s - loss: 0.6768 - accuracy: 0.61 - ETA: 0s - loss: 0.6757 - accuracy: 0.61 - ETA: 0s - loss: 0.6752 - accuracy: 0.61 - ETA: 0s - loss: 0.6732 - accuracy: 0.61 - ETA: 0s - loss: 0.6723 - accuracy: 0.62 - ETA: 0s - loss: 0.6709 - accuracy: 0.62 - ETA: 0s - loss: 0.6700 - accuracy: 0.62 - ETA: 0s - loss: 0.6689 - accuracy: 0.62 - ETA: 0s - loss: 0.6679 - accuracy: 0.62 - ETA: 0s - loss: 0.6670 - accuracy: 0.63 - 2s 143us/step - loss: 0.6668 - accuracy: 0.6313 - val_loss: 0.6284 - val_accuracy: 0.7145\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:59 - loss: 0.6928 - accuracy: 0.37 - ETA: 4s - loss: 0.6919 - accuracy: 0.5377 - ETA: 3s - loss: 0.6914 - accuracy: 0.53 - ETA: 2s - loss: 0.6917 - accuracy: 0.51 - ETA: 2s - loss: 0.6908 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 1s - loss: 0.6897 - accuracy: 0.55 - ETA: 1s - loss: 0.6891 - accuracy: 0.56 - ETA: 1s - loss: 0.6884 - accuracy: 0.56 - ETA: 1s - loss: 0.6881 - accuracy: 0.56 - ETA: 1s - loss: 0.6871 - accuracy: 0.57 - ETA: 1s - loss: 0.6861 - accuracy: 0.57 - ETA: 1s - loss: 0.6847 - accuracy: 0.58 - ETA: 1s - loss: 0.6838 - accuracy: 0.58 - ETA: 0s - loss: 0.6823 - accuracy: 0.58 - ETA: 0s - loss: 0.6808 - accuracy: 0.58 - ETA: 0s - loss: 0.6795 - accuracy: 0.58 - ETA: 0s - loss: 0.6774 - accuracy: 0.59 - ETA: 0s - loss: 0.6764 - accuracy: 0.59 - ETA: 0s - loss: 0.6754 - accuracy: 0.59 - ETA: 0s - loss: 0.6750 - accuracy: 0.59 - ETA: 0s - loss: 0.6741 - accuracy: 0.59 - ETA: 0s - loss: 0.6731 - accuracy: 0.59 - ETA: 0s - loss: 0.6717 - accuracy: 0.59 - ETA: 0s - loss: 0.6705 - accuracy: 0.60 - ETA: 0s - loss: 0.6687 - accuracy: 0.60 - ETA: 0s - loss: 0.6676 - accuracy: 0.60 - ETA: 0s - loss: 0.6664 - accuracy: 0.60 - ETA: 0s - loss: 0.6652 - accuracy: 0.60 - ETA: 0s - loss: 0.6644 - accuracy: 0.61 - ETA: 0s - loss: 0.6633 - accuracy: 0.61 - 2s 141us/step - loss: 0.6636 - accuracy: 0.6109 - val_loss: 0.6196 - val_accuracy: 0.7230\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:54 - loss: 0.6943 - accuracy: 0.62 - ETA: 4s - loss: 0.6937 - accuracy: 0.4828 - ETA: 3s - loss: 0.6935 - accuracy: 0.50 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6929 - accuracy: 0.52 - ETA: 1s - loss: 0.6926 - accuracy: 0.53 - ETA: 1s - loss: 0.6923 - accuracy: 0.53 - ETA: 1s - loss: 0.6921 - accuracy: 0.53 - ETA: 1s - loss: 0.6918 - accuracy: 0.54 - ETA: 1s - loss: 0.6915 - accuracy: 0.54 - ETA: 1s - loss: 0.6909 - accuracy: 0.54 - ETA: 1s - loss: 0.6902 - accuracy: 0.55 - ETA: 1s - loss: 0.6892 - accuracy: 0.55 - ETA: 1s - loss: 0.6884 - accuracy: 0.56 - ETA: 1s - loss: 0.6880 - accuracy: 0.56 - ETA: 0s - loss: 0.6870 - accuracy: 0.56 - ETA: 0s - loss: 0.6856 - accuracy: 0.57 - ETA: 0s - loss: 0.6847 - accuracy: 0.57 - ETA: 0s - loss: 0.6842 - accuracy: 0.57 - ETA: 0s - loss: 0.6832 - accuracy: 0.57 - ETA: 0s - loss: 0.6827 - accuracy: 0.58 - ETA: 0s - loss: 0.6819 - accuracy: 0.58 - ETA: 0s - loss: 0.6809 - accuracy: 0.58 - ETA: 0s - loss: 0.6796 - accuracy: 0.58 - ETA: 0s - loss: 0.6786 - accuracy: 0.58 - ETA: 0s - loss: 0.6772 - accuracy: 0.59 - ETA: 0s - loss: 0.6763 - accuracy: 0.59 - ETA: 0s - loss: 0.6750 - accuracy: 0.59 - ETA: 0s - loss: 0.6737 - accuracy: 0.59 - ETA: 0s - loss: 0.6726 - accuracy: 0.60 - ETA: 0s - loss: 0.6714 - accuracy: 0.60 - 2s 145us/step - loss: 0.6704 - accuracy: 0.6030 - val_loss: 0.6282 - val_accuracy: 0.7081\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:13 - loss: 0.6932 - accuracy: 0.50 - ETA: 4s - loss: 0.6922 - accuracy: 0.5189 - ETA: 4s - loss: 0.6923 - accuracy: 0.51 - ETA: 3s - loss: 0.6926 - accuracy: 0.51 - ETA: 2s - loss: 0.6916 - accuracy: 0.51 - ETA: 2s - loss: 0.6901 - accuracy: 0.51 - ETA: 1s - loss: 0.6883 - accuracy: 0.52 - ETA: 1s - loss: 0.6873 - accuracy: 0.52 - ETA: 1s - loss: 0.6858 - accuracy: 0.53 - ETA: 1s - loss: 0.6846 - accuracy: 0.54 - ETA: 1s - loss: 0.6833 - accuracy: 0.54 - ETA: 1s - loss: 0.6820 - accuracy: 0.55 - ETA: 1s - loss: 0.6803 - accuracy: 0.56 - ETA: 1s - loss: 0.6788 - accuracy: 0.56 - ETA: 1s - loss: 0.6776 - accuracy: 0.57 - ETA: 0s - loss: 0.6764 - accuracy: 0.57 - ETA: 0s - loss: 0.6747 - accuracy: 0.58 - ETA: 0s - loss: 0.6731 - accuracy: 0.58 - ETA: 0s - loss: 0.6719 - accuracy: 0.58 - ETA: 0s - loss: 0.6710 - accuracy: 0.59 - ETA: 0s - loss: 0.6698 - accuracy: 0.59 - ETA: 0s - loss: 0.6682 - accuracy: 0.59 - ETA: 0s - loss: 0.6667 - accuracy: 0.60 - ETA: 0s - loss: 0.6656 - accuracy: 0.60 - ETA: 0s - loss: 0.6645 - accuracy: 0.60 - ETA: 0s - loss: 0.6627 - accuracy: 0.60 - ETA: 0s - loss: 0.6615 - accuracy: 0.61 - ETA: 0s - loss: 0.6594 - accuracy: 0.61 - ETA: 0s - loss: 0.6585 - accuracy: 0.61 - ETA: 0s - loss: 0.6569 - accuracy: 0.62 - ETA: 0s - loss: 0.6561 - accuracy: 0.62 - ETA: 0s - loss: 0.6549 - accuracy: 0.62 - 2s 143us/step - loss: 0.6549 - accuracy: 0.6233 - val_loss: 0.6012 - val_accuracy: 0.7067\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 58us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:00 - loss: 0.6920 - accuracy: 0.62 - ETA: 4s - loss: 0.6927 - accuracy: 0.5472 - ETA: 3s - loss: 0.6921 - accuracy: 0.54 - ETA: 2s - loss: 0.6928 - accuracy: 0.51 - ETA: 2s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6907 - accuracy: 0.53 - ETA: 1s - loss: 0.6895 - accuracy: 0.53 - ETA: 1s - loss: 0.6883 - accuracy: 0.54 - ETA: 1s - loss: 0.6869 - accuracy: 0.55 - ETA: 1s - loss: 0.6857 - accuracy: 0.55 - ETA: 1s - loss: 0.6844 - accuracy: 0.56 - ETA: 1s - loss: 0.6832 - accuracy: 0.56 - ETA: 1s - loss: 0.6819 - accuracy: 0.56 - ETA: 0s - loss: 0.6810 - accuracy: 0.57 - ETA: 0s - loss: 0.6793 - accuracy: 0.57 - ETA: 0s - loss: 0.6776 - accuracy: 0.58 - ETA: 0s - loss: 0.6764 - accuracy: 0.58 - ETA: 0s - loss: 0.6759 - accuracy: 0.58 - ETA: 0s - loss: 0.6744 - accuracy: 0.58 - ETA: 0s - loss: 0.6731 - accuracy: 0.59 - ETA: 0s - loss: 0.6719 - accuracy: 0.59 - ETA: 0s - loss: 0.6701 - accuracy: 0.59 - ETA: 0s - loss: 0.6688 - accuracy: 0.59 - ETA: 0s - loss: 0.6667 - accuracy: 0.60 - ETA: 0s - loss: 0.6651 - accuracy: 0.60 - ETA: 0s - loss: 0.6639 - accuracy: 0.60 - ETA: 0s - loss: 0.6626 - accuracy: 0.60 - ETA: 0s - loss: 0.6614 - accuracy: 0.60 - ETA: 0s - loss: 0.6599 - accuracy: 0.61 - ETA: 0s - loss: 0.6586 - accuracy: 0.61 - 2s 144us/step - loss: 0.6583 - accuracy: 0.6129 - val_loss: 0.6032 - val_accuracy: 0.7124\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 56us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 2:46 - loss: 0.6914 - accuracy: 0.62 - ETA: 4s - loss: 0.6939 - accuracy: 0.5072 - ETA: 2s - loss: 0.6935 - accuracy: 0.50 - ETA: 2s - loss: 0.6907 - accuracy: 0.54 - ETA: 2s - loss: 0.6911 - accuracy: 0.53 - ETA: 1s - loss: 0.6900 - accuracy: 0.53 - ETA: 1s - loss: 0.6898 - accuracy: 0.54 - ETA: 1s - loss: 0.6895 - accuracy: 0.54 - ETA: 1s - loss: 0.6881 - accuracy: 0.55 - ETA: 1s - loss: 0.6873 - accuracy: 0.56 - ETA: 1s - loss: 0.6862 - accuracy: 0.56 - ETA: 1s - loss: 0.6844 - accuracy: 0.57 - ETA: 1s - loss: 0.6836 - accuracy: 0.57 - ETA: 1s - loss: 0.6818 - accuracy: 0.58 - ETA: 0s - loss: 0.6801 - accuracy: 0.58 - ETA: 0s - loss: 0.6782 - accuracy: 0.58 - ETA: 0s - loss: 0.6768 - accuracy: 0.59 - ETA: 0s - loss: 0.6750 - accuracy: 0.59 - ETA: 0s - loss: 0.6730 - accuracy: 0.60 - ETA: 0s - loss: 0.6716 - accuracy: 0.60 - ETA: 0s - loss: 0.6696 - accuracy: 0.60 - ETA: 0s - loss: 0.6677 - accuracy: 0.60 - ETA: 0s - loss: 0.6656 - accuracy: 0.61 - ETA: 0s - loss: 0.6641 - accuracy: 0.61 - ETA: 0s - loss: 0.6628 - accuracy: 0.61 - ETA: 0s - loss: 0.6610 - accuracy: 0.61 - ETA: 0s - loss: 0.6592 - accuracy: 0.62 - ETA: 0s - loss: 0.6576 - accuracy: 0.62 - ETA: 0s - loss: 0.6565 - accuracy: 0.62 - ETA: 0s - loss: 0.6558 - accuracy: 0.62 - ETA: 0s - loss: 0.6546 - accuracy: 0.62 - 2s 140us/step - loss: 0.6534 - accuracy: 0.6292 - val_loss: 0.5971 - val_accuracy: 0.7074\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:52 - loss: 0.6950 - accuracy: 0.37 - ETA: 4s - loss: 0.6920 - accuracy: 0.5409 - ETA: 3s - loss: 0.6918 - accuracy: 0.53 - ETA: 2s - loss: 0.6901 - accuracy: 0.54 - ETA: 2s - loss: 0.6876 - accuracy: 0.55 - ETA: 1s - loss: 0.6872 - accuracy: 0.55 - ETA: 1s - loss: 0.6845 - accuracy: 0.56 - ETA: 1s - loss: 0.6840 - accuracy: 0.56 - ETA: 1s - loss: 0.6823 - accuracy: 0.56 - ETA: 1s - loss: 0.6819 - accuracy: 0.56 - ETA: 1s - loss: 0.6796 - accuracy: 0.57 - ETA: 1s - loss: 0.6775 - accuracy: 0.57 - ETA: 1s - loss: 0.6759 - accuracy: 0.58 - ETA: 1s - loss: 0.6747 - accuracy: 0.58 - ETA: 0s - loss: 0.6733 - accuracy: 0.58 - ETA: 0s - loss: 0.6719 - accuracy: 0.59 - ETA: 0s - loss: 0.6696 - accuracy: 0.59 - ETA: 0s - loss: 0.6681 - accuracy: 0.59 - ETA: 0s - loss: 0.6661 - accuracy: 0.60 - ETA: 0s - loss: 0.6643 - accuracy: 0.60 - ETA: 0s - loss: 0.6632 - accuracy: 0.60 - ETA: 0s - loss: 0.6624 - accuracy: 0.60 - ETA: 0s - loss: 0.6603 - accuracy: 0.61 - ETA: 0s - loss: 0.6600 - accuracy: 0.61 - ETA: 0s - loss: 0.6585 - accuracy: 0.61 - ETA: 0s - loss: 0.6573 - accuracy: 0.61 - ETA: 0s - loss: 0.6558 - accuracy: 0.61 - ETA: 0s - loss: 0.6546 - accuracy: 0.62 - ETA: 0s - loss: 0.6540 - accuracy: 0.62 - ETA: 0s - loss: 0.6531 - accuracy: 0.62 - ETA: 0s - loss: 0.6522 - accuracy: 0.62 - 2s 139us/step - loss: 0.6525 - accuracy: 0.6246 - val_loss: 0.6065 - val_accuracy: 0.7173\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 57us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:55 - loss: 0.6960 - accuracy: 0.37 - ETA: 4s - loss: 0.6940 - accuracy: 0.4835 - ETA: 3s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6915 - accuracy: 0.53 - ETA: 1s - loss: 0.6901 - accuracy: 0.55 - ETA: 1s - loss: 0.6891 - accuracy: 0.55 - ETA: 1s - loss: 0.6872 - accuracy: 0.56 - ETA: 1s - loss: 0.6855 - accuracy: 0.57 - ETA: 1s - loss: 0.6843 - accuracy: 0.57 - ETA: 1s - loss: 0.6828 - accuracy: 0.58 - ETA: 1s - loss: 0.6818 - accuracy: 0.58 - ETA: 1s - loss: 0.6804 - accuracy: 0.58 - ETA: 0s - loss: 0.6786 - accuracy: 0.59 - ETA: 0s - loss: 0.6768 - accuracy: 0.59 - ETA: 0s - loss: 0.6757 - accuracy: 0.59 - ETA: 0s - loss: 0.6739 - accuracy: 0.60 - ETA: 0s - loss: 0.6718 - accuracy: 0.60 - ETA: 0s - loss: 0.6695 - accuracy: 0.60 - ETA: 0s - loss: 0.6672 - accuracy: 0.61 - ETA: 0s - loss: 0.6649 - accuracy: 0.61 - ETA: 0s - loss: 0.6633 - accuracy: 0.61 - ETA: 0s - loss: 0.6610 - accuracy: 0.61 - ETA: 0s - loss: 0.6598 - accuracy: 0.62 - ETA: 0s - loss: 0.6593 - accuracy: 0.62 - ETA: 0s - loss: 0.6580 - accuracy: 0.62 - ETA: 0s - loss: 0.6569 - accuracy: 0.62 - ETA: 0s - loss: 0.6565 - accuracy: 0.62 - ETA: 0s - loss: 0.6550 - accuracy: 0.62 - ETA: 0s - loss: 0.6532 - accuracy: 0.62 - 2s 138us/step - loss: 0.6528 - accuracy: 0.6295 - val_loss: 0.5933 - val_accuracy: 0.7138\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:39 - loss: 0.6948 - accuracy: 0.37 - ETA: 4s - loss: 0.6941 - accuracy: 0.5000 - ETA: 2s - loss: 0.6937 - accuracy: 0.50 - ETA: 2s - loss: 0.6937 - accuracy: 0.50 - ETA: 2s - loss: 0.6933 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.53 - ETA: 1s - loss: 0.6916 - accuracy: 0.54 - ETA: 1s - loss: 0.6904 - accuracy: 0.55 - ETA: 1s - loss: 0.6887 - accuracy: 0.56 - ETA: 1s - loss: 0.6876 - accuracy: 0.56 - ETA: 1s - loss: 0.6866 - accuracy: 0.56 - ETA: 1s - loss: 0.6853 - accuracy: 0.57 - ETA: 1s - loss: 0.6844 - accuracy: 0.57 - ETA: 0s - loss: 0.6831 - accuracy: 0.57 - ETA: 0s - loss: 0.6810 - accuracy: 0.58 - ETA: 0s - loss: 0.6793 - accuracy: 0.58 - ETA: 0s - loss: 0.6773 - accuracy: 0.59 - ETA: 0s - loss: 0.6760 - accuracy: 0.59 - ETA: 0s - loss: 0.6758 - accuracy: 0.59 - ETA: 0s - loss: 0.6740 - accuracy: 0.59 - ETA: 0s - loss: 0.6731 - accuracy: 0.59 - ETA: 0s - loss: 0.6713 - accuracy: 0.60 - ETA: 0s - loss: 0.6703 - accuracy: 0.60 - ETA: 0s - loss: 0.6693 - accuracy: 0.60 - ETA: 0s - loss: 0.6684 - accuracy: 0.60 - ETA: 0s - loss: 0.6669 - accuracy: 0.60 - ETA: 0s - loss: 0.6656 - accuracy: 0.61 - ETA: 0s - loss: 0.6646 - accuracy: 0.61 - ETA: 0s - loss: 0.6636 - accuracy: 0.61 - ETA: 0s - loss: 0.6622 - accuracy: 0.61 - ETA: 0s - loss: 0.6611 - accuracy: 0.61 - 2s 142us/step - loss: 0.6607 - accuracy: 0.6197 - val_loss: 0.6108 - val_accuracy: 0.7067\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:43 - loss: 0.6985 - accuracy: 0.50 - ETA: 4s - loss: 0.6936 - accuracy: 0.5481 - ETA: 2s - loss: 0.6927 - accuracy: 0.55 - ETA: 2s - loss: 0.6911 - accuracy: 0.55 - ETA: 2s - loss: 0.6902 - accuracy: 0.56 - ETA: 2s - loss: 0.6897 - accuracy: 0.56 - ETA: 1s - loss: 0.6882 - accuracy: 0.57 - ETA: 1s - loss: 0.6877 - accuracy: 0.57 - ETA: 1s - loss: 0.6862 - accuracy: 0.58 - ETA: 1s - loss: 0.6846 - accuracy: 0.58 - ETA: 1s - loss: 0.6830 - accuracy: 0.59 - ETA: 1s - loss: 0.6812 - accuracy: 0.59 - ETA: 1s - loss: 0.6806 - accuracy: 0.59 - ETA: 1s - loss: 0.6784 - accuracy: 0.60 - ETA: 1s - loss: 0.6759 - accuracy: 0.60 - ETA: 0s - loss: 0.6735 - accuracy: 0.60 - ETA: 0s - loss: 0.6714 - accuracy: 0.61 - ETA: 0s - loss: 0.6693 - accuracy: 0.61 - ETA: 0s - loss: 0.6680 - accuracy: 0.61 - ETA: 0s - loss: 0.6667 - accuracy: 0.61 - ETA: 0s - loss: 0.6646 - accuracy: 0.62 - ETA: 0s - loss: 0.6619 - accuracy: 0.62 - ETA: 0s - loss: 0.6598 - accuracy: 0.62 - ETA: 0s - loss: 0.6579 - accuracy: 0.62 - ETA: 0s - loss: 0.6555 - accuracy: 0.63 - ETA: 0s - loss: 0.6539 - accuracy: 0.63 - ETA: 0s - loss: 0.6533 - accuracy: 0.63 - ETA: 0s - loss: 0.6521 - accuracy: 0.63 - ETA: 0s - loss: 0.6511 - accuracy: 0.63 - ETA: 0s - loss: 0.6497 - accuracy: 0.63 - ETA: 0s - loss: 0.6492 - accuracy: 0.64 - ETA: 0s - loss: 0.6484 - accuracy: 0.64 - ETA: 0s - loss: 0.6475 - accuracy: 0.64 - 2s 148us/step - loss: 0.6467 - accuracy: 0.6436 - val_loss: 0.5884 - val_accuracy: 0.7223\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:44 - loss: 0.6925 - accuracy: 0.87 - ETA: 4s - loss: 0.6938 - accuracy: 0.5255 - ETA: 2s - loss: 0.6930 - accuracy: 0.52 - ETA: 2s - loss: 0.6906 - accuracy: 0.52 - ETA: 1s - loss: 0.6895 - accuracy: 0.53 - ETA: 1s - loss: 0.6893 - accuracy: 0.53 - ETA: 1s - loss: 0.6871 - accuracy: 0.54 - ETA: 1s - loss: 0.6853 - accuracy: 0.55 - ETA: 1s - loss: 0.6840 - accuracy: 0.56 - ETA: 1s - loss: 0.6823 - accuracy: 0.56 - ETA: 1s - loss: 0.6800 - accuracy: 0.57 - ETA: 1s - loss: 0.6780 - accuracy: 0.58 - ETA: 1s - loss: 0.6765 - accuracy: 0.58 - ETA: 0s - loss: 0.6755 - accuracy: 0.59 - ETA: 0s - loss: 0.6740 - accuracy: 0.59 - ETA: 0s - loss: 0.6726 - accuracy: 0.60 - ETA: 0s - loss: 0.6699 - accuracy: 0.60 - ETA: 0s - loss: 0.6666 - accuracy: 0.61 - ETA: 0s - loss: 0.6645 - accuracy: 0.61 - ETA: 0s - loss: 0.6624 - accuracy: 0.62 - ETA: 0s - loss: 0.6615 - accuracy: 0.62 - ETA: 0s - loss: 0.6598 - accuracy: 0.62 - ETA: 0s - loss: 0.6586 - accuracy: 0.62 - ETA: 0s - loss: 0.6567 - accuracy: 0.62 - ETA: 0s - loss: 0.6559 - accuracy: 0.62 - ETA: 0s - loss: 0.6541 - accuracy: 0.63 - ETA: 0s - loss: 0.6517 - accuracy: 0.63 - ETA: 0s - loss: 0.6504 - accuracy: 0.63 - ETA: 0s - loss: 0.6501 - accuracy: 0.63 - ETA: 0s - loss: 0.6484 - accuracy: 0.63 - ETA: 0s - loss: 0.6481 - accuracy: 0.63 - 2s 140us/step - loss: 0.6477 - accuracy: 0.6402 - val_loss: 0.5960 - val_accuracy: 0.7230\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 48us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:38 - loss: 0.6938 - accuracy: 0.62 - ETA: 4s - loss: 0.6942 - accuracy: 0.5278 - ETA: 2s - loss: 0.6943 - accuracy: 0.51 - ETA: 2s - loss: 0.6929 - accuracy: 0.53 - ETA: 1s - loss: 0.6899 - accuracy: 0.54 - ETA: 1s - loss: 0.6897 - accuracy: 0.54 - ETA: 1s - loss: 0.6880 - accuracy: 0.55 - ETA: 1s - loss: 0.6860 - accuracy: 0.57 - ETA: 1s - loss: 0.6844 - accuracy: 0.57 - ETA: 1s - loss: 0.6832 - accuracy: 0.57 - ETA: 1s - loss: 0.6814 - accuracy: 0.58 - ETA: 1s - loss: 0.6788 - accuracy: 0.59 - ETA: 1s - loss: 0.6771 - accuracy: 0.59 - ETA: 0s - loss: 0.6757 - accuracy: 0.59 - ETA: 0s - loss: 0.6739 - accuracy: 0.60 - ETA: 0s - loss: 0.6714 - accuracy: 0.60 - ETA: 0s - loss: 0.6696 - accuracy: 0.60 - ETA: 0s - loss: 0.6686 - accuracy: 0.61 - ETA: 0s - loss: 0.6666 - accuracy: 0.61 - ETA: 0s - loss: 0.6646 - accuracy: 0.61 - ETA: 0s - loss: 0.6640 - accuracy: 0.61 - ETA: 0s - loss: 0.6610 - accuracy: 0.62 - ETA: 0s - loss: 0.6597 - accuracy: 0.62 - ETA: 0s - loss: 0.6567 - accuracy: 0.63 - ETA: 0s - loss: 0.6556 - accuracy: 0.63 - ETA: 0s - loss: 0.6537 - accuracy: 0.63 - ETA: 0s - loss: 0.6519 - accuracy: 0.63 - ETA: 0s - loss: 0.6505 - accuracy: 0.63 - ETA: 0s - loss: 0.6496 - accuracy: 0.64 - ETA: 0s - loss: 0.6484 - accuracy: 0.64 - 2s 137us/step - loss: 0.6460 - accuracy: 0.6445 - val_loss: 0.5859 - val_accuracy: 0.7088\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 50us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:39 - loss: 0.6936 - accuracy: 0.62 - ETA: 4s - loss: 0.6929 - accuracy: 0.5268 - ETA: 2s - loss: 0.6920 - accuracy: 0.53 - ETA: 2s - loss: 0.6909 - accuracy: 0.53 - ETA: 1s - loss: 0.6893 - accuracy: 0.55 - ETA: 1s - loss: 0.6885 - accuracy: 0.56 - ETA: 1s - loss: 0.6872 - accuracy: 0.57 - ETA: 1s - loss: 0.6851 - accuracy: 0.57 - ETA: 1s - loss: 0.6837 - accuracy: 0.58 - ETA: 1s - loss: 0.6821 - accuracy: 0.58 - ETA: 1s - loss: 0.6797 - accuracy: 0.59 - ETA: 1s - loss: 0.6795 - accuracy: 0.59 - ETA: 0s - loss: 0.6774 - accuracy: 0.59 - ETA: 0s - loss: 0.6760 - accuracy: 0.60 - ETA: 0s - loss: 0.6740 - accuracy: 0.60 - ETA: 0s - loss: 0.6721 - accuracy: 0.60 - ETA: 0s - loss: 0.6699 - accuracy: 0.61 - ETA: 0s - loss: 0.6684 - accuracy: 0.61 - ETA: 0s - loss: 0.6666 - accuracy: 0.61 - ETA: 0s - loss: 0.6651 - accuracy: 0.61 - ETA: 0s - loss: 0.6636 - accuracy: 0.62 - ETA: 0s - loss: 0.6618 - accuracy: 0.62 - ETA: 0s - loss: 0.6604 - accuracy: 0.62 - ETA: 0s - loss: 0.6589 - accuracy: 0.62 - ETA: 0s - loss: 0.6575 - accuracy: 0.63 - ETA: 0s - loss: 0.6564 - accuracy: 0.63 - ETA: 0s - loss: 0.6553 - accuracy: 0.63 - ETA: 0s - loss: 0.6536 - accuracy: 0.63 - ETA: 0s - loss: 0.6526 - accuracy: 0.63 - ETA: 0s - loss: 0.6512 - accuracy: 0.63 - 2s 135us/step - loss: 0.6514 - accuracy: 0.6372 - val_loss: 0.5982 - val_accuracy: 0.7159\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 49us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:38 - loss: 0.6960 - accuracy: 0.37 - ETA: 4s - loss: 0.6934 - accuracy: 0.5463 - ETA: 2s - loss: 0.6919 - accuracy: 0.54 - ETA: 2s - loss: 0.6909 - accuracy: 0.56 - ETA: 1s - loss: 0.6895 - accuracy: 0.56 - ETA: 1s - loss: 0.6887 - accuracy: 0.56 - ETA: 1s - loss: 0.6875 - accuracy: 0.57 - ETA: 1s - loss: 0.6860 - accuracy: 0.58 - ETA: 1s - loss: 0.6855 - accuracy: 0.58 - ETA: 1s - loss: 0.6833 - accuracy: 0.58 - ETA: 1s - loss: 0.6814 - accuracy: 0.59 - ETA: 1s - loss: 0.6791 - accuracy: 0.59 - ETA: 1s - loss: 0.6767 - accuracy: 0.60 - ETA: 0s - loss: 0.6752 - accuracy: 0.60 - ETA: 0s - loss: 0.6735 - accuracy: 0.61 - ETA: 0s - loss: 0.6715 - accuracy: 0.61 - ETA: 0s - loss: 0.6692 - accuracy: 0.61 - ETA: 0s - loss: 0.6667 - accuracy: 0.62 - ETA: 0s - loss: 0.6646 - accuracy: 0.62 - ETA: 0s - loss: 0.6621 - accuracy: 0.62 - ETA: 0s - loss: 0.6620 - accuracy: 0.62 - ETA: 0s - loss: 0.6609 - accuracy: 0.62 - ETA: 0s - loss: 0.6594 - accuracy: 0.63 - ETA: 0s - loss: 0.6580 - accuracy: 0.63 - ETA: 0s - loss: 0.6561 - accuracy: 0.63 - ETA: 0s - loss: 0.6546 - accuracy: 0.63 - ETA: 0s - loss: 0.6532 - accuracy: 0.63 - ETA: 0s - loss: 0.6517 - accuracy: 0.64 - ETA: 0s - loss: 0.6502 - accuracy: 0.64 - ETA: 0s - loss: 0.6488 - accuracy: 0.64 - 2s 136us/step - loss: 0.6486 - accuracy: 0.6449 - val_loss: 0.5910 - val_accuracy: 0.7145\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 49us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:41 - loss: 0.6922 - accuracy: 0.62 - ETA: 4s - loss: 0.6938 - accuracy: 0.5223 - ETA: 2s - loss: 0.6919 - accuracy: 0.54 - ETA: 2s - loss: 0.6914 - accuracy: 0.53 - ETA: 1s - loss: 0.6901 - accuracy: 0.53 - ETA: 1s - loss: 0.6875 - accuracy: 0.54 - ETA: 1s - loss: 0.6851 - accuracy: 0.55 - ETA: 1s - loss: 0.6833 - accuracy: 0.56 - ETA: 1s - loss: 0.6816 - accuracy: 0.57 - ETA: 1s - loss: 0.6797 - accuracy: 0.57 - ETA: 1s - loss: 0.6770 - accuracy: 0.58 - ETA: 1s - loss: 0.6760 - accuracy: 0.58 - ETA: 1s - loss: 0.6739 - accuracy: 0.59 - ETA: 0s - loss: 0.6718 - accuracy: 0.59 - ETA: 0s - loss: 0.6709 - accuracy: 0.60 - ETA: 0s - loss: 0.6691 - accuracy: 0.60 - ETA: 0s - loss: 0.6664 - accuracy: 0.61 - ETA: 0s - loss: 0.6649 - accuracy: 0.61 - ETA: 0s - loss: 0.6636 - accuracy: 0.61 - ETA: 0s - loss: 0.6617 - accuracy: 0.61 - ETA: 0s - loss: 0.6609 - accuracy: 0.61 - ETA: 0s - loss: 0.6601 - accuracy: 0.62 - ETA: 0s - loss: 0.6581 - accuracy: 0.62 - ETA: 0s - loss: 0.6555 - accuracy: 0.62 - ETA: 0s - loss: 0.6532 - accuracy: 0.62 - ETA: 0s - loss: 0.6518 - accuracy: 0.63 - ETA: 0s - loss: 0.6496 - accuracy: 0.63 - ETA: 0s - loss: 0.6489 - accuracy: 0.63 - ETA: 0s - loss: 0.6483 - accuracy: 0.63 - ETA: 0s - loss: 0.6473 - accuracy: 0.63 - 2s 135us/step - loss: 0.6469 - accuracy: 0.6392 - val_loss: 0.6014 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 49us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 2:19 - loss: 0.6932 - accuracy: 0.75 - ETA: 4s - loss: 0.6937 - accuracy: 0.5172 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.49 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.49 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - 2s 135us/step - loss: 0.6931 - accuracy: 0.5138 - val_loss: 0.6912 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:19 - loss: 0.6928 - accuracy: 0.37 - ETA: 3s - loss: 0.6940 - accuracy: 0.4912 - ETA: 2s - loss: 0.6942 - accuracy: 0.48 - ETA: 1s - loss: 0.6939 - accuracy: 0.49 - ETA: 1s - loss: 0.6940 - accuracy: 0.49 - ETA: 1s - loss: 0.6940 - accuracy: 0.49 - ETA: 1s - loss: 0.6939 - accuracy: 0.49 - ETA: 1s - loss: 0.6939 - accuracy: 0.49 - ETA: 1s - loss: 0.6939 - accuracy: 0.49 - ETA: 1s - loss: 0.6938 - accuracy: 0.49 - ETA: 1s - loss: 0.6938 - accuracy: 0.49 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - 2s 127us/step - loss: 0.6931 - accuracy: 0.5107 - val_loss: 0.6920 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 49us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:25 - loss: 0.6931 - accuracy: 0.50 - ETA: 3s - loss: 0.6937 - accuracy: 0.4958 - ETA: 2s - loss: 0.6935 - accuracy: 0.50 - ETA: 2s - loss: 0.6934 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.50 - ETA: 1s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6932 - accuracy: 0.50 - ETA: 0s - loss: 0.6932 - accuracy: 0.50 - ETA: 0s - loss: 0.6932 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6929 - accuracy: 0.50 - ETA: 0s - loss: 0.6929 - accuracy: 0.50 - ETA: 0s - loss: 0.6928 - accuracy: 0.50 - ETA: 0s - loss: 0.6928 - accuracy: 0.50 - ETA: 0s - loss: 0.6927 - accuracy: 0.50 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - 2s 134us/step - loss: 0.6924 - accuracy: 0.5114 - val_loss: 0.6886 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 50us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:24 - loss: 0.6884 - accuracy: 0.75 - ETA: 3s - loss: 0.6939 - accuracy: 0.4737 - ETA: 2s - loss: 0.6937 - accuracy: 0.49 - ETA: 2s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6939 - accuracy: 0.50 - ETA: 1s - loss: 0.6939 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - 2s 128us/step - loss: 0.6927 - accuracy: 0.5158 - val_loss: 0.6898 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:17 - loss: 0.7046 - accuracy: 0.12 - ETA: 3s - loss: 0.6939 - accuracy: 0.5280 - ETA: 2s - loss: 0.6934 - accuracy: 0.54 - ETA: 2s - loss: 0.6931 - accuracy: 0.53 - ETA: 2s - loss: 0.6935 - accuracy: 0.52 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - 2s 134us/step - loss: 0.6926 - accuracy: 0.5213 - val_loss: 0.6905 - val_accuracy: 0.5334\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:30 - loss: 0.6945 - accuracy: 0.62 - ETA: 3s - loss: 0.6940 - accuracy: 0.4777 - ETA: 2s - loss: 0.6939 - accuracy: 0.49 - ETA: 2s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6939 - accuracy: 0.49 - ETA: 1s - loss: 0.6939 - accuracy: 0.49 - ETA: 1s - loss: 0.6939 - accuracy: 0.49 - ETA: 1s - loss: 0.6939 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6939 - accuracy: 0.49 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - 2s 133us/step - loss: 0.6937 - accuracy: 0.5073 - val_loss: 0.6935 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 58us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:43 - loss: 0.6989 - accuracy: 0.37 - ETA: 4s - loss: 0.6926 - accuracy: 0.5463 - ETA: 2s - loss: 0.6933 - accuracy: 0.53 - ETA: 2s - loss: 0.6933 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.53 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - 2s 134us/step - loss: 0.6920 - accuracy: 0.5166 - val_loss: 0.6863 - val_accuracy: 0.5405\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 49us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:25 - loss: 0.6940 - accuracy: 0.50 - ETA: 3s - loss: 0.6938 - accuracy: 0.5285 - ETA: 2s - loss: 0.6941 - accuracy: 0.50 - ETA: 1s - loss: 0.6940 - accuracy: 0.52 - ETA: 1s - loss: 0.6940 - accuracy: 0.52 - ETA: 1s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.52 - ETA: 1s - loss: 0.6937 - accuracy: 0.52 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.52 - ETA: 1s - loss: 0.6936 - accuracy: 0.52 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - 2s 129us/step - loss: 0.6922 - accuracy: 0.5252 - val_loss: 0.6879 - val_accuracy: 0.6158\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 49us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:24 - loss: 0.6923 - accuracy: 0.50 - ETA: 3s - loss: 0.6934 - accuracy: 0.5219 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6940 - accuracy: 0.51 - ETA: 1s - loss: 0.6941 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.52 - ETA: 1s - loss: 0.6935 - accuracy: 0.52 - ETA: 1s - loss: 0.6933 - accuracy: 0.52 - ETA: 1s - loss: 0.6935 - accuracy: 0.52 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - 2s 130us/step - loss: 0.6929 - accuracy: 0.5240 - val_loss: 0.6897 - val_accuracy: 0.5945\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 50us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:24 - loss: 0.6944 - accuracy: 0.50 - ETA: 3s - loss: 0.6942 - accuracy: 0.4873 - ETA: 2s - loss: 0.6941 - accuracy: 0.50 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - 2s 127us/step - loss: 0.6929 - accuracy: 0.5231 - val_loss: 0.6904 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 48us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 2:20 - loss: 0.6950 - accuracy: 0.37 - ETA: 3s - loss: 0.6936 - accuracy: 0.5307 - ETA: 2s - loss: 0.6937 - accuracy: 0.52 - ETA: 1s - loss: 0.6938 - accuracy: 0.52 - ETA: 1s - loss: 0.6942 - accuracy: 0.50 - ETA: 1s - loss: 0.6941 - accuracy: 0.50 - ETA: 1s - loss: 0.6941 - accuracy: 0.50 - ETA: 1s - loss: 0.6941 - accuracy: 0.51 - ETA: 1s - loss: 0.6941 - accuracy: 0.51 - ETA: 1s - loss: 0.6941 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - 2s 134us/step - loss: 0.6936 - accuracy: 0.5140 - val_loss: 0.6913 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:25 - loss: 0.6940 - accuracy: 0.37 - ETA: 3s - loss: 0.6942 - accuracy: 0.5088 - ETA: 2s - loss: 0.6942 - accuracy: 0.49 - ETA: 1s - loss: 0.6939 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6939 - accuracy: 0.50 - ETA: 1s - loss: 0.6939 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.50 - ETA: 0s - loss: 0.6932 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6928 - accuracy: 0.50 - ETA: 0s - loss: 0.6928 - accuracy: 0.50 - ETA: 0s - loss: 0.6927 - accuracy: 0.50 - ETA: 0s - loss: 0.6925 - accuracy: 0.50 - ETA: 0s - loss: 0.6925 - accuracy: 0.50 - ETA: 0s - loss: 0.6924 - accuracy: 0.50 - ETA: 0s - loss: 0.6923 - accuracy: 0.50 - 2s 128us/step - loss: 0.6923 - accuracy: 0.5094 - val_loss: 0.6886 - val_accuracy: 0.5206\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 48us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:35 - loss: 0.7016 - accuracy: 0.25 - ETA: 5s - loss: 0.6950 - accuracy: 0.5194 - ETA: 3s - loss: 0.6949 - accuracy: 0.51 - ETA: 2s - loss: 0.6948 - accuracy: 0.51 - ETA: 2s - loss: 0.6947 - accuracy: 0.51 - ETA: 1s - loss: 0.6943 - accuracy: 0.51 - ETA: 1s - loss: 0.6942 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.52 - ETA: 1s - loss: 0.6938 - accuracy: 0.52 - ETA: 1s - loss: 0.6935 - accuracy: 0.52 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 1s - loss: 0.6931 - accuracy: 0.53 - ETA: 1s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.53 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.53 - ETA: 0s - loss: 0.6927 - accuracy: 0.53 - ETA: 0s - loss: 0.6926 - accuracy: 0.53 - ETA: 0s - loss: 0.6925 - accuracy: 0.53 - ETA: 0s - loss: 0.6923 - accuracy: 0.53 - ETA: 0s - loss: 0.6922 - accuracy: 0.54 - ETA: 0s - loss: 0.6921 - accuracy: 0.54 - ETA: 0s - loss: 0.6919 - accuracy: 0.54 - ETA: 0s - loss: 0.6918 - accuracy: 0.54 - ETA: 0s - loss: 0.6916 - accuracy: 0.54 - ETA: 0s - loss: 0.6912 - accuracy: 0.54 - ETA: 0s - loss: 0.6911 - accuracy: 0.54 - ETA: 0s - loss: 0.6908 - accuracy: 0.54 - ETA: 0s - loss: 0.6907 - accuracy: 0.54 - ETA: 0s - loss: 0.6903 - accuracy: 0.55 - ETA: 0s - loss: 0.6899 - accuracy: 0.55 - 2s 139us/step - loss: 0.6898 - accuracy: 0.5527 - val_loss: 0.6791 - val_accuracy: 0.6364\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 60us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:30 - loss: 0.6884 - accuracy: 0.50 - ETA: 4s - loss: 0.6948 - accuracy: 0.5000 - ETA: 2s - loss: 0.6949 - accuracy: 0.49 - ETA: 2s - loss: 0.6948 - accuracy: 0.49 - ETA: 1s - loss: 0.6943 - accuracy: 0.50 - ETA: 1s - loss: 0.6941 - accuracy: 0.51 - ETA: 1s - loss: 0.6942 - accuracy: 0.50 - ETA: 1s - loss: 0.6943 - accuracy: 0.50 - ETA: 1s - loss: 0.6942 - accuracy: 0.51 - ETA: 1s - loss: 0.6941 - accuracy: 0.51 - ETA: 1s - loss: 0.6941 - accuracy: 0.51 - ETA: 1s - loss: 0.6939 - accuracy: 0.52 - ETA: 0s - loss: 0.6936 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6924 - accuracy: 0.53 - ETA: 0s - loss: 0.6923 - accuracy: 0.53 - ETA: 0s - loss: 0.6921 - accuracy: 0.53 - ETA: 0s - loss: 0.6919 - accuracy: 0.53 - ETA: 0s - loss: 0.6918 - accuracy: 0.53 - ETA: 0s - loss: 0.6915 - accuracy: 0.54 - ETA: 0s - loss: 0.6913 - accuracy: 0.54 - ETA: 0s - loss: 0.6910 - accuracy: 0.54 - 2s 133us/step - loss: 0.6909 - accuracy: 0.5440 - val_loss: 0.6818 - val_accuracy: 0.6023\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 59us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:27 - loss: 0.6933 - accuracy: 0.37 - ETA: 3s - loss: 0.6952 - accuracy: 0.5023 - ETA: 2s - loss: 0.6954 - accuracy: 0.49 - ETA: 2s - loss: 0.6956 - accuracy: 0.49 - ETA: 1s - loss: 0.6956 - accuracy: 0.49 - ETA: 1s - loss: 0.6957 - accuracy: 0.49 - ETA: 1s - loss: 0.6955 - accuracy: 0.49 - ETA: 1s - loss: 0.6954 - accuracy: 0.50 - ETA: 1s - loss: 0.6954 - accuracy: 0.50 - ETA: 1s - loss: 0.6953 - accuracy: 0.50 - ETA: 1s - loss: 0.6952 - accuracy: 0.50 - ETA: 1s - loss: 0.6950 - accuracy: 0.50 - ETA: 1s - loss: 0.6950 - accuracy: 0.50 - ETA: 0s - loss: 0.6949 - accuracy: 0.50 - ETA: 0s - loss: 0.6948 - accuracy: 0.51 - ETA: 0s - loss: 0.6948 - accuracy: 0.50 - ETA: 0s - loss: 0.6949 - accuracy: 0.50 - ETA: 0s - loss: 0.6948 - accuracy: 0.51 - ETA: 0s - loss: 0.6947 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.52 - ETA: 0s - loss: 0.6942 - accuracy: 0.52 - ETA: 0s - loss: 0.6942 - accuracy: 0.52 - ETA: 0s - loss: 0.6940 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6936 - accuracy: 0.52 - 2s 132us/step - loss: 0.6935 - accuracy: 0.5295 - val_loss: 0.6886 - val_accuracy: 0.5455\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 50us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:20 - loss: 0.6908 - accuracy: 0.75 - ETA: 3s - loss: 0.6947 - accuracy: 0.5154 - ETA: 2s - loss: 0.6949 - accuracy: 0.50 - ETA: 2s - loss: 0.6948 - accuracy: 0.51 - ETA: 1s - loss: 0.6950 - accuracy: 0.50 - ETA: 1s - loss: 0.6948 - accuracy: 0.50 - ETA: 1s - loss: 0.6948 - accuracy: 0.50 - ETA: 1s - loss: 0.6949 - accuracy: 0.50 - ETA: 1s - loss: 0.6948 - accuracy: 0.50 - ETA: 1s - loss: 0.6948 - accuracy: 0.50 - ETA: 1s - loss: 0.6948 - accuracy: 0.50 - ETA: 1s - loss: 0.6948 - accuracy: 0.50 - ETA: 0s - loss: 0.6947 - accuracy: 0.50 - ETA: 0s - loss: 0.6947 - accuracy: 0.50 - ETA: 0s - loss: 0.6946 - accuracy: 0.50 - ETA: 0s - loss: 0.6945 - accuracy: 0.50 - ETA: 0s - loss: 0.6944 - accuracy: 0.50 - ETA: 0s - loss: 0.6944 - accuracy: 0.50 - ETA: 0s - loss: 0.6944 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - 2s 133us/step - loss: 0.6931 - accuracy: 0.5139 - val_loss: 0.6889 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 58us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:20 - loss: 0.6984 - accuracy: 0.12 - ETA: 3s - loss: 0.6952 - accuracy: 0.4583 - ETA: 2s - loss: 0.6947 - accuracy: 0.47 - ETA: 2s - loss: 0.6941 - accuracy: 0.49 - ETA: 1s - loss: 0.6945 - accuracy: 0.49 - ETA: 1s - loss: 0.6942 - accuracy: 0.50 - ETA: 1s - loss: 0.6944 - accuracy: 0.49 - ETA: 1s - loss: 0.6942 - accuracy: 0.50 - ETA: 1s - loss: 0.6942 - accuracy: 0.50 - ETA: 1s - loss: 0.6940 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.50 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - ETA: 0s - loss: 0.6919 - accuracy: 0.51 - ETA: 0s - loss: 0.6918 - accuracy: 0.51 - 2s 132us/step - loss: 0.6917 - accuracy: 0.5140 - val_loss: 0.6837 - val_accuracy: 0.5291\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 49us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:22 - loss: 0.6839 - accuracy: 0.75 - ETA: 3s - loss: 0.6944 - accuracy: 0.5273 - ETA: 2s - loss: 0.6950 - accuracy: 0.51 - ETA: 2s - loss: 0.6952 - accuracy: 0.50 - ETA: 1s - loss: 0.6954 - accuracy: 0.49 - ETA: 1s - loss: 0.6950 - accuracy: 0.50 - ETA: 1s - loss: 0.6948 - accuracy: 0.50 - ETA: 1s - loss: 0.6948 - accuracy: 0.50 - ETA: 1s - loss: 0.6946 - accuracy: 0.50 - ETA: 1s - loss: 0.6944 - accuracy: 0.51 - ETA: 1s - loss: 0.6942 - accuracy: 0.51 - ETA: 1s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.52 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.53 - ETA: 0s - loss: 0.6928 - accuracy: 0.53 - ETA: 0s - loss: 0.6926 - accuracy: 0.53 - 2s 131us/step - loss: 0.6925 - accuracy: 0.5352 - val_loss: 0.6881 - val_accuracy: 0.5518\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:58 - loss: 0.6937 - accuracy: 0.62 - ETA: 4s - loss: 0.6935 - accuracy: 0.4907 - ETA: 2s - loss: 0.6934 - accuracy: 0.50 - ETA: 2s - loss: 0.6933 - accuracy: 0.50 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.52 - ETA: 1s - loss: 0.6921 - accuracy: 0.52 - ETA: 1s - loss: 0.6915 - accuracy: 0.53 - ETA: 1s - loss: 0.6903 - accuracy: 0.53 - ETA: 1s - loss: 0.6892 - accuracy: 0.54 - ETA: 1s - loss: 0.6881 - accuracy: 0.54 - ETA: 1s - loss: 0.6875 - accuracy: 0.54 - ETA: 1s - loss: 0.6867 - accuracy: 0.54 - ETA: 1s - loss: 0.6858 - accuracy: 0.54 - ETA: 1s - loss: 0.6843 - accuracy: 0.55 - ETA: 0s - loss: 0.6835 - accuracy: 0.55 - ETA: 0s - loss: 0.6824 - accuracy: 0.55 - ETA: 0s - loss: 0.6816 - accuracy: 0.55 - ETA: 0s - loss: 0.6810 - accuracy: 0.55 - ETA: 0s - loss: 0.6797 - accuracy: 0.55 - ETA: 0s - loss: 0.6785 - accuracy: 0.56 - ETA: 0s - loss: 0.6774 - accuracy: 0.56 - ETA: 0s - loss: 0.6765 - accuracy: 0.56 - ETA: 0s - loss: 0.6754 - accuracy: 0.56 - ETA: 0s - loss: 0.6751 - accuracy: 0.56 - ETA: 0s - loss: 0.6747 - accuracy: 0.56 - ETA: 0s - loss: 0.6745 - accuracy: 0.56 - ETA: 0s - loss: 0.6733 - accuracy: 0.56 - ETA: 0s - loss: 0.6726 - accuracy: 0.56 - ETA: 0s - loss: 0.6720 - accuracy: 0.56 - ETA: 0s - loss: 0.6708 - accuracy: 0.56 - ETA: 0s - loss: 0.6704 - accuracy: 0.57 - 2s 144us/step - loss: 0.6701 - accuracy: 0.5701 - val_loss: 0.6347 - val_accuracy: 0.7102\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 54us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:03 - loss: 0.6914 - accuracy: 0.62 - ETA: 5s - loss: 0.6947 - accuracy: 0.4775 - ETA: 3s - loss: 0.6939 - accuracy: 0.49 - ETA: 2s - loss: 0.6934 - accuracy: 0.51 - ETA: 2s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6904 - accuracy: 0.53 - ETA: 1s - loss: 0.6899 - accuracy: 0.53 - ETA: 1s - loss: 0.6889 - accuracy: 0.54 - ETA: 1s - loss: 0.6871 - accuracy: 0.55 - ETA: 1s - loss: 0.6848 - accuracy: 0.55 - ETA: 1s - loss: 0.6833 - accuracy: 0.56 - ETA: 1s - loss: 0.6822 - accuracy: 0.56 - ETA: 1s - loss: 0.6807 - accuracy: 0.56 - ETA: 0s - loss: 0.6790 - accuracy: 0.56 - ETA: 0s - loss: 0.6783 - accuracy: 0.56 - ETA: 0s - loss: 0.6766 - accuracy: 0.57 - ETA: 0s - loss: 0.6744 - accuracy: 0.57 - ETA: 0s - loss: 0.6731 - accuracy: 0.57 - ETA: 0s - loss: 0.6725 - accuracy: 0.57 - ETA: 0s - loss: 0.6713 - accuracy: 0.58 - ETA: 0s - loss: 0.6702 - accuracy: 0.58 - ETA: 0s - loss: 0.6693 - accuracy: 0.58 - ETA: 0s - loss: 0.6679 - accuracy: 0.58 - ETA: 0s - loss: 0.6667 - accuracy: 0.59 - ETA: 0s - loss: 0.6659 - accuracy: 0.59 - ETA: 0s - loss: 0.6655 - accuracy: 0.59 - ETA: 0s - loss: 0.6644 - accuracy: 0.59 - ETA: 0s - loss: 0.6645 - accuracy: 0.59 - ETA: 0s - loss: 0.6639 - accuracy: 0.59 - ETA: 0s - loss: 0.6629 - accuracy: 0.59 - 2s 145us/step - loss: 0.6623 - accuracy: 0.5972 - val_loss: 0.6134 - val_accuracy: 0.7237\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 50us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 3:05 - loss: 0.6916 - accuracy: 0.75 - ETA: 4s - loss: 0.6932 - accuracy: 0.4884 - ETA: 3s - loss: 0.6926 - accuracy: 0.49 - ETA: 2s - loss: 0.6922 - accuracy: 0.49 - ETA: 2s - loss: 0.6917 - accuracy: 0.50 - ETA: 1s - loss: 0.6912 - accuracy: 0.50 - ETA: 1s - loss: 0.6908 - accuracy: 0.51 - ETA: 1s - loss: 0.6901 - accuracy: 0.52 - ETA: 1s - loss: 0.6896 - accuracy: 0.52 - ETA: 1s - loss: 0.6885 - accuracy: 0.53 - ETA: 1s - loss: 0.6874 - accuracy: 0.53 - ETA: 1s - loss: 0.6869 - accuracy: 0.54 - ETA: 1s - loss: 0.6856 - accuracy: 0.54 - ETA: 1s - loss: 0.6846 - accuracy: 0.54 - ETA: 1s - loss: 0.6829 - accuracy: 0.55 - ETA: 0s - loss: 0.6817 - accuracy: 0.55 - ETA: 0s - loss: 0.6802 - accuracy: 0.56 - ETA: 0s - loss: 0.6796 - accuracy: 0.56 - ETA: 0s - loss: 0.6781 - accuracy: 0.56 - ETA: 0s - loss: 0.6766 - accuracy: 0.56 - ETA: 0s - loss: 0.6758 - accuracy: 0.56 - ETA: 0s - loss: 0.6748 - accuracy: 0.56 - ETA: 0s - loss: 0.6743 - accuracy: 0.57 - ETA: 0s - loss: 0.6726 - accuracy: 0.57 - ETA: 0s - loss: 0.6720 - accuracy: 0.57 - ETA: 0s - loss: 0.6713 - accuracy: 0.57 - ETA: 0s - loss: 0.6700 - accuracy: 0.57 - ETA: 0s - loss: 0.6691 - accuracy: 0.57 - ETA: 0s - loss: 0.6686 - accuracy: 0.58 - ETA: 0s - loss: 0.6676 - accuracy: 0.58 - ETA: 0s - loss: 0.6667 - accuracy: 0.58 - ETA: 0s - loss: 0.6650 - accuracy: 0.58 - 2s 143us/step - loss: 0.6648 - accuracy: 0.5869 - val_loss: 0.6141 - val_accuracy: 0.7109\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 50us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:05 - loss: 0.6932 - accuracy: 0.50 - ETA: 4s - loss: 0.6937 - accuracy: 0.4764 - ETA: 3s - loss: 0.6935 - accuracy: 0.48 - ETA: 2s - loss: 0.6933 - accuracy: 0.48 - ETA: 2s - loss: 0.6931 - accuracy: 0.49 - ETA: 1s - loss: 0.6923 - accuracy: 0.50 - ETA: 1s - loss: 0.6918 - accuracy: 0.51 - ETA: 1s - loss: 0.6910 - accuracy: 0.51 - ETA: 1s - loss: 0.6903 - accuracy: 0.52 - ETA: 1s - loss: 0.6892 - accuracy: 0.52 - ETA: 1s - loss: 0.6882 - accuracy: 0.53 - ETA: 1s - loss: 0.6875 - accuracy: 0.53 - ETA: 1s - loss: 0.6866 - accuracy: 0.53 - ETA: 1s - loss: 0.6859 - accuracy: 0.53 - ETA: 1s - loss: 0.6840 - accuracy: 0.54 - ETA: 0s - loss: 0.6828 - accuracy: 0.54 - ETA: 0s - loss: 0.6811 - accuracy: 0.55 - ETA: 0s - loss: 0.6798 - accuracy: 0.55 - ETA: 0s - loss: 0.6795 - accuracy: 0.55 - ETA: 0s - loss: 0.6773 - accuracy: 0.56 - ETA: 0s - loss: 0.6753 - accuracy: 0.56 - ETA: 0s - loss: 0.6746 - accuracy: 0.56 - ETA: 0s - loss: 0.6726 - accuracy: 0.57 - ETA: 0s - loss: 0.6705 - accuracy: 0.57 - ETA: 0s - loss: 0.6695 - accuracy: 0.57 - ETA: 0s - loss: 0.6685 - accuracy: 0.57 - ETA: 0s - loss: 0.6671 - accuracy: 0.57 - ETA: 0s - loss: 0.6661 - accuracy: 0.58 - ETA: 0s - loss: 0.6647 - accuracy: 0.58 - ETA: 0s - loss: 0.6648 - accuracy: 0.58 - ETA: 0s - loss: 0.6634 - accuracy: 0.58 - ETA: 0s - loss: 0.6623 - accuracy: 0.58 - 2s 145us/step - loss: 0.6616 - accuracy: 0.5868 - val_loss: 0.6271 - val_accuracy: 0.6889\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 49us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:06 - loss: 0.6932 - accuracy: 0.75 - ETA: 5s - loss: 0.6931 - accuracy: 0.5050 - ETA: 3s - loss: 0.6916 - accuracy: 0.51 - ETA: 2s - loss: 0.6901 - accuracy: 0.52 - ETA: 2s - loss: 0.6893 - accuracy: 0.53 - ETA: 1s - loss: 0.6897 - accuracy: 0.52 - ETA: 1s - loss: 0.6885 - accuracy: 0.53 - ETA: 1s - loss: 0.6883 - accuracy: 0.53 - ETA: 1s - loss: 0.6872 - accuracy: 0.52 - ETA: 1s - loss: 0.6865 - accuracy: 0.52 - ETA: 1s - loss: 0.6852 - accuracy: 0.52 - ETA: 1s - loss: 0.6853 - accuracy: 0.52 - ETA: 1s - loss: 0.6850 - accuracy: 0.52 - ETA: 1s - loss: 0.6837 - accuracy: 0.53 - ETA: 1s - loss: 0.6834 - accuracy: 0.53 - ETA: 0s - loss: 0.6829 - accuracy: 0.53 - ETA: 0s - loss: 0.6825 - accuracy: 0.54 - ETA: 0s - loss: 0.6816 - accuracy: 0.54 - ETA: 0s - loss: 0.6814 - accuracy: 0.54 - ETA: 0s - loss: 0.6797 - accuracy: 0.55 - ETA: 0s - loss: 0.6799 - accuracy: 0.55 - ETA: 0s - loss: 0.6797 - accuracy: 0.55 - ETA: 0s - loss: 0.6794 - accuracy: 0.55 - ETA: 0s - loss: 0.6789 - accuracy: 0.56 - ETA: 0s - loss: 0.6783 - accuracy: 0.56 - ETA: 0s - loss: 0.6780 - accuracy: 0.56 - ETA: 0s - loss: 0.6774 - accuracy: 0.57 - ETA: 0s - loss: 0.6768 - accuracy: 0.57 - ETA: 0s - loss: 0.6762 - accuracy: 0.57 - ETA: 0s - loss: 0.6754 - accuracy: 0.57 - ETA: 0s - loss: 0.6748 - accuracy: 0.57 - ETA: 0s - loss: 0.6741 - accuracy: 0.58 - 2s 143us/step - loss: 0.6737 - accuracy: 0.5823 - val_loss: 0.6401 - val_accuracy: 0.6911\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 49us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:03 - loss: 0.6940 - accuracy: 0.37 - ETA: 4s - loss: 0.6923 - accuracy: 0.5849 - ETA: 3s - loss: 0.6910 - accuracy: 0.57 - ETA: 2s - loss: 0.6912 - accuracy: 0.55 - ETA: 2s - loss: 0.6901 - accuracy: 0.55 - ETA: 1s - loss: 0.6892 - accuracy: 0.56 - ETA: 1s - loss: 0.6880 - accuracy: 0.57 - ETA: 1s - loss: 0.6880 - accuracy: 0.56 - ETA: 1s - loss: 0.6868 - accuracy: 0.57 - ETA: 1s - loss: 0.6863 - accuracy: 0.57 - ETA: 1s - loss: 0.6853 - accuracy: 0.57 - ETA: 1s - loss: 0.6837 - accuracy: 0.57 - ETA: 1s - loss: 0.6831 - accuracy: 0.57 - ETA: 1s - loss: 0.6813 - accuracy: 0.58 - ETA: 1s - loss: 0.6804 - accuracy: 0.58 - ETA: 0s - loss: 0.6783 - accuracy: 0.59 - ETA: 0s - loss: 0.6770 - accuracy: 0.59 - ETA: 0s - loss: 0.6756 - accuracy: 0.59 - ETA: 0s - loss: 0.6752 - accuracy: 0.59 - ETA: 0s - loss: 0.6732 - accuracy: 0.59 - ETA: 0s - loss: 0.6716 - accuracy: 0.59 - ETA: 0s - loss: 0.6706 - accuracy: 0.59 - ETA: 0s - loss: 0.6691 - accuracy: 0.60 - ETA: 0s - loss: 0.6670 - accuracy: 0.60 - ETA: 0s - loss: 0.6664 - accuracy: 0.60 - ETA: 0s - loss: 0.6657 - accuracy: 0.60 - ETA: 0s - loss: 0.6650 - accuracy: 0.60 - ETA: 0s - loss: 0.6640 - accuracy: 0.60 - ETA: 0s - loss: 0.6627 - accuracy: 0.60 - ETA: 0s - loss: 0.6615 - accuracy: 0.61 - ETA: 0s - loss: 0.6608 - accuracy: 0.61 - ETA: 0s - loss: 0.6592 - accuracy: 0.61 - 2s 144us/step - loss: 0.6594 - accuracy: 0.6138 - val_loss: 0.6175 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 49us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:08 - loss: 0.6920 - accuracy: 0.62 - ETA: 4s - loss: 0.6935 - accuracy: 0.5236 - ETA: 3s - loss: 0.6933 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6920 - accuracy: 0.53 - ETA: 1s - loss: 0.6914 - accuracy: 0.53 - ETA: 1s - loss: 0.6909 - accuracy: 0.53 - ETA: 1s - loss: 0.6899 - accuracy: 0.54 - ETA: 1s - loss: 0.6888 - accuracy: 0.54 - ETA: 1s - loss: 0.6881 - accuracy: 0.54 - ETA: 1s - loss: 0.6869 - accuracy: 0.55 - ETA: 1s - loss: 0.6850 - accuracy: 0.56 - ETA: 1s - loss: 0.6831 - accuracy: 0.56 - ETA: 1s - loss: 0.6809 - accuracy: 0.57 - ETA: 0s - loss: 0.6792 - accuracy: 0.57 - ETA: 0s - loss: 0.6774 - accuracy: 0.57 - ETA: 0s - loss: 0.6754 - accuracy: 0.58 - ETA: 0s - loss: 0.6742 - accuracy: 0.58 - ETA: 0s - loss: 0.6730 - accuracy: 0.58 - ETA: 0s - loss: 0.6718 - accuracy: 0.59 - ETA: 0s - loss: 0.6700 - accuracy: 0.59 - ETA: 0s - loss: 0.6693 - accuracy: 0.59 - ETA: 0s - loss: 0.6677 - accuracy: 0.59 - ETA: 0s - loss: 0.6667 - accuracy: 0.60 - ETA: 0s - loss: 0.6656 - accuracy: 0.60 - ETA: 0s - loss: 0.6638 - accuracy: 0.60 - ETA: 0s - loss: 0.6626 - accuracy: 0.60 - ETA: 0s - loss: 0.6615 - accuracy: 0.60 - ETA: 0s - loss: 0.6608 - accuracy: 0.61 - ETA: 0s - loss: 0.6596 - accuracy: 0.61 - ETA: 0s - loss: 0.6589 - accuracy: 0.61 - 2s 145us/step - loss: 0.6582 - accuracy: 0.6145 - val_loss: 0.6029 - val_accuracy: 0.7131\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:03 - loss: 0.6935 - accuracy: 0.50 - ETA: 4s - loss: 0.6938 - accuracy: 0.5370 - ETA: 3s - loss: 0.6944 - accuracy: 0.50 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.52 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6922 - accuracy: 0.53 - ETA: 1s - loss: 0.6917 - accuracy: 0.53 - ETA: 1s - loss: 0.6902 - accuracy: 0.54 - ETA: 1s - loss: 0.6889 - accuracy: 0.55 - ETA: 1s - loss: 0.6869 - accuracy: 0.55 - ETA: 1s - loss: 0.6852 - accuracy: 0.56 - ETA: 1s - loss: 0.6843 - accuracy: 0.56 - ETA: 1s - loss: 0.6824 - accuracy: 0.57 - ETA: 1s - loss: 0.6801 - accuracy: 0.57 - ETA: 0s - loss: 0.6784 - accuracy: 0.58 - ETA: 0s - loss: 0.6770 - accuracy: 0.58 - ETA: 0s - loss: 0.6756 - accuracy: 0.58 - ETA: 0s - loss: 0.6746 - accuracy: 0.58 - ETA: 0s - loss: 0.6725 - accuracy: 0.59 - ETA: 0s - loss: 0.6709 - accuracy: 0.59 - ETA: 0s - loss: 0.6685 - accuracy: 0.60 - ETA: 0s - loss: 0.6665 - accuracy: 0.60 - ETA: 0s - loss: 0.6646 - accuracy: 0.60 - ETA: 0s - loss: 0.6635 - accuracy: 0.60 - ETA: 0s - loss: 0.6621 - accuracy: 0.61 - ETA: 0s - loss: 0.6603 - accuracy: 0.61 - ETA: 0s - loss: 0.6596 - accuracy: 0.61 - ETA: 0s - loss: 0.6591 - accuracy: 0.61 - ETA: 0s - loss: 0.6580 - accuracy: 0.61 - ETA: 0s - loss: 0.6574 - accuracy: 0.61 - ETA: 0s - loss: 0.6565 - accuracy: 0.61 - 2s 144us/step - loss: 0.6556 - accuracy: 0.6197 - val_loss: 0.6012 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:13 - loss: 0.6908 - accuracy: 0.37 - ETA: 5s - loss: 0.6930 - accuracy: 0.5025 - ETA: 3s - loss: 0.6932 - accuracy: 0.49 - ETA: 2s - loss: 0.6930 - accuracy: 0.49 - ETA: 2s - loss: 0.6923 - accuracy: 0.50 - ETA: 1s - loss: 0.6915 - accuracy: 0.51 - ETA: 1s - loss: 0.6907 - accuracy: 0.52 - ETA: 1s - loss: 0.6892 - accuracy: 0.53 - ETA: 1s - loss: 0.6880 - accuracy: 0.54 - ETA: 1s - loss: 0.6867 - accuracy: 0.54 - ETA: 1s - loss: 0.6848 - accuracy: 0.55 - ETA: 1s - loss: 0.6836 - accuracy: 0.55 - ETA: 1s - loss: 0.6815 - accuracy: 0.56 - ETA: 1s - loss: 0.6799 - accuracy: 0.56 - ETA: 1s - loss: 0.6789 - accuracy: 0.57 - ETA: 0s - loss: 0.6762 - accuracy: 0.57 - ETA: 0s - loss: 0.6738 - accuracy: 0.58 - ETA: 0s - loss: 0.6723 - accuracy: 0.58 - ETA: 0s - loss: 0.6702 - accuracy: 0.59 - ETA: 0s - loss: 0.6683 - accuracy: 0.59 - ETA: 0s - loss: 0.6671 - accuracy: 0.59 - ETA: 0s - loss: 0.6650 - accuracy: 0.60 - ETA: 0s - loss: 0.6633 - accuracy: 0.60 - ETA: 0s - loss: 0.6626 - accuracy: 0.60 - ETA: 0s - loss: 0.6620 - accuracy: 0.60 - ETA: 0s - loss: 0.6604 - accuracy: 0.61 - ETA: 0s - loss: 0.6583 - accuracy: 0.61 - ETA: 0s - loss: 0.6574 - accuracy: 0.61 - ETA: 0s - loss: 0.6558 - accuracy: 0.61 - ETA: 0s - loss: 0.6554 - accuracy: 0.62 - ETA: 0s - loss: 0.6542 - accuracy: 0.62 - ETA: 0s - loss: 0.6536 - accuracy: 0.62 - 2s 144us/step - loss: 0.6526 - accuracy: 0.6275 - val_loss: 0.5968 - val_accuracy: 0.7166\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:08 - loss: 0.6959 - accuracy: 0.50 - ETA: 4s - loss: 0.6937 - accuracy: 0.5118 - ETA: 3s - loss: 0.6931 - accuracy: 0.52 - ETA: 2s - loss: 0.6921 - accuracy: 0.52 - ETA: 2s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.52 - ETA: 1s - loss: 0.6895 - accuracy: 0.53 - ETA: 1s - loss: 0.6888 - accuracy: 0.53 - ETA: 1s - loss: 0.6878 - accuracy: 0.54 - ETA: 1s - loss: 0.6865 - accuracy: 0.55 - ETA: 1s - loss: 0.6845 - accuracy: 0.55 - ETA: 1s - loss: 0.6832 - accuracy: 0.56 - ETA: 1s - loss: 0.6814 - accuracy: 0.57 - ETA: 1s - loss: 0.6797 - accuracy: 0.57 - ETA: 1s - loss: 0.6781 - accuracy: 0.58 - ETA: 0s - loss: 0.6766 - accuracy: 0.58 - ETA: 0s - loss: 0.6737 - accuracy: 0.59 - ETA: 0s - loss: 0.6724 - accuracy: 0.59 - ETA: 0s - loss: 0.6712 - accuracy: 0.59 - ETA: 0s - loss: 0.6696 - accuracy: 0.59 - ETA: 0s - loss: 0.6677 - accuracy: 0.60 - ETA: 0s - loss: 0.6658 - accuracy: 0.60 - ETA: 0s - loss: 0.6646 - accuracy: 0.60 - ETA: 0s - loss: 0.6620 - accuracy: 0.60 - ETA: 0s - loss: 0.6599 - accuracy: 0.61 - ETA: 0s - loss: 0.6586 - accuracy: 0.61 - ETA: 0s - loss: 0.6571 - accuracy: 0.61 - ETA: 0s - loss: 0.6563 - accuracy: 0.61 - ETA: 0s - loss: 0.6556 - accuracy: 0.61 - ETA: 0s - loss: 0.6543 - accuracy: 0.62 - ETA: 0s - loss: 0.6530 - accuracy: 0.62 - ETA: 0s - loss: 0.6520 - accuracy: 0.62 - 2s 146us/step - loss: 0.6506 - accuracy: 0.6270 - val_loss: 0.5919 - val_accuracy: 0.7195\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 48us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:02 - loss: 0.6900 - accuracy: 0.62 - ETA: 4s - loss: 0.6936 - accuracy: 0.4906 - ETA: 3s - loss: 0.6928 - accuracy: 0.51 - ETA: 2s - loss: 0.6916 - accuracy: 0.52 - ETA: 2s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6909 - accuracy: 0.53 - ETA: 1s - loss: 0.6905 - accuracy: 0.53 - ETA: 1s - loss: 0.6889 - accuracy: 0.55 - ETA: 1s - loss: 0.6879 - accuracy: 0.55 - ETA: 1s - loss: 0.6861 - accuracy: 0.56 - ETA: 1s - loss: 0.6853 - accuracy: 0.56 - ETA: 1s - loss: 0.6836 - accuracy: 0.57 - ETA: 1s - loss: 0.6823 - accuracy: 0.57 - ETA: 1s - loss: 0.6803 - accuracy: 0.58 - ETA: 0s - loss: 0.6792 - accuracy: 0.58 - ETA: 0s - loss: 0.6774 - accuracy: 0.58 - ETA: 0s - loss: 0.6755 - accuracy: 0.59 - ETA: 0s - loss: 0.6731 - accuracy: 0.59 - ETA: 0s - loss: 0.6723 - accuracy: 0.59 - ETA: 0s - loss: 0.6700 - accuracy: 0.60 - ETA: 0s - loss: 0.6682 - accuracy: 0.60 - ETA: 0s - loss: 0.6678 - accuracy: 0.60 - ETA: 0s - loss: 0.6662 - accuracy: 0.60 - ETA: 0s - loss: 0.6652 - accuracy: 0.60 - ETA: 0s - loss: 0.6647 - accuracy: 0.61 - ETA: 0s - loss: 0.6635 - accuracy: 0.61 - ETA: 0s - loss: 0.6614 - accuracy: 0.61 - ETA: 0s - loss: 0.6597 - accuracy: 0.61 - ETA: 0s - loss: 0.6590 - accuracy: 0.61 - ETA: 0s - loss: 0.6579 - accuracy: 0.61 - ETA: 0s - loss: 0.6566 - accuracy: 0.61 - ETA: 0s - loss: 0.6550 - accuracy: 0.62 - 2s 143us/step - loss: 0.6547 - accuracy: 0.6218 - val_loss: 0.5981 - val_accuracy: 0.7159\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 49us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:55 - loss: 0.7000 - accuracy: 0.25 - ETA: 4s - loss: 0.6946 - accuracy: 0.4485 - ETA: 3s - loss: 0.6938 - accuracy: 0.48 - ETA: 2s - loss: 0.6929 - accuracy: 0.51 - ETA: 2s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6902 - accuracy: 0.53 - ETA: 1s - loss: 0.6889 - accuracy: 0.53 - ETA: 1s - loss: 0.6874 - accuracy: 0.54 - ETA: 1s - loss: 0.6865 - accuracy: 0.55 - ETA: 1s - loss: 0.6858 - accuracy: 0.55 - ETA: 1s - loss: 0.6839 - accuracy: 0.55 - ETA: 1s - loss: 0.6828 - accuracy: 0.56 - ETA: 1s - loss: 0.6815 - accuracy: 0.56 - ETA: 1s - loss: 0.6788 - accuracy: 0.57 - ETA: 0s - loss: 0.6759 - accuracy: 0.57 - ETA: 0s - loss: 0.6738 - accuracy: 0.58 - ETA: 0s - loss: 0.6717 - accuracy: 0.58 - ETA: 0s - loss: 0.6708 - accuracy: 0.58 - ETA: 0s - loss: 0.6694 - accuracy: 0.59 - ETA: 0s - loss: 0.6686 - accuracy: 0.59 - ETA: 0s - loss: 0.6671 - accuracy: 0.59 - ETA: 0s - loss: 0.6661 - accuracy: 0.59 - ETA: 0s - loss: 0.6651 - accuracy: 0.59 - ETA: 0s - loss: 0.6642 - accuracy: 0.59 - ETA: 0s - loss: 0.6624 - accuracy: 0.60 - ETA: 0s - loss: 0.6611 - accuracy: 0.60 - ETA: 0s - loss: 0.6601 - accuracy: 0.60 - ETA: 0s - loss: 0.6587 - accuracy: 0.60 - ETA: 0s - loss: 0.6577 - accuracy: 0.60 - ETA: 0s - loss: 0.6562 - accuracy: 0.61 - ETA: 0s - loss: 0.6555 - accuracy: 0.61 - ETA: 0s - loss: 0.6543 - accuracy: 0.61 - 2s 148us/step - loss: 0.6538 - accuracy: 0.6155 - val_loss: 0.6043 - val_accuracy: 0.7237\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 66us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 3:08 - loss: 0.6930 - accuracy: 0.62 - ETA: 4s - loss: 0.6944 - accuracy: 0.5048 - ETA: 3s - loss: 0.6942 - accuracy: 0.49 - ETA: 2s - loss: 0.6941 - accuracy: 0.50 - ETA: 2s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6905 - accuracy: 0.53 - ETA: 1s - loss: 0.6887 - accuracy: 0.54 - ETA: 1s - loss: 0.6870 - accuracy: 0.55 - ETA: 1s - loss: 0.6841 - accuracy: 0.56 - ETA: 1s - loss: 0.6815 - accuracy: 0.57 - ETA: 1s - loss: 0.6782 - accuracy: 0.58 - ETA: 1s - loss: 0.6763 - accuracy: 0.58 - ETA: 1s - loss: 0.6741 - accuracy: 0.59 - ETA: 1s - loss: 0.6726 - accuracy: 0.59 - ETA: 0s - loss: 0.6712 - accuracy: 0.59 - ETA: 0s - loss: 0.6698 - accuracy: 0.59 - ETA: 0s - loss: 0.6670 - accuracy: 0.60 - ETA: 0s - loss: 0.6659 - accuracy: 0.60 - ETA: 0s - loss: 0.6639 - accuracy: 0.60 - ETA: 0s - loss: 0.6623 - accuracy: 0.61 - ETA: 0s - loss: 0.6602 - accuracy: 0.61 - ETA: 0s - loss: 0.6589 - accuracy: 0.61 - ETA: 0s - loss: 0.6569 - accuracy: 0.61 - ETA: 0s - loss: 0.6559 - accuracy: 0.62 - ETA: 0s - loss: 0.6540 - accuracy: 0.62 - ETA: 0s - loss: 0.6526 - accuracy: 0.62 - ETA: 0s - loss: 0.6508 - accuracy: 0.62 - ETA: 0s - loss: 0.6498 - accuracy: 0.62 - ETA: 0s - loss: 0.6485 - accuracy: 0.62 - ETA: 0s - loss: 0.6476 - accuracy: 0.63 - ETA: 0s - loss: 0.6467 - accuracy: 0.63 - 2s 144us/step - loss: 0.6459 - accuracy: 0.6339 - val_loss: 0.5883 - val_accuracy: 0.7216\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:03 - loss: 0.7026 - accuracy: 0.37 - ETA: 4s - loss: 0.6951 - accuracy: 0.4788 - ETA: 3s - loss: 0.6942 - accuracy: 0.50 - ETA: 2s - loss: 0.6927 - accuracy: 0.52 - ETA: 2s - loss: 0.6913 - accuracy: 0.52 - ETA: 1s - loss: 0.6900 - accuracy: 0.53 - ETA: 1s - loss: 0.6893 - accuracy: 0.54 - ETA: 1s - loss: 0.6870 - accuracy: 0.55 - ETA: 1s - loss: 0.6846 - accuracy: 0.56 - ETA: 1s - loss: 0.6835 - accuracy: 0.56 - ETA: 1s - loss: 0.6807 - accuracy: 0.57 - ETA: 1s - loss: 0.6788 - accuracy: 0.58 - ETA: 1s - loss: 0.6760 - accuracy: 0.59 - ETA: 1s - loss: 0.6743 - accuracy: 0.59 - ETA: 1s - loss: 0.6733 - accuracy: 0.59 - ETA: 0s - loss: 0.6719 - accuracy: 0.59 - ETA: 0s - loss: 0.6705 - accuracy: 0.60 - ETA: 0s - loss: 0.6686 - accuracy: 0.60 - ETA: 0s - loss: 0.6671 - accuracy: 0.60 - ETA: 0s - loss: 0.6644 - accuracy: 0.61 - ETA: 0s - loss: 0.6624 - accuracy: 0.61 - ETA: 0s - loss: 0.6598 - accuracy: 0.62 - ETA: 0s - loss: 0.6581 - accuracy: 0.62 - ETA: 0s - loss: 0.6576 - accuracy: 0.62 - ETA: 0s - loss: 0.6568 - accuracy: 0.62 - ETA: 0s - loss: 0.6547 - accuracy: 0.63 - ETA: 0s - loss: 0.6524 - accuracy: 0.63 - ETA: 0s - loss: 0.6500 - accuracy: 0.63 - ETA: 0s - loss: 0.6481 - accuracy: 0.63 - ETA: 0s - loss: 0.6469 - accuracy: 0.64 - ETA: 0s - loss: 0.6461 - accuracy: 0.64 - ETA: 0s - loss: 0.6441 - accuracy: 0.64 - 2s 144us/step - loss: 0.6428 - accuracy: 0.6447 - val_loss: 0.5819 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 50us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:57 - loss: 0.6935 - accuracy: 0.62 - ETA: 4s - loss: 0.6937 - accuracy: 0.5613 - ETA: 3s - loss: 0.6935 - accuracy: 0.55 - ETA: 2s - loss: 0.6916 - accuracy: 0.56 - ETA: 2s - loss: 0.6900 - accuracy: 0.57 - ETA: 1s - loss: 0.6885 - accuracy: 0.57 - ETA: 1s - loss: 0.6874 - accuracy: 0.57 - ETA: 1s - loss: 0.6860 - accuracy: 0.58 - ETA: 1s - loss: 0.6834 - accuracy: 0.59 - ETA: 1s - loss: 0.6807 - accuracy: 0.59 - ETA: 1s - loss: 0.6765 - accuracy: 0.60 - ETA: 1s - loss: 0.6750 - accuracy: 0.60 - ETA: 1s - loss: 0.6718 - accuracy: 0.60 - ETA: 1s - loss: 0.6695 - accuracy: 0.61 - ETA: 0s - loss: 0.6670 - accuracy: 0.61 - ETA: 0s - loss: 0.6640 - accuracy: 0.61 - ETA: 0s - loss: 0.6618 - accuracy: 0.62 - ETA: 0s - loss: 0.6611 - accuracy: 0.62 - ETA: 0s - loss: 0.6587 - accuracy: 0.62 - ETA: 0s - loss: 0.6558 - accuracy: 0.63 - ETA: 0s - loss: 0.6538 - accuracy: 0.63 - ETA: 0s - loss: 0.6516 - accuracy: 0.63 - ETA: 0s - loss: 0.6496 - accuracy: 0.63 - ETA: 0s - loss: 0.6484 - accuracy: 0.64 - ETA: 0s - loss: 0.6471 - accuracy: 0.64 - ETA: 0s - loss: 0.6448 - accuracy: 0.64 - ETA: 0s - loss: 0.6434 - accuracy: 0.64 - ETA: 0s - loss: 0.6412 - accuracy: 0.64 - ETA: 0s - loss: 0.6402 - accuracy: 0.65 - ETA: 0s - loss: 0.6388 - accuracy: 0.65 - ETA: 0s - loss: 0.6370 - accuracy: 0.65 - 2s 139us/step - loss: 0.6367 - accuracy: 0.6560 - val_loss: 0.5821 - val_accuracy: 0.7173\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 49us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:58 - loss: 0.6981 - accuracy: 0.37 - ETA: 4s - loss: 0.6938 - accuracy: 0.4953 - ETA: 2s - loss: 0.6928 - accuracy: 0.53 - ETA: 2s - loss: 0.6920 - accuracy: 0.53 - ETA: 2s - loss: 0.6909 - accuracy: 0.54 - ETA: 1s - loss: 0.6900 - accuracy: 0.54 - ETA: 1s - loss: 0.6887 - accuracy: 0.54 - ETA: 1s - loss: 0.6867 - accuracy: 0.55 - ETA: 1s - loss: 0.6860 - accuracy: 0.56 - ETA: 1s - loss: 0.6841 - accuracy: 0.56 - ETA: 1s - loss: 0.6813 - accuracy: 0.57 - ETA: 1s - loss: 0.6786 - accuracy: 0.58 - ETA: 1s - loss: 0.6756 - accuracy: 0.58 - ETA: 1s - loss: 0.6735 - accuracy: 0.59 - ETA: 1s - loss: 0.6723 - accuracy: 0.59 - ETA: 0s - loss: 0.6717 - accuracy: 0.59 - ETA: 0s - loss: 0.6687 - accuracy: 0.60 - ETA: 0s - loss: 0.6673 - accuracy: 0.60 - ETA: 0s - loss: 0.6654 - accuracy: 0.61 - ETA: 0s - loss: 0.6627 - accuracy: 0.61 - ETA: 0s - loss: 0.6600 - accuracy: 0.61 - ETA: 0s - loss: 0.6586 - accuracy: 0.62 - ETA: 0s - loss: 0.6564 - accuracy: 0.62 - ETA: 0s - loss: 0.6538 - accuracy: 0.62 - ETA: 0s - loss: 0.6521 - accuracy: 0.63 - ETA: 0s - loss: 0.6497 - accuracy: 0.63 - ETA: 0s - loss: 0.6477 - accuracy: 0.63 - ETA: 0s - loss: 0.6452 - accuracy: 0.64 - ETA: 0s - loss: 0.6435 - accuracy: 0.64 - ETA: 0s - loss: 0.6426 - accuracy: 0.64 - ETA: 0s - loss: 0.6408 - accuracy: 0.64 - ETA: 0s - loss: 0.6397 - accuracy: 0.64 - 2s 143us/step - loss: 0.6389 - accuracy: 0.6490 - val_loss: 0.5880 - val_accuracy: 0.7138\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 49us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:02 - loss: 0.6947 - accuracy: 0.50 - ETA: 4s - loss: 0.6934 - accuracy: 0.5425 - ETA: 2s - loss: 0.6930 - accuracy: 0.53 - ETA: 2s - loss: 0.6925 - accuracy: 0.53 - ETA: 2s - loss: 0.6914 - accuracy: 0.55 - ETA: 1s - loss: 0.6899 - accuracy: 0.56 - ETA: 1s - loss: 0.6876 - accuracy: 0.57 - ETA: 1s - loss: 0.6859 - accuracy: 0.58 - ETA: 1s - loss: 0.6837 - accuracy: 0.58 - ETA: 1s - loss: 0.6823 - accuracy: 0.59 - ETA: 1s - loss: 0.6814 - accuracy: 0.59 - ETA: 1s - loss: 0.6795 - accuracy: 0.59 - ETA: 1s - loss: 0.6770 - accuracy: 0.59 - ETA: 1s - loss: 0.6752 - accuracy: 0.60 - ETA: 1s - loss: 0.6727 - accuracy: 0.60 - ETA: 0s - loss: 0.6696 - accuracy: 0.61 - ETA: 0s - loss: 0.6663 - accuracy: 0.61 - ETA: 0s - loss: 0.6648 - accuracy: 0.61 - ETA: 0s - loss: 0.6631 - accuracy: 0.62 - ETA: 0s - loss: 0.6628 - accuracy: 0.62 - ETA: 0s - loss: 0.6605 - accuracy: 0.62 - ETA: 0s - loss: 0.6577 - accuracy: 0.62 - ETA: 0s - loss: 0.6572 - accuracy: 0.63 - ETA: 0s - loss: 0.6561 - accuracy: 0.63 - ETA: 0s - loss: 0.6542 - accuracy: 0.63 - ETA: 0s - loss: 0.6523 - accuracy: 0.63 - ETA: 0s - loss: 0.6512 - accuracy: 0.63 - ETA: 0s - loss: 0.6497 - accuracy: 0.64 - ETA: 0s - loss: 0.6477 - accuracy: 0.64 - ETA: 0s - loss: 0.6460 - accuracy: 0.64 - ETA: 0s - loss: 0.6452 - accuracy: 0.64 - ETA: 0s - loss: 0.6444 - accuracy: 0.64 - 2s 146us/step - loss: 0.6439 - accuracy: 0.6474 - val_loss: 0.5899 - val_accuracy: 0.7259\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 55us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:14 - loss: 0.6987 - accuracy: 0.12 - ETA: 5s - loss: 0.6942 - accuracy: 0.5150 - ETA: 3s - loss: 0.6926 - accuracy: 0.54 - ETA: 2s - loss: 0.6905 - accuracy: 0.54 - ETA: 2s - loss: 0.6899 - accuracy: 0.55 - ETA: 1s - loss: 0.6875 - accuracy: 0.56 - ETA: 1s - loss: 0.6850 - accuracy: 0.57 - ETA: 1s - loss: 0.6828 - accuracy: 0.57 - ETA: 1s - loss: 0.6788 - accuracy: 0.59 - ETA: 1s - loss: 0.6749 - accuracy: 0.60 - ETA: 1s - loss: 0.6726 - accuracy: 0.60 - ETA: 1s - loss: 0.6708 - accuracy: 0.60 - ETA: 1s - loss: 0.6678 - accuracy: 0.61 - ETA: 1s - loss: 0.6653 - accuracy: 0.61 - ETA: 0s - loss: 0.6630 - accuracy: 0.62 - ETA: 0s - loss: 0.6604 - accuracy: 0.62 - ETA: 0s - loss: 0.6580 - accuracy: 0.62 - ETA: 0s - loss: 0.6557 - accuracy: 0.63 - ETA: 0s - loss: 0.6557 - accuracy: 0.63 - ETA: 0s - loss: 0.6541 - accuracy: 0.63 - ETA: 0s - loss: 0.6532 - accuracy: 0.63 - ETA: 0s - loss: 0.6515 - accuracy: 0.63 - ETA: 0s - loss: 0.6482 - accuracy: 0.64 - ETA: 0s - loss: 0.6464 - accuracy: 0.64 - ETA: 0s - loss: 0.6453 - accuracy: 0.64 - ETA: 0s - loss: 0.6444 - accuracy: 0.64 - ETA: 0s - loss: 0.6429 - accuracy: 0.64 - ETA: 0s - loss: 0.6416 - accuracy: 0.64 - ETA: 0s - loss: 0.6409 - accuracy: 0.64 - ETA: 0s - loss: 0.6396 - accuracy: 0.65 - ETA: 0s - loss: 0.6385 - accuracy: 0.65 - 2s 141us/step - loss: 0.6376 - accuracy: 0.6540 - val_loss: 0.5929 - val_accuracy: 0.7216\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 57us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:27 - loss: 0.6920 - accuracy: 0.37 - ETA: 3s - loss: 0.6938 - accuracy: 0.5023 - ETA: 2s - loss: 0.6934 - accuracy: 0.50 - ETA: 2s - loss: 0.6921 - accuracy: 0.51 - ETA: 1s - loss: 0.6912 - accuracy: 0.52 - ETA: 1s - loss: 0.6906 - accuracy: 0.52 - ETA: 1s - loss: 0.6895 - accuracy: 0.53 - ETA: 1s - loss: 0.6893 - accuracy: 0.52 - ETA: 1s - loss: 0.6890 - accuracy: 0.52 - ETA: 1s - loss: 0.6881 - accuracy: 0.53 - ETA: 1s - loss: 0.6875 - accuracy: 0.53 - ETA: 1s - loss: 0.6871 - accuracy: 0.53 - ETA: 0s - loss: 0.6865 - accuracy: 0.53 - ETA: 0s - loss: 0.6859 - accuracy: 0.54 - ETA: 0s - loss: 0.6854 - accuracy: 0.54 - ETA: 0s - loss: 0.6852 - accuracy: 0.54 - ETA: 0s - loss: 0.6849 - accuracy: 0.54 - ETA: 0s - loss: 0.6848 - accuracy: 0.54 - ETA: 0s - loss: 0.6847 - accuracy: 0.54 - ETA: 0s - loss: 0.6843 - accuracy: 0.54 - ETA: 0s - loss: 0.6837 - accuracy: 0.54 - ETA: 0s - loss: 0.6831 - accuracy: 0.55 - ETA: 0s - loss: 0.6828 - accuracy: 0.55 - ETA: 0s - loss: 0.6826 - accuracy: 0.55 - ETA: 0s - loss: 0.6822 - accuracy: 0.55 - ETA: 0s - loss: 0.6818 - accuracy: 0.55 - ETA: 0s - loss: 0.6812 - accuracy: 0.55 - ETA: 0s - loss: 0.6808 - accuracy: 0.55 - ETA: 0s - loss: 0.6802 - accuracy: 0.56 - 2s 130us/step - loss: 0.6797 - accuracy: 0.5639 - val_loss: 0.6610 - val_accuracy: 0.6875\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 48us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:28 - loss: 0.6974 - accuracy: 0.12 - ETA: 3s - loss: 0.6907 - accuracy: 0.5417 - ETA: 2s - loss: 0.6891 - accuracy: 0.53 - ETA: 2s - loss: 0.6886 - accuracy: 0.52 - ETA: 1s - loss: 0.6877 - accuracy: 0.53 - ETA: 1s - loss: 0.6882 - accuracy: 0.53 - ETA: 1s - loss: 0.6881 - accuracy: 0.53 - ETA: 1s - loss: 0.6880 - accuracy: 0.52 - ETA: 1s - loss: 0.6883 - accuracy: 0.52 - ETA: 1s - loss: 0.6886 - accuracy: 0.52 - ETA: 1s - loss: 0.6882 - accuracy: 0.52 - ETA: 1s - loss: 0.6877 - accuracy: 0.52 - ETA: 0s - loss: 0.6875 - accuracy: 0.52 - ETA: 0s - loss: 0.6874 - accuracy: 0.52 - ETA: 0s - loss: 0.6872 - accuracy: 0.53 - ETA: 0s - loss: 0.6868 - accuracy: 0.53 - ETA: 0s - loss: 0.6863 - accuracy: 0.53 - ETA: 0s - loss: 0.6862 - accuracy: 0.53 - ETA: 0s - loss: 0.6862 - accuracy: 0.53 - ETA: 0s - loss: 0.6856 - accuracy: 0.54 - ETA: 0s - loss: 0.6851 - accuracy: 0.54 - ETA: 0s - loss: 0.6844 - accuracy: 0.54 - ETA: 0s - loss: 0.6840 - accuracy: 0.54 - ETA: 0s - loss: 0.6833 - accuracy: 0.55 - ETA: 0s - loss: 0.6828 - accuracy: 0.55 - ETA: 0s - loss: 0.6823 - accuracy: 0.55 - ETA: 0s - loss: 0.6823 - accuracy: 0.55 - ETA: 0s - loss: 0.6820 - accuracy: 0.55 - ETA: 0s - loss: 0.6817 - accuracy: 0.55 - 2s 130us/step - loss: 0.6816 - accuracy: 0.5559 - val_loss: 0.6645 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 50us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:35 - loss: 0.6934 - accuracy: 0.62 - ETA: 4s - loss: 0.6949 - accuracy: 0.5068 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6911 - accuracy: 0.53 - ETA: 1s - loss: 0.6906 - accuracy: 0.53 - ETA: 1s - loss: 0.6888 - accuracy: 0.54 - ETA: 1s - loss: 0.6883 - accuracy: 0.54 - ETA: 1s - loss: 0.6871 - accuracy: 0.54 - ETA: 1s - loss: 0.6868 - accuracy: 0.54 - ETA: 1s - loss: 0.6864 - accuracy: 0.55 - ETA: 1s - loss: 0.6860 - accuracy: 0.55 - ETA: 1s - loss: 0.6854 - accuracy: 0.55 - ETA: 0s - loss: 0.6846 - accuracy: 0.55 - ETA: 0s - loss: 0.6838 - accuracy: 0.55 - ETA: 0s - loss: 0.6833 - accuracy: 0.56 - ETA: 0s - loss: 0.6828 - accuracy: 0.56 - ETA: 0s - loss: 0.6819 - accuracy: 0.56 - ETA: 0s - loss: 0.6814 - accuracy: 0.56 - ETA: 0s - loss: 0.6803 - accuracy: 0.56 - ETA: 0s - loss: 0.6792 - accuracy: 0.57 - ETA: 0s - loss: 0.6783 - accuracy: 0.57 - ETA: 0s - loss: 0.6776 - accuracy: 0.57 - ETA: 0s - loss: 0.6769 - accuracy: 0.57 - ETA: 0s - loss: 0.6764 - accuracy: 0.57 - ETA: 0s - loss: 0.6759 - accuracy: 0.57 - ETA: 0s - loss: 0.6752 - accuracy: 0.57 - ETA: 0s - loss: 0.6746 - accuracy: 0.57 - ETA: 0s - loss: 0.6741 - accuracy: 0.58 - 2s 131us/step - loss: 0.6740 - accuracy: 0.5824 - val_loss: 0.6521 - val_accuracy: 0.7067\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 50us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:35 - loss: 0.6982 - accuracy: 0.37 - ETA: 3s - loss: 0.6935 - accuracy: 0.5066 - ETA: 2s - loss: 0.6936 - accuracy: 0.49 - ETA: 2s - loss: 0.6936 - accuracy: 0.49 - ETA: 1s - loss: 0.6935 - accuracy: 0.48 - ETA: 1s - loss: 0.6935 - accuracy: 0.49 - ETA: 1s - loss: 0.6934 - accuracy: 0.48 - ETA: 1s - loss: 0.6932 - accuracy: 0.50 - ETA: 1s - loss: 0.6931 - accuracy: 0.50 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6926 - accuracy: 0.53 - ETA: 1s - loss: 0.6925 - accuracy: 0.53 - ETA: 0s - loss: 0.6924 - accuracy: 0.53 - ETA: 0s - loss: 0.6922 - accuracy: 0.54 - ETA: 0s - loss: 0.6920 - accuracy: 0.54 - ETA: 0s - loss: 0.6918 - accuracy: 0.54 - ETA: 0s - loss: 0.6916 - accuracy: 0.54 - ETA: 0s - loss: 0.6914 - accuracy: 0.54 - ETA: 0s - loss: 0.6912 - accuracy: 0.54 - ETA: 0s - loss: 0.6910 - accuracy: 0.54 - ETA: 0s - loss: 0.6908 - accuracy: 0.54 - ETA: 0s - loss: 0.6907 - accuracy: 0.54 - ETA: 0s - loss: 0.6906 - accuracy: 0.54 - ETA: 0s - loss: 0.6904 - accuracy: 0.54 - ETA: 0s - loss: 0.6903 - accuracy: 0.54 - ETA: 0s - loss: 0.6902 - accuracy: 0.54 - ETA: 0s - loss: 0.6901 - accuracy: 0.55 - ETA: 0s - loss: 0.6899 - accuracy: 0.55 - ETA: 0s - loss: 0.6898 - accuracy: 0.55 - 2s 133us/step - loss: 0.6898 - accuracy: 0.5535 - val_loss: 0.6833 - val_accuracy: 0.7053\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 49us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 2:41 - loss: 0.6921 - accuracy: 0.75 - ETA: 4s - loss: 0.6933 - accuracy: 0.5023 - ETA: 2s - loss: 0.6931 - accuracy: 0.50 - ETA: 2s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6917 - accuracy: 0.54 - ETA: 1s - loss: 0.6910 - accuracy: 0.54 - ETA: 1s - loss: 0.6905 - accuracy: 0.54 - ETA: 1s - loss: 0.6899 - accuracy: 0.55 - ETA: 1s - loss: 0.6891 - accuracy: 0.55 - ETA: 1s - loss: 0.6887 - accuracy: 0.55 - ETA: 1s - loss: 0.6877 - accuracy: 0.56 - ETA: 1s - loss: 0.6869 - accuracy: 0.56 - ETA: 1s - loss: 0.6862 - accuracy: 0.57 - ETA: 0s - loss: 0.6855 - accuracy: 0.57 - ETA: 0s - loss: 0.6841 - accuracy: 0.57 - ETA: 0s - loss: 0.6832 - accuracy: 0.57 - ETA: 0s - loss: 0.6824 - accuracy: 0.58 - ETA: 0s - loss: 0.6816 - accuracy: 0.58 - ETA: 0s - loss: 0.6809 - accuracy: 0.58 - ETA: 0s - loss: 0.6799 - accuracy: 0.58 - ETA: 0s - loss: 0.6796 - accuracy: 0.59 - ETA: 0s - loss: 0.6790 - accuracy: 0.59 - ETA: 0s - loss: 0.6780 - accuracy: 0.59 - ETA: 0s - loss: 0.6770 - accuracy: 0.59 - ETA: 0s - loss: 0.6765 - accuracy: 0.59 - ETA: 0s - loss: 0.6762 - accuracy: 0.59 - ETA: 0s - loss: 0.6756 - accuracy: 0.59 - ETA: 0s - loss: 0.6749 - accuracy: 0.59 - ETA: 0s - loss: 0.6741 - accuracy: 0.59 - 2s 131us/step - loss: 0.6738 - accuracy: 0.5994 - val_loss: 0.6471 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:35 - loss: 0.6938 - accuracy: 0.50 - ETA: 4s - loss: 0.6935 - accuracy: 0.5208 - ETA: 2s - loss: 0.6923 - accuracy: 0.52 - ETA: 2s - loss: 0.6912 - accuracy: 0.52 - ETA: 1s - loss: 0.6914 - accuracy: 0.51 - ETA: 1s - loss: 0.6913 - accuracy: 0.50 - ETA: 1s - loss: 0.6899 - accuracy: 0.50 - ETA: 1s - loss: 0.6896 - accuracy: 0.50 - ETA: 1s - loss: 0.6890 - accuracy: 0.50 - ETA: 1s - loss: 0.6883 - accuracy: 0.50 - ETA: 1s - loss: 0.6881 - accuracy: 0.50 - ETA: 1s - loss: 0.6880 - accuracy: 0.50 - ETA: 0s - loss: 0.6873 - accuracy: 0.51 - ETA: 0s - loss: 0.6870 - accuracy: 0.51 - ETA: 0s - loss: 0.6868 - accuracy: 0.51 - ETA: 0s - loss: 0.6861 - accuracy: 0.52 - ETA: 0s - loss: 0.6856 - accuracy: 0.52 - ETA: 0s - loss: 0.6852 - accuracy: 0.52 - ETA: 0s - loss: 0.6848 - accuracy: 0.53 - ETA: 0s - loss: 0.6840 - accuracy: 0.53 - ETA: 0s - loss: 0.6839 - accuracy: 0.53 - ETA: 0s - loss: 0.6835 - accuracy: 0.54 - ETA: 0s - loss: 0.6829 - accuracy: 0.54 - ETA: 0s - loss: 0.6821 - accuracy: 0.54 - ETA: 0s - loss: 0.6813 - accuracy: 0.54 - ETA: 0s - loss: 0.6808 - accuracy: 0.55 - ETA: 0s - loss: 0.6801 - accuracy: 0.55 - ETA: 0s - loss: 0.6796 - accuracy: 0.55 - ETA: 0s - loss: 0.6794 - accuracy: 0.55 - 2s 131us/step - loss: 0.6792 - accuracy: 0.5563 - val_loss: 0.6654 - val_accuracy: 0.6357\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 50us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:35 - loss: 0.6948 - accuracy: 0.62 - ETA: 3s - loss: 0.6927 - accuracy: 0.5329 - ETA: 2s - loss: 0.6903 - accuracy: 0.52 - ETA: 2s - loss: 0.6896 - accuracy: 0.52 - ETA: 1s - loss: 0.6885 - accuracy: 0.52 - ETA: 1s - loss: 0.6870 - accuracy: 0.53 - ETA: 1s - loss: 0.6853 - accuracy: 0.53 - ETA: 1s - loss: 0.6846 - accuracy: 0.54 - ETA: 1s - loss: 0.6842 - accuracy: 0.54 - ETA: 1s - loss: 0.6832 - accuracy: 0.54 - ETA: 1s - loss: 0.6826 - accuracy: 0.55 - ETA: 1s - loss: 0.6816 - accuracy: 0.56 - ETA: 0s - loss: 0.6804 - accuracy: 0.56 - ETA: 0s - loss: 0.6798 - accuracy: 0.57 - ETA: 0s - loss: 0.6795 - accuracy: 0.57 - ETA: 0s - loss: 0.6784 - accuracy: 0.57 - ETA: 0s - loss: 0.6779 - accuracy: 0.57 - ETA: 0s - loss: 0.6764 - accuracy: 0.57 - ETA: 0s - loss: 0.6752 - accuracy: 0.58 - ETA: 0s - loss: 0.6748 - accuracy: 0.58 - ETA: 0s - loss: 0.6741 - accuracy: 0.58 - ETA: 0s - loss: 0.6736 - accuracy: 0.58 - ETA: 0s - loss: 0.6728 - accuracy: 0.58 - ETA: 0s - loss: 0.6720 - accuracy: 0.58 - ETA: 0s - loss: 0.6710 - accuracy: 0.59 - ETA: 0s - loss: 0.6705 - accuracy: 0.59 - ETA: 0s - loss: 0.6702 - accuracy: 0.59 - ETA: 0s - loss: 0.6695 - accuracy: 0.59 - 2s 128us/step - loss: 0.6691 - accuracy: 0.5978 - val_loss: 0.6444 - val_accuracy: 0.7074\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:25 - loss: 0.6936 - accuracy: 0.37 - ETA: 3s - loss: 0.6948 - accuracy: 0.4636 - ETA: 2s - loss: 0.6936 - accuracy: 0.49 - ETA: 2s - loss: 0.6927 - accuracy: 0.51 - ETA: 2s - loss: 0.6910 - accuracy: 0.52 - ETA: 1s - loss: 0.6904 - accuracy: 0.52 - ETA: 1s - loss: 0.6895 - accuracy: 0.53 - ETA: 1s - loss: 0.6889 - accuracy: 0.54 - ETA: 1s - loss: 0.6878 - accuracy: 0.55 - ETA: 1s - loss: 0.6870 - accuracy: 0.56 - ETA: 1s - loss: 0.6857 - accuracy: 0.56 - ETA: 1s - loss: 0.6850 - accuracy: 0.56 - ETA: 1s - loss: 0.6835 - accuracy: 0.57 - ETA: 0s - loss: 0.6825 - accuracy: 0.58 - ETA: 0s - loss: 0.6818 - accuracy: 0.58 - ETA: 0s - loss: 0.6810 - accuracy: 0.58 - ETA: 0s - loss: 0.6803 - accuracy: 0.58 - ETA: 0s - loss: 0.6796 - accuracy: 0.59 - ETA: 0s - loss: 0.6785 - accuracy: 0.59 - ETA: 0s - loss: 0.6775 - accuracy: 0.59 - ETA: 0s - loss: 0.6763 - accuracy: 0.59 - ETA: 0s - loss: 0.6758 - accuracy: 0.60 - ETA: 0s - loss: 0.6746 - accuracy: 0.60 - ETA: 0s - loss: 0.6738 - accuracy: 0.60 - ETA: 0s - loss: 0.6726 - accuracy: 0.60 - ETA: 0s - loss: 0.6720 - accuracy: 0.60 - ETA: 0s - loss: 0.6711 - accuracy: 0.61 - ETA: 0s - loss: 0.6703 - accuracy: 0.61 - ETA: 0s - loss: 0.6700 - accuracy: 0.61 - 2s 131us/step - loss: 0.6696 - accuracy: 0.6149 - val_loss: 0.6423 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 49us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:25 - loss: 0.6919 - accuracy: 0.75 - ETA: 3s - loss: 0.6915 - accuracy: 0.5295 - ETA: 2s - loss: 0.6923 - accuracy: 0.51 - ETA: 2s - loss: 0.6915 - accuracy: 0.52 - ETA: 1s - loss: 0.6908 - accuracy: 0.54 - ETA: 1s - loss: 0.6900 - accuracy: 0.54 - ETA: 1s - loss: 0.6891 - accuracy: 0.55 - ETA: 1s - loss: 0.6882 - accuracy: 0.55 - ETA: 1s - loss: 0.6867 - accuracy: 0.56 - ETA: 1s - loss: 0.6858 - accuracy: 0.57 - ETA: 1s - loss: 0.6854 - accuracy: 0.57 - ETA: 0s - loss: 0.6844 - accuracy: 0.58 - ETA: 0s - loss: 0.6835 - accuracy: 0.58 - ETA: 0s - loss: 0.6822 - accuracy: 0.59 - ETA: 0s - loss: 0.6808 - accuracy: 0.59 - ETA: 0s - loss: 0.6802 - accuracy: 0.59 - ETA: 0s - loss: 0.6794 - accuracy: 0.59 - ETA: 0s - loss: 0.6781 - accuracy: 0.60 - ETA: 0s - loss: 0.6772 - accuracy: 0.60 - ETA: 0s - loss: 0.6767 - accuracy: 0.60 - ETA: 0s - loss: 0.6752 - accuracy: 0.60 - ETA: 0s - loss: 0.6742 - accuracy: 0.61 - ETA: 0s - loss: 0.6736 - accuracy: 0.61 - ETA: 0s - loss: 0.6730 - accuracy: 0.61 - ETA: 0s - loss: 0.6723 - accuracy: 0.61 - ETA: 0s - loss: 0.6714 - accuracy: 0.61 - ETA: 0s - loss: 0.6705 - accuracy: 0.61 - ETA: 0s - loss: 0.6703 - accuracy: 0.61 - ETA: 0s - loss: 0.6695 - accuracy: 0.61 - 2s 128us/step - loss: 0.6694 - accuracy: 0.6169 - val_loss: 0.6396 - val_accuracy: 0.7045\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 50us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:28 - loss: 0.6953 - accuracy: 0.50 - ETA: 3s - loss: 0.6942 - accuracy: 0.4710 - ETA: 2s - loss: 0.6927 - accuracy: 0.50 - ETA: 2s - loss: 0.6930 - accuracy: 0.49 - ETA: 1s - loss: 0.6919 - accuracy: 0.51 - ETA: 1s - loss: 0.6916 - accuracy: 0.51 - ETA: 1s - loss: 0.6905 - accuracy: 0.53 - ETA: 1s - loss: 0.6894 - accuracy: 0.54 - ETA: 1s - loss: 0.6885 - accuracy: 0.55 - ETA: 1s - loss: 0.6876 - accuracy: 0.55 - ETA: 1s - loss: 0.6864 - accuracy: 0.56 - ETA: 1s - loss: 0.6851 - accuracy: 0.56 - ETA: 0s - loss: 0.6844 - accuracy: 0.57 - ETA: 0s - loss: 0.6833 - accuracy: 0.57 - ETA: 0s - loss: 0.6821 - accuracy: 0.58 - ETA: 0s - loss: 0.6811 - accuracy: 0.58 - ETA: 0s - loss: 0.6801 - accuracy: 0.58 - ETA: 0s - loss: 0.6786 - accuracy: 0.59 - ETA: 0s - loss: 0.6775 - accuracy: 0.59 - ETA: 0s - loss: 0.6767 - accuracy: 0.59 - ETA: 0s - loss: 0.6760 - accuracy: 0.59 - ETA: 0s - loss: 0.6752 - accuracy: 0.60 - ETA: 0s - loss: 0.6742 - accuracy: 0.60 - ETA: 0s - loss: 0.6733 - accuracy: 0.60 - ETA: 0s - loss: 0.6725 - accuracy: 0.60 - ETA: 0s - loss: 0.6713 - accuracy: 0.61 - ETA: 0s - loss: 0.6705 - accuracy: 0.61 - ETA: 0s - loss: 0.6696 - accuracy: 0.61 - ETA: 0s - loss: 0.6687 - accuracy: 0.61 - 2s 130us/step - loss: 0.6685 - accuracy: 0.6166 - val_loss: 0.6370 - val_accuracy: 0.7159\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 49us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:27 - loss: 0.6869 - accuracy: 0.62 - ETA: 3s - loss: 0.6941 - accuracy: 0.4978 - ETA: 2s - loss: 0.6937 - accuracy: 0.50 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6929 - accuracy: 0.52 - ETA: 1s - loss: 0.6927 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.53 - ETA: 1s - loss: 0.6919 - accuracy: 0.54 - ETA: 1s - loss: 0.6915 - accuracy: 0.54 - ETA: 1s - loss: 0.6911 - accuracy: 0.55 - ETA: 1s - loss: 0.6904 - accuracy: 0.56 - ETA: 0s - loss: 0.6898 - accuracy: 0.57 - ETA: 0s - loss: 0.6888 - accuracy: 0.57 - ETA: 0s - loss: 0.6880 - accuracy: 0.57 - ETA: 0s - loss: 0.6875 - accuracy: 0.58 - ETA: 0s - loss: 0.6870 - accuracy: 0.58 - ETA: 0s - loss: 0.6865 - accuracy: 0.58 - ETA: 0s - loss: 0.6859 - accuracy: 0.58 - ETA: 0s - loss: 0.6857 - accuracy: 0.58 - ETA: 0s - loss: 0.6853 - accuracy: 0.59 - ETA: 0s - loss: 0.6848 - accuracy: 0.59 - ETA: 0s - loss: 0.6840 - accuracy: 0.59 - ETA: 0s - loss: 0.6837 - accuracy: 0.60 - ETA: 0s - loss: 0.6833 - accuracy: 0.60 - ETA: 0s - loss: 0.6824 - accuracy: 0.60 - ETA: 0s - loss: 0.6817 - accuracy: 0.60 - ETA: 0s - loss: 0.6809 - accuracy: 0.60 - ETA: 0s - loss: 0.6804 - accuracy: 0.61 - 2s 129us/step - loss: 0.6804 - accuracy: 0.6100 - val_loss: 0.6628 - val_accuracy: 0.6747\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 49us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:27 - loss: 0.6982 - accuracy: 0.62 - ETA: 3s - loss: 0.6910 - accuracy: 0.5351 - ETA: 2s - loss: 0.6905 - accuracy: 0.54 - ETA: 2s - loss: 0.6901 - accuracy: 0.54 - ETA: 1s - loss: 0.6896 - accuracy: 0.54 - ETA: 1s - loss: 0.6887 - accuracy: 0.54 - ETA: 1s - loss: 0.6871 - accuracy: 0.54 - ETA: 1s - loss: 0.6862 - accuracy: 0.55 - ETA: 1s - loss: 0.6859 - accuracy: 0.55 - ETA: 1s - loss: 0.6856 - accuracy: 0.55 - ETA: 1s - loss: 0.6839 - accuracy: 0.56 - ETA: 1s - loss: 0.6837 - accuracy: 0.55 - ETA: 0s - loss: 0.6829 - accuracy: 0.56 - ETA: 0s - loss: 0.6827 - accuracy: 0.56 - ETA: 0s - loss: 0.6825 - accuracy: 0.56 - ETA: 0s - loss: 0.6820 - accuracy: 0.56 - ETA: 0s - loss: 0.6814 - accuracy: 0.56 - ETA: 0s - loss: 0.6811 - accuracy: 0.56 - ETA: 0s - loss: 0.6808 - accuracy: 0.56 - ETA: 0s - loss: 0.6805 - accuracy: 0.56 - ETA: 0s - loss: 0.6796 - accuracy: 0.56 - ETA: 0s - loss: 0.6785 - accuracy: 0.57 - ETA: 0s - loss: 0.6778 - accuracy: 0.57 - ETA: 0s - loss: 0.6769 - accuracy: 0.57 - ETA: 0s - loss: 0.6766 - accuracy: 0.57 - ETA: 0s - loss: 0.6760 - accuracy: 0.57 - ETA: 0s - loss: 0.6756 - accuracy: 0.57 - ETA: 0s - loss: 0.6753 - accuracy: 0.57 - ETA: 0s - loss: 0.6744 - accuracy: 0.57 - ETA: 0s - loss: 0.6740 - accuracy: 0.57 - 2s 133us/step - loss: 0.6739 - accuracy: 0.5784 - val_loss: 0.6554 - val_accuracy: 0.6982\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 48us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:43 - loss: 0.6978 - accuracy: 0.37 - ETA: 4s - loss: 0.6905 - accuracy: 0.5288 - ETA: 2s - loss: 0.6910 - accuracy: 0.52 - ETA: 2s - loss: 0.6894 - accuracy: 0.54 - ETA: 1s - loss: 0.6885 - accuracy: 0.55 - ETA: 1s - loss: 0.6881 - accuracy: 0.55 - ETA: 1s - loss: 0.6872 - accuracy: 0.56 - ETA: 1s - loss: 0.6857 - accuracy: 0.57 - ETA: 1s - loss: 0.6844 - accuracy: 0.57 - ETA: 1s - loss: 0.6825 - accuracy: 0.58 - ETA: 1s - loss: 0.6812 - accuracy: 0.58 - ETA: 1s - loss: 0.6799 - accuracy: 0.59 - ETA: 1s - loss: 0.6789 - accuracy: 0.59 - ETA: 0s - loss: 0.6775 - accuracy: 0.60 - ETA: 0s - loss: 0.6765 - accuracy: 0.60 - ETA: 0s - loss: 0.6743 - accuracy: 0.60 - ETA: 0s - loss: 0.6733 - accuracy: 0.60 - ETA: 0s - loss: 0.6724 - accuracy: 0.61 - ETA: 0s - loss: 0.6712 - accuracy: 0.61 - ETA: 0s - loss: 0.6699 - accuracy: 0.61 - ETA: 0s - loss: 0.6690 - accuracy: 0.61 - ETA: 0s - loss: 0.6673 - accuracy: 0.62 - ETA: 0s - loss: 0.6670 - accuracy: 0.62 - ETA: 0s - loss: 0.6659 - accuracy: 0.62 - ETA: 0s - loss: 0.6653 - accuracy: 0.62 - ETA: 0s - loss: 0.6649 - accuracy: 0.62 - ETA: 0s - loss: 0.6645 - accuracy: 0.62 - ETA: 0s - loss: 0.6635 - accuracy: 0.62 - ETA: 0s - loss: 0.6624 - accuracy: 0.62 - ETA: 0s - loss: 0.6612 - accuracy: 0.63 - 2s 137us/step - loss: 0.6610 - accuracy: 0.6320 - val_loss: 0.6257 - val_accuracy: 0.7152\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 50us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:35 - loss: 0.7005 - accuracy: 0.12 - ETA: 4s - loss: 0.6867 - accuracy: 0.5591 - ETA: 2s - loss: 0.6911 - accuracy: 0.52 - ETA: 2s - loss: 0.6922 - accuracy: 0.50 - ETA: 1s - loss: 0.6905 - accuracy: 0.51 - ETA: 1s - loss: 0.6897 - accuracy: 0.51 - ETA: 1s - loss: 0.6891 - accuracy: 0.51 - ETA: 1s - loss: 0.6883 - accuracy: 0.52 - ETA: 1s - loss: 0.6875 - accuracy: 0.53 - ETA: 1s - loss: 0.6870 - accuracy: 0.54 - ETA: 1s - loss: 0.6862 - accuracy: 0.55 - ETA: 1s - loss: 0.6856 - accuracy: 0.55 - ETA: 1s - loss: 0.6849 - accuracy: 0.55 - ETA: 0s - loss: 0.6839 - accuracy: 0.56 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6826 - accuracy: 0.57 - ETA: 0s - loss: 0.6814 - accuracy: 0.57 - ETA: 0s - loss: 0.6802 - accuracy: 0.58 - ETA: 0s - loss: 0.6793 - accuracy: 0.58 - ETA: 0s - loss: 0.6785 - accuracy: 0.58 - ETA: 0s - loss: 0.6773 - accuracy: 0.58 - ETA: 0s - loss: 0.6766 - accuracy: 0.58 - ETA: 0s - loss: 0.6759 - accuracy: 0.58 - ETA: 0s - loss: 0.6741 - accuracy: 0.59 - ETA: 0s - loss: 0.6735 - accuracy: 0.59 - ETA: 0s - loss: 0.6730 - accuracy: 0.59 - ETA: 0s - loss: 0.6727 - accuracy: 0.59 - ETA: 0s - loss: 0.6717 - accuracy: 0.60 - ETA: 0s - loss: 0.6709 - accuracy: 0.60 - 2s 132us/step - loss: 0.6701 - accuracy: 0.6030 - val_loss: 0.6403 - val_accuracy: 0.7124\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 50us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 2:33 - loss: 0.6938 - accuracy: 0.50 - ETA: 4s - loss: 0.6885 - accuracy: 0.5636 - ETA: 2s - loss: 0.6859 - accuracy: 0.55 - ETA: 2s - loss: 0.6872 - accuracy: 0.54 - ETA: 1s - loss: 0.6864 - accuracy: 0.55 - ETA: 1s - loss: 0.6858 - accuracy: 0.55 - ETA: 1s - loss: 0.6852 - accuracy: 0.56 - ETA: 1s - loss: 0.6832 - accuracy: 0.57 - ETA: 1s - loss: 0.6807 - accuracy: 0.57 - ETA: 1s - loss: 0.6781 - accuracy: 0.58 - ETA: 1s - loss: 0.6772 - accuracy: 0.58 - ETA: 1s - loss: 0.6766 - accuracy: 0.59 - ETA: 0s - loss: 0.6751 - accuracy: 0.60 - ETA: 0s - loss: 0.6742 - accuracy: 0.60 - ETA: 0s - loss: 0.6731 - accuracy: 0.60 - ETA: 0s - loss: 0.6722 - accuracy: 0.60 - ETA: 0s - loss: 0.6711 - accuracy: 0.60 - ETA: 0s - loss: 0.6702 - accuracy: 0.61 - ETA: 0s - loss: 0.6686 - accuracy: 0.61 - ETA: 0s - loss: 0.6669 - accuracy: 0.62 - ETA: 0s - loss: 0.6660 - accuracy: 0.62 - ETA: 0s - loss: 0.6653 - accuracy: 0.62 - ETA: 0s - loss: 0.6638 - accuracy: 0.62 - ETA: 0s - loss: 0.6633 - accuracy: 0.62 - ETA: 0s - loss: 0.6621 - accuracy: 0.62 - ETA: 0s - loss: 0.6617 - accuracy: 0.62 - ETA: 0s - loss: 0.6614 - accuracy: 0.62 - ETA: 0s - loss: 0.6605 - accuracy: 0.63 - ETA: 0s - loss: 0.6597 - accuracy: 0.63 - 2s 132us/step - loss: 0.6593 - accuracy: 0.6327 - val_loss: 0.6288 - val_accuracy: 0.7095\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 50us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:33 - loss: 0.6948 - accuracy: 0.50 - ETA: 4s - loss: 0.6893 - accuracy: 0.5364 - ETA: 2s - loss: 0.6898 - accuracy: 0.53 - ETA: 2s - loss: 0.6880 - accuracy: 0.54 - ETA: 1s - loss: 0.6871 - accuracy: 0.55 - ETA: 1s - loss: 0.6864 - accuracy: 0.55 - ETA: 1s - loss: 0.6842 - accuracy: 0.56 - ETA: 1s - loss: 0.6824 - accuracy: 0.57 - ETA: 1s - loss: 0.6806 - accuracy: 0.58 - ETA: 1s - loss: 0.6793 - accuracy: 0.58 - ETA: 1s - loss: 0.6770 - accuracy: 0.59 - ETA: 1s - loss: 0.6760 - accuracy: 0.60 - ETA: 1s - loss: 0.6747 - accuracy: 0.60 - ETA: 0s - loss: 0.6726 - accuracy: 0.61 - ETA: 0s - loss: 0.6708 - accuracy: 0.61 - ETA: 0s - loss: 0.6698 - accuracy: 0.61 - ETA: 0s - loss: 0.6682 - accuracy: 0.61 - ETA: 0s - loss: 0.6661 - accuracy: 0.62 - ETA: 0s - loss: 0.6644 - accuracy: 0.62 - ETA: 0s - loss: 0.6646 - accuracy: 0.62 - ETA: 0s - loss: 0.6635 - accuracy: 0.62 - ETA: 0s - loss: 0.6622 - accuracy: 0.63 - ETA: 0s - loss: 0.6610 - accuracy: 0.63 - ETA: 0s - loss: 0.6598 - accuracy: 0.63 - ETA: 0s - loss: 0.6593 - accuracy: 0.63 - ETA: 0s - loss: 0.6581 - accuracy: 0.63 - ETA: 0s - loss: 0.6567 - accuracy: 0.63 - ETA: 0s - loss: 0.6557 - accuracy: 0.63 - ETA: 0s - loss: 0.6538 - accuracy: 0.64 - 2s 132us/step - loss: 0.6529 - accuracy: 0.6440 - val_loss: 0.6136 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 60us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:44 - loss: 0.6978 - accuracy: 0.50 - ETA: 4s - loss: 0.6938 - accuracy: 0.5068 - ETA: 3s - loss: 0.6927 - accuracy: 0.52 - ETA: 2s - loss: 0.6925 - accuracy: 0.52 - ETA: 2s - loss: 0.6913 - accuracy: 0.54 - ETA: 2s - loss: 0.6914 - accuracy: 0.53 - ETA: 2s - loss: 0.6908 - accuracy: 0.54 - ETA: 2s - loss: 0.6904 - accuracy: 0.54 - ETA: 2s - loss: 0.6900 - accuracy: 0.54 - ETA: 2s - loss: 0.6892 - accuracy: 0.55 - ETA: 2s - loss: 0.6886 - accuracy: 0.55 - ETA: 2s - loss: 0.6874 - accuracy: 0.56 - ETA: 2s - loss: 0.6861 - accuracy: 0.56 - ETA: 1s - loss: 0.6852 - accuracy: 0.56 - ETA: 1s - loss: 0.6838 - accuracy: 0.57 - ETA: 1s - loss: 0.6829 - accuracy: 0.57 - ETA: 1s - loss: 0.6819 - accuracy: 0.58 - ETA: 1s - loss: 0.6805 - accuracy: 0.58 - ETA: 1s - loss: 0.6794 - accuracy: 0.59 - ETA: 1s - loss: 0.6781 - accuracy: 0.59 - ETA: 1s - loss: 0.6764 - accuracy: 0.60 - ETA: 0s - loss: 0.6748 - accuracy: 0.60 - ETA: 0s - loss: 0.6734 - accuracy: 0.60 - ETA: 0s - loss: 0.6722 - accuracy: 0.61 - ETA: 0s - loss: 0.6707 - accuracy: 0.61 - ETA: 0s - loss: 0.6694 - accuracy: 0.61 - ETA: 0s - loss: 0.6686 - accuracy: 0.61 - ETA: 0s - loss: 0.6679 - accuracy: 0.61 - ETA: 0s - loss: 0.6676 - accuracy: 0.61 - ETA: 0s - loss: 0.6662 - accuracy: 0.61 - ETA: 0s - loss: 0.6647 - accuracy: 0.62 - ETA: 0s - loss: 0.6644 - accuracy: 0.62 - ETA: 0s - loss: 0.6636 - accuracy: 0.62 - ETA: 0s - loss: 0.6624 - accuracy: 0.62 - ETA: 0s - loss: 0.6616 - accuracy: 0.62 - ETA: 0s - loss: 0.6609 - accuracy: 0.62 - 2s 159us/step - loss: 0.6608 - accuracy: 0.6266 - val_loss: 0.6250 - val_accuracy: 0.7067\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 63us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:43 - loss: 0.6964 - accuracy: 0.50 - ETA: 4s - loss: 0.6924 - accuracy: 0.5354 - ETA: 2s - loss: 0.6913 - accuracy: 0.52 - ETA: 2s - loss: 0.6901 - accuracy: 0.53 - ETA: 1s - loss: 0.6878 - accuracy: 0.55 - ETA: 1s - loss: 0.6870 - accuracy: 0.55 - ETA: 1s - loss: 0.6859 - accuracy: 0.56 - ETA: 1s - loss: 0.6850 - accuracy: 0.56 - ETA: 1s - loss: 0.6841 - accuracy: 0.57 - ETA: 1s - loss: 0.6840 - accuracy: 0.57 - ETA: 1s - loss: 0.6832 - accuracy: 0.57 - ETA: 1s - loss: 0.6816 - accuracy: 0.57 - ETA: 1s - loss: 0.6804 - accuracy: 0.57 - ETA: 1s - loss: 0.6800 - accuracy: 0.57 - ETA: 0s - loss: 0.6792 - accuracy: 0.58 - ETA: 0s - loss: 0.6788 - accuracy: 0.58 - ETA: 0s - loss: 0.6783 - accuracy: 0.58 - ETA: 0s - loss: 0.6772 - accuracy: 0.58 - ETA: 0s - loss: 0.6754 - accuracy: 0.59 - ETA: 0s - loss: 0.6748 - accuracy: 0.59 - ETA: 0s - loss: 0.6737 - accuracy: 0.59 - ETA: 0s - loss: 0.6731 - accuracy: 0.60 - ETA: 0s - loss: 0.6725 - accuracy: 0.60 - ETA: 0s - loss: 0.6717 - accuracy: 0.60 - ETA: 0s - loss: 0.6713 - accuracy: 0.60 - ETA: 0s - loss: 0.6703 - accuracy: 0.60 - ETA: 0s - loss: 0.6696 - accuracy: 0.60 - ETA: 0s - loss: 0.6688 - accuracy: 0.61 - ETA: 0s - loss: 0.6684 - accuracy: 0.61 - ETA: 0s - loss: 0.6679 - accuracy: 0.61 - ETA: 0s - loss: 0.6677 - accuracy: 0.61 - 2s 139us/step - loss: 0.6670 - accuracy: 0.6132 - val_loss: 0.6435 - val_accuracy: 0.7095\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:32 - loss: 0.6923 - accuracy: 0.62 - ETA: 6s - loss: 0.6939 - accuracy: 0.5028 - ETA: 3s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6934 - accuracy: 0.50 - ETA: 2s - loss: 0.6934 - accuracy: 0.50 - ETA: 2s - loss: 0.6933 - accuracy: 0.50 - ETA: 1s - loss: 0.6930 - accuracy: 0.50 - ETA: 1s - loss: 0.6924 - accuracy: 0.51 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6911 - accuracy: 0.52 - ETA: 1s - loss: 0.6897 - accuracy: 0.52 - ETA: 1s - loss: 0.6887 - accuracy: 0.53 - ETA: 1s - loss: 0.6873 - accuracy: 0.53 - ETA: 1s - loss: 0.6855 - accuracy: 0.54 - ETA: 1s - loss: 0.6831 - accuracy: 0.54 - ETA: 1s - loss: 0.6807 - accuracy: 0.55 - ETA: 0s - loss: 0.6792 - accuracy: 0.55 - ETA: 0s - loss: 0.6773 - accuracy: 0.55 - ETA: 0s - loss: 0.6754 - accuracy: 0.56 - ETA: 0s - loss: 0.6739 - accuracy: 0.56 - ETA: 0s - loss: 0.6726 - accuracy: 0.56 - ETA: 0s - loss: 0.6711 - accuracy: 0.56 - ETA: 0s - loss: 0.6698 - accuracy: 0.56 - ETA: 0s - loss: 0.6688 - accuracy: 0.57 - ETA: 0s - loss: 0.6678 - accuracy: 0.57 - ETA: 0s - loss: 0.6668 - accuracy: 0.57 - ETA: 0s - loss: 0.6664 - accuracy: 0.57 - ETA: 0s - loss: 0.6655 - accuracy: 0.57 - ETA: 0s - loss: 0.6640 - accuracy: 0.58 - ETA: 0s - loss: 0.6633 - accuracy: 0.58 - ETA: 0s - loss: 0.6630 - accuracy: 0.58 - ETA: 0s - loss: 0.6621 - accuracy: 0.58 - ETA: 0s - loss: 0.6616 - accuracy: 0.58 - 2s 149us/step - loss: 0.6611 - accuracy: 0.5899 - val_loss: 0.6084 - val_accuracy: 0.7209\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:28 - loss: 0.6933 - accuracy: 0.62 - ETA: 5s - loss: 0.6932 - accuracy: 0.5312 - ETA: 3s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.50 - ETA: 2s - loss: 0.6925 - accuracy: 0.52 - ETA: 2s - loss: 0.6922 - accuracy: 0.53 - ETA: 1s - loss: 0.6913 - accuracy: 0.53 - ETA: 1s - loss: 0.6905 - accuracy: 0.54 - ETA: 1s - loss: 0.6903 - accuracy: 0.54 - ETA: 1s - loss: 0.6895 - accuracy: 0.54 - ETA: 1s - loss: 0.6884 - accuracy: 0.54 - ETA: 1s - loss: 0.6863 - accuracy: 0.55 - ETA: 1s - loss: 0.6855 - accuracy: 0.55 - ETA: 1s - loss: 0.6835 - accuracy: 0.55 - ETA: 1s - loss: 0.6819 - accuracy: 0.56 - ETA: 1s - loss: 0.6795 - accuracy: 0.56 - ETA: 0s - loss: 0.6781 - accuracy: 0.56 - ETA: 0s - loss: 0.6766 - accuracy: 0.57 - ETA: 0s - loss: 0.6743 - accuracy: 0.57 - ETA: 0s - loss: 0.6731 - accuracy: 0.57 - ETA: 0s - loss: 0.6719 - accuracy: 0.58 - ETA: 0s - loss: 0.6709 - accuracy: 0.58 - ETA: 0s - loss: 0.6694 - accuracy: 0.58 - ETA: 0s - loss: 0.6680 - accuracy: 0.58 - ETA: 0s - loss: 0.6666 - accuracy: 0.59 - ETA: 0s - loss: 0.6654 - accuracy: 0.59 - ETA: 0s - loss: 0.6644 - accuracy: 0.59 - ETA: 0s - loss: 0.6641 - accuracy: 0.59 - ETA: 0s - loss: 0.6632 - accuracy: 0.59 - ETA: 0s - loss: 0.6617 - accuracy: 0.59 - ETA: 0s - loss: 0.6601 - accuracy: 0.60 - ETA: 0s - loss: 0.6594 - accuracy: 0.60 - 2s 146us/step - loss: 0.6597 - accuracy: 0.6011 - val_loss: 0.6141 - val_accuracy: 0.7237\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:11 - loss: 0.6990 - accuracy: 0.37 - ETA: 5s - loss: 0.6949 - accuracy: 0.5048 - ETA: 3s - loss: 0.6941 - accuracy: 0.52 - ETA: 2s - loss: 0.6933 - accuracy: 0.53 - ETA: 2s - loss: 0.6930 - accuracy: 0.53 - ETA: 1s - loss: 0.6922 - accuracy: 0.54 - ETA: 1s - loss: 0.6915 - accuracy: 0.54 - ETA: 1s - loss: 0.6909 - accuracy: 0.54 - ETA: 1s - loss: 0.6895 - accuracy: 0.55 - ETA: 1s - loss: 0.6882 - accuracy: 0.55 - ETA: 1s - loss: 0.6871 - accuracy: 0.55 - ETA: 1s - loss: 0.6858 - accuracy: 0.56 - ETA: 1s - loss: 0.6844 - accuracy: 0.56 - ETA: 1s - loss: 0.6823 - accuracy: 0.57 - ETA: 1s - loss: 0.6802 - accuracy: 0.57 - ETA: 0s - loss: 0.6782 - accuracy: 0.57 - ETA: 0s - loss: 0.6756 - accuracy: 0.58 - ETA: 0s - loss: 0.6740 - accuracy: 0.58 - ETA: 0s - loss: 0.6723 - accuracy: 0.58 - ETA: 0s - loss: 0.6707 - accuracy: 0.58 - ETA: 0s - loss: 0.6693 - accuracy: 0.58 - ETA: 0s - loss: 0.6687 - accuracy: 0.59 - ETA: 0s - loss: 0.6669 - accuracy: 0.59 - ETA: 0s - loss: 0.6652 - accuracy: 0.59 - ETA: 0s - loss: 0.6646 - accuracy: 0.59 - ETA: 0s - loss: 0.6634 - accuracy: 0.59 - ETA: 0s - loss: 0.6636 - accuracy: 0.59 - ETA: 0s - loss: 0.6620 - accuracy: 0.60 - ETA: 0s - loss: 0.6615 - accuracy: 0.60 - ETA: 0s - loss: 0.6603 - accuracy: 0.60 - ETA: 0s - loss: 0.6593 - accuracy: 0.60 - ETA: 0s - loss: 0.6581 - accuracy: 0.60 - ETA: 0s - loss: 0.6571 - accuracy: 0.60 - 2s 147us/step - loss: 0.6571 - accuracy: 0.6039 - val_loss: 0.6033 - val_accuracy: 0.7188\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:24 - loss: 0.6953 - accuracy: 0.50 - ETA: 5s - loss: 0.6934 - accuracy: 0.5343 - ETA: 3s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.52 - ETA: 2s - loss: 0.6929 - accuracy: 0.54 - ETA: 2s - loss: 0.6925 - accuracy: 0.54 - ETA: 1s - loss: 0.6922 - accuracy: 0.54 - ETA: 1s - loss: 0.6914 - accuracy: 0.55 - ETA: 1s - loss: 0.6904 - accuracy: 0.56 - ETA: 1s - loss: 0.6891 - accuracy: 0.56 - ETA: 1s - loss: 0.6880 - accuracy: 0.56 - ETA: 1s - loss: 0.6870 - accuracy: 0.57 - ETA: 1s - loss: 0.6855 - accuracy: 0.58 - ETA: 1s - loss: 0.6839 - accuracy: 0.58 - ETA: 1s - loss: 0.6828 - accuracy: 0.58 - ETA: 0s - loss: 0.6814 - accuracy: 0.59 - ETA: 0s - loss: 0.6803 - accuracy: 0.60 - ETA: 0s - loss: 0.6787 - accuracy: 0.60 - ETA: 0s - loss: 0.6767 - accuracy: 0.60 - ETA: 0s - loss: 0.6760 - accuracy: 0.61 - ETA: 0s - loss: 0.6746 - accuracy: 0.61 - ETA: 0s - loss: 0.6736 - accuracy: 0.61 - ETA: 0s - loss: 0.6730 - accuracy: 0.61 - ETA: 0s - loss: 0.6714 - accuracy: 0.62 - ETA: 0s - loss: 0.6705 - accuracy: 0.62 - ETA: 0s - loss: 0.6694 - accuracy: 0.62 - ETA: 0s - loss: 0.6687 - accuracy: 0.62 - ETA: 0s - loss: 0.6678 - accuracy: 0.62 - ETA: 0s - loss: 0.6669 - accuracy: 0.62 - ETA: 0s - loss: 0.6662 - accuracy: 0.63 - ETA: 0s - loss: 0.6647 - accuracy: 0.63 - ETA: 0s - loss: 0.6641 - accuracy: 0.63 - ETA: 0s - loss: 0.6630 - accuracy: 0.63 - 2s 148us/step - loss: 0.6625 - accuracy: 0.6347 - val_loss: 0.6292 - val_accuracy: 0.6619\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:05 - loss: 0.6959 - accuracy: 0.25 - ETA: 4s - loss: 0.6939 - accuracy: 0.4623 - ETA: 3s - loss: 0.6918 - accuracy: 0.51 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 2s - loss: 0.6923 - accuracy: 0.50 - ETA: 1s - loss: 0.6910 - accuracy: 0.50 - ETA: 1s - loss: 0.6897 - accuracy: 0.50 - ETA: 1s - loss: 0.6861 - accuracy: 0.52 - ETA: 1s - loss: 0.6849 - accuracy: 0.52 - ETA: 1s - loss: 0.6818 - accuracy: 0.53 - ETA: 1s - loss: 0.6795 - accuracy: 0.53 - ETA: 1s - loss: 0.6776 - accuracy: 0.54 - ETA: 1s - loss: 0.6776 - accuracy: 0.54 - ETA: 1s - loss: 0.6763 - accuracy: 0.55 - ETA: 1s - loss: 0.6756 - accuracy: 0.56 - ETA: 0s - loss: 0.6737 - accuracy: 0.56 - ETA: 0s - loss: 0.6719 - accuracy: 0.56 - ETA: 0s - loss: 0.6710 - accuracy: 0.57 - ETA: 0s - loss: 0.6706 - accuracy: 0.57 - ETA: 0s - loss: 0.6688 - accuracy: 0.58 - ETA: 0s - loss: 0.6688 - accuracy: 0.58 - ETA: 0s - loss: 0.6688 - accuracy: 0.58 - ETA: 0s - loss: 0.6676 - accuracy: 0.58 - ETA: 0s - loss: 0.6672 - accuracy: 0.59 - ETA: 0s - loss: 0.6662 - accuracy: 0.59 - ETA: 0s - loss: 0.6663 - accuracy: 0.59 - ETA: 0s - loss: 0.6658 - accuracy: 0.59 - ETA: 0s - loss: 0.6654 - accuracy: 0.60 - ETA: 0s - loss: 0.6647 - accuracy: 0.60 - ETA: 0s - loss: 0.6639 - accuracy: 0.60 - ETA: 0s - loss: 0.6634 - accuracy: 0.60 - ETA: 0s - loss: 0.6633 - accuracy: 0.60 - 2s 144us/step - loss: 0.6624 - accuracy: 0.6081 - val_loss: 0.6233 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:05 - loss: 0.6981 - accuracy: 0.50 - ETA: 4s - loss: 0.6944 - accuracy: 0.4788 - ETA: 3s - loss: 0.6930 - accuracy: 0.52 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6917 - accuracy: 0.52 - ETA: 1s - loss: 0.6904 - accuracy: 0.53 - ETA: 1s - loss: 0.6891 - accuracy: 0.54 - ETA: 1s - loss: 0.6868 - accuracy: 0.54 - ETA: 1s - loss: 0.6849 - accuracy: 0.55 - ETA: 1s - loss: 0.6830 - accuracy: 0.55 - ETA: 1s - loss: 0.6815 - accuracy: 0.56 - ETA: 1s - loss: 0.6802 - accuracy: 0.56 - ETA: 0s - loss: 0.6783 - accuracy: 0.56 - ETA: 0s - loss: 0.6771 - accuracy: 0.56 - ETA: 0s - loss: 0.6754 - accuracy: 0.57 - ETA: 0s - loss: 0.6740 - accuracy: 0.57 - ETA: 0s - loss: 0.6727 - accuracy: 0.57 - ETA: 0s - loss: 0.6717 - accuracy: 0.58 - ETA: 0s - loss: 0.6707 - accuracy: 0.58 - ETA: 0s - loss: 0.6697 - accuracy: 0.58 - ETA: 0s - loss: 0.6680 - accuracy: 0.58 - ETA: 0s - loss: 0.6663 - accuracy: 0.58 - ETA: 0s - loss: 0.6657 - accuracy: 0.58 - ETA: 0s - loss: 0.6652 - accuracy: 0.59 - ETA: 0s - loss: 0.6648 - accuracy: 0.59 - ETA: 0s - loss: 0.6635 - accuracy: 0.59 - ETA: 0s - loss: 0.6629 - accuracy: 0.59 - ETA: 0s - loss: 0.6623 - accuracy: 0.59 - ETA: 0s - loss: 0.6612 - accuracy: 0.59 - 2s 144us/step - loss: 0.6608 - accuracy: 0.5989 - val_loss: 0.6162 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 49us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:06 - loss: 0.6971 - accuracy: 0.50 - ETA: 4s - loss: 0.6899 - accuracy: 0.5731 - ETA: 3s - loss: 0.6906 - accuracy: 0.53 - ETA: 2s - loss: 0.6883 - accuracy: 0.54 - ETA: 2s - loss: 0.6880 - accuracy: 0.54 - ETA: 1s - loss: 0.6864 - accuracy: 0.55 - ETA: 1s - loss: 0.6851 - accuracy: 0.55 - ETA: 1s - loss: 0.6833 - accuracy: 0.55 - ETA: 1s - loss: 0.6818 - accuracy: 0.55 - ETA: 1s - loss: 0.6795 - accuracy: 0.56 - ETA: 1s - loss: 0.6771 - accuracy: 0.56 - ETA: 1s - loss: 0.6763 - accuracy: 0.56 - ETA: 1s - loss: 0.6722 - accuracy: 0.57 - ETA: 1s - loss: 0.6701 - accuracy: 0.58 - ETA: 1s - loss: 0.6689 - accuracy: 0.58 - ETA: 0s - loss: 0.6683 - accuracy: 0.58 - ETA: 0s - loss: 0.6674 - accuracy: 0.58 - ETA: 0s - loss: 0.6654 - accuracy: 0.59 - ETA: 0s - loss: 0.6637 - accuracy: 0.59 - ETA: 0s - loss: 0.6614 - accuracy: 0.60 - ETA: 0s - loss: 0.6602 - accuracy: 0.60 - ETA: 0s - loss: 0.6591 - accuracy: 0.60 - ETA: 0s - loss: 0.6562 - accuracy: 0.60 - ETA: 0s - loss: 0.6538 - accuracy: 0.61 - ETA: 0s - loss: 0.6515 - accuracy: 0.61 - ETA: 0s - loss: 0.6502 - accuracy: 0.61 - ETA: 0s - loss: 0.6489 - accuracy: 0.61 - ETA: 0s - loss: 0.6481 - accuracy: 0.61 - ETA: 0s - loss: 0.6482 - accuracy: 0.62 - ETA: 0s - loss: 0.6467 - accuracy: 0.62 - ETA: 0s - loss: 0.6468 - accuracy: 0.62 - ETA: 0s - loss: 0.6462 - accuracy: 0.62 - 2s 144us/step - loss: 0.6460 - accuracy: 0.6247 - val_loss: 0.5912 - val_accuracy: 0.7223\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 49us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:02 - loss: 0.7059 - accuracy: 0.0000e+ - ETA: 4s - loss: 0.6948 - accuracy: 0.4977     - ETA: 2s - loss: 0.6943 - accuracy: 0.50 - ETA: 2s - loss: 0.6933 - accuracy: 0.52 - ETA: 2s - loss: 0.6934 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.53 - ETA: 1s - loss: 0.6915 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.54 - ETA: 1s - loss: 0.6888 - accuracy: 0.55 - ETA: 1s - loss: 0.6866 - accuracy: 0.55 - ETA: 1s - loss: 0.6837 - accuracy: 0.56 - ETA: 1s - loss: 0.6819 - accuracy: 0.56 - ETA: 1s - loss: 0.6786 - accuracy: 0.57 - ETA: 1s - loss: 0.6769 - accuracy: 0.58 - ETA: 1s - loss: 0.6758 - accuracy: 0.58 - ETA: 0s - loss: 0.6738 - accuracy: 0.58 - ETA: 0s - loss: 0.6715 - accuracy: 0.59 - ETA: 0s - loss: 0.6693 - accuracy: 0.59 - ETA: 0s - loss: 0.6686 - accuracy: 0.59 - ETA: 0s - loss: 0.6675 - accuracy: 0.60 - ETA: 0s - loss: 0.6647 - accuracy: 0.60 - ETA: 0s - loss: 0.6635 - accuracy: 0.60 - ETA: 0s - loss: 0.6626 - accuracy: 0.61 - ETA: 0s - loss: 0.6612 - accuracy: 0.61 - ETA: 0s - loss: 0.6594 - accuracy: 0.61 - ETA: 0s - loss: 0.6576 - accuracy: 0.61 - ETA: 0s - loss: 0.6570 - accuracy: 0.61 - ETA: 0s - loss: 0.6545 - accuracy: 0.62 - ETA: 0s - loss: 0.6531 - accuracy: 0.62 - ETA: 0s - loss: 0.6519 - accuracy: 0.62 - ETA: 0s - loss: 0.6510 - accuracy: 0.62 - ETA: 0s - loss: 0.6489 - accuracy: 0.63 - 2s 144us/step - loss: 0.6481 - accuracy: 0.6322 - val_loss: 0.5891 - val_accuracy: 0.7223\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 51us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:08 - loss: 0.6926 - accuracy: 0.50 - ETA: 4s - loss: 0.6935 - accuracy: 0.5495 - ETA: 3s - loss: 0.6934 - accuracy: 0.52 - ETA: 2s - loss: 0.6913 - accuracy: 0.53 - ETA: 2s - loss: 0.6900 - accuracy: 0.53 - ETA: 1s - loss: 0.6890 - accuracy: 0.54 - ETA: 1s - loss: 0.6876 - accuracy: 0.54 - ETA: 1s - loss: 0.6846 - accuracy: 0.55 - ETA: 1s - loss: 0.6823 - accuracy: 0.56 - ETA: 1s - loss: 0.6798 - accuracy: 0.56 - ETA: 1s - loss: 0.6775 - accuracy: 0.57 - ETA: 1s - loss: 0.6763 - accuracy: 0.57 - ETA: 1s - loss: 0.6737 - accuracy: 0.58 - ETA: 1s - loss: 0.6723 - accuracy: 0.58 - ETA: 0s - loss: 0.6699 - accuracy: 0.59 - ETA: 0s - loss: 0.6672 - accuracy: 0.59 - ETA: 0s - loss: 0.6646 - accuracy: 0.60 - ETA: 0s - loss: 0.6624 - accuracy: 0.60 - ETA: 0s - loss: 0.6612 - accuracy: 0.60 - ETA: 0s - loss: 0.6593 - accuracy: 0.61 - ETA: 0s - loss: 0.6581 - accuracy: 0.61 - ETA: 0s - loss: 0.6566 - accuracy: 0.61 - ETA: 0s - loss: 0.6559 - accuracy: 0.61 - ETA: 0s - loss: 0.6546 - accuracy: 0.61 - ETA: 0s - loss: 0.6537 - accuracy: 0.62 - ETA: 0s - loss: 0.6529 - accuracy: 0.62 - ETA: 0s - loss: 0.6509 - accuracy: 0.62 - ETA: 0s - loss: 0.6485 - accuracy: 0.62 - ETA: 0s - loss: 0.6467 - accuracy: 0.63 - ETA: 0s - loss: 0.6445 - accuracy: 0.63 - ETA: 0s - loss: 0.6437 - accuracy: 0.63 - ETA: 0s - loss: 0.6430 - accuracy: 0.63 - 2s 142us/step - loss: 0.6430 - accuracy: 0.6338 - val_loss: 0.5902 - val_accuracy: 0.7237\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 50us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:05 - loss: 0.6931 - accuracy: 0.75 - ETA: 4s - loss: 0.6932 - accuracy: 0.5071 - ETA: 3s - loss: 0.6924 - accuracy: 0.51 - ETA: 2s - loss: 0.6922 - accuracy: 0.51 - ETA: 2s - loss: 0.6906 - accuracy: 0.51 - ETA: 1s - loss: 0.6895 - accuracy: 0.51 - ETA: 1s - loss: 0.6883 - accuracy: 0.52 - ETA: 1s - loss: 0.6869 - accuracy: 0.53 - ETA: 1s - loss: 0.6856 - accuracy: 0.54 - ETA: 1s - loss: 0.6843 - accuracy: 0.54 - ETA: 1s - loss: 0.6818 - accuracy: 0.55 - ETA: 1s - loss: 0.6792 - accuracy: 0.56 - ETA: 1s - loss: 0.6768 - accuracy: 0.56 - ETA: 1s - loss: 0.6749 - accuracy: 0.57 - ETA: 1s - loss: 0.6719 - accuracy: 0.57 - ETA: 0s - loss: 0.6704 - accuracy: 0.57 - ETA: 0s - loss: 0.6690 - accuracy: 0.58 - ETA: 0s - loss: 0.6675 - accuracy: 0.58 - ETA: 0s - loss: 0.6657 - accuracy: 0.58 - ETA: 0s - loss: 0.6640 - accuracy: 0.58 - ETA: 0s - loss: 0.6622 - accuracy: 0.59 - ETA: 0s - loss: 0.6608 - accuracy: 0.59 - ETA: 0s - loss: 0.6587 - accuracy: 0.59 - ETA: 0s - loss: 0.6581 - accuracy: 0.59 - ETA: 0s - loss: 0.6558 - accuracy: 0.60 - ETA: 0s - loss: 0.6540 - accuracy: 0.60 - ETA: 0s - loss: 0.6530 - accuracy: 0.60 - ETA: 0s - loss: 0.6519 - accuracy: 0.60 - ETA: 0s - loss: 0.6505 - accuracy: 0.60 - ETA: 0s - loss: 0.6494 - accuracy: 0.61 - ETA: 0s - loss: 0.6485 - accuracy: 0.61 - ETA: 0s - loss: 0.6470 - accuracy: 0.61 - 2s 144us/step - loss: 0.6469 - accuracy: 0.6159 - val_loss: 0.5953 - val_accuracy: 0.7166\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 50us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:03 - loss: 0.6928 - accuracy: 0.62 - ETA: 4s - loss: 0.6938 - accuracy: 0.5457 - ETA: 3s - loss: 0.6931 - accuracy: 0.54 - ETA: 2s - loss: 0.6922 - accuracy: 0.55 - ETA: 2s - loss: 0.6916 - accuracy: 0.55 - ETA: 1s - loss: 0.6898 - accuracy: 0.56 - ETA: 1s - loss: 0.6872 - accuracy: 0.56 - ETA: 1s - loss: 0.6840 - accuracy: 0.57 - ETA: 1s - loss: 0.6793 - accuracy: 0.58 - ETA: 1s - loss: 0.6749 - accuracy: 0.59 - ETA: 1s - loss: 0.6715 - accuracy: 0.59 - ETA: 1s - loss: 0.6693 - accuracy: 0.59 - ETA: 1s - loss: 0.6669 - accuracy: 0.60 - ETA: 1s - loss: 0.6647 - accuracy: 0.60 - ETA: 1s - loss: 0.6614 - accuracy: 0.60 - ETA: 0s - loss: 0.6578 - accuracy: 0.61 - ETA: 0s - loss: 0.6561 - accuracy: 0.61 - ETA: 0s - loss: 0.6535 - accuracy: 0.61 - ETA: 0s - loss: 0.6513 - accuracy: 0.62 - ETA: 0s - loss: 0.6501 - accuracy: 0.62 - ETA: 0s - loss: 0.6489 - accuracy: 0.62 - ETA: 0s - loss: 0.6473 - accuracy: 0.62 - ETA: 0s - loss: 0.6449 - accuracy: 0.62 - ETA: 0s - loss: 0.6445 - accuracy: 0.62 - ETA: 0s - loss: 0.6442 - accuracy: 0.62 - ETA: 0s - loss: 0.6428 - accuracy: 0.63 - ETA: 0s - loss: 0.6415 - accuracy: 0.63 - ETA: 0s - loss: 0.6402 - accuracy: 0.63 - ETA: 0s - loss: 0.6384 - accuracy: 0.63 - ETA: 0s - loss: 0.6379 - accuracy: 0.63 - ETA: 0s - loss: 0.6371 - accuracy: 0.63 - ETA: 0s - loss: 0.6353 - accuracy: 0.64 - 2s 145us/step - loss: 0.6348 - accuracy: 0.6420 - val_loss: 0.5833 - val_accuracy: 0.7195\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:06 - loss: 0.6909 - accuracy: 0.50 - ETA: 4s - loss: 0.6945 - accuracy: 0.4769 - ETA: 3s - loss: 0.6927 - accuracy: 0.50 - ETA: 2s - loss: 0.6921 - accuracy: 0.50 - ETA: 2s - loss: 0.6907 - accuracy: 0.50 - ETA: 2s - loss: 0.6901 - accuracy: 0.50 - ETA: 1s - loss: 0.6894 - accuracy: 0.51 - ETA: 1s - loss: 0.6889 - accuracy: 0.52 - ETA: 1s - loss: 0.6872 - accuracy: 0.53 - ETA: 1s - loss: 0.6850 - accuracy: 0.53 - ETA: 1s - loss: 0.6841 - accuracy: 0.54 - ETA: 1s - loss: 0.6838 - accuracy: 0.55 - ETA: 1s - loss: 0.6825 - accuracy: 0.56 - ETA: 1s - loss: 0.6806 - accuracy: 0.56 - ETA: 1s - loss: 0.6789 - accuracy: 0.57 - ETA: 0s - loss: 0.6771 - accuracy: 0.57 - ETA: 0s - loss: 0.6756 - accuracy: 0.58 - ETA: 0s - loss: 0.6738 - accuracy: 0.58 - ETA: 0s - loss: 0.6723 - accuracy: 0.58 - ETA: 0s - loss: 0.6711 - accuracy: 0.59 - ETA: 0s - loss: 0.6697 - accuracy: 0.59 - ETA: 0s - loss: 0.6673 - accuracy: 0.59 - ETA: 0s - loss: 0.6652 - accuracy: 0.60 - ETA: 0s - loss: 0.6646 - accuracy: 0.60 - ETA: 0s - loss: 0.6636 - accuracy: 0.60 - ETA: 0s - loss: 0.6636 - accuracy: 0.60 - ETA: 0s - loss: 0.6621 - accuracy: 0.61 - ETA: 0s - loss: 0.6598 - accuracy: 0.61 - ETA: 0s - loss: 0.6572 - accuracy: 0.61 - ETA: 0s - loss: 0.6563 - accuracy: 0.62 - ETA: 0s - loss: 0.6547 - accuracy: 0.62 - ETA: 0s - loss: 0.6533 - accuracy: 0.62 - 2s 145us/step - loss: 0.6523 - accuracy: 0.6276 - val_loss: 0.5992 - val_accuracy: 0.7188\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:06 - loss: 0.6953 - accuracy: 0.50 - ETA: 4s - loss: 0.6952 - accuracy: 0.4676 - ETA: 3s - loss: 0.6946 - accuracy: 0.50 - ETA: 2s - loss: 0.6941 - accuracy: 0.51 - ETA: 2s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6883 - accuracy: 0.54 - ETA: 1s - loss: 0.6865 - accuracy: 0.54 - ETA: 1s - loss: 0.6821 - accuracy: 0.56 - ETA: 1s - loss: 0.6785 - accuracy: 0.57 - ETA: 1s - loss: 0.6767 - accuracy: 0.57 - ETA: 1s - loss: 0.6719 - accuracy: 0.58 - ETA: 1s - loss: 0.6677 - accuracy: 0.59 - ETA: 1s - loss: 0.6629 - accuracy: 0.60 - ETA: 0s - loss: 0.6600 - accuracy: 0.60 - ETA: 0s - loss: 0.6577 - accuracy: 0.61 - ETA: 0s - loss: 0.6555 - accuracy: 0.61 - ETA: 0s - loss: 0.6526 - accuracy: 0.62 - ETA: 0s - loss: 0.6508 - accuracy: 0.62 - ETA: 0s - loss: 0.6483 - accuracy: 0.62 - ETA: 0s - loss: 0.6468 - accuracy: 0.63 - ETA: 0s - loss: 0.6456 - accuracy: 0.63 - ETA: 0s - loss: 0.6455 - accuracy: 0.63 - ETA: 0s - loss: 0.6443 - accuracy: 0.63 - ETA: 0s - loss: 0.6436 - accuracy: 0.63 - ETA: 0s - loss: 0.6421 - accuracy: 0.64 - ETA: 0s - loss: 0.6393 - accuracy: 0.64 - ETA: 0s - loss: 0.6384 - accuracy: 0.64 - ETA: 0s - loss: 0.6366 - accuracy: 0.64 - ETA: 0s - loss: 0.6360 - accuracy: 0.64 - ETA: 0s - loss: 0.6356 - accuracy: 0.64 - ETA: 0s - loss: 0.6348 - accuracy: 0.65 - 2s 147us/step - loss: 0.6334 - accuracy: 0.6528 - val_loss: 0.5787 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 58us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:40 - loss: 0.6939 - accuracy: 0.50 - ETA: 5s - loss: 0.6948 - accuracy: 0.5100 - ETA: 3s - loss: 0.6948 - accuracy: 0.50 - ETA: 2s - loss: 0.6937 - accuracy: 0.51 - ETA: 2s - loss: 0.6925 - accuracy: 0.52 - ETA: 2s - loss: 0.6899 - accuracy: 0.53 - ETA: 1s - loss: 0.6877 - accuracy: 0.53 - ETA: 1s - loss: 0.6858 - accuracy: 0.53 - ETA: 1s - loss: 0.6833 - accuracy: 0.55 - ETA: 1s - loss: 0.6814 - accuracy: 0.55 - ETA: 1s - loss: 0.6779 - accuracy: 0.56 - ETA: 1s - loss: 0.6738 - accuracy: 0.57 - ETA: 1s - loss: 0.6702 - accuracy: 0.58 - ETA: 1s - loss: 0.6693 - accuracy: 0.58 - ETA: 1s - loss: 0.6661 - accuracy: 0.59 - ETA: 1s - loss: 0.6638 - accuracy: 0.59 - ETA: 1s - loss: 0.6618 - accuracy: 0.60 - ETA: 0s - loss: 0.6606 - accuracy: 0.60 - ETA: 0s - loss: 0.6581 - accuracy: 0.60 - ETA: 0s - loss: 0.6565 - accuracy: 0.61 - ETA: 0s - loss: 0.6550 - accuracy: 0.61 - ETA: 0s - loss: 0.6520 - accuracy: 0.61 - ETA: 0s - loss: 0.6506 - accuracy: 0.62 - ETA: 0s - loss: 0.6487 - accuracy: 0.62 - ETA: 0s - loss: 0.6474 - accuracy: 0.62 - ETA: 0s - loss: 0.6465 - accuracy: 0.62 - ETA: 0s - loss: 0.6457 - accuracy: 0.62 - ETA: 0s - loss: 0.6449 - accuracy: 0.62 - ETA: 0s - loss: 0.6438 - accuracy: 0.63 - ETA: 0s - loss: 0.6427 - accuracy: 0.63 - ETA: 0s - loss: 0.6423 - accuracy: 0.63 - ETA: 0s - loss: 0.6404 - accuracy: 0.63 - ETA: 0s - loss: 0.6397 - accuracy: 0.63 - ETA: 0s - loss: 0.6388 - accuracy: 0.64 - 2s 153us/step - loss: 0.6386 - accuracy: 0.6425 - val_loss: 0.5816 - val_accuracy: 0.7237\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 58us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:11 - loss: 0.6946 - accuracy: 0.62 - ETA: 5s - loss: 0.6938 - accuracy: 0.5697 - ETA: 3s - loss: 0.6917 - accuracy: 0.58 - ETA: 2s - loss: 0.6912 - accuracy: 0.57 - ETA: 2s - loss: 0.6885 - accuracy: 0.58 - ETA: 2s - loss: 0.6863 - accuracy: 0.58 - ETA: 1s - loss: 0.6813 - accuracy: 0.59 - ETA: 1s - loss: 0.6772 - accuracy: 0.60 - ETA: 1s - loss: 0.6731 - accuracy: 0.60 - ETA: 1s - loss: 0.6703 - accuracy: 0.61 - ETA: 1s - loss: 0.6692 - accuracy: 0.61 - ETA: 1s - loss: 0.6659 - accuracy: 0.62 - ETA: 1s - loss: 0.6640 - accuracy: 0.62 - ETA: 1s - loss: 0.6606 - accuracy: 0.62 - ETA: 1s - loss: 0.6562 - accuracy: 0.63 - ETA: 1s - loss: 0.6549 - accuracy: 0.63 - ETA: 0s - loss: 0.6534 - accuracy: 0.63 - ETA: 0s - loss: 0.6517 - accuracy: 0.63 - ETA: 0s - loss: 0.6482 - accuracy: 0.64 - ETA: 0s - loss: 0.6472 - accuracy: 0.64 - ETA: 0s - loss: 0.6472 - accuracy: 0.64 - ETA: 0s - loss: 0.6471 - accuracy: 0.64 - ETA: 0s - loss: 0.6465 - accuracy: 0.64 - ETA: 0s - loss: 0.6443 - accuracy: 0.64 - ETA: 0s - loss: 0.6425 - accuracy: 0.65 - ETA: 0s - loss: 0.6409 - accuracy: 0.65 - ETA: 0s - loss: 0.6400 - accuracy: 0.65 - ETA: 0s - loss: 0.6389 - accuracy: 0.65 - ETA: 0s - loss: 0.6381 - accuracy: 0.65 - ETA: 0s - loss: 0.6373 - accuracy: 0.65 - ETA: 0s - loss: 0.6374 - accuracy: 0.65 - ETA: 0s - loss: 0.6369 - accuracy: 0.65 - ETA: 0s - loss: 0.6360 - accuracy: 0.65 - 2s 149us/step - loss: 0.6360 - accuracy: 0.6583 - val_loss: 0.5859 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 53us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:16 - loss: 0.7014 - accuracy: 0.25 - ETA: 5s - loss: 0.6935 - accuracy: 0.5472 - ETA: 3s - loss: 0.6929 - accuracy: 0.53 - ETA: 2s - loss: 0.6917 - accuracy: 0.54 - ETA: 2s - loss: 0.6898 - accuracy: 0.55 - ETA: 1s - loss: 0.6869 - accuracy: 0.55 - ETA: 1s - loss: 0.6861 - accuracy: 0.56 - ETA: 1s - loss: 0.6831 - accuracy: 0.57 - ETA: 1s - loss: 0.6792 - accuracy: 0.57 - ETA: 1s - loss: 0.6777 - accuracy: 0.58 - ETA: 1s - loss: 0.6726 - accuracy: 0.59 - ETA: 1s - loss: 0.6701 - accuracy: 0.59 - ETA: 1s - loss: 0.6684 - accuracy: 0.59 - ETA: 1s - loss: 0.6660 - accuracy: 0.60 - ETA: 1s - loss: 0.6634 - accuracy: 0.60 - ETA: 0s - loss: 0.6598 - accuracy: 0.61 - ETA: 0s - loss: 0.6576 - accuracy: 0.61 - ETA: 0s - loss: 0.6557 - accuracy: 0.62 - ETA: 0s - loss: 0.6536 - accuracy: 0.62 - ETA: 0s - loss: 0.6506 - accuracy: 0.62 - ETA: 0s - loss: 0.6470 - accuracy: 0.63 - ETA: 0s - loss: 0.6454 - accuracy: 0.63 - ETA: 0s - loss: 0.6442 - accuracy: 0.64 - ETA: 0s - loss: 0.6441 - accuracy: 0.64 - ETA: 0s - loss: 0.6432 - accuracy: 0.64 - ETA: 0s - loss: 0.6413 - accuracy: 0.64 - ETA: 0s - loss: 0.6397 - accuracy: 0.64 - ETA: 0s - loss: 0.6388 - accuracy: 0.64 - ETA: 0s - loss: 0.6376 - accuracy: 0.65 - ETA: 0s - loss: 0.6364 - accuracy: 0.65 - ETA: 0s - loss: 0.6355 - accuracy: 0.65 - ETA: 0s - loss: 0.6349 - accuracy: 0.65 - ETA: 0s - loss: 0.6336 - accuracy: 0.65 - 2s 148us/step - loss: 0.6337 - accuracy: 0.6569 - val_loss: 0.5809 - val_accuracy: 0.7209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 54us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:16 - loss: 0.6925 - accuracy: 0.50 - ETA: 5s - loss: 0.6933 - accuracy: 0.5225 - ETA: 3s - loss: 0.6933 - accuracy: 0.52 - ETA: 2s - loss: 0.6921 - accuracy: 0.53 - ETA: 2s - loss: 0.6903 - accuracy: 0.54 - ETA: 2s - loss: 0.6882 - accuracy: 0.55 - ETA: 1s - loss: 0.6858 - accuracy: 0.56 - ETA: 1s - loss: 0.6841 - accuracy: 0.56 - ETA: 1s - loss: 0.6822 - accuracy: 0.57 - ETA: 1s - loss: 0.6792 - accuracy: 0.57 - ETA: 1s - loss: 0.6761 - accuracy: 0.58 - ETA: 1s - loss: 0.6717 - accuracy: 0.59 - ETA: 1s - loss: 0.6703 - accuracy: 0.59 - ETA: 1s - loss: 0.6685 - accuracy: 0.59 - ETA: 1s - loss: 0.6667 - accuracy: 0.60 - ETA: 1s - loss: 0.6642 - accuracy: 0.60 - ETA: 0s - loss: 0.6624 - accuracy: 0.61 - ETA: 0s - loss: 0.6604 - accuracy: 0.61 - ETA: 0s - loss: 0.6590 - accuracy: 0.61 - ETA: 0s - loss: 0.6562 - accuracy: 0.62 - ETA: 0s - loss: 0.6546 - accuracy: 0.62 - ETA: 0s - loss: 0.6526 - accuracy: 0.62 - ETA: 0s - loss: 0.6521 - accuracy: 0.62 - ETA: 0s - loss: 0.6500 - accuracy: 0.63 - ETA: 0s - loss: 0.6489 - accuracy: 0.63 - ETA: 0s - loss: 0.6461 - accuracy: 0.63 - ETA: 0s - loss: 0.6434 - accuracy: 0.64 - ETA: 0s - loss: 0.6424 - accuracy: 0.64 - ETA: 0s - loss: 0.6415 - accuracy: 0.64 - ETA: 0s - loss: 0.6398 - accuracy: 0.64 - ETA: 0s - loss: 0.6387 - accuracy: 0.64 - ETA: 0s - loss: 0.6367 - accuracy: 0.64 - ETA: 0s - loss: 0.6350 - accuracy: 0.64 - 2s 149us/step - loss: 0.6356 - accuracy: 0.6493 - val_loss: 0.5950 - val_accuracy: 0.7031\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 49us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:03 - loss: 0.6919 - accuracy: 0.50 - ETA: 4s - loss: 0.6943 - accuracy: 0.5048 - ETA: 3s - loss: 0.6929 - accuracy: 0.52 - ETA: 2s - loss: 0.6913 - accuracy: 0.54 - ETA: 2s - loss: 0.6891 - accuracy: 0.55 - ETA: 1s - loss: 0.6882 - accuracy: 0.55 - ETA: 1s - loss: 0.6872 - accuracy: 0.56 - ETA: 1s - loss: 0.6845 - accuracy: 0.57 - ETA: 1s - loss: 0.6807 - accuracy: 0.57 - ETA: 1s - loss: 0.6792 - accuracy: 0.58 - ETA: 1s - loss: 0.6774 - accuracy: 0.58 - ETA: 1s - loss: 0.6751 - accuracy: 0.59 - ETA: 1s - loss: 0.6743 - accuracy: 0.59 - ETA: 1s - loss: 0.6721 - accuracy: 0.59 - ETA: 1s - loss: 0.6695 - accuracy: 0.60 - ETA: 0s - loss: 0.6668 - accuracy: 0.60 - ETA: 0s - loss: 0.6642 - accuracy: 0.60 - ETA: 0s - loss: 0.6621 - accuracy: 0.61 - ETA: 0s - loss: 0.6592 - accuracy: 0.61 - ETA: 0s - loss: 0.6576 - accuracy: 0.62 - ETA: 0s - loss: 0.6551 - accuracy: 0.62 - ETA: 0s - loss: 0.6526 - accuracy: 0.62 - ETA: 0s - loss: 0.6503 - accuracy: 0.63 - ETA: 0s - loss: 0.6487 - accuracy: 0.63 - ETA: 0s - loss: 0.6466 - accuracy: 0.63 - ETA: 0s - loss: 0.6458 - accuracy: 0.63 - ETA: 0s - loss: 0.6457 - accuracy: 0.63 - ETA: 0s - loss: 0.6455 - accuracy: 0.64 - ETA: 0s - loss: 0.6451 - accuracy: 0.64 - ETA: 0s - loss: 0.6443 - accuracy: 0.64 - ETA: 0s - loss: 0.6437 - accuracy: 0.64 - ETA: 0s - loss: 0.6425 - accuracy: 0.64 - ETA: 0s - loss: 0.6421 - accuracy: 0.64 - 2s 148us/step - loss: 0.6411 - accuracy: 0.6499 - val_loss: 0.5933 - val_accuracy: 0.6932\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 50us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:06 - loss: 0.6935 - accuracy: 0.60 - ETA: 3s - loss: 0.6931 - accuracy: 0.5073 - ETA: 2s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6916 - accuracy: 0.53 - ETA: 1s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 1s - loss: 0.6897 - accuracy: 0.53 - ETA: 0s - loss: 0.6889 - accuracy: 0.53 - ETA: 0s - loss: 0.6878 - accuracy: 0.53 - ETA: 0s - loss: 0.6872 - accuracy: 0.53 - ETA: 0s - loss: 0.6864 - accuracy: 0.54 - ETA: 0s - loss: 0.6855 - accuracy: 0.54 - ETA: 0s - loss: 0.6849 - accuracy: 0.54 - ETA: 0s - loss: 0.6846 - accuracy: 0.55 - ETA: 0s - loss: 0.6831 - accuracy: 0.55 - ETA: 0s - loss: 0.6823 - accuracy: 0.55 - ETA: 0s - loss: 0.6809 - accuracy: 0.56 - ETA: 0s - loss: 0.6796 - accuracy: 0.56 - ETA: 0s - loss: 0.6789 - accuracy: 0.56 - ETA: 0s - loss: 0.6781 - accuracy: 0.57 - ETA: 0s - loss: 0.6773 - accuracy: 0.57 - ETA: 0s - loss: 0.6765 - accuracy: 0.57 - ETA: 0s - loss: 0.6761 - accuracy: 0.58 - 1s 108us/step - loss: 0.6760 - accuracy: 0.5817 - val_loss: 0.6453 - val_accuracy: 0.6982\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:07 - loss: 0.6949 - accuracy: 0.40 - ETA: 3s - loss: 0.6923 - accuracy: 0.5370 - ETA: 2s - loss: 0.6916 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 1s - loss: 0.6905 - accuracy: 0.53 - ETA: 1s - loss: 0.6897 - accuracy: 0.53 - ETA: 1s - loss: 0.6897 - accuracy: 0.53 - ETA: 1s - loss: 0.6891 - accuracy: 0.53 - ETA: 1s - loss: 0.6886 - accuracy: 0.53 - ETA: 0s - loss: 0.6880 - accuracy: 0.54 - ETA: 0s - loss: 0.6872 - accuracy: 0.54 - ETA: 0s - loss: 0.6867 - accuracy: 0.54 - ETA: 0s - loss: 0.6863 - accuracy: 0.54 - ETA: 0s - loss: 0.6857 - accuracy: 0.54 - ETA: 0s - loss: 0.6853 - accuracy: 0.54 - ETA: 0s - loss: 0.6842 - accuracy: 0.55 - ETA: 0s - loss: 0.6833 - accuracy: 0.55 - ETA: 0s - loss: 0.6824 - accuracy: 0.55 - ETA: 0s - loss: 0.6815 - accuracy: 0.56 - ETA: 0s - loss: 0.6806 - accuracy: 0.56 - ETA: 0s - loss: 0.6800 - accuracy: 0.56 - ETA: 0s - loss: 0.6788 - accuracy: 0.56 - ETA: 0s - loss: 0.6777 - accuracy: 0.57 - ETA: 0s - loss: 0.6765 - accuracy: 0.57 - ETA: 0s - loss: 0.6754 - accuracy: 0.57 - 1s 114us/step - loss: 0.6752 - accuracy: 0.5747 - val_loss: 0.6402 - val_accuracy: 0.7067\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:11 - loss: 0.6907 - accuracy: 0.60 - ETA: 3s - loss: 0.6930 - accuracy: 0.5192 - ETA: 2s - loss: 0.6927 - accuracy: 0.50 - ETA: 1s - loss: 0.6930 - accuracy: 0.50 - ETA: 1s - loss: 0.6923 - accuracy: 0.50 - ETA: 1s - loss: 0.6910 - accuracy: 0.52 - ETA: 1s - loss: 0.6907 - accuracy: 0.52 - ETA: 1s - loss: 0.6899 - accuracy: 0.51 - ETA: 1s - loss: 0.6892 - accuracy: 0.51 - ETA: 0s - loss: 0.6884 - accuracy: 0.52 - ETA: 0s - loss: 0.6878 - accuracy: 0.52 - ETA: 0s - loss: 0.6870 - accuracy: 0.52 - ETA: 0s - loss: 0.6864 - accuracy: 0.53 - ETA: 0s - loss: 0.6858 - accuracy: 0.53 - ETA: 0s - loss: 0.6849 - accuracy: 0.54 - ETA: 0s - loss: 0.6838 - accuracy: 0.54 - ETA: 0s - loss: 0.6821 - accuracy: 0.55 - ETA: 0s - loss: 0.6814 - accuracy: 0.55 - ETA: 0s - loss: 0.6798 - accuracy: 0.56 - ETA: 0s - loss: 0.6790 - accuracy: 0.56 - ETA: 0s - loss: 0.6780 - accuracy: 0.57 - ETA: 0s - loss: 0.6766 - accuracy: 0.57 - ETA: 0s - loss: 0.6752 - accuracy: 0.58 - ETA: 0s - loss: 0.6737 - accuracy: 0.58 - ETA: 0s - loss: 0.6730 - accuracy: 0.58 - 1s 114us/step - loss: 0.6719 - accuracy: 0.5914 - val_loss: 0.6357 - val_accuracy: 0.6889\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:02 - loss: 0.6946 - accuracy: 0.60 - ETA: 3s - loss: 0.6926 - accuracy: 0.4782 - ETA: 2s - loss: 0.6930 - accuracy: 0.47 - ETA: 1s - loss: 0.6925 - accuracy: 0.49 - ETA: 1s - loss: 0.6919 - accuracy: 0.49 - ETA: 1s - loss: 0.6905 - accuracy: 0.50 - ETA: 1s - loss: 0.6899 - accuracy: 0.50 - ETA: 1s - loss: 0.6893 - accuracy: 0.51 - ETA: 0s - loss: 0.6882 - accuracy: 0.51 - ETA: 0s - loss: 0.6873 - accuracy: 0.52 - ETA: 0s - loss: 0.6869 - accuracy: 0.52 - ETA: 0s - loss: 0.6862 - accuracy: 0.52 - ETA: 0s - loss: 0.6852 - accuracy: 0.53 - ETA: 0s - loss: 0.6842 - accuracy: 0.53 - ETA: 0s - loss: 0.6831 - accuracy: 0.54 - ETA: 0s - loss: 0.6816 - accuracy: 0.54 - ETA: 0s - loss: 0.6807 - accuracy: 0.54 - ETA: 0s - loss: 0.6798 - accuracy: 0.55 - ETA: 0s - loss: 0.6789 - accuracy: 0.55 - ETA: 0s - loss: 0.6781 - accuracy: 0.55 - ETA: 0s - loss: 0.6769 - accuracy: 0.56 - ETA: 0s - loss: 0.6756 - accuracy: 0.56 - ETA: 0s - loss: 0.6744 - accuracy: 0.56 - ETA: 0s - loss: 0.6735 - accuracy: 0.56 - 1s 109us/step - loss: 0.6734 - accuracy: 0.5662 - val_loss: 0.6371 - val_accuracy: 0.7102\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:06 - loss: 0.6942 - accuracy: 0.60 - ETA: 3s - loss: 0.6934 - accuracy: 0.5222 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.53 - ETA: 1s - loss: 0.6923 - accuracy: 0.53 - ETA: 1s - loss: 0.6920 - accuracy: 0.53 - ETA: 1s - loss: 0.6913 - accuracy: 0.54 - ETA: 1s - loss: 0.6905 - accuracy: 0.54 - ETA: 1s - loss: 0.6897 - accuracy: 0.55 - ETA: 0s - loss: 0.6888 - accuracy: 0.56 - ETA: 0s - loss: 0.6880 - accuracy: 0.56 - ETA: 0s - loss: 0.6868 - accuracy: 0.57 - ETA: 0s - loss: 0.6856 - accuracy: 0.57 - ETA: 0s - loss: 0.6843 - accuracy: 0.57 - ETA: 0s - loss: 0.6837 - accuracy: 0.58 - ETA: 0s - loss: 0.6829 - accuracy: 0.58 - ETA: 0s - loss: 0.6816 - accuracy: 0.58 - ETA: 0s - loss: 0.6808 - accuracy: 0.58 - ETA: 0s - loss: 0.6796 - accuracy: 0.58 - ETA: 0s - loss: 0.6782 - accuracy: 0.58 - ETA: 0s - loss: 0.6772 - accuracy: 0.58 - ETA: 0s - loss: 0.6763 - accuracy: 0.59 - ETA: 0s - loss: 0.6756 - accuracy: 0.59 - ETA: 0s - loss: 0.6746 - accuracy: 0.59 - ETA: 0s - loss: 0.6738 - accuracy: 0.59 - 1s 113us/step - loss: 0.6736 - accuracy: 0.5969 - val_loss: 0.6360 - val_accuracy: 0.7095\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:09 - loss: 0.6920 - accuracy: 0.70 - ETA: 3s - loss: 0.6940 - accuracy: 0.4727 - ETA: 2s - loss: 0.6927 - accuracy: 0.49 - ETA: 1s - loss: 0.6927 - accuracy: 0.49 - ETA: 1s - loss: 0.6922 - accuracy: 0.50 - ETA: 1s - loss: 0.6915 - accuracy: 0.51 - ETA: 1s - loss: 0.6903 - accuracy: 0.52 - ETA: 1s - loss: 0.6899 - accuracy: 0.52 - ETA: 0s - loss: 0.6893 - accuracy: 0.52 - ETA: 0s - loss: 0.6884 - accuracy: 0.53 - ETA: 0s - loss: 0.6875 - accuracy: 0.54 - ETA: 0s - loss: 0.6865 - accuracy: 0.54 - ETA: 0s - loss: 0.6858 - accuracy: 0.54 - ETA: 0s - loss: 0.6853 - accuracy: 0.54 - ETA: 0s - loss: 0.6844 - accuracy: 0.54 - ETA: 0s - loss: 0.6836 - accuracy: 0.55 - ETA: 0s - loss: 0.6821 - accuracy: 0.55 - ETA: 0s - loss: 0.6813 - accuracy: 0.55 - ETA: 0s - loss: 0.6806 - accuracy: 0.55 - ETA: 0s - loss: 0.6796 - accuracy: 0.55 - ETA: 0s - loss: 0.6791 - accuracy: 0.56 - ETA: 0s - loss: 0.6784 - accuracy: 0.56 - ETA: 0s - loss: 0.6777 - accuracy: 0.56 - ETA: 0s - loss: 0.6767 - accuracy: 0.56 - ETA: 0s - loss: 0.6750 - accuracy: 0.57 - 1s 114us/step - loss: 0.6745 - accuracy: 0.5707 - val_loss: 0.6429 - val_accuracy: 0.7109\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:04 - loss: 0.6891 - accuracy: 0.80 - ETA: 3s - loss: 0.6926 - accuracy: 0.5444 - ETA: 2s - loss: 0.6913 - accuracy: 0.54 - ETA: 1s - loss: 0.6904 - accuracy: 0.54 - ETA: 1s - loss: 0.6896 - accuracy: 0.54 - ETA: 1s - loss: 0.6880 - accuracy: 0.54 - ETA: 1s - loss: 0.6875 - accuracy: 0.55 - ETA: 1s - loss: 0.6862 - accuracy: 0.55 - ETA: 0s - loss: 0.6848 - accuracy: 0.56 - ETA: 0s - loss: 0.6826 - accuracy: 0.57 - ETA: 0s - loss: 0.6810 - accuracy: 0.58 - ETA: 0s - loss: 0.6792 - accuracy: 0.58 - ETA: 0s - loss: 0.6762 - accuracy: 0.59 - ETA: 0s - loss: 0.6757 - accuracy: 0.59 - ETA: 0s - loss: 0.6734 - accuracy: 0.60 - ETA: 0s - loss: 0.6715 - accuracy: 0.60 - ETA: 0s - loss: 0.6697 - accuracy: 0.60 - ETA: 0s - loss: 0.6674 - accuracy: 0.60 - ETA: 0s - loss: 0.6651 - accuracy: 0.61 - ETA: 0s - loss: 0.6637 - accuracy: 0.61 - ETA: 0s - loss: 0.6619 - accuracy: 0.61 - ETA: 0s - loss: 0.6604 - accuracy: 0.61 - ETA: 0s - loss: 0.6587 - accuracy: 0.62 - ETA: 0s - loss: 0.6580 - accuracy: 0.62 - 1s 109us/step - loss: 0.6573 - accuracy: 0.6236 - val_loss: 0.6042 - val_accuracy: 0.7124\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 45us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:05 - loss: 0.6968 - accuracy: 0.40 - ETA: 3s - loss: 0.6935 - accuracy: 0.5255 - ETA: 2s - loss: 0.6929 - accuracy: 0.53 - ETA: 1s - loss: 0.6924 - accuracy: 0.53 - ETA: 1s - loss: 0.6916 - accuracy: 0.54 - ETA: 1s - loss: 0.6909 - accuracy: 0.54 - ETA: 1s - loss: 0.6901 - accuracy: 0.54 - ETA: 1s - loss: 0.6886 - accuracy: 0.55 - ETA: 0s - loss: 0.6870 - accuracy: 0.56 - ETA: 0s - loss: 0.6861 - accuracy: 0.56 - ETA: 0s - loss: 0.6844 - accuracy: 0.56 - ETA: 0s - loss: 0.6838 - accuracy: 0.56 - ETA: 0s - loss: 0.6821 - accuracy: 0.57 - ETA: 0s - loss: 0.6801 - accuracy: 0.57 - ETA: 0s - loss: 0.6792 - accuracy: 0.57 - ETA: 0s - loss: 0.6775 - accuracy: 0.58 - ETA: 0s - loss: 0.6756 - accuracy: 0.58 - ETA: 0s - loss: 0.6743 - accuracy: 0.58 - ETA: 0s - loss: 0.6734 - accuracy: 0.58 - ETA: 0s - loss: 0.6716 - accuracy: 0.59 - ETA: 0s - loss: 0.6709 - accuracy: 0.59 - ETA: 0s - loss: 0.6696 - accuracy: 0.59 - ETA: 0s - loss: 0.6679 - accuracy: 0.60 - ETA: 0s - loss: 0.6663 - accuracy: 0.60 - 1s 109us/step - loss: 0.6656 - accuracy: 0.6072 - val_loss: 0.6213 - val_accuracy: 0.7053\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:05 - loss: 0.7112 - accuracy: 0.10 - ETA: 3s - loss: 0.6940 - accuracy: 0.5037 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6914 - accuracy: 0.53 - ETA: 1s - loss: 0.6904 - accuracy: 0.54 - ETA: 1s - loss: 0.6895 - accuracy: 0.55 - ETA: 1s - loss: 0.6883 - accuracy: 0.56 - ETA: 1s - loss: 0.6868 - accuracy: 0.56 - ETA: 0s - loss: 0.6849 - accuracy: 0.57 - ETA: 0s - loss: 0.6835 - accuracy: 0.57 - ETA: 0s - loss: 0.6812 - accuracy: 0.58 - ETA: 0s - loss: 0.6801 - accuracy: 0.58 - ETA: 0s - loss: 0.6788 - accuracy: 0.59 - ETA: 0s - loss: 0.6766 - accuracy: 0.59 - ETA: 0s - loss: 0.6749 - accuracy: 0.59 - ETA: 0s - loss: 0.6731 - accuracy: 0.60 - ETA: 0s - loss: 0.6714 - accuracy: 0.60 - ETA: 0s - loss: 0.6696 - accuracy: 0.60 - ETA: 0s - loss: 0.6680 - accuracy: 0.60 - ETA: 0s - loss: 0.6663 - accuracy: 0.61 - ETA: 0s - loss: 0.6645 - accuracy: 0.61 - ETA: 0s - loss: 0.6629 - accuracy: 0.61 - ETA: 0s - loss: 0.6616 - accuracy: 0.61 - 1s 111us/step - loss: 0.6601 - accuracy: 0.6220 - val_loss: 0.6069 - val_accuracy: 0.7124\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 2:04 - loss: 0.6937 - accuracy: 0.50 - ETA: 3s - loss: 0.6933 - accuracy: 0.5327 - ETA: 2s - loss: 0.6926 - accuracy: 0.54 - ETA: 1s - loss: 0.6906 - accuracy: 0.55 - ETA: 1s - loss: 0.6896 - accuracy: 0.55 - ETA: 1s - loss: 0.6879 - accuracy: 0.55 - ETA: 1s - loss: 0.6878 - accuracy: 0.55 - ETA: 1s - loss: 0.6860 - accuracy: 0.56 - ETA: 0s - loss: 0.6847 - accuracy: 0.57 - ETA: 0s - loss: 0.6836 - accuracy: 0.57 - ETA: 0s - loss: 0.6819 - accuracy: 0.58 - ETA: 0s - loss: 0.6801 - accuracy: 0.58 - ETA: 0s - loss: 0.6785 - accuracy: 0.58 - ETA: 0s - loss: 0.6770 - accuracy: 0.59 - ETA: 0s - loss: 0.6758 - accuracy: 0.59 - ETA: 0s - loss: 0.6746 - accuracy: 0.59 - ETA: 0s - loss: 0.6723 - accuracy: 0.60 - ETA: 0s - loss: 0.6709 - accuracy: 0.60 - ETA: 0s - loss: 0.6693 - accuracy: 0.60 - ETA: 0s - loss: 0.6672 - accuracy: 0.60 - ETA: 0s - loss: 0.6659 - accuracy: 0.60 - ETA: 0s - loss: 0.6636 - accuracy: 0.61 - ETA: 0s - loss: 0.6621 - accuracy: 0.61 - ETA: 0s - loss: 0.6602 - accuracy: 0.61 - 1s 110us/step - loss: 0.6595 - accuracy: 0.6200 - val_loss: 0.6109 - val_accuracy: 0.7209\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 39us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:05 - loss: 0.6933 - accuracy: 0.80 - ETA: 3s - loss: 0.6934 - accuracy: 0.5255 - ETA: 2s - loss: 0.6930 - accuracy: 0.53 - ETA: 1s - loss: 0.6925 - accuracy: 0.54 - ETA: 1s - loss: 0.6917 - accuracy: 0.54 - ETA: 1s - loss: 0.6903 - accuracy: 0.54 - ETA: 1s - loss: 0.6885 - accuracy: 0.55 - ETA: 1s - loss: 0.6864 - accuracy: 0.56 - ETA: 0s - loss: 0.6841 - accuracy: 0.57 - ETA: 0s - loss: 0.6828 - accuracy: 0.57 - ETA: 0s - loss: 0.6812 - accuracy: 0.58 - ETA: 0s - loss: 0.6802 - accuracy: 0.58 - ETA: 0s - loss: 0.6778 - accuracy: 0.59 - ETA: 0s - loss: 0.6747 - accuracy: 0.59 - ETA: 0s - loss: 0.6733 - accuracy: 0.59 - ETA: 0s - loss: 0.6717 - accuracy: 0.60 - ETA: 0s - loss: 0.6696 - accuracy: 0.60 - ETA: 0s - loss: 0.6678 - accuracy: 0.60 - ETA: 0s - loss: 0.6665 - accuracy: 0.61 - ETA: 0s - loss: 0.6649 - accuracy: 0.61 - ETA: 0s - loss: 0.6636 - accuracy: 0.61 - ETA: 0s - loss: 0.6623 - accuracy: 0.61 - ETA: 0s - loss: 0.6604 - accuracy: 0.61 - ETA: 0s - loss: 0.6592 - accuracy: 0.62 - 1s 111us/step - loss: 0.6574 - accuracy: 0.6237 - val_loss: 0.6029 - val_accuracy: 0.7195\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 45us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:11 - loss: 0.6945 - accuracy: 0.60 - ETA: 3s - loss: 0.6926 - accuracy: 0.5185 - ETA: 2s - loss: 0.6923 - accuracy: 0.51 - ETA: 1s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6885 - accuracy: 0.55 - ETA: 1s - loss: 0.6873 - accuracy: 0.55 - ETA: 1s - loss: 0.6859 - accuracy: 0.56 - ETA: 1s - loss: 0.6845 - accuracy: 0.57 - ETA: 1s - loss: 0.6829 - accuracy: 0.58 - ETA: 0s - loss: 0.6816 - accuracy: 0.58 - ETA: 0s - loss: 0.6796 - accuracy: 0.59 - ETA: 0s - loss: 0.6776 - accuracy: 0.59 - ETA: 0s - loss: 0.6750 - accuracy: 0.60 - ETA: 0s - loss: 0.6739 - accuracy: 0.60 - ETA: 0s - loss: 0.6724 - accuracy: 0.60 - ETA: 0s - loss: 0.6699 - accuracy: 0.60 - ETA: 0s - loss: 0.6684 - accuracy: 0.61 - ETA: 0s - loss: 0.6662 - accuracy: 0.61 - ETA: 0s - loss: 0.6648 - accuracy: 0.61 - ETA: 0s - loss: 0.6628 - accuracy: 0.61 - ETA: 0s - loss: 0.6616 - accuracy: 0.62 - ETA: 0s - loss: 0.6595 - accuracy: 0.62 - ETA: 0s - loss: 0.6575 - accuracy: 0.62 - ETA: 0s - loss: 0.6552 - accuracy: 0.62 - ETA: 0s - loss: 0.6534 - accuracy: 0.63 - 1s 114us/step - loss: 0.6520 - accuracy: 0.6334 - val_loss: 0.6035 - val_accuracy: 0.7102\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:10 - loss: 0.6951 - accuracy: 0.40 - ETA: 3s - loss: 0.6938 - accuracy: 0.5170 - ETA: 2s - loss: 0.6925 - accuracy: 0.54 - ETA: 1s - loss: 0.6932 - accuracy: 0.52 - ETA: 1s - loss: 0.6913 - accuracy: 0.53 - ETA: 1s - loss: 0.6898 - accuracy: 0.53 - ETA: 1s - loss: 0.6891 - accuracy: 0.53 - ETA: 1s - loss: 0.6874 - accuracy: 0.54 - ETA: 1s - loss: 0.6863 - accuracy: 0.54 - ETA: 0s - loss: 0.6848 - accuracy: 0.55 - ETA: 0s - loss: 0.6819 - accuracy: 0.57 - ETA: 0s - loss: 0.6799 - accuracy: 0.57 - ETA: 0s - loss: 0.6776 - accuracy: 0.58 - ETA: 0s - loss: 0.6765 - accuracy: 0.58 - ETA: 0s - loss: 0.6748 - accuracy: 0.58 - ETA: 0s - loss: 0.6728 - accuracy: 0.59 - ETA: 0s - loss: 0.6703 - accuracy: 0.59 - ETA: 0s - loss: 0.6682 - accuracy: 0.60 - ETA: 0s - loss: 0.6660 - accuracy: 0.60 - ETA: 0s - loss: 0.6621 - accuracy: 0.61 - ETA: 0s - loss: 0.6608 - accuracy: 0.61 - ETA: 0s - loss: 0.6590 - accuracy: 0.61 - ETA: 0s - loss: 0.6565 - accuracy: 0.62 - ETA: 0s - loss: 0.6548 - accuracy: 0.62 - ETA: 0s - loss: 0.6530 - accuracy: 0.62 - 1s 113us/step - loss: 0.6525 - accuracy: 0.6270 - val_loss: 0.5939 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 3:16 - loss: 0.6916 - accuracy: 0.70 - ETA: 4s - loss: 0.6940 - accuracy: 0.5264 - ETA: 2s - loss: 0.6923 - accuracy: 0.53 - ETA: 2s - loss: 0.6905 - accuracy: 0.55 - ETA: 1s - loss: 0.6894 - accuracy: 0.55 - ETA: 1s - loss: 0.6880 - accuracy: 0.56 - ETA: 1s - loss: 0.6867 - accuracy: 0.57 - ETA: 1s - loss: 0.6847 - accuracy: 0.57 - ETA: 1s - loss: 0.6833 - accuracy: 0.58 - ETA: 1s - loss: 0.6806 - accuracy: 0.59 - ETA: 1s - loss: 0.6782 - accuracy: 0.60 - ETA: 0s - loss: 0.6755 - accuracy: 0.60 - ETA: 0s - loss: 0.6725 - accuracy: 0.61 - ETA: 0s - loss: 0.6703 - accuracy: 0.61 - ETA: 0s - loss: 0.6690 - accuracy: 0.61 - ETA: 0s - loss: 0.6665 - accuracy: 0.61 - ETA: 0s - loss: 0.6638 - accuracy: 0.62 - ETA: 0s - loss: 0.6610 - accuracy: 0.62 - ETA: 0s - loss: 0.6598 - accuracy: 0.62 - ETA: 0s - loss: 0.6584 - accuracy: 0.63 - ETA: 0s - loss: 0.6568 - accuracy: 0.63 - ETA: 0s - loss: 0.6552 - accuracy: 0.63 - ETA: 0s - loss: 0.6540 - accuracy: 0.63 - ETA: 0s - loss: 0.6518 - accuracy: 0.64 - ETA: 0s - loss: 0.6499 - accuracy: 0.64 - 1s 118us/step - loss: 0.6486 - accuracy: 0.6432 - val_loss: 0.5938 - val_accuracy: 0.7202\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 39us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:06 - loss: 0.6947 - accuracy: 0.60 - ETA: 3s - loss: 0.6936 - accuracy: 0.5370 - ETA: 2s - loss: 0.6926 - accuracy: 0.53 - ETA: 1s - loss: 0.6917 - accuracy: 0.54 - ETA: 1s - loss: 0.6907 - accuracy: 0.55 - ETA: 1s - loss: 0.6890 - accuracy: 0.56 - ETA: 1s - loss: 0.6876 - accuracy: 0.56 - ETA: 1s - loss: 0.6857 - accuracy: 0.57 - ETA: 0s - loss: 0.6837 - accuracy: 0.58 - ETA: 0s - loss: 0.6815 - accuracy: 0.59 - ETA: 0s - loss: 0.6788 - accuracy: 0.59 - ETA: 0s - loss: 0.6752 - accuracy: 0.60 - ETA: 0s - loss: 0.6733 - accuracy: 0.61 - ETA: 0s - loss: 0.6722 - accuracy: 0.61 - ETA: 0s - loss: 0.6690 - accuracy: 0.62 - ETA: 0s - loss: 0.6671 - accuracy: 0.62 - ETA: 0s - loss: 0.6642 - accuracy: 0.62 - ETA: 0s - loss: 0.6608 - accuracy: 0.63 - ETA: 0s - loss: 0.6582 - accuracy: 0.63 - ETA: 0s - loss: 0.6562 - accuracy: 0.63 - ETA: 0s - loss: 0.6549 - accuracy: 0.63 - ETA: 0s - loss: 0.6542 - accuracy: 0.63 - ETA: 0s - loss: 0.6525 - accuracy: 0.63 - ETA: 0s - loss: 0.6519 - accuracy: 0.64 - 1s 110us/step - loss: 0.6511 - accuracy: 0.6409 - val_loss: 0.5982 - val_accuracy: 0.7216\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 3:01 - loss: 0.6970 - accuracy: 0.30 - ETA: 4s - loss: 0.6926 - accuracy: 0.5275 - ETA: 2s - loss: 0.6918 - accuracy: 0.52 - ETA: 2s - loss: 0.6912 - accuracy: 0.53 - ETA: 2s - loss: 0.6904 - accuracy: 0.54 - ETA: 2s - loss: 0.6906 - accuracy: 0.54 - ETA: 2s - loss: 0.6889 - accuracy: 0.55 - ETA: 1s - loss: 0.6872 - accuracy: 0.56 - ETA: 1s - loss: 0.6853 - accuracy: 0.57 - ETA: 1s - loss: 0.6834 - accuracy: 0.58 - ETA: 1s - loss: 0.6804 - accuracy: 0.59 - ETA: 1s - loss: 0.6778 - accuracy: 0.59 - ETA: 1s - loss: 0.6757 - accuracy: 0.59 - ETA: 0s - loss: 0.6730 - accuracy: 0.60 - ETA: 0s - loss: 0.6703 - accuracy: 0.60 - ETA: 0s - loss: 0.6679 - accuracy: 0.61 - ETA: 0s - loss: 0.6650 - accuracy: 0.61 - ETA: 0s - loss: 0.6614 - accuracy: 0.62 - ETA: 0s - loss: 0.6587 - accuracy: 0.62 - ETA: 0s - loss: 0.6562 - accuracy: 0.63 - ETA: 0s - loss: 0.6543 - accuracy: 0.63 - ETA: 0s - loss: 0.6519 - accuracy: 0.63 - ETA: 0s - loss: 0.6498 - accuracy: 0.63 - ETA: 0s - loss: 0.6477 - accuracy: 0.64 - ETA: 0s - loss: 0.6464 - accuracy: 0.64 - ETA: 0s - loss: 0.6455 - accuracy: 0.64 - 2s 121us/step - loss: 0.6442 - accuracy: 0.6486 - val_loss: 0.5950 - val_accuracy: 0.7159\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:09 - loss: 0.6910 - accuracy: 0.60 - ETA: 3s - loss: 0.6933 - accuracy: 0.5309 - ETA: 2s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.54 - ETA: 1s - loss: 0.6895 - accuracy: 0.56 - ETA: 1s - loss: 0.6885 - accuracy: 0.56 - ETA: 1s - loss: 0.6859 - accuracy: 0.57 - ETA: 1s - loss: 0.6835 - accuracy: 0.58 - ETA: 1s - loss: 0.6813 - accuracy: 0.59 - ETA: 1s - loss: 0.6787 - accuracy: 0.59 - ETA: 0s - loss: 0.6764 - accuracy: 0.60 - ETA: 0s - loss: 0.6747 - accuracy: 0.60 - ETA: 0s - loss: 0.6724 - accuracy: 0.60 - ETA: 0s - loss: 0.6700 - accuracy: 0.61 - ETA: 0s - loss: 0.6675 - accuracy: 0.61 - ETA: 0s - loss: 0.6664 - accuracy: 0.61 - ETA: 0s - loss: 0.6639 - accuracy: 0.61 - ETA: 0s - loss: 0.6620 - accuracy: 0.62 - ETA: 0s - loss: 0.6596 - accuracy: 0.62 - ETA: 0s - loss: 0.6577 - accuracy: 0.62 - ETA: 0s - loss: 0.6568 - accuracy: 0.62 - ETA: 0s - loss: 0.6545 - accuracy: 0.63 - ETA: 0s - loss: 0.6529 - accuracy: 0.63 - ETA: 0s - loss: 0.6517 - accuracy: 0.63 - ETA: 0s - loss: 0.6504 - accuracy: 0.63 - ETA: 0s - loss: 0.6491 - accuracy: 0.63 - 1s 116us/step - loss: 0.6489 - accuracy: 0.6385 - val_loss: 0.5955 - val_accuracy: 0.7202\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:07 - loss: 0.6939 - accuracy: 0.50 - ETA: 3s - loss: 0.6951 - accuracy: 0.4893 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6891 - accuracy: 0.55 - ETA: 1s - loss: 0.6882 - accuracy: 0.55 - ETA: 1s - loss: 0.6861 - accuracy: 0.57 - ETA: 1s - loss: 0.6848 - accuracy: 0.57 - ETA: 0s - loss: 0.6836 - accuracy: 0.57 - ETA: 0s - loss: 0.6823 - accuracy: 0.58 - ETA: 0s - loss: 0.6807 - accuracy: 0.58 - ETA: 0s - loss: 0.6789 - accuracy: 0.59 - ETA: 0s - loss: 0.6761 - accuracy: 0.59 - ETA: 0s - loss: 0.6729 - accuracy: 0.60 - ETA: 0s - loss: 0.6708 - accuracy: 0.60 - ETA: 0s - loss: 0.6689 - accuracy: 0.61 - ETA: 0s - loss: 0.6672 - accuracy: 0.61 - ETA: 0s - loss: 0.6646 - accuracy: 0.61 - ETA: 0s - loss: 0.6631 - accuracy: 0.62 - ETA: 0s - loss: 0.6607 - accuracy: 0.62 - ETA: 0s - loss: 0.6588 - accuracy: 0.62 - ETA: 0s - loss: 0.6566 - accuracy: 0.63 - ETA: 0s - loss: 0.6541 - accuracy: 0.63 - ETA: 0s - loss: 0.6518 - accuracy: 0.63 - 1s 113us/step - loss: 0.6508 - accuracy: 0.6398 - val_loss: 0.5997 - val_accuracy: 0.7188\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:51 - loss: 0.6936 - accuracy: 0.60 - ETA: 2s - loss: 0.6937 - accuracy: 0.4930 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - 1s 108us/step - loss: 0.6934 - accuracy: 0.5129 - val_loss: 0.6924 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:50 - loss: 0.6939 - accuracy: 0.60 - ETA: 2s - loss: 0.6931 - accuracy: 0.5411 - ETA: 1s - loss: 0.6928 - accuracy: 0.53 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.53 - ETA: 1s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6922 - accuracy: 0.52 - ETA: 0s - loss: 0.6922 - accuracy: 0.52 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - ETA: 0s - loss: 0.6919 - accuracy: 0.51 - 1s 106us/step - loss: 0.6918 - accuracy: 0.5190 - val_loss: 0.6886 - val_accuracy: 0.5710\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:52 - loss: 0.6950 - accuracy: 0.50 - ETA: 3s - loss: 0.6936 - accuracy: 0.4778 - ETA: 2s - loss: 0.6928 - accuracy: 0.50 - ETA: 1s - loss: 0.6926 - accuracy: 0.50 - ETA: 1s - loss: 0.6923 - accuracy: 0.51 - ETA: 1s - loss: 0.6913 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.52 - ETA: 1s - loss: 0.6915 - accuracy: 0.52 - ETA: 0s - loss: 0.6913 - accuracy: 0.52 - ETA: 0s - loss: 0.6916 - accuracy: 0.52 - ETA: 0s - loss: 0.6916 - accuracy: 0.52 - ETA: 0s - loss: 0.6913 - accuracy: 0.52 - ETA: 0s - loss: 0.6913 - accuracy: 0.52 - ETA: 0s - loss: 0.6913 - accuracy: 0.51 - ETA: 0s - loss: 0.6914 - accuracy: 0.51 - ETA: 0s - loss: 0.6912 - accuracy: 0.51 - ETA: 0s - loss: 0.6910 - accuracy: 0.51 - ETA: 0s - loss: 0.6910 - accuracy: 0.51 - ETA: 0s - loss: 0.6908 - accuracy: 0.51 - ETA: 0s - loss: 0.6905 - accuracy: 0.51 - ETA: 0s - loss: 0.6905 - accuracy: 0.51 - ETA: 0s - loss: 0.6903 - accuracy: 0.51 - ETA: 0s - loss: 0.6902 - accuracy: 0.52 - 1s 104us/step - loss: 0.6901 - accuracy: 0.5211 - val_loss: 0.6841 - val_accuracy: 0.5810\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 1:57 - loss: 0.6957 - accuracy: 0.30 - ETA: 3s - loss: 0.6933 - accuracy: 0.5426 - ETA: 2s - loss: 0.6934 - accuracy: 0.53 - ETA: 1s - loss: 0.6933 - accuracy: 0.52 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 1s - loss: 0.6935 - accuracy: 0.52 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - 1s 106us/step - loss: 0.6934 - accuracy: 0.5149 - val_loss: 0.6925 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:47 - loss: 0.6959 - accuracy: 0.40 - ETA: 2s - loss: 0.6936 - accuracy: 0.5158 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - 1s 104us/step - loss: 0.6934 - accuracy: 0.5134 - val_loss: 0.6926 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:50 - loss: 0.6955 - accuracy: 0.50 - ETA: 2s - loss: 0.6936 - accuracy: 0.5018 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.53 - ETA: 0s - loss: 0.6927 - accuracy: 0.53 - ETA: 0s - loss: 0.6926 - accuracy: 0.53 - ETA: 0s - loss: 0.6925 - accuracy: 0.53 - ETA: 0s - loss: 0.6925 - accuracy: 0.53 - ETA: 0s - loss: 0.6923 - accuracy: 0.53 - ETA: 0s - loss: 0.6922 - accuracy: 0.54 - ETA: 0s - loss: 0.6921 - accuracy: 0.54 - ETA: 0s - loss: 0.6919 - accuracy: 0.54 - 1s 105us/step - loss: 0.6918 - accuracy: 0.5465 - val_loss: 0.6889 - val_accuracy: 0.5767\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:50 - loss: 0.6989 - accuracy: 0.40 - ETA: 2s - loss: 0.6944 - accuracy: 0.5105 - ETA: 1s - loss: 0.6944 - accuracy: 0.50 - ETA: 1s - loss: 0.6943 - accuracy: 0.50 - ETA: 1s - loss: 0.6942 - accuracy: 0.51 - ETA: 1s - loss: 0.6942 - accuracy: 0.51 - ETA: 1s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - 1s 104us/step - loss: 0.6937 - accuracy: 0.5145 - val_loss: 0.6919 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:47 - loss: 0.6952 - accuracy: 0.50 - ETA: 2s - loss: 0.6928 - accuracy: 0.5518 - ETA: 1s - loss: 0.6927 - accuracy: 0.54 - ETA: 1s - loss: 0.6932 - accuracy: 0.53 - ETA: 1s - loss: 0.6933 - accuracy: 0.53 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 1s - loss: 0.6931 - accuracy: 0.53 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.53 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.53 - ETA: 0s - loss: 0.6929 - accuracy: 0.53 - ETA: 0s - loss: 0.6928 - accuracy: 0.53 - ETA: 0s - loss: 0.6927 - accuracy: 0.53 - ETA: 0s - loss: 0.6928 - accuracy: 0.53 - ETA: 0s - loss: 0.6927 - accuracy: 0.53 - ETA: 0s - loss: 0.6926 - accuracy: 0.53 - ETA: 0s - loss: 0.6925 - accuracy: 0.53 - 1s 104us/step - loss: 0.6924 - accuracy: 0.5361 - val_loss: 0.6884 - val_accuracy: 0.5483\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:48 - loss: 0.6928 - accuracy: 0.60 - ETA: 2s - loss: 0.6948 - accuracy: 0.4860 - ETA: 1s - loss: 0.6946 - accuracy: 0.49 - ETA: 1s - loss: 0.6945 - accuracy: 0.49 - ETA: 1s - loss: 0.6945 - accuracy: 0.49 - ETA: 1s - loss: 0.6945 - accuracy: 0.49 - ETA: 1s - loss: 0.6944 - accuracy: 0.49 - ETA: 0s - loss: 0.6944 - accuracy: 0.49 - ETA: 0s - loss: 0.6943 - accuracy: 0.49 - ETA: 0s - loss: 0.6943 - accuracy: 0.49 - ETA: 0s - loss: 0.6942 - accuracy: 0.49 - ETA: 0s - loss: 0.6941 - accuracy: 0.49 - ETA: 0s - loss: 0.6941 - accuracy: 0.49 - ETA: 0s - loss: 0.6941 - accuracy: 0.49 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - 1s 105us/step - loss: 0.6933 - accuracy: 0.5089 - val_loss: 0.6903 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:52 - loss: 0.6958 - accuracy: 0.40 - ETA: 2s - loss: 0.6936 - accuracy: 0.5561 - ETA: 1s - loss: 0.6934 - accuracy: 0.54 - ETA: 1s - loss: 0.6940 - accuracy: 0.52 - ETA: 1s - loss: 0.6940 - accuracy: 0.51 - ETA: 1s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - 1s 104us/step - loss: 0.6934 - accuracy: 0.5161 - val_loss: 0.6915 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 39us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:50 - loss: 0.6933 - accuracy: 0.60 - ETA: 2s - loss: 0.6941 - accuracy: 0.5368 - ETA: 1s - loss: 0.6945 - accuracy: 0.51 - ETA: 1s - loss: 0.6945 - accuracy: 0.51 - ETA: 1s - loss: 0.6944 - accuracy: 0.51 - ETA: 1s - loss: 0.6943 - accuracy: 0.51 - ETA: 1s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - 1s 106us/step - loss: 0.6939 - accuracy: 0.5149 - val_loss: 0.6926 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:52 - loss: 0.6943 - accuracy: 0.60 - ETA: 2s - loss: 0.6934 - accuracy: 0.5316 - ETA: 1s - loss: 0.6932 - accuracy: 0.52 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.53 - ETA: 0s - loss: 0.6919 - accuracy: 0.53 - ETA: 0s - loss: 0.6917 - accuracy: 0.53 - ETA: 0s - loss: 0.6916 - accuracy: 0.53 - ETA: 0s - loss: 0.6913 - accuracy: 0.53 - 1s 103us/step - loss: 0.6913 - accuracy: 0.5349 - val_loss: 0.6874 - val_accuracy: 0.5170\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:53 - loss: 0.6939 - accuracy: 0.40 - ETA: 3s - loss: 0.6939 - accuracy: 0.5375 - ETA: 1s - loss: 0.6936 - accuracy: 0.53 - ETA: 1s - loss: 0.6937 - accuracy: 0.53 - ETA: 1s - loss: 0.6935 - accuracy: 0.53 - ETA: 1s - loss: 0.6935 - accuracy: 0.52 - ETA: 1s - loss: 0.6936 - accuracy: 0.52 - ETA: 1s - loss: 0.6936 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - 1s 106us/step - loss: 0.6924 - accuracy: 0.5223 - val_loss: 0.6879 - val_accuracy: 0.5568\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:53 - loss: 0.6926 - accuracy: 0.40 - ETA: 2s - loss: 0.6931 - accuracy: 0.5333 - ETA: 1s - loss: 0.6941 - accuracy: 0.52 - ETA: 1s - loss: 0.6933 - accuracy: 0.52 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6922 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6922 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - 1s 104us/step - loss: 0.6920 - accuracy: 0.5247 - val_loss: 0.6877 - val_accuracy: 0.5874\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 39us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:50 - loss: 0.6937 - accuracy: 0.70 - ETA: 2s - loss: 0.6939 - accuracy: 0.5614 - ETA: 1s - loss: 0.6937 - accuracy: 0.55 - ETA: 1s - loss: 0.6933 - accuracy: 0.54 - ETA: 1s - loss: 0.6936 - accuracy: 0.54 - ETA: 1s - loss: 0.6935 - accuracy: 0.53 - ETA: 1s - loss: 0.6935 - accuracy: 0.53 - ETA: 0s - loss: 0.6935 - accuracy: 0.53 - ETA: 0s - loss: 0.6934 - accuracy: 0.53 - ETA: 0s - loss: 0.6937 - accuracy: 0.53 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - ETA: 0s - loss: 0.6940 - accuracy: 0.52 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.52 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.52 - ETA: 0s - loss: 0.6942 - accuracy: 0.52 - ETA: 0s - loss: 0.6941 - accuracy: 0.52 - ETA: 0s - loss: 0.6941 - accuracy: 0.52 - 1s 104us/step - loss: 0.6941 - accuracy: 0.5249 - val_loss: 0.6927 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 39us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 1:52 - loss: 0.6979 - accuracy: 0.30 - ETA: 2s - loss: 0.6946 - accuracy: 0.5246 - ETA: 1s - loss: 0.6948 - accuracy: 0.50 - ETA: 1s - loss: 0.6949 - accuracy: 0.50 - ETA: 1s - loss: 0.6944 - accuracy: 0.51 - ETA: 1s - loss: 0.6945 - accuracy: 0.51 - ETA: 1s - loss: 0.6944 - accuracy: 0.51 - ETA: 1s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - 1s 117us/step - loss: 0.6928 - accuracy: 0.5226 - val_loss: 0.6886 - val_accuracy: 0.5312\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:48 - loss: 0.6939 - accuracy: 0.50 - ETA: 2s - loss: 0.6953 - accuracy: 0.4929 - ETA: 1s - loss: 0.6952 - accuracy: 0.48 - ETA: 1s - loss: 0.6947 - accuracy: 0.50 - ETA: 1s - loss: 0.6948 - accuracy: 0.49 - ETA: 1s - loss: 0.6947 - accuracy: 0.49 - ETA: 1s - loss: 0.6944 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - 1s 104us/step - loss: 0.6935 - accuracy: 0.5180 - val_loss: 0.6905 - val_accuracy: 0.5284\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:02 - loss: 0.6970 - accuracy: 0.30 - ETA: 3s - loss: 0.6944 - accuracy: 0.4911 - ETA: 2s - loss: 0.6943 - accuracy: 0.50 - ETA: 1s - loss: 0.6946 - accuracy: 0.49 - ETA: 1s - loss: 0.6945 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.53 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6920 - accuracy: 0.53 - ETA: 0s - loss: 0.6919 - accuracy: 0.53 - 1s 106us/step - loss: 0.6917 - accuracy: 0.5334 - val_loss: 0.6865 - val_accuracy: 0.5426\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:21 - loss: 0.6935 - accuracy: 0.70 - ETA: 3s - loss: 0.6934 - accuracy: 0.5358 - ETA: 2s - loss: 0.6929 - accuracy: 0.55 - ETA: 1s - loss: 0.6929 - accuracy: 0.53 - ETA: 1s - loss: 0.6922 - accuracy: 0.53 - ETA: 1s - loss: 0.6921 - accuracy: 0.53 - ETA: 1s - loss: 0.6912 - accuracy: 0.54 - ETA: 1s - loss: 0.6907 - accuracy: 0.54 - ETA: 1s - loss: 0.6898 - accuracy: 0.55 - ETA: 0s - loss: 0.6889 - accuracy: 0.55 - ETA: 0s - loss: 0.6880 - accuracy: 0.55 - ETA: 0s - loss: 0.6871 - accuracy: 0.55 - ETA: 0s - loss: 0.6866 - accuracy: 0.55 - ETA: 0s - loss: 0.6850 - accuracy: 0.55 - ETA: 0s - loss: 0.6837 - accuracy: 0.56 - ETA: 0s - loss: 0.6827 - accuracy: 0.56 - ETA: 0s - loss: 0.6813 - accuracy: 0.56 - ETA: 0s - loss: 0.6809 - accuracy: 0.56 - ETA: 0s - loss: 0.6803 - accuracy: 0.56 - ETA: 0s - loss: 0.6794 - accuracy: 0.56 - ETA: 0s - loss: 0.6786 - accuracy: 0.56 - ETA: 0s - loss: 0.6777 - accuracy: 0.56 - ETA: 0s - loss: 0.6771 - accuracy: 0.56 - ETA: 0s - loss: 0.6764 - accuracy: 0.56 - ETA: 0s - loss: 0.6752 - accuracy: 0.56 - 1s 116us/step - loss: 0.6745 - accuracy: 0.5704 - val_loss: 0.6364 - val_accuracy: 0.7180\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:21 - loss: 0.6929 - accuracy: 0.70 - ETA: 3s - loss: 0.6936 - accuracy: 0.4981 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6921 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6895 - accuracy: 0.55 - ETA: 1s - loss: 0.6877 - accuracy: 0.55 - ETA: 0s - loss: 0.6864 - accuracy: 0.55 - ETA: 0s - loss: 0.6854 - accuracy: 0.55 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6813 - accuracy: 0.56 - ETA: 0s - loss: 0.6792 - accuracy: 0.57 - ETA: 0s - loss: 0.6776 - accuracy: 0.57 - ETA: 0s - loss: 0.6751 - accuracy: 0.58 - ETA: 0s - loss: 0.6728 - accuracy: 0.58 - ETA: 0s - loss: 0.6710 - accuracy: 0.58 - ETA: 0s - loss: 0.6699 - accuracy: 0.58 - ETA: 0s - loss: 0.6682 - accuracy: 0.59 - ETA: 0s - loss: 0.6669 - accuracy: 0.59 - ETA: 0s - loss: 0.6662 - accuracy: 0.59 - ETA: 0s - loss: 0.6652 - accuracy: 0.59 - ETA: 0s - loss: 0.6633 - accuracy: 0.60 - ETA: 0s - loss: 0.6620 - accuracy: 0.60 - 1s 115us/step - loss: 0.6611 - accuracy: 0.6020 - val_loss: 0.6112 - val_accuracy: 0.7216\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:23 - loss: 0.6930 - accuracy: 0.50 - ETA: 3s - loss: 0.6937 - accuracy: 0.4885 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6924 - accuracy: 0.51 - ETA: 1s - loss: 0.6920 - accuracy: 0.51 - ETA: 1s - loss: 0.6915 - accuracy: 0.52 - ETA: 1s - loss: 0.6905 - accuracy: 0.52 - ETA: 1s - loss: 0.6897 - accuracy: 0.52 - ETA: 0s - loss: 0.6880 - accuracy: 0.53 - ETA: 0s - loss: 0.6860 - accuracy: 0.53 - ETA: 0s - loss: 0.6845 - accuracy: 0.53 - ETA: 0s - loss: 0.6835 - accuracy: 0.54 - ETA: 0s - loss: 0.6827 - accuracy: 0.54 - ETA: 0s - loss: 0.6825 - accuracy: 0.54 - ETA: 0s - loss: 0.6809 - accuracy: 0.55 - ETA: 0s - loss: 0.6797 - accuracy: 0.55 - ETA: 0s - loss: 0.6790 - accuracy: 0.55 - ETA: 0s - loss: 0.6784 - accuracy: 0.55 - ETA: 0s - loss: 0.6774 - accuracy: 0.55 - ETA: 0s - loss: 0.6762 - accuracy: 0.55 - ETA: 0s - loss: 0.6754 - accuracy: 0.56 - ETA: 0s - loss: 0.6747 - accuracy: 0.56 - ETA: 0s - loss: 0.6735 - accuracy: 0.56 - ETA: 0s - loss: 0.6726 - accuracy: 0.56 - ETA: 0s - loss: 0.6720 - accuracy: 0.56 - 1s 118us/step - loss: 0.6717 - accuracy: 0.5655 - val_loss: 0.6339 - val_accuracy: 0.7088\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:21 - loss: 0.6937 - accuracy: 0.40 - ETA: 3s - loss: 0.6937 - accuracy: 0.4962 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6903 - accuracy: 0.54 - ETA: 1s - loss: 0.6884 - accuracy: 0.54 - ETA: 0s - loss: 0.6874 - accuracy: 0.55 - ETA: 0s - loss: 0.6853 - accuracy: 0.56 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6814 - accuracy: 0.57 - ETA: 0s - loss: 0.6800 - accuracy: 0.57 - ETA: 0s - loss: 0.6782 - accuracy: 0.57 - ETA: 0s - loss: 0.6760 - accuracy: 0.58 - ETA: 0s - loss: 0.6737 - accuracy: 0.58 - ETA: 0s - loss: 0.6722 - accuracy: 0.58 - ETA: 0s - loss: 0.6706 - accuracy: 0.59 - ETA: 0s - loss: 0.6683 - accuracy: 0.59 - ETA: 0s - loss: 0.6669 - accuracy: 0.60 - ETA: 0s - loss: 0.6652 - accuracy: 0.60 - ETA: 0s - loss: 0.6640 - accuracy: 0.60 - ETA: 0s - loss: 0.6626 - accuracy: 0.61 - ETA: 0s - loss: 0.6610 - accuracy: 0.61 - 1s 117us/step - loss: 0.6609 - accuracy: 0.6131 - val_loss: 0.6145 - val_accuracy: 0.7045\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:28 - loss: 0.6906 - accuracy: 0.60 - ETA: 3s - loss: 0.6919 - accuracy: 0.5283 - ETA: 2s - loss: 0.6917 - accuracy: 0.52 - ETA: 1s - loss: 0.6911 - accuracy: 0.52 - ETA: 1s - loss: 0.6913 - accuracy: 0.51 - ETA: 1s - loss: 0.6907 - accuracy: 0.52 - ETA: 1s - loss: 0.6899 - accuracy: 0.53 - ETA: 1s - loss: 0.6892 - accuracy: 0.53 - ETA: 1s - loss: 0.6879 - accuracy: 0.54 - ETA: 0s - loss: 0.6868 - accuracy: 0.55 - ETA: 0s - loss: 0.6857 - accuracy: 0.55 - ETA: 0s - loss: 0.6840 - accuracy: 0.56 - ETA: 0s - loss: 0.6827 - accuracy: 0.56 - ETA: 0s - loss: 0.6809 - accuracy: 0.56 - ETA: 0s - loss: 0.6793 - accuracy: 0.57 - ETA: 0s - loss: 0.6771 - accuracy: 0.57 - ETA: 0s - loss: 0.6752 - accuracy: 0.57 - ETA: 0s - loss: 0.6742 - accuracy: 0.58 - ETA: 0s - loss: 0.6737 - accuracy: 0.58 - ETA: 0s - loss: 0.6724 - accuracy: 0.58 - ETA: 0s - loss: 0.6720 - accuracy: 0.58 - ETA: 0s - loss: 0.6705 - accuracy: 0.59 - ETA: 0s - loss: 0.6690 - accuracy: 0.59 - ETA: 0s - loss: 0.6678 - accuracy: 0.59 - ETA: 0s - loss: 0.6657 - accuracy: 0.60 - ETA: 0s - loss: 0.6651 - accuracy: 0.60 - 1s 118us/step - loss: 0.6647 - accuracy: 0.6032 - val_loss: 0.6173 - val_accuracy: 0.7173\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:25 - loss: 0.6960 - accuracy: 0.40 - ETA: 3s - loss: 0.6939 - accuracy: 0.4846 - ETA: 2s - loss: 0.6937 - accuracy: 0.48 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.53 - ETA: 1s - loss: 0.6919 - accuracy: 0.53 - ETA: 1s - loss: 0.6911 - accuracy: 0.53 - ETA: 1s - loss: 0.6902 - accuracy: 0.54 - ETA: 1s - loss: 0.6890 - accuracy: 0.54 - ETA: 0s - loss: 0.6882 - accuracy: 0.54 - ETA: 0s - loss: 0.6878 - accuracy: 0.54 - ETA: 0s - loss: 0.6866 - accuracy: 0.55 - ETA: 0s - loss: 0.6854 - accuracy: 0.55 - ETA: 0s - loss: 0.6839 - accuracy: 0.55 - ETA: 0s - loss: 0.6826 - accuracy: 0.55 - ETA: 0s - loss: 0.6815 - accuracy: 0.55 - ETA: 0s - loss: 0.6802 - accuracy: 0.55 - ETA: 0s - loss: 0.6787 - accuracy: 0.56 - ETA: 0s - loss: 0.6776 - accuracy: 0.56 - ETA: 0s - loss: 0.6764 - accuracy: 0.56 - ETA: 0s - loss: 0.6752 - accuracy: 0.56 - ETA: 0s - loss: 0.6741 - accuracy: 0.56 - ETA: 0s - loss: 0.6731 - accuracy: 0.56 - ETA: 0s - loss: 0.6717 - accuracy: 0.57 - ETA: 0s - loss: 0.6710 - accuracy: 0.57 - 2s 119us/step - loss: 0.6706 - accuracy: 0.5722 - val_loss: 0.6353 - val_accuracy: 0.6974\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:29 - loss: 0.6887 - accuracy: 0.80 - ETA: 3s - loss: 0.6926 - accuracy: 0.5208 - ETA: 2s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6914 - accuracy: 0.52 - ETA: 1s - loss: 0.6911 - accuracy: 0.52 - ETA: 1s - loss: 0.6904 - accuracy: 0.53 - ETA: 1s - loss: 0.6888 - accuracy: 0.54 - ETA: 1s - loss: 0.6866 - accuracy: 0.55 - ETA: 1s - loss: 0.6845 - accuracy: 0.56 - ETA: 1s - loss: 0.6813 - accuracy: 0.57 - ETA: 0s - loss: 0.6789 - accuracy: 0.57 - ETA: 0s - loss: 0.6767 - accuracy: 0.58 - ETA: 0s - loss: 0.6760 - accuracy: 0.58 - ETA: 0s - loss: 0.6724 - accuracy: 0.58 - ETA: 0s - loss: 0.6700 - accuracy: 0.59 - ETA: 0s - loss: 0.6691 - accuracy: 0.59 - ETA: 0s - loss: 0.6671 - accuracy: 0.60 - ETA: 0s - loss: 0.6647 - accuracy: 0.60 - ETA: 0s - loss: 0.6623 - accuracy: 0.60 - ETA: 0s - loss: 0.6599 - accuracy: 0.61 - ETA: 0s - loss: 0.6590 - accuracy: 0.61 - ETA: 0s - loss: 0.6582 - accuracy: 0.61 - ETA: 0s - loss: 0.6561 - accuracy: 0.61 - ETA: 0s - loss: 0.6547 - accuracy: 0.62 - ETA: 0s - loss: 0.6543 - accuracy: 0.62 - ETA: 0s - loss: 0.6530 - accuracy: 0.62 - 2s 120us/step - loss: 0.6524 - accuracy: 0.6263 - val_loss: 0.6012 - val_accuracy: 0.7180\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:29 - loss: 0.6948 - accuracy: 0.50 - ETA: 3s - loss: 0.6935 - accuracy: 0.5222 - ETA: 2s - loss: 0.6924 - accuracy: 0.55 - ETA: 1s - loss: 0.6913 - accuracy: 0.55 - ETA: 1s - loss: 0.6896 - accuracy: 0.57 - ETA: 1s - loss: 0.6879 - accuracy: 0.57 - ETA: 1s - loss: 0.6853 - accuracy: 0.58 - ETA: 1s - loss: 0.6835 - accuracy: 0.58 - ETA: 1s - loss: 0.6804 - accuracy: 0.59 - ETA: 1s - loss: 0.6773 - accuracy: 0.59 - ETA: 0s - loss: 0.6753 - accuracy: 0.59 - ETA: 0s - loss: 0.6733 - accuracy: 0.60 - ETA: 0s - loss: 0.6716 - accuracy: 0.60 - ETA: 0s - loss: 0.6691 - accuracy: 0.60 - ETA: 0s - loss: 0.6660 - accuracy: 0.61 - ETA: 0s - loss: 0.6633 - accuracy: 0.61 - ETA: 0s - loss: 0.6615 - accuracy: 0.61 - ETA: 0s - loss: 0.6597 - accuracy: 0.62 - ETA: 0s - loss: 0.6588 - accuracy: 0.62 - ETA: 0s - loss: 0.6570 - accuracy: 0.62 - ETA: 0s - loss: 0.6559 - accuracy: 0.62 - ETA: 0s - loss: 0.6536 - accuracy: 0.63 - ETA: 0s - loss: 0.6532 - accuracy: 0.63 - ETA: 0s - loss: 0.6523 - accuracy: 0.63 - ETA: 0s - loss: 0.6510 - accuracy: 0.63 - ETA: 0s - loss: 0.6503 - accuracy: 0.63 - 1s 118us/step - loss: 0.6501 - accuracy: 0.6392 - val_loss: 0.5957 - val_accuracy: 0.7131\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:37 - loss: 0.6950 - accuracy: 0.50 - ETA: 4s - loss: 0.6929 - accuracy: 0.5235 - ETA: 2s - loss: 0.6919 - accuracy: 0.54 - ETA: 2s - loss: 0.6903 - accuracy: 0.55 - ETA: 1s - loss: 0.6883 - accuracy: 0.56 - ETA: 1s - loss: 0.6854 - accuracy: 0.57 - ETA: 1s - loss: 0.6823 - accuracy: 0.58 - ETA: 1s - loss: 0.6815 - accuracy: 0.57 - ETA: 1s - loss: 0.6787 - accuracy: 0.58 - ETA: 1s - loss: 0.6756 - accuracy: 0.59 - ETA: 0s - loss: 0.6736 - accuracy: 0.59 - ETA: 0s - loss: 0.6709 - accuracy: 0.60 - ETA: 0s - loss: 0.6687 - accuracy: 0.60 - ETA: 0s - loss: 0.6662 - accuracy: 0.61 - ETA: 0s - loss: 0.6644 - accuracy: 0.61 - ETA: 0s - loss: 0.6626 - accuracy: 0.61 - ETA: 0s - loss: 0.6608 - accuracy: 0.61 - ETA: 0s - loss: 0.6596 - accuracy: 0.61 - ETA: 0s - loss: 0.6586 - accuracy: 0.62 - ETA: 0s - loss: 0.6564 - accuracy: 0.62 - ETA: 0s - loss: 0.6538 - accuracy: 0.62 - ETA: 0s - loss: 0.6521 - accuracy: 0.63 - ETA: 0s - loss: 0.6510 - accuracy: 0.63 - ETA: 0s - loss: 0.6498 - accuracy: 0.63 - ETA: 0s - loss: 0.6483 - accuracy: 0.63 - ETA: 0s - loss: 0.6470 - accuracy: 0.63 - 1s 118us/step - loss: 0.6474 - accuracy: 0.6351 - val_loss: 0.5985 - val_accuracy: 0.7216\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:26 - loss: 0.6953 - accuracy: 0.30 - ETA: 3s - loss: 0.6924 - accuracy: 0.5472 - ETA: 2s - loss: 0.6923 - accuracy: 0.53 - ETA: 1s - loss: 0.6916 - accuracy: 0.53 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6907 - accuracy: 0.53 - ETA: 1s - loss: 0.6893 - accuracy: 0.53 - ETA: 1s - loss: 0.6883 - accuracy: 0.54 - ETA: 1s - loss: 0.6878 - accuracy: 0.54 - ETA: 1s - loss: 0.6867 - accuracy: 0.55 - ETA: 0s - loss: 0.6857 - accuracy: 0.55 - ETA: 0s - loss: 0.6841 - accuracy: 0.55 - ETA: 0s - loss: 0.6824 - accuracy: 0.56 - ETA: 0s - loss: 0.6804 - accuracy: 0.56 - ETA: 0s - loss: 0.6777 - accuracy: 0.57 - ETA: 0s - loss: 0.6748 - accuracy: 0.58 - ETA: 0s - loss: 0.6724 - accuracy: 0.58 - ETA: 0s - loss: 0.6701 - accuracy: 0.59 - ETA: 0s - loss: 0.6676 - accuracy: 0.59 - ETA: 0s - loss: 0.6671 - accuracy: 0.59 - ETA: 0s - loss: 0.6641 - accuracy: 0.60 - ETA: 0s - loss: 0.6621 - accuracy: 0.60 - ETA: 0s - loss: 0.6603 - accuracy: 0.60 - ETA: 0s - loss: 0.6591 - accuracy: 0.60 - ETA: 0s - loss: 0.6586 - accuracy: 0.60 - 1s 116us/step - loss: 0.6569 - accuracy: 0.6123 - val_loss: 0.5998 - val_accuracy: 0.7173\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:23 - loss: 0.6937 - accuracy: 0.70 - ETA: 3s - loss: 0.6918 - accuracy: 0.5660 - ETA: 2s - loss: 0.6924 - accuracy: 0.54 - ETA: 1s - loss: 0.6919 - accuracy: 0.54 - ETA: 1s - loss: 0.6907 - accuracy: 0.55 - ETA: 1s - loss: 0.6897 - accuracy: 0.55 - ETA: 1s - loss: 0.6877 - accuracy: 0.56 - ETA: 1s - loss: 0.6867 - accuracy: 0.56 - ETA: 1s - loss: 0.6848 - accuracy: 0.57 - ETA: 0s - loss: 0.6823 - accuracy: 0.58 - ETA: 0s - loss: 0.6803 - accuracy: 0.58 - ETA: 0s - loss: 0.6781 - accuracy: 0.59 - ETA: 0s - loss: 0.6762 - accuracy: 0.59 - ETA: 0s - loss: 0.6741 - accuracy: 0.59 - ETA: 0s - loss: 0.6730 - accuracy: 0.59 - ETA: 0s - loss: 0.6713 - accuracy: 0.60 - ETA: 0s - loss: 0.6692 - accuracy: 0.60 - ETA: 0s - loss: 0.6673 - accuracy: 0.60 - ETA: 0s - loss: 0.6653 - accuracy: 0.61 - ETA: 0s - loss: 0.6634 - accuracy: 0.61 - ETA: 0s - loss: 0.6617 - accuracy: 0.61 - ETA: 0s - loss: 0.6600 - accuracy: 0.61 - ETA: 0s - loss: 0.6591 - accuracy: 0.62 - ETA: 0s - loss: 0.6575 - accuracy: 0.62 - ETA: 0s - loss: 0.6551 - accuracy: 0.62 - ETA: 0s - loss: 0.6540 - accuracy: 0.62 - 1s 117us/step - loss: 0.6538 - accuracy: 0.6281 - val_loss: 0.6010 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:24 - loss: 0.6976 - accuracy: 0.40 - ETA: 3s - loss: 0.6942 - accuracy: 0.5058 - ETA: 2s - loss: 0.6933 - accuracy: 0.53 - ETA: 1s - loss: 0.6923 - accuracy: 0.54 - ETA: 1s - loss: 0.6902 - accuracy: 0.55 - ETA: 1s - loss: 0.6899 - accuracy: 0.54 - ETA: 1s - loss: 0.6887 - accuracy: 0.55 - ETA: 1s - loss: 0.6871 - accuracy: 0.55 - ETA: 1s - loss: 0.6847 - accuracy: 0.56 - ETA: 1s - loss: 0.6825 - accuracy: 0.57 - ETA: 0s - loss: 0.6803 - accuracy: 0.57 - ETA: 0s - loss: 0.6787 - accuracy: 0.58 - ETA: 0s - loss: 0.6768 - accuracy: 0.58 - ETA: 0s - loss: 0.6746 - accuracy: 0.59 - ETA: 0s - loss: 0.6730 - accuracy: 0.59 - ETA: 0s - loss: 0.6719 - accuracy: 0.59 - ETA: 0s - loss: 0.6698 - accuracy: 0.60 - ETA: 0s - loss: 0.6678 - accuracy: 0.60 - ETA: 0s - loss: 0.6657 - accuracy: 0.61 - ETA: 0s - loss: 0.6639 - accuracy: 0.61 - ETA: 0s - loss: 0.6613 - accuracy: 0.61 - ETA: 0s - loss: 0.6587 - accuracy: 0.62 - ETA: 0s - loss: 0.6571 - accuracy: 0.62 - ETA: 0s - loss: 0.6557 - accuracy: 0.62 - ETA: 0s - loss: 0.6545 - accuracy: 0.62 - ETA: 0s - loss: 0.6530 - accuracy: 0.63 - 2s 119us/step - loss: 0.6524 - accuracy: 0.6310 - val_loss: 0.6005 - val_accuracy: 0.7159\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:28 - loss: 0.6922 - accuracy: 0.50 - ETA: 3s - loss: 0.6919 - accuracy: 0.5490 - ETA: 2s - loss: 0.6928 - accuracy: 0.52 - ETA: 2s - loss: 0.6911 - accuracy: 0.54 - ETA: 2s - loss: 0.6907 - accuracy: 0.54 - ETA: 1s - loss: 0.6894 - accuracy: 0.55 - ETA: 1s - loss: 0.6874 - accuracy: 0.56 - ETA: 1s - loss: 0.6852 - accuracy: 0.57 - ETA: 1s - loss: 0.6824 - accuracy: 0.58 - ETA: 1s - loss: 0.6785 - accuracy: 0.59 - ETA: 1s - loss: 0.6748 - accuracy: 0.59 - ETA: 1s - loss: 0.6714 - accuracy: 0.60 - ETA: 0s - loss: 0.6688 - accuracy: 0.61 - ETA: 0s - loss: 0.6665 - accuracy: 0.61 - ETA: 0s - loss: 0.6640 - accuracy: 0.61 - ETA: 0s - loss: 0.6617 - accuracy: 0.61 - ETA: 0s - loss: 0.6592 - accuracy: 0.62 - ETA: 0s - loss: 0.6553 - accuracy: 0.63 - ETA: 0s - loss: 0.6546 - accuracy: 0.63 - ETA: 0s - loss: 0.6519 - accuracy: 0.63 - ETA: 0s - loss: 0.6509 - accuracy: 0.63 - ETA: 0s - loss: 0.6501 - accuracy: 0.63 - ETA: 0s - loss: 0.6480 - accuracy: 0.64 - ETA: 0s - loss: 0.6468 - accuracy: 0.64 - ETA: 0s - loss: 0.6459 - accuracy: 0.64 - ETA: 0s - loss: 0.6446 - accuracy: 0.64 - ETA: 0s - loss: 0.6435 - accuracy: 0.64 - 2s 123us/step - loss: 0.6423 - accuracy: 0.6487 - val_loss: 0.5891 - val_accuracy: 0.7223\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:24 - loss: 0.7015 - accuracy: 0.50 - ETA: 3s - loss: 0.6928 - accuracy: 0.5216 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6906 - accuracy: 0.52 - ETA: 1s - loss: 0.6886 - accuracy: 0.53 - ETA: 1s - loss: 0.6855 - accuracy: 0.54 - ETA: 1s - loss: 0.6836 - accuracy: 0.55 - ETA: 1s - loss: 0.6815 - accuracy: 0.56 - ETA: 0s - loss: 0.6806 - accuracy: 0.56 - ETA: 0s - loss: 0.6772 - accuracy: 0.57 - ETA: 0s - loss: 0.6756 - accuracy: 0.58 - ETA: 0s - loss: 0.6734 - accuracy: 0.58 - ETA: 0s - loss: 0.6700 - accuracy: 0.59 - ETA: 0s - loss: 0.6676 - accuracy: 0.60 - ETA: 0s - loss: 0.6655 - accuracy: 0.60 - ETA: 0s - loss: 0.6632 - accuracy: 0.60 - ETA: 0s - loss: 0.6609 - accuracy: 0.61 - ETA: 0s - loss: 0.6584 - accuracy: 0.61 - ETA: 0s - loss: 0.6562 - accuracy: 0.62 - ETA: 0s - loss: 0.6539 - accuracy: 0.62 - ETA: 0s - loss: 0.6525 - accuracy: 0.62 - ETA: 0s - loss: 0.6518 - accuracy: 0.63 - ETA: 0s - loss: 0.6505 - accuracy: 0.63 - ETA: 0s - loss: 0.6486 - accuracy: 0.63 - 1s 117us/step - loss: 0.6488 - accuracy: 0.6355 - val_loss: 0.6005 - val_accuracy: 0.7251\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:20 - loss: 0.6911 - accuracy: 0.60 - ETA: 3s - loss: 0.6943 - accuracy: 0.5058 - ETA: 2s - loss: 0.6927 - accuracy: 0.50 - ETA: 1s - loss: 0.6910 - accuracy: 0.54 - ETA: 1s - loss: 0.6888 - accuracy: 0.54 - ETA: 1s - loss: 0.6877 - accuracy: 0.55 - ETA: 1s - loss: 0.6868 - accuracy: 0.55 - ETA: 1s - loss: 0.6857 - accuracy: 0.56 - ETA: 1s - loss: 0.6822 - accuracy: 0.58 - ETA: 1s - loss: 0.6799 - accuracy: 0.58 - ETA: 0s - loss: 0.6767 - accuracy: 0.59 - ETA: 0s - loss: 0.6737 - accuracy: 0.59 - ETA: 0s - loss: 0.6708 - accuracy: 0.60 - ETA: 0s - loss: 0.6672 - accuracy: 0.60 - ETA: 0s - loss: 0.6649 - accuracy: 0.61 - ETA: 0s - loss: 0.6630 - accuracy: 0.61 - ETA: 0s - loss: 0.6595 - accuracy: 0.62 - ETA: 0s - loss: 0.6553 - accuracy: 0.62 - ETA: 0s - loss: 0.6535 - accuracy: 0.63 - ETA: 0s - loss: 0.6512 - accuracy: 0.63 - ETA: 0s - loss: 0.6495 - accuracy: 0.63 - ETA: 0s - loss: 0.6471 - accuracy: 0.64 - ETA: 0s - loss: 0.6455 - accuracy: 0.64 - ETA: 0s - loss: 0.6437 - accuracy: 0.64 - ETA: 0s - loss: 0.6421 - accuracy: 0.64 - ETA: 0s - loss: 0.6406 - accuracy: 0.65 - 2s 120us/step - loss: 0.6391 - accuracy: 0.6527 - val_loss: 0.5849 - val_accuracy: 0.7237\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:26 - loss: 0.6890 - accuracy: 0.90 - ETA: 3s - loss: 0.6941 - accuracy: 0.5154 - ETA: 2s - loss: 0.6931 - accuracy: 0.53 - ETA: 2s - loss: 0.6913 - accuracy: 0.55 - ETA: 1s - loss: 0.6902 - accuracy: 0.54 - ETA: 1s - loss: 0.6885 - accuracy: 0.56 - ETA: 1s - loss: 0.6867 - accuracy: 0.57 - ETA: 1s - loss: 0.6850 - accuracy: 0.57 - ETA: 1s - loss: 0.6823 - accuracy: 0.58 - ETA: 1s - loss: 0.6801 - accuracy: 0.59 - ETA: 0s - loss: 0.6769 - accuracy: 0.59 - ETA: 0s - loss: 0.6743 - accuracy: 0.60 - ETA: 0s - loss: 0.6720 - accuracy: 0.60 - ETA: 0s - loss: 0.6698 - accuracy: 0.61 - ETA: 0s - loss: 0.6665 - accuracy: 0.61 - ETA: 0s - loss: 0.6642 - accuracy: 0.61 - ETA: 0s - loss: 0.6611 - accuracy: 0.62 - ETA: 0s - loss: 0.6588 - accuracy: 0.62 - ETA: 0s - loss: 0.6561 - accuracy: 0.63 - ETA: 0s - loss: 0.6537 - accuracy: 0.63 - ETA: 0s - loss: 0.6526 - accuracy: 0.63 - ETA: 0s - loss: 0.6512 - accuracy: 0.63 - ETA: 0s - loss: 0.6495 - accuracy: 0.63 - ETA: 0s - loss: 0.6483 - accuracy: 0.64 - ETA: 0s - loss: 0.6468 - accuracy: 0.64 - ETA: 0s - loss: 0.6458 - accuracy: 0.64 - 2s 119us/step - loss: 0.6454 - accuracy: 0.6445 - val_loss: 0.5944 - val_accuracy: 0.7138\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:34 - loss: 0.6994 - accuracy: 0.40 - ETA: 3s - loss: 0.6935 - accuracy: 0.5019 - ETA: 2s - loss: 0.6905 - accuracy: 0.51 - ETA: 1s - loss: 0.6902 - accuracy: 0.51 - ETA: 1s - loss: 0.6896 - accuracy: 0.52 - ETA: 1s - loss: 0.6875 - accuracy: 0.53 - ETA: 1s - loss: 0.6865 - accuracy: 0.54 - ETA: 1s - loss: 0.6847 - accuracy: 0.55 - ETA: 1s - loss: 0.6828 - accuracy: 0.56 - ETA: 1s - loss: 0.6799 - accuracy: 0.57 - ETA: 0s - loss: 0.6779 - accuracy: 0.57 - ETA: 0s - loss: 0.6757 - accuracy: 0.58 - ETA: 0s - loss: 0.6749 - accuracy: 0.58 - ETA: 0s - loss: 0.6737 - accuracy: 0.58 - ETA: 0s - loss: 0.6706 - accuracy: 0.59 - ETA: 0s - loss: 0.6684 - accuracy: 0.59 - ETA: 0s - loss: 0.6664 - accuracy: 0.60 - ETA: 0s - loss: 0.6647 - accuracy: 0.60 - ETA: 0s - loss: 0.6624 - accuracy: 0.60 - ETA: 0s - loss: 0.6608 - accuracy: 0.61 - ETA: 0s - loss: 0.6580 - accuracy: 0.61 - ETA: 0s - loss: 0.6560 - accuracy: 0.62 - ETA: 0s - loss: 0.6545 - accuracy: 0.62 - ETA: 0s - loss: 0.6528 - accuracy: 0.62 - ETA: 0s - loss: 0.6517 - accuracy: 0.63 - ETA: 0s - loss: 0.6493 - accuracy: 0.63 - 2s 120us/step - loss: 0.6485 - accuracy: 0.6347 - val_loss: 0.5967 - val_accuracy: 0.7152\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:34 - loss: 0.6977 - accuracy: 0.40 - ETA: 4s - loss: 0.6944 - accuracy: 0.5265 - ETA: 2s - loss: 0.6940 - accuracy: 0.52 - ETA: 2s - loss: 0.6933 - accuracy: 0.53 - ETA: 1s - loss: 0.6922 - accuracy: 0.54 - ETA: 1s - loss: 0.6914 - accuracy: 0.55 - ETA: 1s - loss: 0.6899 - accuracy: 0.55 - ETA: 1s - loss: 0.6879 - accuracy: 0.56 - ETA: 1s - loss: 0.6861 - accuracy: 0.57 - ETA: 1s - loss: 0.6833 - accuracy: 0.58 - ETA: 1s - loss: 0.6807 - accuracy: 0.58 - ETA: 0s - loss: 0.6773 - accuracy: 0.59 - ETA: 0s - loss: 0.6743 - accuracy: 0.59 - ETA: 0s - loss: 0.6721 - accuracy: 0.60 - ETA: 0s - loss: 0.6692 - accuracy: 0.60 - ETA: 0s - loss: 0.6660 - accuracy: 0.61 - ETA: 0s - loss: 0.6639 - accuracy: 0.61 - ETA: 0s - loss: 0.6621 - accuracy: 0.61 - ETA: 0s - loss: 0.6591 - accuracy: 0.62 - ETA: 0s - loss: 0.6575 - accuracy: 0.62 - ETA: 0s - loss: 0.6544 - accuracy: 0.62 - ETA: 0s - loss: 0.6544 - accuracy: 0.62 - ETA: 0s - loss: 0.6527 - accuracy: 0.63 - ETA: 0s - loss: 0.6509 - accuracy: 0.63 - ETA: 0s - loss: 0.6489 - accuracy: 0.63 - ETA: 0s - loss: 0.6470 - accuracy: 0.63 - ETA: 0s - loss: 0.6459 - accuracy: 0.64 - 2s 122us/step - loss: 0.6457 - accuracy: 0.6399 - val_loss: 0.5959 - val_accuracy: 0.7237\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:05 - loss: 0.6929 - accuracy: 0.70 - ETA: 3s - loss: 0.6923 - accuracy: 0.5415 - ETA: 2s - loss: 0.6922 - accuracy: 0.53 - ETA: 1s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6914 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.52 - ETA: 1s - loss: 0.6906 - accuracy: 0.52 - ETA: 1s - loss: 0.6895 - accuracy: 0.52 - ETA: 0s - loss: 0.6894 - accuracy: 0.52 - ETA: 0s - loss: 0.6890 - accuracy: 0.52 - ETA: 0s - loss: 0.6883 - accuracy: 0.52 - ETA: 0s - loss: 0.6882 - accuracy: 0.51 - ETA: 0s - loss: 0.6878 - accuracy: 0.51 - ETA: 0s - loss: 0.6876 - accuracy: 0.51 - ETA: 0s - loss: 0.6875 - accuracy: 0.51 - ETA: 0s - loss: 0.6867 - accuracy: 0.51 - ETA: 0s - loss: 0.6864 - accuracy: 0.51 - ETA: 0s - loss: 0.6858 - accuracy: 0.51 - ETA: 0s - loss: 0.6856 - accuracy: 0.51 - ETA: 0s - loss: 0.6852 - accuracy: 0.51 - ETA: 0s - loss: 0.6849 - accuracy: 0.51 - ETA: 0s - loss: 0.6846 - accuracy: 0.51 - ETA: 0s - loss: 0.6844 - accuracy: 0.51 - 1s 106us/step - loss: 0.6844 - accuracy: 0.5187 - val_loss: 0.6732 - val_accuracy: 0.6122\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 48us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:57 - loss: 0.6957 - accuracy: 0.40 - ETA: 3s - loss: 0.6936 - accuracy: 0.4945 - ETA: 2s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6907 - accuracy: 0.54 - ETA: 1s - loss: 0.6900 - accuracy: 0.55 - ETA: 1s - loss: 0.6887 - accuracy: 0.56 - ETA: 1s - loss: 0.6879 - accuracy: 0.56 - ETA: 1s - loss: 0.6869 - accuracy: 0.57 - ETA: 1s - loss: 0.6857 - accuracy: 0.58 - ETA: 0s - loss: 0.6847 - accuracy: 0.58 - ETA: 0s - loss: 0.6841 - accuracy: 0.58 - ETA: 0s - loss: 0.6829 - accuracy: 0.59 - ETA: 0s - loss: 0.6820 - accuracy: 0.59 - ETA: 0s - loss: 0.6812 - accuracy: 0.59 - ETA: 0s - loss: 0.6807 - accuracy: 0.59 - ETA: 0s - loss: 0.6795 - accuracy: 0.60 - ETA: 0s - loss: 0.6782 - accuracy: 0.60 - ETA: 0s - loss: 0.6773 - accuracy: 0.60 - ETA: 0s - loss: 0.6763 - accuracy: 0.60 - ETA: 0s - loss: 0.6753 - accuracy: 0.60 - ETA: 0s - loss: 0.6747 - accuracy: 0.61 - ETA: 0s - loss: 0.6738 - accuracy: 0.61 - ETA: 0s - loss: 0.6732 - accuracy: 0.61 - 1s 107us/step - loss: 0.6720 - accuracy: 0.6140 - val_loss: 0.6500 - val_accuracy: 0.6790\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:04 - loss: 0.6926 - accuracy: 0.60 - ETA: 3s - loss: 0.6935 - accuracy: 0.5037 - ETA: 2s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6892 - accuracy: 0.54 - ETA: 1s - loss: 0.6894 - accuracy: 0.53 - ETA: 1s - loss: 0.6892 - accuracy: 0.53 - ETA: 1s - loss: 0.6883 - accuracy: 0.54 - ETA: 1s - loss: 0.6875 - accuracy: 0.54 - ETA: 0s - loss: 0.6870 - accuracy: 0.54 - ETA: 0s - loss: 0.6863 - accuracy: 0.55 - ETA: 0s - loss: 0.6859 - accuracy: 0.55 - ETA: 0s - loss: 0.6855 - accuracy: 0.55 - ETA: 0s - loss: 0.6848 - accuracy: 0.55 - ETA: 0s - loss: 0.6839 - accuracy: 0.55 - ETA: 0s - loss: 0.6832 - accuracy: 0.55 - ETA: 0s - loss: 0.6825 - accuracy: 0.56 - ETA: 0s - loss: 0.6818 - accuracy: 0.56 - ETA: 0s - loss: 0.6806 - accuracy: 0.56 - ETA: 0s - loss: 0.6799 - accuracy: 0.56 - ETA: 0s - loss: 0.6796 - accuracy: 0.56 - ETA: 0s - loss: 0.6784 - accuracy: 0.57 - ETA: 0s - loss: 0.6779 - accuracy: 0.57 - ETA: 0s - loss: 0.6770 - accuracy: 0.57 - ETA: 0s - loss: 0.6763 - accuracy: 0.57 - 1s 108us/step - loss: 0.6762 - accuracy: 0.5762 - val_loss: 0.6539 - val_accuracy: 0.6967\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 1:59 - loss: 0.6957 - accuracy: 0.40 - ETA: 3s - loss: 0.6929 - accuracy: 0.5204 - ETA: 2s - loss: 0.6923 - accuracy: 0.51 - ETA: 1s - loss: 0.6920 - accuracy: 0.50 - ETA: 1s - loss: 0.6915 - accuracy: 0.51 - ETA: 1s - loss: 0.6912 - accuracy: 0.50 - ETA: 1s - loss: 0.6910 - accuracy: 0.51 - ETA: 1s - loss: 0.6904 - accuracy: 0.51 - ETA: 1s - loss: 0.6893 - accuracy: 0.51 - ETA: 0s - loss: 0.6892 - accuracy: 0.51 - ETA: 0s - loss: 0.6879 - accuracy: 0.51 - ETA: 0s - loss: 0.6878 - accuracy: 0.51 - ETA: 0s - loss: 0.6868 - accuracy: 0.52 - ETA: 0s - loss: 0.6861 - accuracy: 0.52 - ETA: 0s - loss: 0.6859 - accuracy: 0.52 - ETA: 0s - loss: 0.6857 - accuracy: 0.52 - ETA: 0s - loss: 0.6853 - accuracy: 0.52 - ETA: 0s - loss: 0.6849 - accuracy: 0.53 - ETA: 0s - loss: 0.6840 - accuracy: 0.53 - ETA: 0s - loss: 0.6834 - accuracy: 0.53 - ETA: 0s - loss: 0.6826 - accuracy: 0.54 - ETA: 0s - loss: 0.6822 - accuracy: 0.54 - ETA: 0s - loss: 0.6816 - accuracy: 0.54 - ETA: 0s - loss: 0.6814 - accuracy: 0.54 - 1s 108us/step - loss: 0.6813 - accuracy: 0.5475 - val_loss: 0.6631 - val_accuracy: 0.6967\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 52us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:59 - loss: 0.6941 - accuracy: 0.50 - ETA: 3s - loss: 0.6932 - accuracy: 0.5109 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.50 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.50 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - 1s 108us/step - loss: 0.6929 - accuracy: 0.5137 - val_loss: 0.6922 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:00 - loss: 0.6915 - accuracy: 0.70 - ETA: 3s - loss: 0.6903 - accuracy: 0.5298 - ETA: 1s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6917 - accuracy: 0.52 - ETA: 1s - loss: 0.6907 - accuracy: 0.52 - ETA: 1s - loss: 0.6893 - accuracy: 0.53 - ETA: 1s - loss: 0.6887 - accuracy: 0.53 - ETA: 1s - loss: 0.6877 - accuracy: 0.54 - ETA: 0s - loss: 0.6869 - accuracy: 0.54 - ETA: 0s - loss: 0.6863 - accuracy: 0.55 - ETA: 0s - loss: 0.6854 - accuracy: 0.56 - ETA: 0s - loss: 0.6844 - accuracy: 0.56 - ETA: 0s - loss: 0.6832 - accuracy: 0.57 - ETA: 0s - loss: 0.6826 - accuracy: 0.57 - ETA: 0s - loss: 0.6821 - accuracy: 0.57 - ETA: 0s - loss: 0.6813 - accuracy: 0.57 - ETA: 0s - loss: 0.6807 - accuracy: 0.57 - ETA: 0s - loss: 0.6799 - accuracy: 0.57 - ETA: 0s - loss: 0.6793 - accuracy: 0.57 - ETA: 0s - loss: 0.6787 - accuracy: 0.58 - ETA: 0s - loss: 0.6780 - accuracy: 0.58 - ETA: 0s - loss: 0.6764 - accuracy: 0.58 - ETA: 0s - loss: 0.6760 - accuracy: 0.58 - ETA: 0s - loss: 0.6754 - accuracy: 0.58 - 1s 109us/step - loss: 0.6749 - accuracy: 0.5874 - val_loss: 0.6559 - val_accuracy: 0.7010\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:10 - loss: 0.6940 - accuracy: 0.40 - ETA: 3s - loss: 0.6873 - accuracy: 0.5491 - ETA: 2s - loss: 0.6902 - accuracy: 0.53 - ETA: 1s - loss: 0.6900 - accuracy: 0.53 - ETA: 1s - loss: 0.6902 - accuracy: 0.53 - ETA: 1s - loss: 0.6902 - accuracy: 0.53 - ETA: 1s - loss: 0.6899 - accuracy: 0.53 - ETA: 1s - loss: 0.6891 - accuracy: 0.54 - ETA: 0s - loss: 0.6883 - accuracy: 0.54 - ETA: 0s - loss: 0.6876 - accuracy: 0.55 - ETA: 0s - loss: 0.6870 - accuracy: 0.55 - ETA: 0s - loss: 0.6864 - accuracy: 0.55 - ETA: 0s - loss: 0.6859 - accuracy: 0.55 - ETA: 0s - loss: 0.6853 - accuracy: 0.56 - ETA: 0s - loss: 0.6845 - accuracy: 0.56 - ETA: 0s - loss: 0.6836 - accuracy: 0.57 - ETA: 0s - loss: 0.6828 - accuracy: 0.57 - ETA: 0s - loss: 0.6821 - accuracy: 0.57 - ETA: 0s - loss: 0.6815 - accuracy: 0.58 - ETA: 0s - loss: 0.6807 - accuracy: 0.58 - ETA: 0s - loss: 0.6801 - accuracy: 0.58 - ETA: 0s - loss: 0.6794 - accuracy: 0.58 - ETA: 0s - loss: 0.6790 - accuracy: 0.59 - 1s 107us/step - loss: 0.6785 - accuracy: 0.5913 - val_loss: 0.6576 - val_accuracy: 0.7067\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:02 - loss: 0.7005 - accuracy: 0.40 - ETA: 3s - loss: 0.6877 - accuracy: 0.5518 - ETA: 2s - loss: 0.6915 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.52 - ETA: 1s - loss: 0.6898 - accuracy: 0.53 - ETA: 1s - loss: 0.6890 - accuracy: 0.54 - ETA: 1s - loss: 0.6885 - accuracy: 0.54 - ETA: 0s - loss: 0.6874 - accuracy: 0.55 - ETA: 0s - loss: 0.6863 - accuracy: 0.55 - ETA: 0s - loss: 0.6849 - accuracy: 0.56 - ETA: 0s - loss: 0.6836 - accuracy: 0.57 - ETA: 0s - loss: 0.6817 - accuracy: 0.57 - ETA: 0s - loss: 0.6810 - accuracy: 0.58 - ETA: 0s - loss: 0.6800 - accuracy: 0.58 - ETA: 0s - loss: 0.6790 - accuracy: 0.58 - ETA: 0s - loss: 0.6777 - accuracy: 0.58 - ETA: 0s - loss: 0.6769 - accuracy: 0.59 - ETA: 0s - loss: 0.6761 - accuracy: 0.59 - ETA: 0s - loss: 0.6751 - accuracy: 0.59 - ETA: 0s - loss: 0.6741 - accuracy: 0.59 - ETA: 0s - loss: 0.6735 - accuracy: 0.59 - ETA: 0s - loss: 0.6732 - accuracy: 0.59 - ETA: 0s - loss: 0.6727 - accuracy: 0.59 - 1s 107us/step - loss: 0.6724 - accuracy: 0.5990 - val_loss: 0.6468 - val_accuracy: 0.7067\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 46us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:04 - loss: 0.6917 - accuracy: 0.70 - ETA: 3s - loss: 0.6884 - accuracy: 0.5364 - ETA: 2s - loss: 0.6881 - accuracy: 0.52 - ETA: 1s - loss: 0.6866 - accuracy: 0.53 - ETA: 1s - loss: 0.6861 - accuracy: 0.53 - ETA: 1s - loss: 0.6852 - accuracy: 0.54 - ETA: 1s - loss: 0.6832 - accuracy: 0.55 - ETA: 1s - loss: 0.6825 - accuracy: 0.56 - ETA: 0s - loss: 0.6810 - accuracy: 0.56 - ETA: 0s - loss: 0.6798 - accuracy: 0.57 - ETA: 0s - loss: 0.6789 - accuracy: 0.57 - ETA: 0s - loss: 0.6779 - accuracy: 0.58 - ETA: 0s - loss: 0.6764 - accuracy: 0.58 - ETA: 0s - loss: 0.6753 - accuracy: 0.59 - ETA: 0s - loss: 0.6740 - accuracy: 0.59 - ETA: 0s - loss: 0.6720 - accuracy: 0.60 - ETA: 0s - loss: 0.6707 - accuracy: 0.60 - ETA: 0s - loss: 0.6689 - accuracy: 0.60 - ETA: 0s - loss: 0.6679 - accuracy: 0.61 - ETA: 0s - loss: 0.6671 - accuracy: 0.61 - ETA: 0s - loss: 0.6662 - accuracy: 0.61 - ETA: 0s - loss: 0.6648 - accuracy: 0.61 - ETA: 0s - loss: 0.6642 - accuracy: 0.61 - 1s 107us/step - loss: 0.6629 - accuracy: 0.6217 - val_loss: 0.6297 - val_accuracy: 0.7053\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:54 - loss: 0.6968 - accuracy: 0.20 - ETA: 5s - loss: 0.6937 - accuracy: 0.4976 - ETA: 2s - loss: 0.6915 - accuracy: 0.52 - ETA: 2s - loss: 0.6897 - accuracy: 0.52 - ETA: 1s - loss: 0.6897 - accuracy: 0.52 - ETA: 1s - loss: 0.6881 - accuracy: 0.54 - ETA: 1s - loss: 0.6866 - accuracy: 0.54 - ETA: 1s - loss: 0.6860 - accuracy: 0.54 - ETA: 1s - loss: 0.6845 - accuracy: 0.55 - ETA: 0s - loss: 0.6831 - accuracy: 0.55 - ETA: 0s - loss: 0.6814 - accuracy: 0.56 - ETA: 0s - loss: 0.6799 - accuracy: 0.56 - ETA: 0s - loss: 0.6788 - accuracy: 0.57 - ETA: 0s - loss: 0.6781 - accuracy: 0.57 - ETA: 0s - loss: 0.6774 - accuracy: 0.57 - ETA: 0s - loss: 0.6763 - accuracy: 0.57 - ETA: 0s - loss: 0.6752 - accuracy: 0.57 - ETA: 0s - loss: 0.6746 - accuracy: 0.58 - ETA: 0s - loss: 0.6735 - accuracy: 0.58 - ETA: 0s - loss: 0.6728 - accuracy: 0.58 - ETA: 0s - loss: 0.6721 - accuracy: 0.58 - ETA: 0s - loss: 0.6710 - accuracy: 0.58 - ETA: 0s - loss: 0.6707 - accuracy: 0.58 - 1s 110us/step - loss: 0.6697 - accuracy: 0.5888 - val_loss: 0.6439 - val_accuracy: 0.7053\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:57 - loss: 0.6969 - accuracy: 0.30 - ETA: 3s - loss: 0.6911 - accuracy: 0.5273 - ETA: 2s - loss: 0.6881 - accuracy: 0.54 - ETA: 1s - loss: 0.6874 - accuracy: 0.55 - ETA: 1s - loss: 0.6865 - accuracy: 0.56 - ETA: 1s - loss: 0.6847 - accuracy: 0.57 - ETA: 1s - loss: 0.6842 - accuracy: 0.56 - ETA: 1s - loss: 0.6835 - accuracy: 0.57 - ETA: 0s - loss: 0.6821 - accuracy: 0.57 - ETA: 0s - loss: 0.6804 - accuracy: 0.58 - ETA: 0s - loss: 0.6785 - accuracy: 0.59 - ETA: 0s - loss: 0.6775 - accuracy: 0.59 - ETA: 0s - loss: 0.6763 - accuracy: 0.60 - ETA: 0s - loss: 0.6755 - accuracy: 0.60 - ETA: 0s - loss: 0.6746 - accuracy: 0.60 - ETA: 0s - loss: 0.6735 - accuracy: 0.60 - ETA: 0s - loss: 0.6722 - accuracy: 0.61 - ETA: 0s - loss: 0.6709 - accuracy: 0.61 - ETA: 0s - loss: 0.6698 - accuracy: 0.61 - ETA: 0s - loss: 0.6690 - accuracy: 0.61 - ETA: 0s - loss: 0.6678 - accuracy: 0.62 - ETA: 0s - loss: 0.6673 - accuracy: 0.62 - ETA: 0s - loss: 0.6660 - accuracy: 0.62 - 1s 106us/step - loss: 0.6644 - accuracy: 0.6265 - val_loss: 0.6341 - val_accuracy: 0.7166\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:57 - loss: 0.6971 - accuracy: 0.50 - ETA: 3s - loss: 0.6923 - accuracy: 0.5411 - ETA: 2s - loss: 0.6915 - accuracy: 0.53 - ETA: 1s - loss: 0.6900 - accuracy: 0.55 - ETA: 1s - loss: 0.6884 - accuracy: 0.56 - ETA: 1s - loss: 0.6870 - accuracy: 0.56 - ETA: 1s - loss: 0.6843 - accuracy: 0.58 - ETA: 1s - loss: 0.6829 - accuracy: 0.58 - ETA: 0s - loss: 0.6820 - accuracy: 0.58 - ETA: 0s - loss: 0.6811 - accuracy: 0.58 - ETA: 0s - loss: 0.6807 - accuracy: 0.58 - ETA: 0s - loss: 0.6798 - accuracy: 0.58 - ETA: 0s - loss: 0.6790 - accuracy: 0.58 - ETA: 0s - loss: 0.6774 - accuracy: 0.59 - ETA: 0s - loss: 0.6772 - accuracy: 0.59 - ETA: 0s - loss: 0.6765 - accuracy: 0.59 - ETA: 0s - loss: 0.6758 - accuracy: 0.59 - ETA: 0s - loss: 0.6756 - accuracy: 0.59 - ETA: 0s - loss: 0.6748 - accuracy: 0.59 - ETA: 0s - loss: 0.6742 - accuracy: 0.59 - ETA: 0s - loss: 0.6734 - accuracy: 0.60 - ETA: 0s - loss: 0.6729 - accuracy: 0.59 - ETA: 0s - loss: 0.6725 - accuracy: 0.59 - 1s 106us/step - loss: 0.6716 - accuracy: 0.6002 - val_loss: 0.6501 - val_accuracy: 0.7067\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 39us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:59 - loss: 0.6935 - accuracy: 0.60 - ETA: 3s - loss: 0.6916 - accuracy: 0.5204 - ETA: 2s - loss: 0.6890 - accuracy: 0.53 - ETA: 1s - loss: 0.6896 - accuracy: 0.52 - ETA: 1s - loss: 0.6887 - accuracy: 0.53 - ETA: 1s - loss: 0.6878 - accuracy: 0.54 - ETA: 1s - loss: 0.6871 - accuracy: 0.55 - ETA: 1s - loss: 0.6864 - accuracy: 0.55 - ETA: 0s - loss: 0.6853 - accuracy: 0.56 - ETA: 0s - loss: 0.6839 - accuracy: 0.56 - ETA: 0s - loss: 0.6829 - accuracy: 0.57 - ETA: 0s - loss: 0.6825 - accuracy: 0.57 - ETA: 0s - loss: 0.6820 - accuracy: 0.57 - ETA: 0s - loss: 0.6803 - accuracy: 0.58 - ETA: 0s - loss: 0.6800 - accuracy: 0.57 - ETA: 0s - loss: 0.6789 - accuracy: 0.58 - ETA: 0s - loss: 0.6780 - accuracy: 0.58 - ETA: 0s - loss: 0.6769 - accuracy: 0.58 - ETA: 0s - loss: 0.6762 - accuracy: 0.58 - ETA: 0s - loss: 0.6755 - accuracy: 0.58 - ETA: 0s - loss: 0.6745 - accuracy: 0.59 - ETA: 0s - loss: 0.6737 - accuracy: 0.59 - ETA: 0s - loss: 0.6730 - accuracy: 0.59 - ETA: 0s - loss: 0.6721 - accuracy: 0.59 - 1s 108us/step - loss: 0.6720 - accuracy: 0.5952 - val_loss: 0.6447 - val_accuracy: 0.6989\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:57 - loss: 0.6959 - accuracy: 0.40 - ETA: 3s - loss: 0.6927 - accuracy: 0.5143 - ETA: 1s - loss: 0.6919 - accuracy: 0.50 - ETA: 1s - loss: 0.6912 - accuracy: 0.51 - ETA: 1s - loss: 0.6896 - accuracy: 0.53 - ETA: 1s - loss: 0.6882 - accuracy: 0.55 - ETA: 1s - loss: 0.6868 - accuracy: 0.56 - ETA: 1s - loss: 0.6853 - accuracy: 0.57 - ETA: 0s - loss: 0.6840 - accuracy: 0.57 - ETA: 0s - loss: 0.6825 - accuracy: 0.58 - ETA: 0s - loss: 0.6815 - accuracy: 0.58 - ETA: 0s - loss: 0.6805 - accuracy: 0.58 - ETA: 0s - loss: 0.6792 - accuracy: 0.59 - ETA: 0s - loss: 0.6782 - accuracy: 0.59 - ETA: 0s - loss: 0.6770 - accuracy: 0.59 - ETA: 0s - loss: 0.6760 - accuracy: 0.59 - ETA: 0s - loss: 0.6749 - accuracy: 0.60 - ETA: 0s - loss: 0.6740 - accuracy: 0.60 - ETA: 0s - loss: 0.6731 - accuracy: 0.60 - ETA: 0s - loss: 0.6720 - accuracy: 0.60 - ETA: 0s - loss: 0.6710 - accuracy: 0.61 - ETA: 0s - loss: 0.6704 - accuracy: 0.61 - ETA: 0s - loss: 0.6697 - accuracy: 0.61 - 1s 106us/step - loss: 0.6694 - accuracy: 0.6138 - val_loss: 0.6426 - val_accuracy: 0.7145\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 39us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:04 - loss: 0.6944 - accuracy: 0.50 - ETA: 4s - loss: 0.6939 - accuracy: 0.4919 - ETA: 2s - loss: 0.6935 - accuracy: 0.51 - ETA: 2s - loss: 0.6916 - accuracy: 0.53 - ETA: 1s - loss: 0.6905 - accuracy: 0.55 - ETA: 1s - loss: 0.6887 - accuracy: 0.56 - ETA: 1s - loss: 0.6877 - accuracy: 0.57 - ETA: 1s - loss: 0.6853 - accuracy: 0.58 - ETA: 0s - loss: 0.6835 - accuracy: 0.58 - ETA: 0s - loss: 0.6815 - accuracy: 0.59 - ETA: 0s - loss: 0.6788 - accuracy: 0.60 - ETA: 0s - loss: 0.6767 - accuracy: 0.60 - ETA: 0s - loss: 0.6749 - accuracy: 0.61 - ETA: 0s - loss: 0.6725 - accuracy: 0.61 - ETA: 0s - loss: 0.6703 - accuracy: 0.61 - ETA: 0s - loss: 0.6687 - accuracy: 0.62 - ETA: 0s - loss: 0.6666 - accuracy: 0.62 - ETA: 0s - loss: 0.6659 - accuracy: 0.62 - ETA: 0s - loss: 0.6647 - accuracy: 0.62 - ETA: 0s - loss: 0.6633 - accuracy: 0.63 - ETA: 0s - loss: 0.6624 - accuracy: 0.63 - ETA: 0s - loss: 0.6617 - accuracy: 0.63 - ETA: 0s - loss: 0.6607 - accuracy: 0.63 - 1s 107us/step - loss: 0.6599 - accuracy: 0.6343 - val_loss: 0.6248 - val_accuracy: 0.7102\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 2:01 - loss: 0.6899 - accuracy: 0.60 - ETA: 3s - loss: 0.6954 - accuracy: 0.5036 - ETA: 2s - loss: 0.6941 - accuracy: 0.52 - ETA: 1s - loss: 0.6919 - accuracy: 0.55 - ETA: 1s - loss: 0.6894 - accuracy: 0.57 - ETA: 1s - loss: 0.6883 - accuracy: 0.57 - ETA: 1s - loss: 0.6870 - accuracy: 0.57 - ETA: 1s - loss: 0.6853 - accuracy: 0.58 - ETA: 0s - loss: 0.6833 - accuracy: 0.59 - ETA: 0s - loss: 0.6818 - accuracy: 0.59 - ETA: 0s - loss: 0.6805 - accuracy: 0.60 - ETA: 0s - loss: 0.6790 - accuracy: 0.60 - ETA: 0s - loss: 0.6772 - accuracy: 0.61 - ETA: 0s - loss: 0.6746 - accuracy: 0.61 - ETA: 0s - loss: 0.6730 - accuracy: 0.62 - ETA: 0s - loss: 0.6715 - accuracy: 0.62 - ETA: 0s - loss: 0.6707 - accuracy: 0.63 - ETA: 0s - loss: 0.6697 - accuracy: 0.63 - ETA: 0s - loss: 0.6681 - accuracy: 0.63 - ETA: 0s - loss: 0.6665 - accuracy: 0.63 - ETA: 0s - loss: 0.6646 - accuracy: 0.63 - ETA: 0s - loss: 0.6635 - accuracy: 0.63 - ETA: 0s - loss: 0.6625 - accuracy: 0.64 - ETA: 0s - loss: 0.6616 - accuracy: 0.64 - 1s 108us/step - loss: 0.6617 - accuracy: 0.6417 - val_loss: 0.6290 - val_accuracy: 0.7131\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:04 - loss: 0.6938 - accuracy: 0.50 - ETA: 3s - loss: 0.6911 - accuracy: 0.5309 - ETA: 2s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.52 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 1s - loss: 0.6896 - accuracy: 0.54 - ETA: 1s - loss: 0.6886 - accuracy: 0.55 - ETA: 1s - loss: 0.6875 - accuracy: 0.56 - ETA: 0s - loss: 0.6861 - accuracy: 0.56 - ETA: 0s - loss: 0.6849 - accuracy: 0.57 - ETA: 0s - loss: 0.6830 - accuracy: 0.58 - ETA: 0s - loss: 0.6817 - accuracy: 0.58 - ETA: 0s - loss: 0.6804 - accuracy: 0.59 - ETA: 0s - loss: 0.6790 - accuracy: 0.59 - ETA: 0s - loss: 0.6782 - accuracy: 0.59 - ETA: 0s - loss: 0.6772 - accuracy: 0.60 - ETA: 0s - loss: 0.6757 - accuracy: 0.60 - ETA: 0s - loss: 0.6739 - accuracy: 0.61 - ETA: 0s - loss: 0.6729 - accuracy: 0.61 - ETA: 0s - loss: 0.6719 - accuracy: 0.61 - ETA: 0s - loss: 0.6708 - accuracy: 0.61 - ETA: 0s - loss: 0.6696 - accuracy: 0.61 - ETA: 0s - loss: 0.6683 - accuracy: 0.62 - ETA: 0s - loss: 0.6674 - accuracy: 0.62 - 1s 108us/step - loss: 0.6671 - accuracy: 0.6244 - val_loss: 0.6336 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:01 - loss: 0.6918 - accuracy: 0.70 - ETA: 3s - loss: 0.6903 - accuracy: 0.5345 - ETA: 2s - loss: 0.6888 - accuracy: 0.53 - ETA: 1s - loss: 0.6873 - accuracy: 0.54 - ETA: 1s - loss: 0.6860 - accuracy: 0.55 - ETA: 1s - loss: 0.6841 - accuracy: 0.56 - ETA: 1s - loss: 0.6809 - accuracy: 0.57 - ETA: 1s - loss: 0.6789 - accuracy: 0.58 - ETA: 0s - loss: 0.6773 - accuracy: 0.58 - ETA: 0s - loss: 0.6757 - accuracy: 0.59 - ETA: 0s - loss: 0.6744 - accuracy: 0.60 - ETA: 0s - loss: 0.6722 - accuracy: 0.60 - ETA: 0s - loss: 0.6708 - accuracy: 0.61 - ETA: 0s - loss: 0.6694 - accuracy: 0.61 - ETA: 0s - loss: 0.6674 - accuracy: 0.62 - ETA: 0s - loss: 0.6664 - accuracy: 0.62 - ETA: 0s - loss: 0.6649 - accuracy: 0.62 - ETA: 0s - loss: 0.6641 - accuracy: 0.62 - ETA: 0s - loss: 0.6632 - accuracy: 0.62 - ETA: 0s - loss: 0.6625 - accuracy: 0.63 - ETA: 0s - loss: 0.6613 - accuracy: 0.63 - ETA: 0s - loss: 0.6598 - accuracy: 0.63 - ETA: 0s - loss: 0.6588 - accuracy: 0.63 - ETA: 0s - loss: 0.6573 - accuracy: 0.63 - 1s 111us/step - loss: 0.6563 - accuracy: 0.6414 - val_loss: 0.6259 - val_accuracy: 0.7131\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:33 - loss: 0.6944 - accuracy: 0.10 - ETA: 3s - loss: 0.6938 - accuracy: 0.4925 - ETA: 2s - loss: 0.6937 - accuracy: 0.49 - ETA: 2s - loss: 0.6937 - accuracy: 0.48 - ETA: 1s - loss: 0.6935 - accuracy: 0.49 - ETA: 1s - loss: 0.6935 - accuracy: 0.49 - ETA: 1s - loss: 0.6934 - accuracy: 0.50 - ETA: 1s - loss: 0.6931 - accuracy: 0.50 - ETA: 1s - loss: 0.6930 - accuracy: 0.50 - ETA: 1s - loss: 0.6926 - accuracy: 0.50 - ETA: 1s - loss: 0.6923 - accuracy: 0.50 - ETA: 0s - loss: 0.6914 - accuracy: 0.50 - ETA: 0s - loss: 0.6909 - accuracy: 0.50 - ETA: 0s - loss: 0.6896 - accuracy: 0.51 - ETA: 0s - loss: 0.6889 - accuracy: 0.51 - ETA: 0s - loss: 0.6879 - accuracy: 0.52 - ETA: 0s - loss: 0.6876 - accuracy: 0.52 - ETA: 0s - loss: 0.6866 - accuracy: 0.52 - ETA: 0s - loss: 0.6859 - accuracy: 0.52 - ETA: 0s - loss: 0.6847 - accuracy: 0.53 - ETA: 0s - loss: 0.6842 - accuracy: 0.53 - ETA: 0s - loss: 0.6830 - accuracy: 0.53 - ETA: 0s - loss: 0.6824 - accuracy: 0.53 - ETA: 0s - loss: 0.6818 - accuracy: 0.53 - ETA: 0s - loss: 0.6816 - accuracy: 0.53 - ETA: 0s - loss: 0.6811 - accuracy: 0.54 - ETA: 0s - loss: 0.6805 - accuracy: 0.54 - 2s 122us/step - loss: 0.6807 - accuracy: 0.5420 - val_loss: 0.6532 - val_accuracy: 0.7145\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 45us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:25 - loss: 0.6933 - accuracy: 0.70 - ETA: 3s - loss: 0.6934 - accuracy: 0.5226 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6924 - accuracy: 0.53 - ETA: 1s - loss: 0.6909 - accuracy: 0.54 - ETA: 1s - loss: 0.6898 - accuracy: 0.54 - ETA: 1s - loss: 0.6878 - accuracy: 0.54 - ETA: 1s - loss: 0.6848 - accuracy: 0.55 - ETA: 1s - loss: 0.6831 - accuracy: 0.55 - ETA: 1s - loss: 0.6790 - accuracy: 0.56 - ETA: 0s - loss: 0.6769 - accuracy: 0.56 - ETA: 0s - loss: 0.6748 - accuracy: 0.57 - ETA: 0s - loss: 0.6721 - accuracy: 0.57 - ETA: 0s - loss: 0.6711 - accuracy: 0.58 - ETA: 0s - loss: 0.6694 - accuracy: 0.58 - ETA: 0s - loss: 0.6673 - accuracy: 0.58 - ETA: 0s - loss: 0.6661 - accuracy: 0.58 - ETA: 0s - loss: 0.6640 - accuracy: 0.59 - ETA: 0s - loss: 0.6620 - accuracy: 0.59 - ETA: 0s - loss: 0.6594 - accuracy: 0.59 - ETA: 0s - loss: 0.6576 - accuracy: 0.59 - ETA: 0s - loss: 0.6562 - accuracy: 0.60 - ETA: 0s - loss: 0.6563 - accuracy: 0.60 - ETA: 0s - loss: 0.6558 - accuracy: 0.60 - ETA: 0s - loss: 0.6557 - accuracy: 0.60 - ETA: 0s - loss: 0.6544 - accuracy: 0.60 - ETA: 0s - loss: 0.6538 - accuracy: 0.60 - 2s 121us/step - loss: 0.6540 - accuracy: 0.6068 - val_loss: 0.6128 - val_accuracy: 0.7209\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 45us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:38 - loss: 0.6889 - accuracy: 0.70 - ETA: 4s - loss: 0.6926 - accuracy: 0.5204 - ETA: 2s - loss: 0.6925 - accuracy: 0.52 - ETA: 2s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6899 - accuracy: 0.53 - ETA: 1s - loss: 0.6877 - accuracy: 0.54 - ETA: 1s - loss: 0.6866 - accuracy: 0.54 - ETA: 1s - loss: 0.6844 - accuracy: 0.55 - ETA: 1s - loss: 0.6817 - accuracy: 0.55 - ETA: 1s - loss: 0.6793 - accuracy: 0.56 - ETA: 0s - loss: 0.6778 - accuracy: 0.56 - ETA: 0s - loss: 0.6750 - accuracy: 0.57 - ETA: 0s - loss: 0.6727 - accuracy: 0.57 - ETA: 0s - loss: 0.6704 - accuracy: 0.58 - ETA: 0s - loss: 0.6679 - accuracy: 0.58 - ETA: 0s - loss: 0.6659 - accuracy: 0.58 - ETA: 0s - loss: 0.6640 - accuracy: 0.59 - ETA: 0s - loss: 0.6639 - accuracy: 0.59 - ETA: 0s - loss: 0.6621 - accuracy: 0.59 - ETA: 0s - loss: 0.6599 - accuracy: 0.59 - ETA: 0s - loss: 0.6587 - accuracy: 0.59 - ETA: 0s - loss: 0.6584 - accuracy: 0.60 - ETA: 0s - loss: 0.6574 - accuracy: 0.60 - ETA: 0s - loss: 0.6558 - accuracy: 0.60 - ETA: 0s - loss: 0.6550 - accuracy: 0.60 - ETA: 0s - loss: 0.6530 - accuracy: 0.60 - 2s 122us/step - loss: 0.6529 - accuracy: 0.6081 - val_loss: 0.5983 - val_accuracy: 0.7173\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:33 - loss: 0.6936 - accuracy: 0.40 - ETA: 3s - loss: 0.6934 - accuracy: 0.5208 - ETA: 2s - loss: 0.6917 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6903 - accuracy: 0.52 - ETA: 1s - loss: 0.6882 - accuracy: 0.53 - ETA: 1s - loss: 0.6850 - accuracy: 0.53 - ETA: 1s - loss: 0.6812 - accuracy: 0.53 - ETA: 1s - loss: 0.6785 - accuracy: 0.55 - ETA: 1s - loss: 0.6753 - accuracy: 0.55 - ETA: 0s - loss: 0.6728 - accuracy: 0.56 - ETA: 0s - loss: 0.6694 - accuracy: 0.57 - ETA: 0s - loss: 0.6679 - accuracy: 0.57 - ETA: 0s - loss: 0.6653 - accuracy: 0.58 - ETA: 0s - loss: 0.6633 - accuracy: 0.58 - ETA: 0s - loss: 0.6633 - accuracy: 0.58 - ETA: 0s - loss: 0.6623 - accuracy: 0.58 - ETA: 0s - loss: 0.6607 - accuracy: 0.59 - ETA: 0s - loss: 0.6601 - accuracy: 0.59 - ETA: 0s - loss: 0.6593 - accuracy: 0.59 - ETA: 0s - loss: 0.6571 - accuracy: 0.59 - ETA: 0s - loss: 0.6552 - accuracy: 0.60 - ETA: 0s - loss: 0.6539 - accuracy: 0.60 - ETA: 0s - loss: 0.6534 - accuracy: 0.60 - ETA: 0s - loss: 0.6517 - accuracy: 0.60 - 2s 119us/step - loss: 0.6503 - accuracy: 0.6106 - val_loss: 0.5950 - val_accuracy: 0.7188\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:34 - loss: 0.6921 - accuracy: 0.60 - ETA: 4s - loss: 0.6943 - accuracy: 0.4820 - ETA: 2s - loss: 0.6939 - accuracy: 0.49 - ETA: 2s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6914 - accuracy: 0.53 - ETA: 1s - loss: 0.6905 - accuracy: 0.53 - ETA: 1s - loss: 0.6889 - accuracy: 0.53 - ETA: 0s - loss: 0.6874 - accuracy: 0.53 - ETA: 0s - loss: 0.6860 - accuracy: 0.53 - ETA: 0s - loss: 0.6839 - accuracy: 0.54 - ETA: 0s - loss: 0.6823 - accuracy: 0.54 - ETA: 0s - loss: 0.6791 - accuracy: 0.55 - ETA: 0s - loss: 0.6771 - accuracy: 0.55 - ETA: 0s - loss: 0.6756 - accuracy: 0.55 - ETA: 0s - loss: 0.6745 - accuracy: 0.55 - ETA: 0s - loss: 0.6735 - accuracy: 0.56 - ETA: 0s - loss: 0.6711 - accuracy: 0.56 - ETA: 0s - loss: 0.6694 - accuracy: 0.56 - ETA: 0s - loss: 0.6681 - accuracy: 0.56 - ETA: 0s - loss: 0.6676 - accuracy: 0.57 - ETA: 0s - loss: 0.6668 - accuracy: 0.57 - ETA: 0s - loss: 0.6654 - accuracy: 0.57 - ETA: 0s - loss: 0.6640 - accuracy: 0.57 - 2s 125us/step - loss: 0.6627 - accuracy: 0.5807 - val_loss: 0.6084 - val_accuracy: 0.7244\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 48us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:26 - loss: 0.6926 - accuracy: 0.70 - ETA: 3s - loss: 0.6930 - accuracy: 0.5231 - ETA: 2s - loss: 0.6924 - accuracy: 0.51 - ETA: 1s - loss: 0.6912 - accuracy: 0.52 - ETA: 1s - loss: 0.6883 - accuracy: 0.54 - ETA: 1s - loss: 0.6875 - accuracy: 0.53 - ETA: 1s - loss: 0.6863 - accuracy: 0.54 - ETA: 1s - loss: 0.6843 - accuracy: 0.54 - ETA: 1s - loss: 0.6819 - accuracy: 0.55 - ETA: 1s - loss: 0.6806 - accuracy: 0.55 - ETA: 0s - loss: 0.6795 - accuracy: 0.55 - ETA: 0s - loss: 0.6759 - accuracy: 0.56 - ETA: 0s - loss: 0.6738 - accuracy: 0.56 - ETA: 0s - loss: 0.6712 - accuracy: 0.56 - ETA: 0s - loss: 0.6693 - accuracy: 0.57 - ETA: 0s - loss: 0.6663 - accuracy: 0.57 - ETA: 0s - loss: 0.6656 - accuracy: 0.57 - ETA: 0s - loss: 0.6641 - accuracy: 0.58 - ETA: 0s - loss: 0.6629 - accuracy: 0.58 - ETA: 0s - loss: 0.6616 - accuracy: 0.58 - ETA: 0s - loss: 0.6599 - accuracy: 0.58 - ETA: 0s - loss: 0.6584 - accuracy: 0.58 - ETA: 0s - loss: 0.6573 - accuracy: 0.58 - ETA: 0s - loss: 0.6563 - accuracy: 0.59 - ETA: 0s - loss: 0.6553 - accuracy: 0.59 - ETA: 0s - loss: 0.6548 - accuracy: 0.59 - 1s 118us/step - loss: 0.6548 - accuracy: 0.5965 - val_loss: 0.6115 - val_accuracy: 0.7124\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:29 - loss: 0.6957 - accuracy: 0.40 - ETA: 3s - loss: 0.6936 - accuracy: 0.5204 - ETA: 2s - loss: 0.6933 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.54 - ETA: 1s - loss: 0.6898 - accuracy: 0.55 - ETA: 1s - loss: 0.6872 - accuracy: 0.56 - ETA: 1s - loss: 0.6858 - accuracy: 0.56 - ETA: 1s - loss: 0.6842 - accuracy: 0.57 - ETA: 1s - loss: 0.6826 - accuracy: 0.57 - ETA: 1s - loss: 0.6794 - accuracy: 0.57 - ETA: 0s - loss: 0.6766 - accuracy: 0.58 - ETA: 0s - loss: 0.6746 - accuracy: 0.58 - ETA: 0s - loss: 0.6729 - accuracy: 0.59 - ETA: 0s - loss: 0.6713 - accuracy: 0.59 - ETA: 0s - loss: 0.6688 - accuracy: 0.59 - ETA: 0s - loss: 0.6670 - accuracy: 0.60 - ETA: 0s - loss: 0.6649 - accuracy: 0.60 - ETA: 0s - loss: 0.6629 - accuracy: 0.60 - ETA: 0s - loss: 0.6601 - accuracy: 0.60 - ETA: 0s - loss: 0.6585 - accuracy: 0.61 - ETA: 0s - loss: 0.6570 - accuracy: 0.61 - ETA: 0s - loss: 0.6554 - accuracy: 0.61 - ETA: 0s - loss: 0.6539 - accuracy: 0.62 - ETA: 0s - loss: 0.6523 - accuracy: 0.62 - ETA: 0s - loss: 0.6503 - accuracy: 0.62 - ETA: 0s - loss: 0.6493 - accuracy: 0.62 - 1s 118us/step - loss: 0.6490 - accuracy: 0.6284 - val_loss: 0.5928 - val_accuracy: 0.7237\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:28 - loss: 0.6926 - accuracy: 0.60 - ETA: 3s - loss: 0.6940 - accuracy: 0.5077 - ETA: 2s - loss: 0.6934 - accuracy: 0.52 - ETA: 1s - loss: 0.6900 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.52 - ETA: 1s - loss: 0.6881 - accuracy: 0.52 - ETA: 1s - loss: 0.6862 - accuracy: 0.53 - ETA: 1s - loss: 0.6851 - accuracy: 0.53 - ETA: 1s - loss: 0.6845 - accuracy: 0.54 - ETA: 1s - loss: 0.6810 - accuracy: 0.55 - ETA: 0s - loss: 0.6776 - accuracy: 0.56 - ETA: 0s - loss: 0.6748 - accuracy: 0.57 - ETA: 0s - loss: 0.6725 - accuracy: 0.57 - ETA: 0s - loss: 0.6711 - accuracy: 0.58 - ETA: 0s - loss: 0.6685 - accuracy: 0.58 - ETA: 0s - loss: 0.6673 - accuracy: 0.59 - ETA: 0s - loss: 0.6656 - accuracy: 0.59 - ETA: 0s - loss: 0.6634 - accuracy: 0.59 - ETA: 0s - loss: 0.6610 - accuracy: 0.60 - ETA: 0s - loss: 0.6582 - accuracy: 0.60 - ETA: 0s - loss: 0.6555 - accuracy: 0.61 - ETA: 0s - loss: 0.6546 - accuracy: 0.61 - ETA: 0s - loss: 0.6533 - accuracy: 0.61 - ETA: 0s - loss: 0.6504 - accuracy: 0.61 - ETA: 0s - loss: 0.6491 - accuracy: 0.62 - ETA: 0s - loss: 0.6479 - accuracy: 0.62 - 2s 119us/step - loss: 0.6467 - accuracy: 0.6239 - val_loss: 0.5847 - val_accuracy: 0.7259\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 44us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:25 - loss: 0.6951 - accuracy: 0.40 - ETA: 3s - loss: 0.6939 - accuracy: 0.5396 - ETA: 2s - loss: 0.6940 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.52 - ETA: 1s - loss: 0.6934 - accuracy: 0.53 - ETA: 1s - loss: 0.6927 - accuracy: 0.54 - ETA: 1s - loss: 0.6920 - accuracy: 0.55 - ETA: 1s - loss: 0.6907 - accuracy: 0.56 - ETA: 1s - loss: 0.6888 - accuracy: 0.57 - ETA: 1s - loss: 0.6870 - accuracy: 0.57 - ETA: 0s - loss: 0.6849 - accuracy: 0.58 - ETA: 0s - loss: 0.6836 - accuracy: 0.58 - ETA: 0s - loss: 0.6816 - accuracy: 0.59 - ETA: 0s - loss: 0.6787 - accuracy: 0.59 - ETA: 0s - loss: 0.6761 - accuracy: 0.60 - ETA: 0s - loss: 0.6724 - accuracy: 0.61 - ETA: 0s - loss: 0.6704 - accuracy: 0.61 - ETA: 0s - loss: 0.6672 - accuracy: 0.61 - ETA: 0s - loss: 0.6663 - accuracy: 0.61 - ETA: 0s - loss: 0.6636 - accuracy: 0.62 - ETA: 0s - loss: 0.6617 - accuracy: 0.62 - ETA: 0s - loss: 0.6607 - accuracy: 0.62 - ETA: 0s - loss: 0.6586 - accuracy: 0.62 - ETA: 0s - loss: 0.6570 - accuracy: 0.63 - ETA: 0s - loss: 0.6551 - accuracy: 0.63 - ETA: 0s - loss: 0.6541 - accuracy: 0.63 - 1s 118us/step - loss: 0.6537 - accuracy: 0.6353 - val_loss: 0.5955 - val_accuracy: 0.7166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 44us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:29 - loss: 0.7097 - accuracy: 0.0000e+ - ETA: 3s - loss: 0.6944 - accuracy: 0.4788     - ETA: 2s - loss: 0.6943 - accuracy: 0.47 - ETA: 1s - loss: 0.6926 - accuracy: 0.50 - ETA: 1s - loss: 0.6911 - accuracy: 0.51 - ETA: 1s - loss: 0.6886 - accuracy: 0.51 - ETA: 1s - loss: 0.6876 - accuracy: 0.52 - ETA: 1s - loss: 0.6858 - accuracy: 0.52 - ETA: 1s - loss: 0.6833 - accuracy: 0.53 - ETA: 1s - loss: 0.6808 - accuracy: 0.54 - ETA: 0s - loss: 0.6786 - accuracy: 0.55 - ETA: 0s - loss: 0.6757 - accuracy: 0.56 - ETA: 0s - loss: 0.6730 - accuracy: 0.57 - ETA: 0s - loss: 0.6719 - accuracy: 0.57 - ETA: 0s - loss: 0.6689 - accuracy: 0.58 - ETA: 0s - loss: 0.6668 - accuracy: 0.59 - ETA: 0s - loss: 0.6647 - accuracy: 0.59 - ETA: 0s - loss: 0.6616 - accuracy: 0.60 - ETA: 0s - loss: 0.6598 - accuracy: 0.60 - ETA: 0s - loss: 0.6587 - accuracy: 0.60 - ETA: 0s - loss: 0.6567 - accuracy: 0.61 - ETA: 0s - loss: 0.6543 - accuracy: 0.61 - ETA: 0s - loss: 0.6526 - accuracy: 0.61 - ETA: 0s - loss: 0.6514 - accuracy: 0.62 - ETA: 0s - loss: 0.6494 - accuracy: 0.62 - ETA: 0s - loss: 0.6479 - accuracy: 0.62 - 2s 118us/step - loss: 0.6476 - accuracy: 0.6268 - val_loss: 0.5883 - val_accuracy: 0.7223\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:29 - loss: 0.6956 - accuracy: 0.50 - ETA: 3s - loss: 0.6935 - accuracy: 0.5358 - ETA: 2s - loss: 0.6931 - accuracy: 0.53 - ETA: 1s - loss: 0.6922 - accuracy: 0.54 - ETA: 1s - loss: 0.6898 - accuracy: 0.55 - ETA: 1s - loss: 0.6871 - accuracy: 0.55 - ETA: 1s - loss: 0.6845 - accuracy: 0.56 - ETA: 1s - loss: 0.6827 - accuracy: 0.56 - ETA: 1s - loss: 0.6790 - accuracy: 0.58 - ETA: 1s - loss: 0.6757 - accuracy: 0.58 - ETA: 0s - loss: 0.6735 - accuracy: 0.59 - ETA: 0s - loss: 0.6697 - accuracy: 0.60 - ETA: 0s - loss: 0.6682 - accuracy: 0.60 - ETA: 0s - loss: 0.6658 - accuracy: 0.60 - ETA: 0s - loss: 0.6623 - accuracy: 0.61 - ETA: 0s - loss: 0.6589 - accuracy: 0.61 - ETA: 0s - loss: 0.6572 - accuracy: 0.62 - ETA: 0s - loss: 0.6534 - accuracy: 0.62 - ETA: 0s - loss: 0.6516 - accuracy: 0.63 - ETA: 0s - loss: 0.6488 - accuracy: 0.63 - ETA: 0s - loss: 0.6472 - accuracy: 0.63 - ETA: 0s - loss: 0.6460 - accuracy: 0.63 - ETA: 0s - loss: 0.6434 - accuracy: 0.64 - ETA: 0s - loss: 0.6418 - accuracy: 0.64 - ETA: 0s - loss: 0.6398 - accuracy: 0.64 - ETA: 0s - loss: 0.6385 - accuracy: 0.64 - 1s 118us/step - loss: 0.6376 - accuracy: 0.6493 - val_loss: 0.5831 - val_accuracy: 0.7188\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:28 - loss: 0.6956 - accuracy: 0.40 - ETA: 3s - loss: 0.6943 - accuracy: 0.4943 - ETA: 2s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6917 - accuracy: 0.53 - ETA: 1s - loss: 0.6909 - accuracy: 0.53 - ETA: 1s - loss: 0.6879 - accuracy: 0.54 - ETA: 1s - loss: 0.6845 - accuracy: 0.55 - ETA: 1s - loss: 0.6826 - accuracy: 0.56 - ETA: 1s - loss: 0.6786 - accuracy: 0.57 - ETA: 0s - loss: 0.6739 - accuracy: 0.57 - ETA: 0s - loss: 0.6719 - accuracy: 0.58 - ETA: 0s - loss: 0.6703 - accuracy: 0.59 - ETA: 0s - loss: 0.6679 - accuracy: 0.59 - ETA: 0s - loss: 0.6661 - accuracy: 0.60 - ETA: 0s - loss: 0.6653 - accuracy: 0.60 - ETA: 0s - loss: 0.6651 - accuracy: 0.60 - ETA: 0s - loss: 0.6632 - accuracy: 0.60 - ETA: 0s - loss: 0.6623 - accuracy: 0.60 - ETA: 0s - loss: 0.6603 - accuracy: 0.61 - ETA: 0s - loss: 0.6581 - accuracy: 0.61 - ETA: 0s - loss: 0.6567 - accuracy: 0.61 - ETA: 0s - loss: 0.6549 - accuracy: 0.62 - ETA: 0s - loss: 0.6523 - accuracy: 0.62 - ETA: 0s - loss: 0.6510 - accuracy: 0.62 - ETA: 0s - loss: 0.6487 - accuracy: 0.63 - 2s 120us/step - loss: 0.6476 - accuracy: 0.6329 - val_loss: 0.5911 - val_accuracy: 0.7166\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:25 - loss: 0.6967 - accuracy: 0.40 - ETA: 3s - loss: 0.6944 - accuracy: 0.5078 - ETA: 2s - loss: 0.6928 - accuracy: 0.53 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6899 - accuracy: 0.54 - ETA: 1s - loss: 0.6882 - accuracy: 0.55 - ETA: 1s - loss: 0.6841 - accuracy: 0.56 - ETA: 1s - loss: 0.6811 - accuracy: 0.57 - ETA: 1s - loss: 0.6786 - accuracy: 0.58 - ETA: 1s - loss: 0.6746 - accuracy: 0.58 - ETA: 0s - loss: 0.6709 - accuracy: 0.59 - ETA: 0s - loss: 0.6698 - accuracy: 0.59 - ETA: 0s - loss: 0.6676 - accuracy: 0.60 - ETA: 0s - loss: 0.6657 - accuracy: 0.60 - ETA: 0s - loss: 0.6618 - accuracy: 0.61 - ETA: 0s - loss: 0.6576 - accuracy: 0.61 - ETA: 0s - loss: 0.6559 - accuracy: 0.62 - ETA: 0s - loss: 0.6545 - accuracy: 0.62 - ETA: 0s - loss: 0.6523 - accuracy: 0.62 - ETA: 0s - loss: 0.6516 - accuracy: 0.62 - ETA: 0s - loss: 0.6496 - accuracy: 0.62 - ETA: 0s - loss: 0.6472 - accuracy: 0.63 - ETA: 0s - loss: 0.6464 - accuracy: 0.63 - ETA: 0s - loss: 0.6450 - accuracy: 0.63 - ETA: 0s - loss: 0.6438 - accuracy: 0.63 - ETA: 0s - loss: 0.6432 - accuracy: 0.63 - 2s 119us/step - loss: 0.6414 - accuracy: 0.6415 - val_loss: 0.5931 - val_accuracy: 0.7188\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:25 - loss: 0.6977 - accuracy: 0.40 - ETA: 3s - loss: 0.6936 - accuracy: 0.5509 - ETA: 2s - loss: 0.6914 - accuracy: 0.56 - ETA: 1s - loss: 0.6899 - accuracy: 0.56 - ETA: 1s - loss: 0.6870 - accuracy: 0.56 - ETA: 1s - loss: 0.6814 - accuracy: 0.57 - ETA: 1s - loss: 0.6765 - accuracy: 0.58 - ETA: 1s - loss: 0.6712 - accuracy: 0.59 - ETA: 1s - loss: 0.6662 - accuracy: 0.60 - ETA: 0s - loss: 0.6653 - accuracy: 0.60 - ETA: 0s - loss: 0.6618 - accuracy: 0.61 - ETA: 0s - loss: 0.6599 - accuracy: 0.61 - ETA: 0s - loss: 0.6555 - accuracy: 0.61 - ETA: 0s - loss: 0.6517 - accuracy: 0.62 - ETA: 0s - loss: 0.6493 - accuracy: 0.62 - ETA: 0s - loss: 0.6463 - accuracy: 0.63 - ETA: 0s - loss: 0.6451 - accuracy: 0.63 - ETA: 0s - loss: 0.6446 - accuracy: 0.63 - ETA: 0s - loss: 0.6439 - accuracy: 0.63 - ETA: 0s - loss: 0.6423 - accuracy: 0.64 - ETA: 0s - loss: 0.6402 - accuracy: 0.64 - ETA: 0s - loss: 0.6397 - accuracy: 0.64 - ETA: 0s - loss: 0.6373 - accuracy: 0.64 - ETA: 0s - loss: 0.6351 - accuracy: 0.64 - ETA: 0s - loss: 0.6336 - accuracy: 0.65 - 1s 117us/step - loss: 0.6320 - accuracy: 0.6521 - val_loss: 0.5789 - val_accuracy: 0.7251\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:26 - loss: 0.6977 - accuracy: 0.40 - ETA: 3s - loss: 0.6952 - accuracy: 0.4885 - ETA: 2s - loss: 0.6942 - accuracy: 0.50 - ETA: 1s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.52 - ETA: 1s - loss: 0.6892 - accuracy: 0.53 - ETA: 1s - loss: 0.6874 - accuracy: 0.53 - ETA: 1s - loss: 0.6835 - accuracy: 0.55 - ETA: 1s - loss: 0.6797 - accuracy: 0.56 - ETA: 1s - loss: 0.6758 - accuracy: 0.57 - ETA: 0s - loss: 0.6723 - accuracy: 0.58 - ETA: 0s - loss: 0.6694 - accuracy: 0.58 - ETA: 0s - loss: 0.6664 - accuracy: 0.59 - ETA: 0s - loss: 0.6656 - accuracy: 0.59 - ETA: 0s - loss: 0.6628 - accuracy: 0.60 - ETA: 0s - loss: 0.6599 - accuracy: 0.60 - ETA: 0s - loss: 0.6579 - accuracy: 0.61 - ETA: 0s - loss: 0.6565 - accuracy: 0.61 - ETA: 0s - loss: 0.6525 - accuracy: 0.62 - ETA: 0s - loss: 0.6515 - accuracy: 0.62 - ETA: 0s - loss: 0.6488 - accuracy: 0.62 - ETA: 0s - loss: 0.6468 - accuracy: 0.63 - ETA: 0s - loss: 0.6449 - accuracy: 0.63 - ETA: 0s - loss: 0.6436 - accuracy: 0.63 - ETA: 0s - loss: 0.6415 - accuracy: 0.64 - ETA: 0s - loss: 0.6408 - accuracy: 0.64 - 2s 119us/step - loss: 0.6394 - accuracy: 0.6436 - val_loss: 0.5841 - val_accuracy: 0.7074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:39 - loss: 0.6945 - accuracy: 0.50 - ETA: 4s - loss: 0.6937 - accuracy: 0.5000 - ETA: 2s - loss: 0.6930 - accuracy: 0.50 - ETA: 2s - loss: 0.6907 - accuracy: 0.52 - ETA: 1s - loss: 0.6875 - accuracy: 0.54 - ETA: 1s - loss: 0.6844 - accuracy: 0.55 - ETA: 1s - loss: 0.6821 - accuracy: 0.55 - ETA: 1s - loss: 0.6777 - accuracy: 0.57 - ETA: 1s - loss: 0.6729 - accuracy: 0.58 - ETA: 1s - loss: 0.6704 - accuracy: 0.58 - ETA: 0s - loss: 0.6675 - accuracy: 0.58 - ETA: 0s - loss: 0.6626 - accuracy: 0.59 - ETA: 0s - loss: 0.6594 - accuracy: 0.60 - ETA: 0s - loss: 0.6539 - accuracy: 0.61 - ETA: 0s - loss: 0.6516 - accuracy: 0.61 - ETA: 0s - loss: 0.6467 - accuracy: 0.62 - ETA: 0s - loss: 0.6440 - accuracy: 0.62 - ETA: 0s - loss: 0.6416 - accuracy: 0.62 - ETA: 0s - loss: 0.6391 - accuracy: 0.63 - ETA: 0s - loss: 0.6385 - accuracy: 0.63 - ETA: 0s - loss: 0.6372 - accuracy: 0.63 - ETA: 0s - loss: 0.6365 - accuracy: 0.63 - ETA: 0s - loss: 0.6352 - accuracy: 0.64 - ETA: 0s - loss: 0.6340 - accuracy: 0.64 - ETA: 0s - loss: 0.6324 - accuracy: 0.64 - ETA: 0s - loss: 0.6311 - accuracy: 0.64 - 2s 119us/step - loss: 0.6306 - accuracy: 0.6497 - val_loss: 0.5784 - val_accuracy: 0.7173\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:24 - loss: 0.6941 - accuracy: 0.60 - ETA: 3s - loss: 0.6942 - accuracy: 0.5231 - ETA: 2s - loss: 0.6937 - accuracy: 0.52 - ETA: 1s - loss: 0.6927 - accuracy: 0.53 - ETA: 1s - loss: 0.6910 - accuracy: 0.54 - ETA: 1s - loss: 0.6871 - accuracy: 0.56 - ETA: 1s - loss: 0.6830 - accuracy: 0.56 - ETA: 1s - loss: 0.6791 - accuracy: 0.57 - ETA: 1s - loss: 0.6757 - accuracy: 0.58 - ETA: 1s - loss: 0.6719 - accuracy: 0.59 - ETA: 0s - loss: 0.6690 - accuracy: 0.60 - ETA: 0s - loss: 0.6669 - accuracy: 0.60 - ETA: 0s - loss: 0.6642 - accuracy: 0.60 - ETA: 0s - loss: 0.6622 - accuracy: 0.61 - ETA: 0s - loss: 0.6613 - accuracy: 0.61 - ETA: 0s - loss: 0.6586 - accuracy: 0.61 - ETA: 0s - loss: 0.6558 - accuracy: 0.62 - ETA: 0s - loss: 0.6529 - accuracy: 0.62 - ETA: 0s - loss: 0.6510 - accuracy: 0.63 - ETA: 0s - loss: 0.6499 - accuracy: 0.63 - ETA: 0s - loss: 0.6482 - accuracy: 0.63 - ETA: 0s - loss: 0.6458 - accuracy: 0.63 - ETA: 0s - loss: 0.6451 - accuracy: 0.64 - ETA: 0s - loss: 0.6434 - accuracy: 0.64 - ETA: 0s - loss: 0.6414 - accuracy: 0.64 - ETA: 0s - loss: 0.6406 - accuracy: 0.64 - 1s 118us/step - loss: 0.6401 - accuracy: 0.6484 - val_loss: 0.5810 - val_accuracy: 0.7251\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:24 - loss: 0.6950 - accuracy: 0.60 - ETA: 3s - loss: 0.6934 - accuracy: 0.5096 - ETA: 2s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6907 - accuracy: 0.52 - ETA: 1s - loss: 0.6894 - accuracy: 0.52 - ETA: 1s - loss: 0.6875 - accuracy: 0.53 - ETA: 1s - loss: 0.6832 - accuracy: 0.55 - ETA: 1s - loss: 0.6791 - accuracy: 0.56 - ETA: 1s - loss: 0.6778 - accuracy: 0.57 - ETA: 1s - loss: 0.6751 - accuracy: 0.58 - ETA: 0s - loss: 0.6732 - accuracy: 0.58 - ETA: 0s - loss: 0.6707 - accuracy: 0.59 - ETA: 0s - loss: 0.6661 - accuracy: 0.60 - ETA: 0s - loss: 0.6631 - accuracy: 0.60 - ETA: 0s - loss: 0.6607 - accuracy: 0.61 - ETA: 0s - loss: 0.6581 - accuracy: 0.61 - ETA: 0s - loss: 0.6568 - accuracy: 0.61 - ETA: 0s - loss: 0.6565 - accuracy: 0.61 - ETA: 0s - loss: 0.6541 - accuracy: 0.62 - ETA: 0s - loss: 0.6514 - accuracy: 0.62 - ETA: 0s - loss: 0.6500 - accuracy: 0.63 - ETA: 0s - loss: 0.6485 - accuracy: 0.63 - ETA: 0s - loss: 0.6471 - accuracy: 0.63 - ETA: 0s - loss: 0.6452 - accuracy: 0.63 - ETA: 0s - loss: 0.6438 - accuracy: 0.63 - ETA: 0s - loss: 0.6429 - accuracy: 0.64 - 2s 119us/step - loss: 0.6418 - accuracy: 0.6411 - val_loss: 0.5903 - val_accuracy: 0.7195\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:07 - loss: 0.6925 - accuracy: 0.70 - ETA: 3s - loss: 0.6925 - accuracy: 0.5273 - ETA: 2s - loss: 0.6921 - accuracy: 0.53 - ETA: 1s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.54 - ETA: 1s - loss: 0.6899 - accuracy: 0.54 - ETA: 1s - loss: 0.6898 - accuracy: 0.54 - ETA: 1s - loss: 0.6890 - accuracy: 0.54 - ETA: 1s - loss: 0.6874 - accuracy: 0.55 - ETA: 0s - loss: 0.6864 - accuracy: 0.55 - ETA: 0s - loss: 0.6851 - accuracy: 0.55 - ETA: 0s - loss: 0.6838 - accuracy: 0.56 - ETA: 0s - loss: 0.6828 - accuracy: 0.56 - ETA: 0s - loss: 0.6820 - accuracy: 0.56 - ETA: 0s - loss: 0.6810 - accuracy: 0.57 - ETA: 0s - loss: 0.6795 - accuracy: 0.57 - ETA: 0s - loss: 0.6783 - accuracy: 0.57 - ETA: 0s - loss: 0.6773 - accuracy: 0.58 - ETA: 0s - loss: 0.6760 - accuracy: 0.58 - ETA: 0s - loss: 0.6747 - accuracy: 0.58 - ETA: 0s - loss: 0.6734 - accuracy: 0.59 - ETA: 0s - loss: 0.6720 - accuracy: 0.59 - ETA: 0s - loss: 0.6709 - accuracy: 0.59 - ETA: 0s - loss: 0.6698 - accuracy: 0.59 - ETA: 0s - loss: 0.6690 - accuracy: 0.59 - 1s 112us/step - loss: 0.6689 - accuracy: 0.5982 - val_loss: 0.6267 - val_accuracy: 0.7223\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:04 - loss: 0.6938 - accuracy: 0.30 - ETA: 3s - loss: 0.6928 - accuracy: 0.5113 - ETA: 2s - loss: 0.6923 - accuracy: 0.54 - ETA: 1s - loss: 0.6918 - accuracy: 0.54 - ETA: 1s - loss: 0.6916 - accuracy: 0.54 - ETA: 1s - loss: 0.6909 - accuracy: 0.55 - ETA: 1s - loss: 0.6904 - accuracy: 0.56 - ETA: 1s - loss: 0.6896 - accuracy: 0.57 - ETA: 0s - loss: 0.6880 - accuracy: 0.57 - ETA: 0s - loss: 0.6864 - accuracy: 0.58 - ETA: 0s - loss: 0.6854 - accuracy: 0.58 - ETA: 0s - loss: 0.6844 - accuracy: 0.58 - ETA: 0s - loss: 0.6832 - accuracy: 0.59 - ETA: 0s - loss: 0.6821 - accuracy: 0.59 - ETA: 0s - loss: 0.6809 - accuracy: 0.59 - ETA: 0s - loss: 0.6795 - accuracy: 0.59 - ETA: 0s - loss: 0.6785 - accuracy: 0.60 - ETA: 0s - loss: 0.6768 - accuracy: 0.60 - ETA: 0s - loss: 0.6762 - accuracy: 0.60 - ETA: 0s - loss: 0.6747 - accuracy: 0.60 - ETA: 0s - loss: 0.6737 - accuracy: 0.61 - ETA: 0s - loss: 0.6724 - accuracy: 0.61 - ETA: 0s - loss: 0.6707 - accuracy: 0.61 - ETA: 0s - loss: 0.6700 - accuracy: 0.61 - ETA: 0s - loss: 0.6690 - accuracy: 0.61 - 1s 112us/step - loss: 0.6689 - accuracy: 0.6186 - val_loss: 0.6284 - val_accuracy: 0.7074\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:11 - loss: 0.6952 - accuracy: 0.30 - ETA: 3s - loss: 0.6936 - accuracy: 0.4877 - ETA: 2s - loss: 0.6936 - accuracy: 0.48 - ETA: 2s - loss: 0.6932 - accuracy: 0.50 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.53 - ETA: 1s - loss: 0.6923 - accuracy: 0.53 - ETA: 1s - loss: 0.6919 - accuracy: 0.54 - ETA: 0s - loss: 0.6912 - accuracy: 0.55 - ETA: 0s - loss: 0.6906 - accuracy: 0.55 - ETA: 0s - loss: 0.6897 - accuracy: 0.56 - ETA: 0s - loss: 0.6891 - accuracy: 0.56 - ETA: 0s - loss: 0.6884 - accuracy: 0.57 - ETA: 0s - loss: 0.6876 - accuracy: 0.57 - ETA: 0s - loss: 0.6867 - accuracy: 0.58 - ETA: 0s - loss: 0.6858 - accuracy: 0.58 - ETA: 0s - loss: 0.6850 - accuracy: 0.58 - ETA: 0s - loss: 0.6842 - accuracy: 0.59 - ETA: 0s - loss: 0.6834 - accuracy: 0.59 - ETA: 0s - loss: 0.6826 - accuracy: 0.59 - ETA: 0s - loss: 0.6816 - accuracy: 0.60 - ETA: 0s - loss: 0.6806 - accuracy: 0.60 - 1s 111us/step - loss: 0.6802 - accuracy: 0.6048 - val_loss: 0.6521 - val_accuracy: 0.7145\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:10 - loss: 0.6961 - accuracy: 0.40 - ETA: 3s - loss: 0.6916 - accuracy: 0.5574 - ETA: 2s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.52 - ETA: 1s - loss: 0.6906 - accuracy: 0.52 - ETA: 1s - loss: 0.6896 - accuracy: 0.53 - ETA: 1s - loss: 0.6893 - accuracy: 0.53 - ETA: 0s - loss: 0.6882 - accuracy: 0.54 - ETA: 0s - loss: 0.6878 - accuracy: 0.54 - ETA: 0s - loss: 0.6873 - accuracy: 0.54 - ETA: 0s - loss: 0.6864 - accuracy: 0.54 - ETA: 0s - loss: 0.6853 - accuracy: 0.55 - ETA: 0s - loss: 0.6843 - accuracy: 0.55 - ETA: 0s - loss: 0.6824 - accuracy: 0.56 - ETA: 0s - loss: 0.6809 - accuracy: 0.56 - ETA: 0s - loss: 0.6791 - accuracy: 0.57 - ETA: 0s - loss: 0.6781 - accuracy: 0.57 - ETA: 0s - loss: 0.6766 - accuracy: 0.57 - ETA: 0s - loss: 0.6752 - accuracy: 0.58 - ETA: 0s - loss: 0.6740 - accuracy: 0.58 - ETA: 0s - loss: 0.6727 - accuracy: 0.58 - ETA: 0s - loss: 0.6718 - accuracy: 0.58 - ETA: 0s - loss: 0.6708 - accuracy: 0.58 - 1s 111us/step - loss: 0.6687 - accuracy: 0.5931 - val_loss: 0.6239 - val_accuracy: 0.7017\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:10 - loss: 0.6970 - accuracy: 0.40 - ETA: 3s - loss: 0.6934 - accuracy: 0.5109 - ETA: 2s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.52 - ETA: 1s - loss: 0.6920 - accuracy: 0.53 - ETA: 1s - loss: 0.6909 - accuracy: 0.54 - ETA: 1s - loss: 0.6902 - accuracy: 0.55 - ETA: 0s - loss: 0.6895 - accuracy: 0.55 - ETA: 0s - loss: 0.6883 - accuracy: 0.56 - ETA: 0s - loss: 0.6874 - accuracy: 0.56 - ETA: 0s - loss: 0.6856 - accuracy: 0.57 - ETA: 0s - loss: 0.6846 - accuracy: 0.57 - ETA: 0s - loss: 0.6829 - accuracy: 0.58 - ETA: 0s - loss: 0.6819 - accuracy: 0.58 - ETA: 0s - loss: 0.6803 - accuracy: 0.58 - ETA: 0s - loss: 0.6796 - accuracy: 0.58 - ETA: 0s - loss: 0.6780 - accuracy: 0.59 - ETA: 0s - loss: 0.6771 - accuracy: 0.59 - ETA: 0s - loss: 0.6754 - accuracy: 0.59 - ETA: 0s - loss: 0.6737 - accuracy: 0.59 - ETA: 0s - loss: 0.6724 - accuracy: 0.60 - ETA: 0s - loss: 0.6710 - accuracy: 0.60 - ETA: 0s - loss: 0.6693 - accuracy: 0.60 - 1s 110us/step - loss: 0.6685 - accuracy: 0.6052 - val_loss: 0.6217 - val_accuracy: 0.7124\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:06 - loss: 0.6941 - accuracy: 0.40 - ETA: 3s - loss: 0.6933 - accuracy: 0.5145 - ETA: 2s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.53 - ETA: 1s - loss: 0.6925 - accuracy: 0.53 - ETA: 1s - loss: 0.6916 - accuracy: 0.54 - ETA: 1s - loss: 0.6911 - accuracy: 0.55 - ETA: 1s - loss: 0.6903 - accuracy: 0.55 - ETA: 0s - loss: 0.6889 - accuracy: 0.56 - ETA: 0s - loss: 0.6884 - accuracy: 0.56 - ETA: 0s - loss: 0.6881 - accuracy: 0.56 - ETA: 0s - loss: 0.6876 - accuracy: 0.56 - ETA: 0s - loss: 0.6870 - accuracy: 0.56 - ETA: 0s - loss: 0.6858 - accuracy: 0.57 - ETA: 0s - loss: 0.6849 - accuracy: 0.57 - ETA: 0s - loss: 0.6842 - accuracy: 0.57 - ETA: 0s - loss: 0.6832 - accuracy: 0.57 - ETA: 0s - loss: 0.6822 - accuracy: 0.57 - ETA: 0s - loss: 0.6806 - accuracy: 0.58 - ETA: 0s - loss: 0.6791 - accuracy: 0.58 - ETA: 0s - loss: 0.6777 - accuracy: 0.58 - ETA: 0s - loss: 0.6766 - accuracy: 0.58 - ETA: 0s - loss: 0.6761 - accuracy: 0.58 - ETA: 0s - loss: 0.6751 - accuracy: 0.59 - 1s 110us/step - loss: 0.6746 - accuracy: 0.5909 - val_loss: 0.6446 - val_accuracy: 0.6776\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:07 - loss: 0.6948 - accuracy: 0.50 - ETA: 3s - loss: 0.6917 - accuracy: 0.5815 - ETA: 2s - loss: 0.6916 - accuracy: 0.56 - ETA: 1s - loss: 0.6905 - accuracy: 0.56 - ETA: 1s - loss: 0.6888 - accuracy: 0.57 - ETA: 1s - loss: 0.6877 - accuracy: 0.57 - ETA: 1s - loss: 0.6862 - accuracy: 0.58 - ETA: 1s - loss: 0.6844 - accuracy: 0.58 - ETA: 0s - loss: 0.6838 - accuracy: 0.58 - ETA: 0s - loss: 0.6814 - accuracy: 0.59 - ETA: 0s - loss: 0.6798 - accuracy: 0.59 - ETA: 0s - loss: 0.6775 - accuracy: 0.59 - ETA: 0s - loss: 0.6757 - accuracy: 0.60 - ETA: 0s - loss: 0.6743 - accuracy: 0.60 - ETA: 0s - loss: 0.6717 - accuracy: 0.61 - ETA: 0s - loss: 0.6701 - accuracy: 0.61 - ETA: 0s - loss: 0.6686 - accuracy: 0.61 - ETA: 0s - loss: 0.6666 - accuracy: 0.61 - ETA: 0s - loss: 0.6649 - accuracy: 0.62 - ETA: 0s - loss: 0.6630 - accuracy: 0.62 - ETA: 0s - loss: 0.6625 - accuracy: 0.62 - ETA: 0s - loss: 0.6611 - accuracy: 0.62 - ETA: 0s - loss: 0.6598 - accuracy: 0.62 - ETA: 0s - loss: 0.6585 - accuracy: 0.62 - ETA: 0s - loss: 0.6573 - accuracy: 0.62 - 1s 112us/step - loss: 0.6573 - accuracy: 0.6272 - val_loss: 0.6042 - val_accuracy: 0.7152\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:09 - loss: 0.6947 - accuracy: 0.50 - ETA: 3s - loss: 0.6932 - accuracy: 0.5782 - ETA: 2s - loss: 0.6923 - accuracy: 0.55 - ETA: 1s - loss: 0.6916 - accuracy: 0.55 - ETA: 1s - loss: 0.6909 - accuracy: 0.54 - ETA: 1s - loss: 0.6893 - accuracy: 0.55 - ETA: 1s - loss: 0.6881 - accuracy: 0.54 - ETA: 1s - loss: 0.6863 - accuracy: 0.55 - ETA: 0s - loss: 0.6850 - accuracy: 0.56 - ETA: 0s - loss: 0.6840 - accuracy: 0.56 - ETA: 0s - loss: 0.6828 - accuracy: 0.57 - ETA: 0s - loss: 0.6804 - accuracy: 0.57 - ETA: 0s - loss: 0.6779 - accuracy: 0.58 - ETA: 0s - loss: 0.6765 - accuracy: 0.58 - ETA: 0s - loss: 0.6741 - accuracy: 0.59 - ETA: 0s - loss: 0.6729 - accuracy: 0.59 - ETA: 0s - loss: 0.6714 - accuracy: 0.59 - ETA: 0s - loss: 0.6694 - accuracy: 0.60 - ETA: 0s - loss: 0.6676 - accuracy: 0.60 - ETA: 0s - loss: 0.6659 - accuracy: 0.60 - ETA: 0s - loss: 0.6643 - accuracy: 0.60 - ETA: 0s - loss: 0.6625 - accuracy: 0.61 - ETA: 0s - loss: 0.6610 - accuracy: 0.61 - ETA: 0s - loss: 0.6594 - accuracy: 0.61 - 1s 110us/step - loss: 0.6585 - accuracy: 0.6173 - val_loss: 0.6046 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:07 - loss: 0.6958 - accuracy: 0.40 - ETA: 3s - loss: 0.6936 - accuracy: 0.5093 - ETA: 2s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6919 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.54 - ETA: 1s - loss: 0.6900 - accuracy: 0.55 - ETA: 1s - loss: 0.6878 - accuracy: 0.56 - ETA: 1s - loss: 0.6854 - accuracy: 0.57 - ETA: 0s - loss: 0.6837 - accuracy: 0.57 - ETA: 0s - loss: 0.6825 - accuracy: 0.58 - ETA: 0s - loss: 0.6807 - accuracy: 0.58 - ETA: 0s - loss: 0.6784 - accuracy: 0.59 - ETA: 0s - loss: 0.6767 - accuracy: 0.59 - ETA: 0s - loss: 0.6750 - accuracy: 0.59 - ETA: 0s - loss: 0.6733 - accuracy: 0.60 - ETA: 0s - loss: 0.6713 - accuracy: 0.60 - ETA: 0s - loss: 0.6702 - accuracy: 0.60 - ETA: 0s - loss: 0.6687 - accuracy: 0.60 - ETA: 0s - loss: 0.6677 - accuracy: 0.61 - ETA: 0s - loss: 0.6656 - accuracy: 0.61 - ETA: 0s - loss: 0.6636 - accuracy: 0.61 - ETA: 0s - loss: 0.6613 - accuracy: 0.61 - ETA: 0s - loss: 0.6596 - accuracy: 0.62 - 1s 111us/step - loss: 0.6586 - accuracy: 0.6233 - val_loss: 0.6063 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 2:09 - loss: 0.6978 - accuracy: 0.30 - ETA: 3s - loss: 0.6936 - accuracy: 0.4907 - ETA: 2s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6921 - accuracy: 0.52 - ETA: 1s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6914 - accuracy: 0.52 - ETA: 1s - loss: 0.6905 - accuracy: 0.53 - ETA: 1s - loss: 0.6896 - accuracy: 0.54 - ETA: 1s - loss: 0.6883 - accuracy: 0.54 - ETA: 0s - loss: 0.6872 - accuracy: 0.55 - ETA: 0s - loss: 0.6856 - accuracy: 0.55 - ETA: 0s - loss: 0.6841 - accuracy: 0.56 - ETA: 0s - loss: 0.6826 - accuracy: 0.56 - ETA: 0s - loss: 0.6814 - accuracy: 0.57 - ETA: 0s - loss: 0.6795 - accuracy: 0.57 - ETA: 0s - loss: 0.6780 - accuracy: 0.57 - ETA: 0s - loss: 0.6756 - accuracy: 0.58 - ETA: 0s - loss: 0.6730 - accuracy: 0.58 - ETA: 0s - loss: 0.6721 - accuracy: 0.59 - ETA: 0s - loss: 0.6696 - accuracy: 0.59 - ETA: 0s - loss: 0.6678 - accuracy: 0.59 - ETA: 0s - loss: 0.6662 - accuracy: 0.60 - ETA: 0s - loss: 0.6647 - accuracy: 0.60 - ETA: 0s - loss: 0.6641 - accuracy: 0.60 - 1s 111us/step - loss: 0.6627 - accuracy: 0.6081 - val_loss: 0.6147 - val_accuracy: 0.7017\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:35 - loss: 0.6921 - accuracy: 0.40 - ETA: 4s - loss: 0.6935 - accuracy: 0.5137 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6906 - accuracy: 0.53 - ETA: 1s - loss: 0.6881 - accuracy: 0.54 - ETA: 1s - loss: 0.6869 - accuracy: 0.54 - ETA: 1s - loss: 0.6858 - accuracy: 0.55 - ETA: 1s - loss: 0.6843 - accuracy: 0.55 - ETA: 0s - loss: 0.6835 - accuracy: 0.55 - ETA: 0s - loss: 0.6822 - accuracy: 0.56 - ETA: 0s - loss: 0.6795 - accuracy: 0.57 - ETA: 0s - loss: 0.6782 - accuracy: 0.57 - ETA: 0s - loss: 0.6761 - accuracy: 0.57 - ETA: 0s - loss: 0.6744 - accuracy: 0.58 - ETA: 0s - loss: 0.6733 - accuracy: 0.58 - ETA: 0s - loss: 0.6725 - accuracy: 0.58 - ETA: 0s - loss: 0.6707 - accuracy: 0.59 - ETA: 0s - loss: 0.6695 - accuracy: 0.59 - ETA: 0s - loss: 0.6670 - accuracy: 0.60 - ETA: 0s - loss: 0.6660 - accuracy: 0.60 - ETA: 0s - loss: 0.6644 - accuracy: 0.60 - ETA: 0s - loss: 0.6627 - accuracy: 0.61 - ETA: 0s - loss: 0.6613 - accuracy: 0.61 - 1s 112us/step - loss: 0.6604 - accuracy: 0.6137 - val_loss: 0.6140 - val_accuracy: 0.7067\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:07 - loss: 0.6996 - accuracy: 0.30 - ETA: 3s - loss: 0.6932 - accuracy: 0.5170 - ETA: 2s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6917 - accuracy: 0.51 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 1s - loss: 0.6892 - accuracy: 0.53 - ETA: 1s - loss: 0.6888 - accuracy: 0.53 - ETA: 1s - loss: 0.6873 - accuracy: 0.55 - ETA: 1s - loss: 0.6859 - accuracy: 0.55 - ETA: 0s - loss: 0.6846 - accuracy: 0.56 - ETA: 0s - loss: 0.6836 - accuracy: 0.56 - ETA: 0s - loss: 0.6825 - accuracy: 0.57 - ETA: 0s - loss: 0.6806 - accuracy: 0.57 - ETA: 0s - loss: 0.6789 - accuracy: 0.57 - ETA: 0s - loss: 0.6757 - accuracy: 0.58 - ETA: 0s - loss: 0.6731 - accuracy: 0.59 - ETA: 0s - loss: 0.6715 - accuracy: 0.59 - ETA: 0s - loss: 0.6700 - accuracy: 0.59 - ETA: 0s - loss: 0.6689 - accuracy: 0.60 - ETA: 0s - loss: 0.6678 - accuracy: 0.60 - ETA: 0s - loss: 0.6661 - accuracy: 0.60 - ETA: 0s - loss: 0.6645 - accuracy: 0.60 - ETA: 0s - loss: 0.6631 - accuracy: 0.61 - ETA: 0s - loss: 0.6614 - accuracy: 0.61 - ETA: 0s - loss: 0.6596 - accuracy: 0.61 - 1s 112us/step - loss: 0.6595 - accuracy: 0.6185 - val_loss: 0.6105 - val_accuracy: 0.7095\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:09 - loss: 0.6956 - accuracy: 0.30 - ETA: 3s - loss: 0.6910 - accuracy: 0.5302 - ETA: 2s - loss: 0.6895 - accuracy: 0.52 - ETA: 1s - loss: 0.6882 - accuracy: 0.53 - ETA: 1s - loss: 0.6877 - accuracy: 0.53 - ETA: 1s - loss: 0.6863 - accuracy: 0.53 - ETA: 1s - loss: 0.6843 - accuracy: 0.55 - ETA: 1s - loss: 0.6828 - accuracy: 0.56 - ETA: 0s - loss: 0.6808 - accuracy: 0.57 - ETA: 0s - loss: 0.6782 - accuracy: 0.57 - ETA: 0s - loss: 0.6766 - accuracy: 0.58 - ETA: 0s - loss: 0.6744 - accuracy: 0.58 - ETA: 0s - loss: 0.6711 - accuracy: 0.59 - ETA: 0s - loss: 0.6697 - accuracy: 0.60 - ETA: 0s - loss: 0.6682 - accuracy: 0.60 - ETA: 0s - loss: 0.6669 - accuracy: 0.60 - ETA: 0s - loss: 0.6642 - accuracy: 0.61 - ETA: 0s - loss: 0.6632 - accuracy: 0.61 - ETA: 0s - loss: 0.6609 - accuracy: 0.61 - ETA: 0s - loss: 0.6596 - accuracy: 0.62 - ETA: 0s - loss: 0.6578 - accuracy: 0.62 - ETA: 0s - loss: 0.6558 - accuracy: 0.62 - ETA: 0s - loss: 0.6543 - accuracy: 0.62 - ETA: 0s - loss: 0.6519 - accuracy: 0.63 - 1s 111us/step - loss: 0.6497 - accuracy: 0.6359 - val_loss: 0.5915 - val_accuracy: 0.7188\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:06 - loss: 0.6862 - accuracy: 0.80 - ETA: 3s - loss: 0.6924 - accuracy: 0.5759 - ETA: 2s - loss: 0.6905 - accuracy: 0.57 - ETA: 1s - loss: 0.6891 - accuracy: 0.58 - ETA: 1s - loss: 0.6885 - accuracy: 0.57 - ETA: 1s - loss: 0.6870 - accuracy: 0.58 - ETA: 1s - loss: 0.6856 - accuracy: 0.58 - ETA: 1s - loss: 0.6842 - accuracy: 0.58 - ETA: 0s - loss: 0.6818 - accuracy: 0.59 - ETA: 0s - loss: 0.6796 - accuracy: 0.60 - ETA: 0s - loss: 0.6784 - accuracy: 0.60 - ETA: 0s - loss: 0.6765 - accuracy: 0.60 - ETA: 0s - loss: 0.6753 - accuracy: 0.60 - ETA: 0s - loss: 0.6730 - accuracy: 0.60 - ETA: 0s - loss: 0.6710 - accuracy: 0.61 - ETA: 0s - loss: 0.6692 - accuracy: 0.61 - ETA: 0s - loss: 0.6671 - accuracy: 0.61 - ETA: 0s - loss: 0.6659 - accuracy: 0.61 - ETA: 0s - loss: 0.6639 - accuracy: 0.62 - ETA: 0s - loss: 0.6622 - accuracy: 0.62 - ETA: 0s - loss: 0.6592 - accuracy: 0.62 - ETA: 0s - loss: 0.6578 - accuracy: 0.63 - ETA: 0s - loss: 0.6560 - accuracy: 0.63 - ETA: 0s - loss: 0.6534 - accuracy: 0.63 - 1s 111us/step - loss: 0.6518 - accuracy: 0.6388 - val_loss: 0.5953 - val_accuracy: 0.7173\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:07 - loss: 0.6985 - accuracy: 0.40 - ETA: 3s - loss: 0.6941 - accuracy: 0.5151 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6920 - accuracy: 0.54 - ETA: 1s - loss: 0.6904 - accuracy: 0.54 - ETA: 1s - loss: 0.6892 - accuracy: 0.55 - ETA: 1s - loss: 0.6873 - accuracy: 0.56 - ETA: 1s - loss: 0.6850 - accuracy: 0.57 - ETA: 1s - loss: 0.6834 - accuracy: 0.57 - ETA: 0s - loss: 0.6815 - accuracy: 0.58 - ETA: 0s - loss: 0.6780 - accuracy: 0.59 - ETA: 0s - loss: 0.6756 - accuracy: 0.60 - ETA: 0s - loss: 0.6741 - accuracy: 0.60 - ETA: 0s - loss: 0.6711 - accuracy: 0.60 - ETA: 0s - loss: 0.6695 - accuracy: 0.61 - ETA: 0s - loss: 0.6673 - accuracy: 0.61 - ETA: 0s - loss: 0.6644 - accuracy: 0.62 - ETA: 0s - loss: 0.6609 - accuracy: 0.62 - ETA: 0s - loss: 0.6587 - accuracy: 0.62 - ETA: 0s - loss: 0.6566 - accuracy: 0.63 - ETA: 0s - loss: 0.6546 - accuracy: 0.63 - ETA: 0s - loss: 0.6522 - accuracy: 0.63 - ETA: 0s - loss: 0.6504 - accuracy: 0.64 - ETA: 0s - loss: 0.6495 - accuracy: 0.64 - ETA: 0s - loss: 0.6474 - accuracy: 0.64 - 1s 112us/step - loss: 0.6471 - accuracy: 0.6453 - val_loss: 0.5963 - val_accuracy: 0.7145\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 45us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:07 - loss: 0.6947 - accuracy: 0.70 - ETA: 3s - loss: 0.6913 - accuracy: 0.5667 - ETA: 2s - loss: 0.6925 - accuracy: 0.53 - ETA: 1s - loss: 0.6921 - accuracy: 0.53 - ETA: 1s - loss: 0.6905 - accuracy: 0.54 - ETA: 1s - loss: 0.6888 - accuracy: 0.55 - ETA: 1s - loss: 0.6881 - accuracy: 0.56 - ETA: 1s - loss: 0.6863 - accuracy: 0.57 - ETA: 1s - loss: 0.6855 - accuracy: 0.57 - ETA: 0s - loss: 0.6842 - accuracy: 0.57 - ETA: 0s - loss: 0.6825 - accuracy: 0.58 - ETA: 0s - loss: 0.6809 - accuracy: 0.59 - ETA: 0s - loss: 0.6782 - accuracy: 0.59 - ETA: 0s - loss: 0.6751 - accuracy: 0.60 - ETA: 0s - loss: 0.6727 - accuracy: 0.60 - ETA: 0s - loss: 0.6703 - accuracy: 0.61 - ETA: 0s - loss: 0.6688 - accuracy: 0.61 - ETA: 0s - loss: 0.6663 - accuracy: 0.61 - ETA: 0s - loss: 0.6630 - accuracy: 0.62 - ETA: 0s - loss: 0.6618 - accuracy: 0.62 - ETA: 0s - loss: 0.6609 - accuracy: 0.62 - ETA: 0s - loss: 0.6598 - accuracy: 0.62 - ETA: 0s - loss: 0.6579 - accuracy: 0.62 - ETA: 0s - loss: 0.6559 - accuracy: 0.63 - ETA: 0s - loss: 0.6551 - accuracy: 0.63 - 1s 113us/step - loss: 0.6538 - accuracy: 0.6343 - val_loss: 0.6015 - val_accuracy: 0.6974\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:06 - loss: 0.6948 - accuracy: 0.50 - ETA: 3s - loss: 0.6918 - accuracy: 0.5473 - ETA: 2s - loss: 0.6894 - accuracy: 0.55 - ETA: 1s - loss: 0.6894 - accuracy: 0.55 - ETA: 1s - loss: 0.6884 - accuracy: 0.56 - ETA: 1s - loss: 0.6856 - accuracy: 0.58 - ETA: 1s - loss: 0.6846 - accuracy: 0.58 - ETA: 1s - loss: 0.6824 - accuracy: 0.59 - ETA: 0s - loss: 0.6784 - accuracy: 0.60 - ETA: 0s - loss: 0.6762 - accuracy: 0.60 - ETA: 0s - loss: 0.6737 - accuracy: 0.61 - ETA: 0s - loss: 0.6699 - accuracy: 0.61 - ETA: 0s - loss: 0.6674 - accuracy: 0.62 - ETA: 0s - loss: 0.6643 - accuracy: 0.62 - ETA: 0s - loss: 0.6609 - accuracy: 0.63 - ETA: 0s - loss: 0.6585 - accuracy: 0.63 - ETA: 0s - loss: 0.6563 - accuracy: 0.63 - ETA: 0s - loss: 0.6546 - accuracy: 0.63 - ETA: 0s - loss: 0.6528 - accuracy: 0.63 - ETA: 0s - loss: 0.6510 - accuracy: 0.64 - ETA: 0s - loss: 0.6495 - accuracy: 0.64 - ETA: 0s - loss: 0.6481 - accuracy: 0.64 - ETA: 0s - loss: 0.6462 - accuracy: 0.64 - ETA: 0s - loss: 0.6441 - accuracy: 0.65 - 1s 110us/step - loss: 0.6430 - accuracy: 0.6515 - val_loss: 0.5961 - val_accuracy: 0.7195\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:10 - loss: 0.6903 - accuracy: 0.80 - ETA: 3s - loss: 0.6940 - accuracy: 0.5185 - ETA: 2s - loss: 0.6928 - accuracy: 0.54 - ETA: 1s - loss: 0.6911 - accuracy: 0.56 - ETA: 1s - loss: 0.6904 - accuracy: 0.56 - ETA: 1s - loss: 0.6886 - accuracy: 0.57 - ETA: 1s - loss: 0.6861 - accuracy: 0.58 - ETA: 1s - loss: 0.6841 - accuracy: 0.59 - ETA: 1s - loss: 0.6826 - accuracy: 0.59 - ETA: 0s - loss: 0.6803 - accuracy: 0.60 - ETA: 0s - loss: 0.6772 - accuracy: 0.60 - ETA: 0s - loss: 0.6744 - accuracy: 0.61 - ETA: 0s - loss: 0.6720 - accuracy: 0.61 - ETA: 0s - loss: 0.6686 - accuracy: 0.62 - ETA: 0s - loss: 0.6655 - accuracy: 0.62 - ETA: 0s - loss: 0.6633 - accuracy: 0.62 - ETA: 0s - loss: 0.6622 - accuracy: 0.63 - ETA: 0s - loss: 0.6610 - accuracy: 0.63 - ETA: 0s - loss: 0.6588 - accuracy: 0.63 - ETA: 0s - loss: 0.6573 - accuracy: 0.63 - ETA: 0s - loss: 0.6547 - accuracy: 0.64 - ETA: 0s - loss: 0.6525 - accuracy: 0.64 - ETA: 0s - loss: 0.6510 - accuracy: 0.64 - ETA: 0s - loss: 0.6497 - accuracy: 0.64 - ETA: 0s - loss: 0.6482 - accuracy: 0.65 - 1s 114us/step - loss: 0.6472 - accuracy: 0.6509 - val_loss: 0.5976 - val_accuracy: 0.6967\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:52 - loss: 0.6933 - accuracy: 0.70 - ETA: 2s - loss: 0.6935 - accuracy: 0.5153 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.49 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - 1s 105us/step - loss: 0.6929 - accuracy: 0.5117 - val_loss: 0.6909 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:51 - loss: 0.6945 - accuracy: 0.70 - ETA: 2s - loss: 0.6938 - accuracy: 0.4964 - ETA: 1s - loss: 0.6939 - accuracy: 0.49 - ETA: 1s - loss: 0.6941 - accuracy: 0.47 - ETA: 1s - loss: 0.6941 - accuracy: 0.47 - ETA: 1s - loss: 0.6941 - accuracy: 0.47 - ETA: 1s - loss: 0.6940 - accuracy: 0.48 - ETA: 1s - loss: 0.6940 - accuracy: 0.48 - ETA: 0s - loss: 0.6938 - accuracy: 0.48 - ETA: 0s - loss: 0.6935 - accuracy: 0.49 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - 1s 108us/step - loss: 0.6933 - accuracy: 0.5076 - val_loss: 0.6918 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:51 - loss: 0.6937 - accuracy: 0.20 - ETA: 2s - loss: 0.6936 - accuracy: 0.5017 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - 1s 104us/step - loss: 0.6933 - accuracy: 0.5123 - val_loss: 0.6922 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 1:50 - loss: 0.6963 - accuracy: 0.50 - ETA: 2s - loss: 0.6927 - accuracy: 0.5571 - ETA: 1s - loss: 0.6937 - accuracy: 0.52 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - 1s 107us/step - loss: 0.6924 - accuracy: 0.5146 - val_loss: 0.6888 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:51 - loss: 0.6936 - accuracy: 0.60 - ETA: 3s - loss: 0.6926 - accuracy: 0.5291 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - 1s 106us/step - loss: 0.6922 - accuracy: 0.5143 - val_loss: 0.6893 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:52 - loss: 0.6969 - accuracy: 0.40 - ETA: 3s - loss: 0.6940 - accuracy: 0.5000 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6939 - accuracy: 0.49 - ETA: 1s - loss: 0.6938 - accuracy: 0.49 - ETA: 1s - loss: 0.6938 - accuracy: 0.49 - ETA: 1s - loss: 0.6939 - accuracy: 0.49 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - 1s 105us/step - loss: 0.6936 - accuracy: 0.5068 - val_loss: 0.6934 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:52 - loss: 0.6992 - accuracy: 0.30 - ETA: 2s - loss: 0.6945 - accuracy: 0.4895 - ETA: 1s - loss: 0.6944 - accuracy: 0.48 - ETA: 1s - loss: 0.6942 - accuracy: 0.50 - ETA: 1s - loss: 0.6942 - accuracy: 0.50 - ETA: 1s - loss: 0.6943 - accuracy: 0.50 - ETA: 1s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - 1s 106us/step - loss: 0.6932 - accuracy: 0.5168 - val_loss: 0.6909 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:55 - loss: 0.6917 - accuracy: 0.60 - ETA: 3s - loss: 0.6945 - accuracy: 0.5179 - ETA: 1s - loss: 0.6945 - accuracy: 0.51 - ETA: 1s - loss: 0.6944 - accuracy: 0.50 - ETA: 1s - loss: 0.6943 - accuracy: 0.50 - ETA: 1s - loss: 0.6942 - accuracy: 0.50 - ETA: 1s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - 1s 105us/step - loss: 0.6928 - accuracy: 0.5159 - val_loss: 0.6892 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:50 - loss: 0.6941 - accuracy: 0.60 - ETA: 2s - loss: 0.6950 - accuracy: 0.4750 - ETA: 1s - loss: 0.6948 - accuracy: 0.48 - ETA: 1s - loss: 0.6951 - accuracy: 0.47 - ETA: 1s - loss: 0.6950 - accuracy: 0.48 - ETA: 1s - loss: 0.6949 - accuracy: 0.49 - ETA: 1s - loss: 0.6948 - accuracy: 0.49 - ETA: 0s - loss: 0.6946 - accuracy: 0.49 - ETA: 0s - loss: 0.6945 - accuracy: 0.49 - ETA: 0s - loss: 0.6943 - accuracy: 0.49 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6929 - accuracy: 0.50 - 1s 105us/step - loss: 0.6930 - accuracy: 0.5095 - val_loss: 0.6892 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:51 - loss: 0.7018 - accuracy: 0.30 - ETA: 2s - loss: 0.6963 - accuracy: 0.4607 - ETA: 1s - loss: 0.6952 - accuracy: 0.48 - ETA: 1s - loss: 0.6948 - accuracy: 0.49 - ETA: 1s - loss: 0.6946 - accuracy: 0.49 - ETA: 1s - loss: 0.6947 - accuracy: 0.49 - ETA: 1s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - 1s 104us/step - loss: 0.6932 - accuracy: 0.5113 - val_loss: 0.6901 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:50 - loss: 0.6882 - accuracy: 0.60 - ETA: 2s - loss: 0.6935 - accuracy: 0.4862 - ETA: 1s - loss: 0.6931 - accuracy: 0.50 - ETA: 1s - loss: 0.6933 - accuracy: 0.50 - ETA: 1s - loss: 0.6929 - accuracy: 0.50 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6917 - accuracy: 0.52 - ETA: 0s - loss: 0.6915 - accuracy: 0.52 - ETA: 0s - loss: 0.6912 - accuracy: 0.52 - ETA: 0s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6909 - accuracy: 0.53 - ETA: 0s - loss: 0.6910 - accuracy: 0.53 - ETA: 0s - loss: 0.6908 - accuracy: 0.53 - ETA: 0s - loss: 0.6906 - accuracy: 0.53 - ETA: 0s - loss: 0.6905 - accuracy: 0.53 - ETA: 0s - loss: 0.6904 - accuracy: 0.53 - ETA: 0s - loss: 0.6904 - accuracy: 0.53 - ETA: 0s - loss: 0.6903 - accuracy: 0.53 - 1s 104us/step - loss: 0.6901 - accuracy: 0.5330 - val_loss: 0.6829 - val_accuracy: 0.5838\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:52 - loss: 0.6935 - accuracy: 0.50 - ETA: 3s - loss: 0.6943 - accuracy: 0.4818 - ETA: 1s - loss: 0.6944 - accuracy: 0.49 - ETA: 1s - loss: 0.6943 - accuracy: 0.49 - ETA: 1s - loss: 0.6941 - accuracy: 0.50 - ETA: 1s - loss: 0.6939 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - 1s 105us/step - loss: 0.6932 - accuracy: 0.5188 - val_loss: 0.6915 - val_accuracy: 0.5099\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:52 - loss: 0.6955 - accuracy: 0.30 - ETA: 3s - loss: 0.6944 - accuracy: 0.5127 - ETA: 1s - loss: 0.6940 - accuracy: 0.52 - ETA: 1s - loss: 0.6937 - accuracy: 0.52 - ETA: 1s - loss: 0.6940 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.52 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - 1s 106us/step - loss: 0.6924 - accuracy: 0.5179 - val_loss: 0.6875 - val_accuracy: 0.5518\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:50 - loss: 0.6944 - accuracy: 0.50 - ETA: 2s - loss: 0.6947 - accuracy: 0.5036 - ETA: 1s - loss: 0.6947 - accuracy: 0.50 - ETA: 1s - loss: 0.6946 - accuracy: 0.50 - ETA: 1s - loss: 0.6943 - accuracy: 0.50 - ETA: 1s - loss: 0.6942 - accuracy: 0.50 - ETA: 1s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6918 - accuracy: 0.51 - 1s 104us/step - loss: 0.6918 - accuracy: 0.5149 - val_loss: 0.6865 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:50 - loss: 0.6946 - accuracy: 0.50 - ETA: 2s - loss: 0.6927 - accuracy: 0.5304 - ETA: 1s - loss: 0.6927 - accuracy: 0.53 - ETA: 1s - loss: 0.6927 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6922 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6919 - accuracy: 0.53 - ETA: 0s - loss: 0.6918 - accuracy: 0.53 - ETA: 0s - loss: 0.6917 - accuracy: 0.53 - ETA: 0s - loss: 0.6914 - accuracy: 0.53 - ETA: 0s - loss: 0.6912 - accuracy: 0.53 - ETA: 0s - loss: 0.6911 - accuracy: 0.53 - 1s 107us/step - loss: 0.6911 - accuracy: 0.5384 - val_loss: 0.6843 - val_accuracy: 0.5881\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 1:50 - loss: 0.6919 - accuracy: 0.70 - ETA: 2s - loss: 0.6945 - accuracy: 0.4825 - ETA: 1s - loss: 0.6944 - accuracy: 0.49 - ETA: 1s - loss: 0.6941 - accuracy: 0.50 - ETA: 1s - loss: 0.6940 - accuracy: 0.51 - ETA: 1s - loss: 0.6940 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6919 - accuracy: 0.51 - ETA: 0s - loss: 0.6917 - accuracy: 0.51 - 1s 107us/step - loss: 0.6916 - accuracy: 0.5182 - val_loss: 0.6858 - val_accuracy: 0.5298\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:50 - loss: 0.6929 - accuracy: 0.70 - ETA: 2s - loss: 0.6938 - accuracy: 0.5298 - ETA: 1s - loss: 0.6942 - accuracy: 0.52 - ETA: 1s - loss: 0.6938 - accuracy: 0.52 - ETA: 1s - loss: 0.6938 - accuracy: 0.52 - ETA: 1s - loss: 0.6941 - accuracy: 0.51 - ETA: 1s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.52 - ETA: 0s - loss: 0.6936 - accuracy: 0.52 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.52 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - 1s 105us/step - loss: 0.6933 - accuracy: 0.5230 - val_loss: 0.6900 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:50 - loss: 0.6921 - accuracy: 0.60 - ETA: 2s - loss: 0.6947 - accuracy: 0.5105 - ETA: 1s - loss: 0.6944 - accuracy: 0.53 - ETA: 1s - loss: 0.6940 - accuracy: 0.54 - ETA: 1s - loss: 0.6939 - accuracy: 0.54 - ETA: 1s - loss: 0.6940 - accuracy: 0.54 - ETA: 1s - loss: 0.6939 - accuracy: 0.54 - ETA: 0s - loss: 0.6936 - accuracy: 0.54 - ETA: 0s - loss: 0.6933 - accuracy: 0.55 - ETA: 0s - loss: 0.6932 - accuracy: 0.54 - ETA: 0s - loss: 0.6930 - accuracy: 0.54 - ETA: 0s - loss: 0.6930 - accuracy: 0.54 - ETA: 0s - loss: 0.6929 - accuracy: 0.54 - ETA: 0s - loss: 0.6928 - accuracy: 0.54 - ETA: 0s - loss: 0.6925 - accuracy: 0.54 - ETA: 0s - loss: 0.6923 - accuracy: 0.54 - ETA: 0s - loss: 0.6920 - accuracy: 0.55 - ETA: 0s - loss: 0.6918 - accuracy: 0.55 - ETA: 0s - loss: 0.6916 - accuracy: 0.55 - ETA: 0s - loss: 0.6912 - accuracy: 0.55 - ETA: 0s - loss: 0.6912 - accuracy: 0.55 - ETA: 0s - loss: 0.6911 - accuracy: 0.55 - ETA: 0s - loss: 0.6908 - accuracy: 0.55 - ETA: 0s - loss: 0.6906 - accuracy: 0.55 - 1s 107us/step - loss: 0.6906 - accuracy: 0.5552 - val_loss: 0.6850 - val_accuracy: 0.6669\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:24 - loss: 0.6951 - accuracy: 0.50 - ETA: 3s - loss: 0.6936 - accuracy: 0.4962 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6923 - accuracy: 0.53 - ETA: 1s - loss: 0.6916 - accuracy: 0.54 - ETA: 1s - loss: 0.6909 - accuracy: 0.54 - ETA: 1s - loss: 0.6899 - accuracy: 0.55 - ETA: 1s - loss: 0.6884 - accuracy: 0.55 - ETA: 0s - loss: 0.6871 - accuracy: 0.55 - ETA: 0s - loss: 0.6864 - accuracy: 0.55 - ETA: 0s - loss: 0.6848 - accuracy: 0.56 - ETA: 0s - loss: 0.6826 - accuracy: 0.56 - ETA: 0s - loss: 0.6814 - accuracy: 0.57 - ETA: 0s - loss: 0.6797 - accuracy: 0.57 - ETA: 0s - loss: 0.6781 - accuracy: 0.57 - ETA: 0s - loss: 0.6762 - accuracy: 0.57 - ETA: 0s - loss: 0.6745 - accuracy: 0.58 - ETA: 0s - loss: 0.6739 - accuracy: 0.58 - ETA: 0s - loss: 0.6721 - accuracy: 0.58 - ETA: 0s - loss: 0.6712 - accuracy: 0.58 - ETA: 0s - loss: 0.6694 - accuracy: 0.58 - ETA: 0s - loss: 0.6680 - accuracy: 0.59 - ETA: 0s - loss: 0.6673 - accuracy: 0.59 - ETA: 0s - loss: 0.6666 - accuracy: 0.59 - 1s 114us/step - loss: 0.6661 - accuracy: 0.5952 - val_loss: 0.6216 - val_accuracy: 0.7216\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:25 - loss: 0.6859 - accuracy: 0.60 - ETA: 3s - loss: 0.6891 - accuracy: 0.5275 - ETA: 2s - loss: 0.6908 - accuracy: 0.50 - ETA: 1s - loss: 0.6896 - accuracy: 0.50 - ETA: 1s - loss: 0.6885 - accuracy: 0.51 - ETA: 1s - loss: 0.6887 - accuracy: 0.51 - ETA: 1s - loss: 0.6876 - accuracy: 0.51 - ETA: 1s - loss: 0.6871 - accuracy: 0.51 - ETA: 1s - loss: 0.6871 - accuracy: 0.51 - ETA: 1s - loss: 0.6864 - accuracy: 0.52 - ETA: 0s - loss: 0.6858 - accuracy: 0.52 - ETA: 0s - loss: 0.6845 - accuracy: 0.53 - ETA: 0s - loss: 0.6841 - accuracy: 0.53 - ETA: 0s - loss: 0.6829 - accuracy: 0.53 - ETA: 0s - loss: 0.6820 - accuracy: 0.54 - ETA: 0s - loss: 0.6815 - accuracy: 0.54 - ETA: 0s - loss: 0.6796 - accuracy: 0.55 - ETA: 0s - loss: 0.6784 - accuracy: 0.55 - ETA: 0s - loss: 0.6779 - accuracy: 0.55 - ETA: 0s - loss: 0.6768 - accuracy: 0.56 - ETA: 0s - loss: 0.6764 - accuracy: 0.56 - ETA: 0s - loss: 0.6749 - accuracy: 0.57 - ETA: 0s - loss: 0.6736 - accuracy: 0.57 - ETA: 0s - loss: 0.6725 - accuracy: 0.57 - ETA: 0s - loss: 0.6715 - accuracy: 0.58 - ETA: 0s - loss: 0.6708 - accuracy: 0.58 - 2s 118us/step - loss: 0.6705 - accuracy: 0.5899 - val_loss: 0.6354 - val_accuracy: 0.7173\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:24 - loss: 0.6939 - accuracy: 0.30 - ETA: 3s - loss: 0.6935 - accuracy: 0.4923 - ETA: 2s - loss: 0.6934 - accuracy: 0.50 - ETA: 1s - loss: 0.6934 - accuracy: 0.50 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6929 - accuracy: 0.52 - ETA: 1s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6922 - accuracy: 0.53 - ETA: 0s - loss: 0.6912 - accuracy: 0.54 - ETA: 0s - loss: 0.6905 - accuracy: 0.54 - ETA: 0s - loss: 0.6895 - accuracy: 0.55 - ETA: 0s - loss: 0.6884 - accuracy: 0.55 - ETA: 0s - loss: 0.6876 - accuracy: 0.55 - ETA: 0s - loss: 0.6863 - accuracy: 0.55 - ETA: 0s - loss: 0.6857 - accuracy: 0.55 - ETA: 0s - loss: 0.6846 - accuracy: 0.55 - ETA: 0s - loss: 0.6832 - accuracy: 0.56 - ETA: 0s - loss: 0.6815 - accuracy: 0.56 - ETA: 0s - loss: 0.6803 - accuracy: 0.56 - ETA: 0s - loss: 0.6801 - accuracy: 0.56 - ETA: 0s - loss: 0.6785 - accuracy: 0.56 - ETA: 0s - loss: 0.6776 - accuracy: 0.56 - ETA: 0s - loss: 0.6766 - accuracy: 0.56 - ETA: 0s - loss: 0.6756 - accuracy: 0.56 - 1s 117us/step - loss: 0.6755 - accuracy: 0.5700 - val_loss: 0.6365 - val_accuracy: 0.7109\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:26 - loss: 0.6926 - accuracy: 0.60 - ETA: 3s - loss: 0.6935 - accuracy: 0.5077 - ETA: 2s - loss: 0.6931 - accuracy: 0.54 - ETA: 1s - loss: 0.6923 - accuracy: 0.54 - ETA: 1s - loss: 0.6910 - accuracy: 0.55 - ETA: 1s - loss: 0.6890 - accuracy: 0.55 - ETA: 1s - loss: 0.6884 - accuracy: 0.56 - ETA: 1s - loss: 0.6862 - accuracy: 0.56 - ETA: 1s - loss: 0.6853 - accuracy: 0.56 - ETA: 1s - loss: 0.6836 - accuracy: 0.57 - ETA: 0s - loss: 0.6822 - accuracy: 0.57 - ETA: 0s - loss: 0.6804 - accuracy: 0.57 - ETA: 0s - loss: 0.6793 - accuracy: 0.57 - ETA: 0s - loss: 0.6777 - accuracy: 0.58 - ETA: 0s - loss: 0.6758 - accuracy: 0.58 - ETA: 0s - loss: 0.6744 - accuracy: 0.58 - ETA: 0s - loss: 0.6729 - accuracy: 0.59 - ETA: 0s - loss: 0.6708 - accuracy: 0.59 - ETA: 0s - loss: 0.6681 - accuracy: 0.59 - ETA: 0s - loss: 0.6665 - accuracy: 0.59 - ETA: 0s - loss: 0.6652 - accuracy: 0.60 - ETA: 0s - loss: 0.6646 - accuracy: 0.60 - ETA: 0s - loss: 0.6635 - accuracy: 0.60 - ETA: 0s - loss: 0.6628 - accuracy: 0.60 - ETA: 0s - loss: 0.6614 - accuracy: 0.60 - ETA: 0s - loss: 0.6595 - accuracy: 0.60 - 1s 118us/step - loss: 0.6585 - accuracy: 0.6108 - val_loss: 0.6051 - val_accuracy: 0.7209\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:29 - loss: 0.6904 - accuracy: 0.50 - ETA: 3s - loss: 0.6938 - accuracy: 0.5019 - ETA: 2s - loss: 0.6933 - accuracy: 0.49 - ETA: 1s - loss: 0.6929 - accuracy: 0.50 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6921 - accuracy: 0.52 - ETA: 1s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6906 - accuracy: 0.53 - ETA: 1s - loss: 0.6898 - accuracy: 0.52 - ETA: 1s - loss: 0.6889 - accuracy: 0.53 - ETA: 0s - loss: 0.6880 - accuracy: 0.54 - ETA: 0s - loss: 0.6870 - accuracy: 0.54 - ETA: 0s - loss: 0.6857 - accuracy: 0.54 - ETA: 0s - loss: 0.6847 - accuracy: 0.54 - ETA: 0s - loss: 0.6831 - accuracy: 0.55 - ETA: 0s - loss: 0.6816 - accuracy: 0.55 - ETA: 0s - loss: 0.6806 - accuracy: 0.55 - ETA: 0s - loss: 0.6793 - accuracy: 0.55 - ETA: 0s - loss: 0.6778 - accuracy: 0.56 - ETA: 0s - loss: 0.6770 - accuracy: 0.56 - ETA: 0s - loss: 0.6762 - accuracy: 0.56 - ETA: 0s - loss: 0.6755 - accuracy: 0.56 - ETA: 0s - loss: 0.6748 - accuracy: 0.56 - ETA: 0s - loss: 0.6736 - accuracy: 0.57 - ETA: 0s - loss: 0.6719 - accuracy: 0.57 - ETA: 0s - loss: 0.6716 - accuracy: 0.57 - 1s 118us/step - loss: 0.6714 - accuracy: 0.5754 - val_loss: 0.6279 - val_accuracy: 0.7138\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:26 - loss: 0.6941 - accuracy: 0.40 - ETA: 3s - loss: 0.6935 - accuracy: 0.4944 - ETA: 2s - loss: 0.6935 - accuracy: 0.48 - ETA: 1s - loss: 0.6933 - accuracy: 0.50 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6918 - accuracy: 0.53 - ETA: 1s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6902 - accuracy: 0.54 - ETA: 0s - loss: 0.6892 - accuracy: 0.54 - ETA: 0s - loss: 0.6881 - accuracy: 0.55 - ETA: 0s - loss: 0.6867 - accuracy: 0.55 - ETA: 0s - loss: 0.6856 - accuracy: 0.56 - ETA: 0s - loss: 0.6844 - accuracy: 0.56 - ETA: 0s - loss: 0.6836 - accuracy: 0.56 - ETA: 0s - loss: 0.6826 - accuracy: 0.57 - ETA: 0s - loss: 0.6816 - accuracy: 0.57 - ETA: 0s - loss: 0.6801 - accuracy: 0.57 - ETA: 0s - loss: 0.6789 - accuracy: 0.58 - ETA: 0s - loss: 0.6772 - accuracy: 0.58 - ETA: 0s - loss: 0.6763 - accuracy: 0.58 - ETA: 0s - loss: 0.6749 - accuracy: 0.58 - ETA: 0s - loss: 0.6731 - accuracy: 0.59 - ETA: 0s - loss: 0.6722 - accuracy: 0.59 - ETA: 0s - loss: 0.6713 - accuracy: 0.59 - 1s 115us/step - loss: 0.6701 - accuracy: 0.5962 - val_loss: 0.6285 - val_accuracy: 0.6896\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:26 - loss: 0.6963 - accuracy: 0.50 - ETA: 3s - loss: 0.6932 - accuracy: 0.5250 - ETA: 2s - loss: 0.6925 - accuracy: 0.53 - ETA: 1s - loss: 0.6914 - accuracy: 0.54 - ETA: 1s - loss: 0.6906 - accuracy: 0.55 - ETA: 1s - loss: 0.6895 - accuracy: 0.55 - ETA: 1s - loss: 0.6889 - accuracy: 0.55 - ETA: 1s - loss: 0.6882 - accuracy: 0.55 - ETA: 1s - loss: 0.6866 - accuracy: 0.56 - ETA: 1s - loss: 0.6844 - accuracy: 0.57 - ETA: 0s - loss: 0.6828 - accuracy: 0.57 - ETA: 0s - loss: 0.6806 - accuracy: 0.57 - ETA: 0s - loss: 0.6775 - accuracy: 0.58 - ETA: 0s - loss: 0.6760 - accuracy: 0.59 - ETA: 0s - loss: 0.6738 - accuracy: 0.59 - ETA: 0s - loss: 0.6726 - accuracy: 0.59 - ETA: 0s - loss: 0.6705 - accuracy: 0.60 - ETA: 0s - loss: 0.6684 - accuracy: 0.60 - ETA: 0s - loss: 0.6666 - accuracy: 0.60 - ETA: 0s - loss: 0.6657 - accuracy: 0.61 - ETA: 0s - loss: 0.6642 - accuracy: 0.61 - ETA: 0s - loss: 0.6616 - accuracy: 0.61 - ETA: 0s - loss: 0.6588 - accuracy: 0.61 - ETA: 0s - loss: 0.6576 - accuracy: 0.61 - ETA: 0s - loss: 0.6565 - accuracy: 0.61 - ETA: 0s - loss: 0.6551 - accuracy: 0.62 - 2s 119us/step - loss: 0.6535 - accuracy: 0.6237 - val_loss: 0.5984 - val_accuracy: 0.7223\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:28 - loss: 0.6925 - accuracy: 0.40 - ETA: 3s - loss: 0.6921 - accuracy: 0.5157 - ETA: 2s - loss: 0.6896 - accuracy: 0.53 - ETA: 1s - loss: 0.6871 - accuracy: 0.54 - ETA: 1s - loss: 0.6872 - accuracy: 0.54 - ETA: 1s - loss: 0.6849 - accuracy: 0.56 - ETA: 1s - loss: 0.6832 - accuracy: 0.56 - ETA: 1s - loss: 0.6822 - accuracy: 0.57 - ETA: 1s - loss: 0.6813 - accuracy: 0.57 - ETA: 1s - loss: 0.6798 - accuracy: 0.58 - ETA: 0s - loss: 0.6780 - accuracy: 0.58 - ETA: 0s - loss: 0.6767 - accuracy: 0.58 - ETA: 0s - loss: 0.6745 - accuracy: 0.59 - ETA: 0s - loss: 0.6721 - accuracy: 0.59 - ETA: 0s - loss: 0.6702 - accuracy: 0.59 - ETA: 0s - loss: 0.6674 - accuracy: 0.60 - ETA: 0s - loss: 0.6656 - accuracy: 0.60 - ETA: 0s - loss: 0.6640 - accuracy: 0.60 - ETA: 0s - loss: 0.6616 - accuracy: 0.61 - ETA: 0s - loss: 0.6595 - accuracy: 0.61 - ETA: 0s - loss: 0.6578 - accuracy: 0.61 - ETA: 0s - loss: 0.6572 - accuracy: 0.61 - ETA: 0s - loss: 0.6564 - accuracy: 0.61 - ETA: 0s - loss: 0.6545 - accuracy: 0.62 - ETA: 0s - loss: 0.6526 - accuracy: 0.62 - ETA: 0s - loss: 0.6513 - accuracy: 0.62 - 1s 118us/step - loss: 0.6510 - accuracy: 0.6258 - val_loss: 0.5948 - val_accuracy: 0.7166\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:30 - loss: 0.6927 - accuracy: 0.50 - ETA: 4s - loss: 0.6935 - accuracy: 0.5020 - ETA: 2s - loss: 0.6925 - accuracy: 0.51 - ETA: 2s - loss: 0.6923 - accuracy: 0.51 - ETA: 1s - loss: 0.6919 - accuracy: 0.53 - ETA: 1s - loss: 0.6913 - accuracy: 0.54 - ETA: 1s - loss: 0.6902 - accuracy: 0.55 - ETA: 1s - loss: 0.6878 - accuracy: 0.56 - ETA: 1s - loss: 0.6864 - accuracy: 0.57 - ETA: 1s - loss: 0.6847 - accuracy: 0.58 - ETA: 0s - loss: 0.6816 - accuracy: 0.59 - ETA: 0s - loss: 0.6785 - accuracy: 0.59 - ETA: 0s - loss: 0.6763 - accuracy: 0.60 - ETA: 0s - loss: 0.6754 - accuracy: 0.60 - ETA: 0s - loss: 0.6729 - accuracy: 0.60 - ETA: 0s - loss: 0.6714 - accuracy: 0.61 - ETA: 0s - loss: 0.6690 - accuracy: 0.61 - ETA: 0s - loss: 0.6684 - accuracy: 0.61 - ETA: 0s - loss: 0.6663 - accuracy: 0.61 - ETA: 0s - loss: 0.6646 - accuracy: 0.62 - ETA: 0s - loss: 0.6631 - accuracy: 0.62 - ETA: 0s - loss: 0.6628 - accuracy: 0.62 - ETA: 0s - loss: 0.6615 - accuracy: 0.63 - ETA: 0s - loss: 0.6599 - accuracy: 0.63 - ETA: 0s - loss: 0.6589 - accuracy: 0.63 - ETA: 0s - loss: 0.6572 - accuracy: 0.63 - 2s 120us/step - loss: 0.6556 - accuracy: 0.6405 - val_loss: 0.6042 - val_accuracy: 0.7195\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:25 - loss: 0.6948 - accuracy: 0.50 - ETA: 3s - loss: 0.6930 - accuracy: 0.5058 - ETA: 2s - loss: 0.6921 - accuracy: 0.52 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6914 - accuracy: 0.52 - ETA: 1s - loss: 0.6900 - accuracy: 0.54 - ETA: 1s - loss: 0.6893 - accuracy: 0.55 - ETA: 1s - loss: 0.6874 - accuracy: 0.55 - ETA: 1s - loss: 0.6854 - accuracy: 0.56 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6807 - accuracy: 0.57 - ETA: 0s - loss: 0.6784 - accuracy: 0.57 - ETA: 0s - loss: 0.6755 - accuracy: 0.58 - ETA: 0s - loss: 0.6735 - accuracy: 0.58 - ETA: 0s - loss: 0.6713 - accuracy: 0.59 - ETA: 0s - loss: 0.6691 - accuracy: 0.59 - ETA: 0s - loss: 0.6676 - accuracy: 0.60 - ETA: 0s - loss: 0.6662 - accuracy: 0.60 - ETA: 0s - loss: 0.6640 - accuracy: 0.60 - ETA: 0s - loss: 0.6637 - accuracy: 0.60 - ETA: 0s - loss: 0.6618 - accuracy: 0.60 - ETA: 0s - loss: 0.6609 - accuracy: 0.60 - ETA: 0s - loss: 0.6596 - accuracy: 0.61 - ETA: 0s - loss: 0.6582 - accuracy: 0.61 - ETA: 0s - loss: 0.6556 - accuracy: 0.61 - 1s 115us/step - loss: 0.6550 - accuracy: 0.6176 - val_loss: 0.5972 - val_accuracy: 0.7095\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:26 - loss: 0.6946 - accuracy: 0.30 - ETA: 3s - loss: 0.6939 - accuracy: 0.5038 - ETA: 2s - loss: 0.6934 - accuracy: 0.54 - ETA: 1s - loss: 0.6928 - accuracy: 0.54 - ETA: 1s - loss: 0.6920 - accuracy: 0.55 - ETA: 1s - loss: 0.6907 - accuracy: 0.56 - ETA: 1s - loss: 0.6891 - accuracy: 0.57 - ETA: 1s - loss: 0.6874 - accuracy: 0.57 - ETA: 1s - loss: 0.6846 - accuracy: 0.58 - ETA: 0s - loss: 0.6819 - accuracy: 0.59 - ETA: 0s - loss: 0.6803 - accuracy: 0.59 - ETA: 0s - loss: 0.6780 - accuracy: 0.59 - ETA: 0s - loss: 0.6760 - accuracy: 0.60 - ETA: 0s - loss: 0.6735 - accuracy: 0.60 - ETA: 0s - loss: 0.6713 - accuracy: 0.60 - ETA: 0s - loss: 0.6702 - accuracy: 0.60 - ETA: 0s - loss: 0.6686 - accuracy: 0.60 - ETA: 0s - loss: 0.6663 - accuracy: 0.61 - ETA: 0s - loss: 0.6640 - accuracy: 0.61 - ETA: 0s - loss: 0.6624 - accuracy: 0.61 - ETA: 0s - loss: 0.6610 - accuracy: 0.62 - ETA: 0s - loss: 0.6592 - accuracy: 0.62 - ETA: 0s - loss: 0.6580 - accuracy: 0.62 - ETA: 0s - loss: 0.6572 - accuracy: 0.62 - ETA: 0s - loss: 0.6556 - accuracy: 0.63 - ETA: 0s - loss: 0.6544 - accuracy: 0.63 - 2s 120us/step - loss: 0.6535 - accuracy: 0.6332 - val_loss: 0.6025 - val_accuracy: 0.7166\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 45us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:25 - loss: 0.6965 - accuracy: 0.20 - ETA: 3s - loss: 0.6938 - accuracy: 0.5208 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6922 - accuracy: 0.52 - ETA: 1s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6911 - accuracy: 0.53 - ETA: 1s - loss: 0.6894 - accuracy: 0.53 - ETA: 1s - loss: 0.6880 - accuracy: 0.54 - ETA: 1s - loss: 0.6860 - accuracy: 0.55 - ETA: 0s - loss: 0.6838 - accuracy: 0.55 - ETA: 0s - loss: 0.6814 - accuracy: 0.56 - ETA: 0s - loss: 0.6794 - accuracy: 0.56 - ETA: 0s - loss: 0.6776 - accuracy: 0.56 - ETA: 0s - loss: 0.6761 - accuracy: 0.57 - ETA: 0s - loss: 0.6740 - accuracy: 0.57 - ETA: 0s - loss: 0.6721 - accuracy: 0.58 - ETA: 0s - loss: 0.6706 - accuracy: 0.58 - ETA: 0s - loss: 0.6699 - accuracy: 0.58 - ETA: 0s - loss: 0.6680 - accuracy: 0.58 - ETA: 0s - loss: 0.6660 - accuracy: 0.59 - ETA: 0s - loss: 0.6645 - accuracy: 0.59 - ETA: 0s - loss: 0.6632 - accuracy: 0.59 - ETA: 0s - loss: 0.6616 - accuracy: 0.59 - ETA: 0s - loss: 0.6607 - accuracy: 0.59 - ETA: 0s - loss: 0.6597 - accuracy: 0.60 - 2s 119us/step - loss: 0.6591 - accuracy: 0.6027 - val_loss: 0.6132 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:26 - loss: 0.6907 - accuracy: 0.60 - ETA: 3s - loss: 0.6925 - accuracy: 0.5231 - ETA: 2s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6914 - accuracy: 0.52 - ETA: 1s - loss: 0.6915 - accuracy: 0.51 - ETA: 1s - loss: 0.6903 - accuracy: 0.52 - ETA: 1s - loss: 0.6879 - accuracy: 0.53 - ETA: 1s - loss: 0.6868 - accuracy: 0.53 - ETA: 1s - loss: 0.6845 - accuracy: 0.55 - ETA: 1s - loss: 0.6821 - accuracy: 0.56 - ETA: 0s - loss: 0.6790 - accuracy: 0.56 - ETA: 0s - loss: 0.6760 - accuracy: 0.57 - ETA: 0s - loss: 0.6750 - accuracy: 0.57 - ETA: 0s - loss: 0.6720 - accuracy: 0.58 - ETA: 0s - loss: 0.6698 - accuracy: 0.59 - ETA: 0s - loss: 0.6672 - accuracy: 0.59 - ETA: 0s - loss: 0.6629 - accuracy: 0.60 - ETA: 0s - loss: 0.6620 - accuracy: 0.60 - ETA: 0s - loss: 0.6608 - accuracy: 0.61 - ETA: 0s - loss: 0.6586 - accuracy: 0.61 - ETA: 0s - loss: 0.6569 - accuracy: 0.61 - ETA: 0s - loss: 0.6550 - accuracy: 0.62 - ETA: 0s - loss: 0.6533 - accuracy: 0.62 - ETA: 0s - loss: 0.6510 - accuracy: 0.62 - ETA: 0s - loss: 0.6500 - accuracy: 0.63 - ETA: 0s - loss: 0.6483 - accuracy: 0.63 - 2s 119us/step - loss: 0.6471 - accuracy: 0.6349 - val_loss: 0.5897 - val_accuracy: 0.7145\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:25 - loss: 0.6871 - accuracy: 0.70 - ETA: 3s - loss: 0.6921 - accuracy: 0.5321 - ETA: 2s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6921 - accuracy: 0.52 - ETA: 1s - loss: 0.6900 - accuracy: 0.54 - ETA: 1s - loss: 0.6881 - accuracy: 0.55 - ETA: 1s - loss: 0.6852 - accuracy: 0.56 - ETA: 1s - loss: 0.6822 - accuracy: 0.57 - ETA: 1s - loss: 0.6787 - accuracy: 0.58 - ETA: 0s - loss: 0.6760 - accuracy: 0.59 - ETA: 0s - loss: 0.6725 - accuracy: 0.60 - ETA: 0s - loss: 0.6702 - accuracy: 0.60 - ETA: 0s - loss: 0.6683 - accuracy: 0.61 - ETA: 0s - loss: 0.6652 - accuracy: 0.61 - ETA: 0s - loss: 0.6614 - accuracy: 0.62 - ETA: 0s - loss: 0.6581 - accuracy: 0.62 - ETA: 0s - loss: 0.6559 - accuracy: 0.63 - ETA: 0s - loss: 0.6527 - accuracy: 0.63 - ETA: 0s - loss: 0.6517 - accuracy: 0.63 - ETA: 0s - loss: 0.6486 - accuracy: 0.63 - ETA: 0s - loss: 0.6473 - accuracy: 0.64 - ETA: 0s - loss: 0.6453 - accuracy: 0.64 - ETA: 0s - loss: 0.6435 - accuracy: 0.64 - ETA: 0s - loss: 0.6406 - accuracy: 0.64 - ETA: 0s - loss: 0.6394 - accuracy: 0.64 - 1s 116us/step - loss: 0.6394 - accuracy: 0.6500 - val_loss: 0.5871 - val_accuracy: 0.7152\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:26 - loss: 0.6940 - accuracy: 0.50 - ETA: 3s - loss: 0.6916 - accuracy: 0.5558 - ETA: 2s - loss: 0.6934 - accuracy: 0.52 - ETA: 1s - loss: 0.6926 - accuracy: 0.53 - ETA: 1s - loss: 0.6917 - accuracy: 0.53 - ETA: 1s - loss: 0.6905 - accuracy: 0.54 - ETA: 1s - loss: 0.6890 - accuracy: 0.55 - ETA: 1s - loss: 0.6876 - accuracy: 0.55 - ETA: 1s - loss: 0.6854 - accuracy: 0.56 - ETA: 1s - loss: 0.6836 - accuracy: 0.57 - ETA: 0s - loss: 0.6815 - accuracy: 0.57 - ETA: 0s - loss: 0.6798 - accuracy: 0.57 - ETA: 0s - loss: 0.6776 - accuracy: 0.58 - ETA: 0s - loss: 0.6758 - accuracy: 0.58 - ETA: 0s - loss: 0.6738 - accuracy: 0.58 - ETA: 0s - loss: 0.6713 - accuracy: 0.59 - ETA: 0s - loss: 0.6693 - accuracy: 0.59 - ETA: 0s - loss: 0.6673 - accuracy: 0.60 - ETA: 0s - loss: 0.6656 - accuracy: 0.60 - ETA: 0s - loss: 0.6634 - accuracy: 0.60 - ETA: 0s - loss: 0.6615 - accuracy: 0.60 - ETA: 0s - loss: 0.6592 - accuracy: 0.61 - ETA: 0s - loss: 0.6573 - accuracy: 0.61 - ETA: 0s - loss: 0.6549 - accuracy: 0.61 - ETA: 0s - loss: 0.6525 - accuracy: 0.62 - ETA: 0s - loss: 0.6506 - accuracy: 0.62 - 2s 119us/step - loss: 0.6493 - accuracy: 0.6287 - val_loss: 0.5930 - val_accuracy: 0.7216\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:26 - loss: 0.6894 - accuracy: 0.60 - ETA: 3s - loss: 0.6949 - accuracy: 0.4981 - ETA: 2s - loss: 0.6928 - accuracy: 0.53 - ETA: 1s - loss: 0.6918 - accuracy: 0.53 - ETA: 1s - loss: 0.6898 - accuracy: 0.54 - ETA: 1s - loss: 0.6885 - accuracy: 0.55 - ETA: 1s - loss: 0.6860 - accuracy: 0.56 - ETA: 1s - loss: 0.6836 - accuracy: 0.57 - ETA: 1s - loss: 0.6809 - accuracy: 0.58 - ETA: 0s - loss: 0.6788 - accuracy: 0.58 - ETA: 0s - loss: 0.6762 - accuracy: 0.59 - ETA: 0s - loss: 0.6727 - accuracy: 0.60 - ETA: 0s - loss: 0.6700 - accuracy: 0.60 - ETA: 0s - loss: 0.6680 - accuracy: 0.60 - ETA: 0s - loss: 0.6647 - accuracy: 0.61 - ETA: 0s - loss: 0.6625 - accuracy: 0.61 - ETA: 0s - loss: 0.6598 - accuracy: 0.62 - ETA: 0s - loss: 0.6571 - accuracy: 0.62 - ETA: 0s - loss: 0.6567 - accuracy: 0.62 - ETA: 0s - loss: 0.6546 - accuracy: 0.63 - ETA: 0s - loss: 0.6516 - accuracy: 0.63 - ETA: 0s - loss: 0.6494 - accuracy: 0.63 - ETA: 0s - loss: 0.6474 - accuracy: 0.63 - ETA: 0s - loss: 0.6459 - accuracy: 0.64 - ETA: 0s - loss: 0.6445 - accuracy: 0.64 - 1s 115us/step - loss: 0.6436 - accuracy: 0.6443 - val_loss: 0.5870 - val_accuracy: 0.7188\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:29 - loss: 0.6914 - accuracy: 0.60 - ETA: 4s - loss: 0.6937 - accuracy: 0.4980 - ETA: 2s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6913 - accuracy: 0.53 - ETA: 1s - loss: 0.6892 - accuracy: 0.54 - ETA: 1s - loss: 0.6881 - accuracy: 0.55 - ETA: 1s - loss: 0.6867 - accuracy: 0.56 - ETA: 1s - loss: 0.6846 - accuracy: 0.57 - ETA: 1s - loss: 0.6834 - accuracy: 0.57 - ETA: 1s - loss: 0.6800 - accuracy: 0.58 - ETA: 0s - loss: 0.6771 - accuracy: 0.59 - ETA: 0s - loss: 0.6738 - accuracy: 0.60 - ETA: 0s - loss: 0.6694 - accuracy: 0.60 - ETA: 0s - loss: 0.6674 - accuracy: 0.61 - ETA: 0s - loss: 0.6635 - accuracy: 0.61 - ETA: 0s - loss: 0.6614 - accuracy: 0.61 - ETA: 0s - loss: 0.6582 - accuracy: 0.62 - ETA: 0s - loss: 0.6562 - accuracy: 0.62 - ETA: 0s - loss: 0.6546 - accuracy: 0.62 - ETA: 0s - loss: 0.6536 - accuracy: 0.62 - ETA: 0s - loss: 0.6519 - accuracy: 0.63 - ETA: 0s - loss: 0.6504 - accuracy: 0.63 - ETA: 0s - loss: 0.6491 - accuracy: 0.63 - ETA: 0s - loss: 0.6474 - accuracy: 0.63 - ETA: 0s - loss: 0.6461 - accuracy: 0.64 - ETA: 0s - loss: 0.6453 - accuracy: 0.64 - 2s 120us/step - loss: 0.6437 - accuracy: 0.6445 - val_loss: 0.5861 - val_accuracy: 0.7131\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:29 - loss: 0.6937 - accuracy: 0.40 - ETA: 4s - loss: 0.6943 - accuracy: 0.5000 - ETA: 2s - loss: 0.6927 - accuracy: 0.55 - ETA: 1s - loss: 0.6911 - accuracy: 0.54 - ETA: 1s - loss: 0.6892 - accuracy: 0.56 - ETA: 1s - loss: 0.6878 - accuracy: 0.57 - ETA: 1s - loss: 0.6853 - accuracy: 0.58 - ETA: 1s - loss: 0.6835 - accuracy: 0.59 - ETA: 1s - loss: 0.6802 - accuracy: 0.60 - ETA: 1s - loss: 0.6773 - accuracy: 0.61 - ETA: 0s - loss: 0.6747 - accuracy: 0.61 - ETA: 0s - loss: 0.6726 - accuracy: 0.61 - ETA: 0s - loss: 0.6703 - accuracy: 0.61 - ETA: 0s - loss: 0.6678 - accuracy: 0.62 - ETA: 0s - loss: 0.6654 - accuracy: 0.62 - ETA: 0s - loss: 0.6638 - accuracy: 0.62 - ETA: 0s - loss: 0.6604 - accuracy: 0.63 - ETA: 0s - loss: 0.6581 - accuracy: 0.63 - ETA: 0s - loss: 0.6547 - accuracy: 0.63 - ETA: 0s - loss: 0.6540 - accuracy: 0.63 - ETA: 0s - loss: 0.6527 - accuracy: 0.64 - ETA: 0s - loss: 0.6511 - accuracy: 0.64 - ETA: 0s - loss: 0.6498 - accuracy: 0.64 - ETA: 0s - loss: 0.6464 - accuracy: 0.64 - ETA: 0s - loss: 0.6444 - accuracy: 0.64 - 1s 117us/step - loss: 0.6439 - accuracy: 0.6500 - val_loss: 0.5930 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:59 - loss: 0.6939 - accuracy: 0.70 - ETA: 3s - loss: 0.6932 - accuracy: 0.5218 - ETA: 2s - loss: 0.6926 - accuracy: 0.53 - ETA: 1s - loss: 0.6920 - accuracy: 0.53 - ETA: 1s - loss: 0.6916 - accuracy: 0.55 - ETA: 1s - loss: 0.6910 - accuracy: 0.56 - ETA: 1s - loss: 0.6903 - accuracy: 0.57 - ETA: 1s - loss: 0.6895 - accuracy: 0.58 - ETA: 0s - loss: 0.6888 - accuracy: 0.58 - ETA: 0s - loss: 0.6881 - accuracy: 0.59 - ETA: 0s - loss: 0.6871 - accuracy: 0.59 - ETA: 0s - loss: 0.6864 - accuracy: 0.59 - ETA: 0s - loss: 0.6851 - accuracy: 0.60 - ETA: 0s - loss: 0.6842 - accuracy: 0.60 - ETA: 0s - loss: 0.6836 - accuracy: 0.60 - ETA: 0s - loss: 0.6825 - accuracy: 0.60 - ETA: 0s - loss: 0.6818 - accuracy: 0.60 - ETA: 0s - loss: 0.6809 - accuracy: 0.61 - ETA: 0s - loss: 0.6802 - accuracy: 0.61 - ETA: 0s - loss: 0.6793 - accuracy: 0.61 - ETA: 0s - loss: 0.6788 - accuracy: 0.62 - ETA: 0s - loss: 0.6780 - accuracy: 0.62 - ETA: 0s - loss: 0.6774 - accuracy: 0.62 - 1s 106us/step - loss: 0.6768 - accuracy: 0.6256 - val_loss: 0.6592 - val_accuracy: 0.7024\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:01 - loss: 0.6958 - accuracy: 0.40 - ETA: 3s - loss: 0.6936 - accuracy: 0.5000 - ETA: 2s - loss: 0.6931 - accuracy: 0.53 - ETA: 1s - loss: 0.6925 - accuracy: 0.55 - ETA: 1s - loss: 0.6919 - accuracy: 0.57 - ETA: 1s - loss: 0.6914 - accuracy: 0.57 - ETA: 1s - loss: 0.6909 - accuracy: 0.58 - ETA: 1s - loss: 0.6902 - accuracy: 0.58 - ETA: 0s - loss: 0.6893 - accuracy: 0.59 - ETA: 0s - loss: 0.6891 - accuracy: 0.59 - ETA: 0s - loss: 0.6887 - accuracy: 0.59 - ETA: 0s - loss: 0.6877 - accuracy: 0.59 - ETA: 0s - loss: 0.6868 - accuracy: 0.60 - ETA: 0s - loss: 0.6863 - accuracy: 0.60 - ETA: 0s - loss: 0.6857 - accuracy: 0.60 - ETA: 0s - loss: 0.6853 - accuracy: 0.61 - ETA: 0s - loss: 0.6844 - accuracy: 0.61 - ETA: 0s - loss: 0.6837 - accuracy: 0.61 - ETA: 0s - loss: 0.6833 - accuracy: 0.61 - ETA: 0s - loss: 0.6827 - accuracy: 0.61 - ETA: 0s - loss: 0.6822 - accuracy: 0.62 - ETA: 0s - loss: 0.6817 - accuracy: 0.62 - ETA: 0s - loss: 0.6811 - accuracy: 0.62 - ETA: 0s - loss: 0.6803 - accuracy: 0.62 - 1s 108us/step - loss: 0.6802 - accuracy: 0.6243 - val_loss: 0.6650 - val_accuracy: 0.6697\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:01 - loss: 0.6944 - accuracy: 0.30 - ETA: 3s - loss: 0.6932 - accuracy: 0.5327 - ETA: 2s - loss: 0.6927 - accuracy: 0.55 - ETA: 1s - loss: 0.6916 - accuracy: 0.55 - ETA: 1s - loss: 0.6912 - accuracy: 0.56 - ETA: 1s - loss: 0.6910 - accuracy: 0.57 - ETA: 1s - loss: 0.6905 - accuracy: 0.57 - ETA: 1s - loss: 0.6898 - accuracy: 0.58 - ETA: 0s - loss: 0.6889 - accuracy: 0.58 - ETA: 0s - loss: 0.6882 - accuracy: 0.58 - ETA: 0s - loss: 0.6875 - accuracy: 0.58 - ETA: 0s - loss: 0.6868 - accuracy: 0.59 - ETA: 0s - loss: 0.6860 - accuracy: 0.59 - ETA: 0s - loss: 0.6851 - accuracy: 0.59 - ETA: 0s - loss: 0.6847 - accuracy: 0.60 - ETA: 0s - loss: 0.6842 - accuracy: 0.60 - ETA: 0s - loss: 0.6837 - accuracy: 0.60 - ETA: 0s - loss: 0.6830 - accuracy: 0.60 - ETA: 0s - loss: 0.6823 - accuracy: 0.60 - ETA: 0s - loss: 0.6820 - accuracy: 0.60 - ETA: 0s - loss: 0.6815 - accuracy: 0.60 - ETA: 0s - loss: 0.6810 - accuracy: 0.61 - ETA: 0s - loss: 0.6810 - accuracy: 0.61 - ETA: 0s - loss: 0.6805 - accuracy: 0.61 - 1s 110us/step - loss: 0.6800 - accuracy: 0.6169 - val_loss: 0.6653 - val_accuracy: 0.7166\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 2:00 - loss: 0.6860 - accuracy: 0.70 - ETA: 3s - loss: 0.6930 - accuracy: 0.5145 - ETA: 2s - loss: 0.6922 - accuracy: 0.53 - ETA: 1s - loss: 0.6916 - accuracy: 0.56 - ETA: 1s - loss: 0.6909 - accuracy: 0.56 - ETA: 1s - loss: 0.6902 - accuracy: 0.57 - ETA: 1s - loss: 0.6894 - accuracy: 0.57 - ETA: 1s - loss: 0.6887 - accuracy: 0.58 - ETA: 0s - loss: 0.6881 - accuracy: 0.58 - ETA: 0s - loss: 0.6869 - accuracy: 0.59 - ETA: 0s - loss: 0.6851 - accuracy: 0.60 - ETA: 0s - loss: 0.6845 - accuracy: 0.60 - ETA: 0s - loss: 0.6835 - accuracy: 0.60 - ETA: 0s - loss: 0.6826 - accuracy: 0.60 - ETA: 0s - loss: 0.6818 - accuracy: 0.60 - ETA: 0s - loss: 0.6807 - accuracy: 0.60 - ETA: 0s - loss: 0.6796 - accuracy: 0.61 - ETA: 0s - loss: 0.6781 - accuracy: 0.61 - ETA: 0s - loss: 0.6772 - accuracy: 0.61 - ETA: 0s - loss: 0.6766 - accuracy: 0.61 - ETA: 0s - loss: 0.6756 - accuracy: 0.61 - ETA: 0s - loss: 0.6750 - accuracy: 0.61 - ETA: 0s - loss: 0.6742 - accuracy: 0.61 - 1s 106us/step - loss: 0.6739 - accuracy: 0.6186 - val_loss: 0.6489 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:00 - loss: 0.6973 - accuracy: 0.30 - ETA: 3s - loss: 0.6924 - accuracy: 0.5164 - ETA: 2s - loss: 0.6905 - accuracy: 0.52 - ETA: 1s - loss: 0.6903 - accuracy: 0.52 - ETA: 1s - loss: 0.6904 - accuracy: 0.51 - ETA: 1s - loss: 0.6893 - accuracy: 0.52 - ETA: 1s - loss: 0.6888 - accuracy: 0.52 - ETA: 1s - loss: 0.6881 - accuracy: 0.52 - ETA: 0s - loss: 0.6876 - accuracy: 0.52 - ETA: 0s - loss: 0.6868 - accuracy: 0.52 - ETA: 0s - loss: 0.6863 - accuracy: 0.52 - ETA: 0s - loss: 0.6856 - accuracy: 0.52 - ETA: 0s - loss: 0.6847 - accuracy: 0.52 - ETA: 0s - loss: 0.6848 - accuracy: 0.52 - ETA: 0s - loss: 0.6845 - accuracy: 0.52 - ETA: 0s - loss: 0.6840 - accuracy: 0.52 - ETA: 0s - loss: 0.6839 - accuracy: 0.52 - ETA: 0s - loss: 0.6837 - accuracy: 0.52 - ETA: 0s - loss: 0.6836 - accuracy: 0.52 - ETA: 0s - loss: 0.6833 - accuracy: 0.53 - ETA: 0s - loss: 0.6828 - accuracy: 0.53 - ETA: 0s - loss: 0.6826 - accuracy: 0.53 - ETA: 0s - loss: 0.6823 - accuracy: 0.53 - ETA: 0s - loss: 0.6820 - accuracy: 0.54 - 1s 109us/step - loss: 0.6818 - accuracy: 0.5423 - val_loss: 0.6683 - val_accuracy: 0.6271\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:00 - loss: 0.6905 - accuracy: 0.60 - ETA: 3s - loss: 0.6903 - accuracy: 0.5250 - ETA: 2s - loss: 0.6900 - accuracy: 0.52 - ETA: 1s - loss: 0.6902 - accuracy: 0.52 - ETA: 1s - loss: 0.6896 - accuracy: 0.52 - ETA: 1s - loss: 0.6889 - accuracy: 0.52 - ETA: 1s - loss: 0.6883 - accuracy: 0.51 - ETA: 1s - loss: 0.6877 - accuracy: 0.51 - ETA: 0s - loss: 0.6875 - accuracy: 0.51 - ETA: 0s - loss: 0.6873 - accuracy: 0.51 - ETA: 0s - loss: 0.6871 - accuracy: 0.51 - ETA: 0s - loss: 0.6870 - accuracy: 0.51 - ETA: 0s - loss: 0.6868 - accuracy: 0.52 - ETA: 0s - loss: 0.6859 - accuracy: 0.52 - ETA: 0s - loss: 0.6856 - accuracy: 0.52 - ETA: 0s - loss: 0.6853 - accuracy: 0.52 - ETA: 0s - loss: 0.6848 - accuracy: 0.53 - ETA: 0s - loss: 0.6844 - accuracy: 0.53 - ETA: 0s - loss: 0.6840 - accuracy: 0.53 - ETA: 0s - loss: 0.6835 - accuracy: 0.53 - ETA: 0s - loss: 0.6830 - accuracy: 0.53 - ETA: 0s - loss: 0.6822 - accuracy: 0.54 - ETA: 0s - loss: 0.6814 - accuracy: 0.54 - 1s 107us/step - loss: 0.6814 - accuracy: 0.5439 - val_loss: 0.6715 - val_accuracy: 0.6328\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:57 - loss: 0.6939 - accuracy: 0.50 - ETA: 3s - loss: 0.6904 - accuracy: 0.5296 - ETA: 2s - loss: 0.6892 - accuracy: 0.51 - ETA: 1s - loss: 0.6883 - accuracy: 0.52 - ETA: 1s - loss: 0.6884 - accuracy: 0.51 - ETA: 1s - loss: 0.6870 - accuracy: 0.51 - ETA: 1s - loss: 0.6869 - accuracy: 0.51 - ETA: 1s - loss: 0.6864 - accuracy: 0.52 - ETA: 0s - loss: 0.6863 - accuracy: 0.52 - ETA: 0s - loss: 0.6857 - accuracy: 0.53 - ETA: 0s - loss: 0.6854 - accuracy: 0.53 - ETA: 0s - loss: 0.6844 - accuracy: 0.54 - ETA: 0s - loss: 0.6843 - accuracy: 0.55 - ETA: 0s - loss: 0.6832 - accuracy: 0.55 - ETA: 0s - loss: 0.6822 - accuracy: 0.56 - ETA: 0s - loss: 0.6814 - accuracy: 0.56 - ETA: 0s - loss: 0.6809 - accuracy: 0.56 - ETA: 0s - loss: 0.6800 - accuracy: 0.57 - ETA: 0s - loss: 0.6792 - accuracy: 0.57 - ETA: 0s - loss: 0.6793 - accuracy: 0.57 - ETA: 0s - loss: 0.6784 - accuracy: 0.57 - ETA: 0s - loss: 0.6781 - accuracy: 0.58 - ETA: 0s - loss: 0.6773 - accuracy: 0.58 - ETA: 0s - loss: 0.6770 - accuracy: 0.58 - 1s 109us/step - loss: 0.6766 - accuracy: 0.5875 - val_loss: 0.6565 - val_accuracy: 0.6626\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:59 - loss: 0.6963 - accuracy: 0.50 - ETA: 3s - loss: 0.6912 - accuracy: 0.5250 - ETA: 2s - loss: 0.6905 - accuracy: 0.52 - ETA: 1s - loss: 0.6899 - accuracy: 0.52 - ETA: 1s - loss: 0.6882 - accuracy: 0.54 - ETA: 1s - loss: 0.6881 - accuracy: 0.55 - ETA: 1s - loss: 0.6866 - accuracy: 0.56 - ETA: 1s - loss: 0.6854 - accuracy: 0.56 - ETA: 0s - loss: 0.6840 - accuracy: 0.57 - ETA: 0s - loss: 0.6823 - accuracy: 0.58 - ETA: 0s - loss: 0.6809 - accuracy: 0.58 - ETA: 0s - loss: 0.6795 - accuracy: 0.59 - ETA: 0s - loss: 0.6775 - accuracy: 0.59 - ETA: 0s - loss: 0.6756 - accuracy: 0.60 - ETA: 0s - loss: 0.6739 - accuracy: 0.60 - ETA: 0s - loss: 0.6725 - accuracy: 0.61 - ETA: 0s - loss: 0.6716 - accuracy: 0.61 - ETA: 0s - loss: 0.6704 - accuracy: 0.61 - ETA: 0s - loss: 0.6693 - accuracy: 0.61 - ETA: 0s - loss: 0.6677 - accuracy: 0.61 - ETA: 0s - loss: 0.6671 - accuracy: 0.61 - ETA: 0s - loss: 0.6660 - accuracy: 0.61 - ETA: 0s - loss: 0.6655 - accuracy: 0.61 - ETA: 0s - loss: 0.6649 - accuracy: 0.62 - ETA: 0s - loss: 0.6637 - accuracy: 0.62 - 1s 112us/step - loss: 0.6636 - accuracy: 0.6234 - val_loss: 0.6296 - val_accuracy: 0.7145\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:57 - loss: 0.6894 - accuracy: 0.70 - ETA: 3s - loss: 0.6939 - accuracy: 0.5073 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6914 - accuracy: 0.52 - ETA: 1s - loss: 0.6914 - accuracy: 0.51 - ETA: 1s - loss: 0.6905 - accuracy: 0.52 - ETA: 1s - loss: 0.6897 - accuracy: 0.53 - ETA: 1s - loss: 0.6890 - accuracy: 0.54 - ETA: 0s - loss: 0.6877 - accuracy: 0.55 - ETA: 0s - loss: 0.6868 - accuracy: 0.55 - ETA: 0s - loss: 0.6855 - accuracy: 0.56 - ETA: 0s - loss: 0.6842 - accuracy: 0.57 - ETA: 0s - loss: 0.6828 - accuracy: 0.57 - ETA: 0s - loss: 0.6818 - accuracy: 0.58 - ETA: 0s - loss: 0.6797 - accuracy: 0.58 - ETA: 0s - loss: 0.6790 - accuracy: 0.59 - ETA: 0s - loss: 0.6777 - accuracy: 0.59 - ETA: 0s - loss: 0.6771 - accuracy: 0.59 - ETA: 0s - loss: 0.6765 - accuracy: 0.59 - ETA: 0s - loss: 0.6753 - accuracy: 0.59 - ETA: 0s - loss: 0.6742 - accuracy: 0.60 - ETA: 0s - loss: 0.6728 - accuracy: 0.60 - ETA: 0s - loss: 0.6718 - accuracy: 0.60 - ETA: 0s - loss: 0.6707 - accuracy: 0.60 - 1s 108us/step - loss: 0.6707 - accuracy: 0.6080 - val_loss: 0.6408 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:00 - loss: 0.6921 - accuracy: 0.50 - ETA: 3s - loss: 0.6925 - accuracy: 0.5286 - ETA: 2s - loss: 0.6904 - accuracy: 0.54 - ETA: 1s - loss: 0.6897 - accuracy: 0.54 - ETA: 1s - loss: 0.6875 - accuracy: 0.55 - ETA: 1s - loss: 0.6850 - accuracy: 0.57 - ETA: 1s - loss: 0.6836 - accuracy: 0.57 - ETA: 1s - loss: 0.6818 - accuracy: 0.58 - ETA: 0s - loss: 0.6799 - accuracy: 0.58 - ETA: 0s - loss: 0.6783 - accuracy: 0.59 - ETA: 0s - loss: 0.6767 - accuracy: 0.59 - ETA: 0s - loss: 0.6752 - accuracy: 0.60 - ETA: 0s - loss: 0.6737 - accuracy: 0.60 - ETA: 0s - loss: 0.6725 - accuracy: 0.61 - ETA: 0s - loss: 0.6714 - accuracy: 0.61 - ETA: 0s - loss: 0.6694 - accuracy: 0.61 - ETA: 0s - loss: 0.6683 - accuracy: 0.62 - ETA: 0s - loss: 0.6669 - accuracy: 0.62 - ETA: 0s - loss: 0.6651 - accuracy: 0.62 - ETA: 0s - loss: 0.6641 - accuracy: 0.62 - ETA: 0s - loss: 0.6633 - accuracy: 0.62 - ETA: 0s - loss: 0.6621 - accuracy: 0.63 - ETA: 0s - loss: 0.6606 - accuracy: 0.63 - ETA: 0s - loss: 0.6599 - accuracy: 0.63 - 1s 108us/step - loss: 0.6598 - accuracy: 0.6332 - val_loss: 0.6275 - val_accuracy: 0.7067\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:57 - loss: 0.6951 - accuracy: 0.40 - ETA: 3s - loss: 0.6928 - accuracy: 0.5309 - ETA: 2s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6914 - accuracy: 0.51 - ETA: 1s - loss: 0.6905 - accuracy: 0.51 - ETA: 1s - loss: 0.6893 - accuracy: 0.52 - ETA: 1s - loss: 0.6881 - accuracy: 0.53 - ETA: 1s - loss: 0.6872 - accuracy: 0.54 - ETA: 0s - loss: 0.6852 - accuracy: 0.55 - ETA: 0s - loss: 0.6845 - accuracy: 0.55 - ETA: 0s - loss: 0.6836 - accuracy: 0.55 - ETA: 0s - loss: 0.6827 - accuracy: 0.56 - ETA: 0s - loss: 0.6813 - accuracy: 0.56 - ETA: 0s - loss: 0.6801 - accuracy: 0.57 - ETA: 0s - loss: 0.6788 - accuracy: 0.57 - ETA: 0s - loss: 0.6770 - accuracy: 0.58 - ETA: 0s - loss: 0.6763 - accuracy: 0.58 - ETA: 0s - loss: 0.6756 - accuracy: 0.58 - ETA: 0s - loss: 0.6746 - accuracy: 0.59 - ETA: 0s - loss: 0.6733 - accuracy: 0.59 - ETA: 0s - loss: 0.6722 - accuracy: 0.59 - ETA: 0s - loss: 0.6705 - accuracy: 0.60 - ETA: 0s - loss: 0.6699 - accuracy: 0.60 - ETA: 0s - loss: 0.6689 - accuracy: 0.60 - 1s 107us/step - loss: 0.6690 - accuracy: 0.6069 - val_loss: 0.6379 - val_accuracy: 0.7067\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:57 - loss: 0.6938 - accuracy: 0.60 - ETA: 3s - loss: 0.6932 - accuracy: 0.5109 - ETA: 2s - loss: 0.6928 - accuracy: 0.49 - ETA: 1s - loss: 0.6912 - accuracy: 0.52 - ETA: 1s - loss: 0.6893 - accuracy: 0.54 - ETA: 1s - loss: 0.6888 - accuracy: 0.55 - ETA: 1s - loss: 0.6872 - accuracy: 0.57 - ETA: 1s - loss: 0.6860 - accuracy: 0.57 - ETA: 1s - loss: 0.6853 - accuracy: 0.57 - ETA: 0s - loss: 0.6847 - accuracy: 0.57 - ETA: 0s - loss: 0.6841 - accuracy: 0.57 - ETA: 0s - loss: 0.6832 - accuracy: 0.58 - ETA: 0s - loss: 0.6816 - accuracy: 0.59 - ETA: 0s - loss: 0.6799 - accuracy: 0.59 - ETA: 0s - loss: 0.6791 - accuracy: 0.59 - ETA: 0s - loss: 0.6780 - accuracy: 0.60 - ETA: 0s - loss: 0.6770 - accuracy: 0.60 - ETA: 0s - loss: 0.6762 - accuracy: 0.60 - ETA: 0s - loss: 0.6753 - accuracy: 0.60 - ETA: 0s - loss: 0.6744 - accuracy: 0.61 - ETA: 0s - loss: 0.6739 - accuracy: 0.61 - ETA: 0s - loss: 0.6731 - accuracy: 0.61 - ETA: 0s - loss: 0.6725 - accuracy: 0.61 - ETA: 0s - loss: 0.6714 - accuracy: 0.61 - ETA: 0s - loss: 0.6706 - accuracy: 0.61 - 1s 111us/step - loss: 0.6705 - accuracy: 0.6190 - val_loss: 0.6479 - val_accuracy: 0.6861\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:57 - loss: 0.6969 - accuracy: 0.50 - ETA: 3s - loss: 0.6939 - accuracy: 0.4981 - ETA: 2s - loss: 0.6925 - accuracy: 0.50 - ETA: 1s - loss: 0.6901 - accuracy: 0.52 - ETA: 1s - loss: 0.6875 - accuracy: 0.54 - ETA: 1s - loss: 0.6864 - accuracy: 0.54 - ETA: 1s - loss: 0.6845 - accuracy: 0.55 - ETA: 1s - loss: 0.6823 - accuracy: 0.57 - ETA: 0s - loss: 0.6810 - accuracy: 0.58 - ETA: 0s - loss: 0.6790 - accuracy: 0.58 - ETA: 0s - loss: 0.6766 - accuracy: 0.59 - ETA: 0s - loss: 0.6747 - accuracy: 0.60 - ETA: 0s - loss: 0.6722 - accuracy: 0.60 - ETA: 0s - loss: 0.6705 - accuracy: 0.60 - ETA: 0s - loss: 0.6690 - accuracy: 0.61 - ETA: 0s - loss: 0.6673 - accuracy: 0.61 - ETA: 0s - loss: 0.6656 - accuracy: 0.62 - ETA: 0s - loss: 0.6644 - accuracy: 0.62 - ETA: 0s - loss: 0.6631 - accuracy: 0.62 - ETA: 0s - loss: 0.6620 - accuracy: 0.62 - ETA: 0s - loss: 0.6613 - accuracy: 0.62 - ETA: 0s - loss: 0.6602 - accuracy: 0.62 - ETA: 0s - loss: 0.6591 - accuracy: 0.62 - 1s 107us/step - loss: 0.6579 - accuracy: 0.6326 - val_loss: 0.6201 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:59 - loss: 0.6947 - accuracy: 0.60 - ETA: 3s - loss: 0.6916 - accuracy: 0.5321 - ETA: 2s - loss: 0.6906 - accuracy: 0.53 - ETA: 1s - loss: 0.6889 - accuracy: 0.54 - ETA: 1s - loss: 0.6868 - accuracy: 0.55 - ETA: 1s - loss: 0.6844 - accuracy: 0.57 - ETA: 1s - loss: 0.6825 - accuracy: 0.57 - ETA: 1s - loss: 0.6805 - accuracy: 0.58 - ETA: 0s - loss: 0.6786 - accuracy: 0.59 - ETA: 0s - loss: 0.6770 - accuracy: 0.60 - ETA: 0s - loss: 0.6752 - accuracy: 0.60 - ETA: 0s - loss: 0.6736 - accuracy: 0.60 - ETA: 0s - loss: 0.6719 - accuracy: 0.61 - ETA: 0s - loss: 0.6699 - accuracy: 0.61 - ETA: 0s - loss: 0.6687 - accuracy: 0.61 - ETA: 0s - loss: 0.6674 - accuracy: 0.62 - ETA: 0s - loss: 0.6660 - accuracy: 0.62 - ETA: 0s - loss: 0.6651 - accuracy: 0.62 - ETA: 0s - loss: 0.6637 - accuracy: 0.63 - ETA: 0s - loss: 0.6622 - accuracy: 0.63 - ETA: 0s - loss: 0.6609 - accuracy: 0.63 - ETA: 0s - loss: 0.6595 - accuracy: 0.63 - ETA: 0s - loss: 0.6591 - accuracy: 0.63 - 1s 107us/step - loss: 0.6578 - accuracy: 0.6380 - val_loss: 0.6216 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 44us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:02 - loss: 0.6952 - accuracy: 0.60 - ETA: 3s - loss: 0.6918 - accuracy: 0.5148 - ETA: 2s - loss: 0.6878 - accuracy: 0.52 - ETA: 1s - loss: 0.6870 - accuracy: 0.53 - ETA: 1s - loss: 0.6860 - accuracy: 0.53 - ETA: 1s - loss: 0.6831 - accuracy: 0.54 - ETA: 1s - loss: 0.6821 - accuracy: 0.54 - ETA: 1s - loss: 0.6811 - accuracy: 0.55 - ETA: 0s - loss: 0.6805 - accuracy: 0.55 - ETA: 0s - loss: 0.6798 - accuracy: 0.55 - ETA: 0s - loss: 0.6796 - accuracy: 0.56 - ETA: 0s - loss: 0.6782 - accuracy: 0.56 - ETA: 0s - loss: 0.6774 - accuracy: 0.57 - ETA: 0s - loss: 0.6757 - accuracy: 0.58 - ETA: 0s - loss: 0.6742 - accuracy: 0.58 - ETA: 0s - loss: 0.6737 - accuracy: 0.58 - ETA: 0s - loss: 0.6727 - accuracy: 0.59 - ETA: 0s - loss: 0.6719 - accuracy: 0.59 - ETA: 0s - loss: 0.6709 - accuracy: 0.59 - ETA: 0s - loss: 0.6699 - accuracy: 0.60 - ETA: 0s - loss: 0.6684 - accuracy: 0.60 - ETA: 0s - loss: 0.6676 - accuracy: 0.60 - ETA: 0s - loss: 0.6668 - accuracy: 0.61 - ETA: 0s - loss: 0.6656 - accuracy: 0.61 - 1s 109us/step - loss: 0.6652 - accuracy: 0.6138 - val_loss: 0.6347 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 1:59 - loss: 0.6897 - accuracy: 0.70 - ETA: 3s - loss: 0.6927 - accuracy: 0.5393 - ETA: 2s - loss: 0.6912 - accuracy: 0.54 - ETA: 1s - loss: 0.6881 - accuracy: 0.56 - ETA: 1s - loss: 0.6868 - accuracy: 0.56 - ETA: 1s - loss: 0.6852 - accuracy: 0.57 - ETA: 1s - loss: 0.6825 - accuracy: 0.58 - ETA: 1s - loss: 0.6811 - accuracy: 0.59 - ETA: 0s - loss: 0.6795 - accuracy: 0.59 - ETA: 0s - loss: 0.6781 - accuracy: 0.60 - ETA: 0s - loss: 0.6767 - accuracy: 0.60 - ETA: 0s - loss: 0.6752 - accuracy: 0.60 - ETA: 0s - loss: 0.6738 - accuracy: 0.61 - ETA: 0s - loss: 0.6720 - accuracy: 0.61 - ETA: 0s - loss: 0.6705 - accuracy: 0.61 - ETA: 0s - loss: 0.6692 - accuracy: 0.62 - ETA: 0s - loss: 0.6681 - accuracy: 0.62 - ETA: 0s - loss: 0.6671 - accuracy: 0.62 - ETA: 0s - loss: 0.6659 - accuracy: 0.62 - ETA: 0s - loss: 0.6650 - accuracy: 0.62 - ETA: 0s - loss: 0.6634 - accuracy: 0.62 - ETA: 0s - loss: 0.6624 - accuracy: 0.62 - ETA: 0s - loss: 0.6613 - accuracy: 0.63 - ETA: 0s - loss: 0.6606 - accuracy: 0.63 - 1s 107us/step - loss: 0.6606 - accuracy: 0.6313 - val_loss: 0.6280 - val_accuracy: 0.7017\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:59 - loss: 0.6933 - accuracy: 0.60 - ETA: 3s - loss: 0.6923 - accuracy: 0.5218 - ETA: 2s - loss: 0.6895 - accuracy: 0.53 - ETA: 1s - loss: 0.6857 - accuracy: 0.54 - ETA: 1s - loss: 0.6834 - accuracy: 0.55 - ETA: 1s - loss: 0.6808 - accuracy: 0.56 - ETA: 1s - loss: 0.6790 - accuracy: 0.56 - ETA: 1s - loss: 0.6777 - accuracy: 0.57 - ETA: 0s - loss: 0.6758 - accuracy: 0.58 - ETA: 0s - loss: 0.6742 - accuracy: 0.59 - ETA: 0s - loss: 0.6723 - accuracy: 0.59 - ETA: 0s - loss: 0.6710 - accuracy: 0.60 - ETA: 0s - loss: 0.6693 - accuracy: 0.60 - ETA: 0s - loss: 0.6674 - accuracy: 0.61 - ETA: 0s - loss: 0.6663 - accuracy: 0.61 - ETA: 0s - loss: 0.6648 - accuracy: 0.61 - ETA: 0s - loss: 0.6643 - accuracy: 0.61 - ETA: 0s - loss: 0.6629 - accuracy: 0.62 - ETA: 0s - loss: 0.6617 - accuracy: 0.62 - ETA: 0s - loss: 0.6614 - accuracy: 0.62 - ETA: 0s - loss: 0.6599 - accuracy: 0.62 - ETA: 0s - loss: 0.6594 - accuracy: 0.62 - ETA: 0s - loss: 0.6580 - accuracy: 0.62 - ETA: 0s - loss: 0.6568 - accuracy: 0.63 - 1s 110us/step - loss: 0.6559 - accuracy: 0.6340 - val_loss: 0.6216 - val_accuracy: 0.7095\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:01 - loss: 0.6967 - accuracy: 0.50 - ETA: 3s - loss: 0.6922 - accuracy: 0.5278 - ETA: 2s - loss: 0.6900 - accuracy: 0.52 - ETA: 1s - loss: 0.6900 - accuracy: 0.53 - ETA: 1s - loss: 0.6884 - accuracy: 0.54 - ETA: 1s - loss: 0.6869 - accuracy: 0.55 - ETA: 1s - loss: 0.6851 - accuracy: 0.56 - ETA: 1s - loss: 0.6827 - accuracy: 0.57 - ETA: 0s - loss: 0.6806 - accuracy: 0.57 - ETA: 0s - loss: 0.6792 - accuracy: 0.58 - ETA: 0s - loss: 0.6772 - accuracy: 0.59 - ETA: 0s - loss: 0.6752 - accuracy: 0.59 - ETA: 0s - loss: 0.6741 - accuracy: 0.59 - ETA: 0s - loss: 0.6725 - accuracy: 0.60 - ETA: 0s - loss: 0.6714 - accuracy: 0.60 - ETA: 0s - loss: 0.6703 - accuracy: 0.61 - ETA: 0s - loss: 0.6688 - accuracy: 0.61 - ETA: 0s - loss: 0.6675 - accuracy: 0.61 - ETA: 0s - loss: 0.6661 - accuracy: 0.61 - ETA: 0s - loss: 0.6657 - accuracy: 0.62 - ETA: 0s - loss: 0.6646 - accuracy: 0.62 - ETA: 0s - loss: 0.6637 - accuracy: 0.62 - ETA: 0s - loss: 0.6628 - accuracy: 0.62 - ETA: 0s - loss: 0.6616 - accuracy: 0.62 - 1s 109us/step - loss: 0.6614 - accuracy: 0.6277 - val_loss: 0.6335 - val_accuracy: 0.7024\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:24 - loss: 0.6932 - accuracy: 0.50 - ETA: 3s - loss: 0.6933 - accuracy: 0.5077 - ETA: 2s - loss: 0.6929 - accuracy: 0.53 - ETA: 1s - loss: 0.6925 - accuracy: 0.53 - ETA: 1s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6888 - accuracy: 0.53 - ETA: 1s - loss: 0.6864 - accuracy: 0.54 - ETA: 1s - loss: 0.6814 - accuracy: 0.55 - ETA: 1s - loss: 0.6793 - accuracy: 0.56 - ETA: 1s - loss: 0.6767 - accuracy: 0.56 - ETA: 0s - loss: 0.6729 - accuracy: 0.57 - ETA: 0s - loss: 0.6689 - accuracy: 0.58 - ETA: 0s - loss: 0.6671 - accuracy: 0.58 - ETA: 0s - loss: 0.6652 - accuracy: 0.59 - ETA: 0s - loss: 0.6646 - accuracy: 0.59 - ETA: 0s - loss: 0.6629 - accuracy: 0.59 - ETA: 0s - loss: 0.6607 - accuracy: 0.60 - ETA: 0s - loss: 0.6597 - accuracy: 0.60 - ETA: 0s - loss: 0.6582 - accuracy: 0.60 - ETA: 0s - loss: 0.6575 - accuracy: 0.60 - ETA: 0s - loss: 0.6568 - accuracy: 0.60 - ETA: 0s - loss: 0.6546 - accuracy: 0.60 - ETA: 0s - loss: 0.6538 - accuracy: 0.60 - ETA: 0s - loss: 0.6527 - accuracy: 0.61 - ETA: 0s - loss: 0.6511 - accuracy: 0.61 - ETA: 0s - loss: 0.6501 - accuracy: 0.61 - 1s 117us/step - loss: 0.6501 - accuracy: 0.6140 - val_loss: 0.5982 - val_accuracy: 0.7202\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:26 - loss: 0.6938 - accuracy: 0.50 - ETA: 3s - loss: 0.6942 - accuracy: 0.4712 - ETA: 2s - loss: 0.6940 - accuracy: 0.47 - ETA: 1s - loss: 0.6935 - accuracy: 0.49 - ETA: 1s - loss: 0.6934 - accuracy: 0.49 - ETA: 1s - loss: 0.6924 - accuracy: 0.50 - ETA: 1s - loss: 0.6920 - accuracy: 0.50 - ETA: 1s - loss: 0.6911 - accuracy: 0.50 - ETA: 1s - loss: 0.6899 - accuracy: 0.50 - ETA: 1s - loss: 0.6892 - accuracy: 0.51 - ETA: 0s - loss: 0.6873 - accuracy: 0.52 - ETA: 0s - loss: 0.6863 - accuracy: 0.52 - ETA: 0s - loss: 0.6846 - accuracy: 0.53 - ETA: 0s - loss: 0.6836 - accuracy: 0.54 - ETA: 0s - loss: 0.6827 - accuracy: 0.54 - ETA: 0s - loss: 0.6817 - accuracy: 0.55 - ETA: 0s - loss: 0.6797 - accuracy: 0.56 - ETA: 0s - loss: 0.6788 - accuracy: 0.56 - ETA: 0s - loss: 0.6774 - accuracy: 0.57 - ETA: 0s - loss: 0.6763 - accuracy: 0.57 - ETA: 0s - loss: 0.6750 - accuracy: 0.57 - ETA: 0s - loss: 0.6736 - accuracy: 0.58 - ETA: 0s - loss: 0.6721 - accuracy: 0.58 - ETA: 0s - loss: 0.6709 - accuracy: 0.59 - ETA: 0s - loss: 0.6699 - accuracy: 0.59 - ETA: 0s - loss: 0.6687 - accuracy: 0.59 - 1s 118us/step - loss: 0.6681 - accuracy: 0.5995 - val_loss: 0.6267 - val_accuracy: 0.6967\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:28 - loss: 0.6947 - accuracy: 0.40 - ETA: 3s - loss: 0.6941 - accuracy: 0.4830 - ETA: 2s - loss: 0.6939 - accuracy: 0.49 - ETA: 1s - loss: 0.6937 - accuracy: 0.49 - ETA: 1s - loss: 0.6937 - accuracy: 0.49 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.52 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 0s - loss: 0.6906 - accuracy: 0.53 - ETA: 0s - loss: 0.6893 - accuracy: 0.53 - ETA: 0s - loss: 0.6878 - accuracy: 0.53 - ETA: 0s - loss: 0.6867 - accuracy: 0.54 - ETA: 0s - loss: 0.6855 - accuracy: 0.54 - ETA: 0s - loss: 0.6838 - accuracy: 0.54 - ETA: 0s - loss: 0.6827 - accuracy: 0.54 - ETA: 0s - loss: 0.6813 - accuracy: 0.55 - ETA: 0s - loss: 0.6794 - accuracy: 0.55 - ETA: 0s - loss: 0.6774 - accuracy: 0.55 - ETA: 0s - loss: 0.6740 - accuracy: 0.56 - ETA: 0s - loss: 0.6720 - accuracy: 0.56 - ETA: 0s - loss: 0.6690 - accuracy: 0.57 - ETA: 0s - loss: 0.6670 - accuracy: 0.57 - ETA: 0s - loss: 0.6659 - accuracy: 0.57 - ETA: 0s - loss: 0.6645 - accuracy: 0.58 - 1s 116us/step - loss: 0.6630 - accuracy: 0.5828 - val_loss: 0.6041 - val_accuracy: 0.7230\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:28 - loss: 0.6869 - accuracy: 0.50 - ETA: 3s - loss: 0.6930 - accuracy: 0.5226 - ETA: 2s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6903 - accuracy: 0.54 - ETA: 1s - loss: 0.6894 - accuracy: 0.54 - ETA: 1s - loss: 0.6880 - accuracy: 0.54 - ETA: 1s - loss: 0.6868 - accuracy: 0.54 - ETA: 1s - loss: 0.6850 - accuracy: 0.55 - ETA: 1s - loss: 0.6843 - accuracy: 0.55 - ETA: 0s - loss: 0.6828 - accuracy: 0.56 - ETA: 0s - loss: 0.6806 - accuracy: 0.56 - ETA: 0s - loss: 0.6782 - accuracy: 0.57 - ETA: 0s - loss: 0.6767 - accuracy: 0.57 - ETA: 0s - loss: 0.6748 - accuracy: 0.57 - ETA: 0s - loss: 0.6725 - accuracy: 0.58 - ETA: 0s - loss: 0.6720 - accuracy: 0.58 - ETA: 0s - loss: 0.6690 - accuracy: 0.58 - ETA: 0s - loss: 0.6681 - accuracy: 0.59 - ETA: 0s - loss: 0.6664 - accuracy: 0.59 - ETA: 0s - loss: 0.6650 - accuracy: 0.59 - ETA: 0s - loss: 0.6639 - accuracy: 0.60 - ETA: 0s - loss: 0.6634 - accuracy: 0.60 - ETA: 0s - loss: 0.6623 - accuracy: 0.60 - ETA: 0s - loss: 0.6617 - accuracy: 0.60 - ETA: 0s - loss: 0.6604 - accuracy: 0.60 - 2s 119us/step - loss: 0.6601 - accuracy: 0.6082 - val_loss: 0.6137 - val_accuracy: 0.7138\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:30 - loss: 0.6950 - accuracy: 0.50 - ETA: 3s - loss: 0.6927 - accuracy: 0.5577 - ETA: 2s - loss: 0.6927 - accuracy: 0.53 - ETA: 1s - loss: 0.6924 - accuracy: 0.54 - ETA: 1s - loss: 0.6915 - accuracy: 0.54 - ETA: 1s - loss: 0.6906 - accuracy: 0.54 - ETA: 1s - loss: 0.6887 - accuracy: 0.55 - ETA: 1s - loss: 0.6867 - accuracy: 0.55 - ETA: 1s - loss: 0.6847 - accuracy: 0.55 - ETA: 1s - loss: 0.6824 - accuracy: 0.56 - ETA: 0s - loss: 0.6826 - accuracy: 0.55 - ETA: 0s - loss: 0.6806 - accuracy: 0.55 - ETA: 0s - loss: 0.6788 - accuracy: 0.56 - ETA: 0s - loss: 0.6780 - accuracy: 0.56 - ETA: 0s - loss: 0.6764 - accuracy: 0.56 - ETA: 0s - loss: 0.6740 - accuracy: 0.57 - ETA: 0s - loss: 0.6724 - accuracy: 0.57 - ETA: 0s - loss: 0.6706 - accuracy: 0.57 - ETA: 0s - loss: 0.6692 - accuracy: 0.57 - ETA: 0s - loss: 0.6680 - accuracy: 0.58 - ETA: 0s - loss: 0.6666 - accuracy: 0.58 - ETA: 0s - loss: 0.6654 - accuracy: 0.58 - ETA: 0s - loss: 0.6642 - accuracy: 0.58 - ETA: 0s - loss: 0.6625 - accuracy: 0.59 - ETA: 0s - loss: 0.6611 - accuracy: 0.59 - ETA: 0s - loss: 0.6596 - accuracy: 0.59 - 2s 119us/step - loss: 0.6593 - accuracy: 0.5970 - val_loss: 0.6039 - val_accuracy: 0.7259\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:26 - loss: 0.6932 - accuracy: 0.50 - ETA: 3s - loss: 0.6934 - accuracy: 0.5038 - ETA: 2s - loss: 0.6934 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6922 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.52 - ETA: 1s - loss: 0.6887 - accuracy: 0.53 - ETA: 1s - loss: 0.6862 - accuracy: 0.54 - ETA: 1s - loss: 0.6827 - accuracy: 0.55 - ETA: 1s - loss: 0.6813 - accuracy: 0.55 - ETA: 0s - loss: 0.6798 - accuracy: 0.55 - ETA: 0s - loss: 0.6774 - accuracy: 0.56 - ETA: 0s - loss: 0.6750 - accuracy: 0.56 - ETA: 0s - loss: 0.6728 - accuracy: 0.57 - ETA: 0s - loss: 0.6714 - accuracy: 0.57 - ETA: 0s - loss: 0.6688 - accuracy: 0.57 - ETA: 0s - loss: 0.6655 - accuracy: 0.58 - ETA: 0s - loss: 0.6648 - accuracy: 0.58 - ETA: 0s - loss: 0.6635 - accuracy: 0.58 - ETA: 0s - loss: 0.6618 - accuracy: 0.59 - ETA: 0s - loss: 0.6606 - accuracy: 0.59 - ETA: 0s - loss: 0.6598 - accuracy: 0.59 - ETA: 0s - loss: 0.6581 - accuracy: 0.59 - ETA: 0s - loss: 0.6568 - accuracy: 0.60 - ETA: 0s - loss: 0.6556 - accuracy: 0.60 - ETA: 0s - loss: 0.6528 - accuracy: 0.60 - 1s 118us/step - loss: 0.6525 - accuracy: 0.6060 - val_loss: 0.5966 - val_accuracy: 0.7308\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:26 - loss: 0.6909 - accuracy: 0.70 - ETA: 3s - loss: 0.6940 - accuracy: 0.4942 - ETA: 2s - loss: 0.6924 - accuracy: 0.50 - ETA: 1s - loss: 0.6903 - accuracy: 0.51 - ETA: 1s - loss: 0.6898 - accuracy: 0.50 - ETA: 1s - loss: 0.6878 - accuracy: 0.52 - ETA: 1s - loss: 0.6864 - accuracy: 0.53 - ETA: 1s - loss: 0.6838 - accuracy: 0.54 - ETA: 1s - loss: 0.6816 - accuracy: 0.55 - ETA: 0s - loss: 0.6786 - accuracy: 0.56 - ETA: 0s - loss: 0.6762 - accuracy: 0.57 - ETA: 0s - loss: 0.6733 - accuracy: 0.57 - ETA: 0s - loss: 0.6709 - accuracy: 0.58 - ETA: 0s - loss: 0.6694 - accuracy: 0.58 - ETA: 0s - loss: 0.6684 - accuracy: 0.59 - ETA: 0s - loss: 0.6664 - accuracy: 0.59 - ETA: 0s - loss: 0.6642 - accuracy: 0.59 - ETA: 0s - loss: 0.6631 - accuracy: 0.59 - ETA: 0s - loss: 0.6613 - accuracy: 0.60 - ETA: 0s - loss: 0.6588 - accuracy: 0.60 - ETA: 0s - loss: 0.6581 - accuracy: 0.60 - ETA: 0s - loss: 0.6569 - accuracy: 0.61 - ETA: 0s - loss: 0.6560 - accuracy: 0.61 - ETA: 0s - loss: 0.6546 - accuracy: 0.61 - ETA: 0s - loss: 0.6534 - accuracy: 0.61 - 1s 115us/step - loss: 0.6530 - accuracy: 0.6203 - val_loss: 0.6000 - val_accuracy: 0.7287\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:29 - loss: 0.6956 - accuracy: 0.30 - ETA: 3s - loss: 0.6932 - accuracy: 0.5415 - ETA: 2s - loss: 0.6924 - accuracy: 0.53 - ETA: 1s - loss: 0.6917 - accuracy: 0.53 - ETA: 1s - loss: 0.6900 - accuracy: 0.54 - ETA: 1s - loss: 0.6887 - accuracy: 0.54 - ETA: 1s - loss: 0.6865 - accuracy: 0.54 - ETA: 1s - loss: 0.6835 - accuracy: 0.55 - ETA: 1s - loss: 0.6812 - accuracy: 0.56 - ETA: 1s - loss: 0.6793 - accuracy: 0.57 - ETA: 0s - loss: 0.6779 - accuracy: 0.57 - ETA: 0s - loss: 0.6752 - accuracy: 0.58 - ETA: 0s - loss: 0.6718 - accuracy: 0.58 - ETA: 0s - loss: 0.6673 - accuracy: 0.59 - ETA: 0s - loss: 0.6642 - accuracy: 0.60 - ETA: 0s - loss: 0.6616 - accuracy: 0.60 - ETA: 0s - loss: 0.6601 - accuracy: 0.60 - ETA: 0s - loss: 0.6582 - accuracy: 0.61 - ETA: 0s - loss: 0.6564 - accuracy: 0.61 - ETA: 0s - loss: 0.6538 - accuracy: 0.61 - ETA: 0s - loss: 0.6516 - accuracy: 0.62 - ETA: 0s - loss: 0.6515 - accuracy: 0.62 - ETA: 0s - loss: 0.6501 - accuracy: 0.62 - ETA: 0s - loss: 0.6487 - accuracy: 0.62 - ETA: 0s - loss: 0.6468 - accuracy: 0.63 - ETA: 0s - loss: 0.6462 - accuracy: 0.63 - 2s 120us/step - loss: 0.6449 - accuracy: 0.6353 - val_loss: 0.5888 - val_accuracy: 0.7287\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:24 - loss: 0.6807 - accuracy: 0.70 - ETA: 3s - loss: 0.6941 - accuracy: 0.5096 - ETA: 2s - loss: 0.6934 - accuracy: 0.52 - ETA: 1s - loss: 0.6922 - accuracy: 0.53 - ETA: 1s - loss: 0.6914 - accuracy: 0.55 - ETA: 1s - loss: 0.6896 - accuracy: 0.56 - ETA: 1s - loss: 0.6863 - accuracy: 0.57 - ETA: 1s - loss: 0.6830 - accuracy: 0.58 - ETA: 1s - loss: 0.6799 - accuracy: 0.59 - ETA: 1s - loss: 0.6789 - accuracy: 0.59 - ETA: 0s - loss: 0.6768 - accuracy: 0.59 - ETA: 0s - loss: 0.6740 - accuracy: 0.60 - ETA: 0s - loss: 0.6717 - accuracy: 0.60 - ETA: 0s - loss: 0.6687 - accuracy: 0.60 - ETA: 0s - loss: 0.6660 - accuracy: 0.61 - ETA: 0s - loss: 0.6650 - accuracy: 0.61 - ETA: 0s - loss: 0.6623 - accuracy: 0.61 - ETA: 0s - loss: 0.6607 - accuracy: 0.61 - ETA: 0s - loss: 0.6594 - accuracy: 0.62 - ETA: 0s - loss: 0.6572 - accuracy: 0.62 - ETA: 0s - loss: 0.6552 - accuracy: 0.62 - ETA: 0s - loss: 0.6529 - accuracy: 0.62 - ETA: 0s - loss: 0.6507 - accuracy: 0.63 - ETA: 0s - loss: 0.6491 - accuracy: 0.63 - ETA: 0s - loss: 0.6479 - accuracy: 0.63 - ETA: 0s - loss: 0.6472 - accuracy: 0.63 - 1s 118us/step - loss: 0.6467 - accuracy: 0.6353 - val_loss: 0.5907 - val_accuracy: 0.7216\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:25 - loss: 0.6937 - accuracy: 0.50 - ETA: 3s - loss: 0.6932 - accuracy: 0.5509 - ETA: 2s - loss: 0.6929 - accuracy: 0.53 - ETA: 1s - loss: 0.6915 - accuracy: 0.54 - ETA: 1s - loss: 0.6890 - accuracy: 0.55 - ETA: 1s - loss: 0.6873 - accuracy: 0.55 - ETA: 1s - loss: 0.6861 - accuracy: 0.55 - ETA: 1s - loss: 0.6838 - accuracy: 0.55 - ETA: 1s - loss: 0.6811 - accuracy: 0.55 - ETA: 1s - loss: 0.6782 - accuracy: 0.56 - ETA: 0s - loss: 0.6766 - accuracy: 0.56 - ETA: 0s - loss: 0.6728 - accuracy: 0.57 - ETA: 0s - loss: 0.6702 - accuracy: 0.57 - ETA: 0s - loss: 0.6679 - accuracy: 0.58 - ETA: 0s - loss: 0.6669 - accuracy: 0.58 - ETA: 0s - loss: 0.6644 - accuracy: 0.58 - ETA: 0s - loss: 0.6611 - accuracy: 0.59 - ETA: 0s - loss: 0.6602 - accuracy: 0.59 - ETA: 0s - loss: 0.6586 - accuracy: 0.60 - ETA: 0s - loss: 0.6567 - accuracy: 0.60 - ETA: 0s - loss: 0.6549 - accuracy: 0.60 - ETA: 0s - loss: 0.6527 - accuracy: 0.61 - ETA: 0s - loss: 0.6520 - accuracy: 0.61 - ETA: 0s - loss: 0.6503 - accuracy: 0.61 - ETA: 0s - loss: 0.6490 - accuracy: 0.61 - ETA: 0s - loss: 0.6471 - accuracy: 0.62 - 1s 118us/step - loss: 0.6470 - accuracy: 0.6237 - val_loss: 0.5960 - val_accuracy: 0.7223\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:28 - loss: 0.6933 - accuracy: 0.70 - ETA: 3s - loss: 0.6905 - accuracy: 0.5377 - ETA: 2s - loss: 0.6910 - accuracy: 0.53 - ETA: 1s - loss: 0.6880 - accuracy: 0.54 - ETA: 1s - loss: 0.6869 - accuracy: 0.53 - ETA: 1s - loss: 0.6860 - accuracy: 0.53 - ETA: 1s - loss: 0.6855 - accuracy: 0.54 - ETA: 1s - loss: 0.6829 - accuracy: 0.55 - ETA: 1s - loss: 0.6820 - accuracy: 0.56 - ETA: 1s - loss: 0.6806 - accuracy: 0.56 - ETA: 0s - loss: 0.6791 - accuracy: 0.57 - ETA: 0s - loss: 0.6767 - accuracy: 0.57 - ETA: 0s - loss: 0.6747 - accuracy: 0.58 - ETA: 0s - loss: 0.6731 - accuracy: 0.58 - ETA: 0s - loss: 0.6723 - accuracy: 0.59 - ETA: 0s - loss: 0.6710 - accuracy: 0.59 - ETA: 0s - loss: 0.6693 - accuracy: 0.60 - ETA: 0s - loss: 0.6679 - accuracy: 0.60 - ETA: 0s - loss: 0.6665 - accuracy: 0.60 - ETA: 0s - loss: 0.6648 - accuracy: 0.60 - ETA: 0s - loss: 0.6627 - accuracy: 0.61 - ETA: 0s - loss: 0.6619 - accuracy: 0.61 - ETA: 0s - loss: 0.6598 - accuracy: 0.61 - ETA: 0s - loss: 0.6582 - accuracy: 0.61 - ETA: 0s - loss: 0.6579 - accuracy: 0.62 - ETA: 0s - loss: 0.6569 - accuracy: 0.62 - 1s 118us/step - loss: 0.6569 - accuracy: 0.6233 - val_loss: 0.6125 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:25 - loss: 0.6946 - accuracy: 0.40 - ETA: 3s - loss: 0.6940 - accuracy: 0.5019 - ETA: 2s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6928 - accuracy: 0.51 - ETA: 1s - loss: 0.6920 - accuracy: 0.51 - ETA: 1s - loss: 0.6907 - accuracy: 0.52 - ETA: 1s - loss: 0.6888 - accuracy: 0.52 - ETA: 1s - loss: 0.6874 - accuracy: 0.53 - ETA: 1s - loss: 0.6861 - accuracy: 0.53 - ETA: 0s - loss: 0.6848 - accuracy: 0.54 - ETA: 0s - loss: 0.6818 - accuracy: 0.55 - ETA: 0s - loss: 0.6786 - accuracy: 0.55 - ETA: 0s - loss: 0.6762 - accuracy: 0.56 - ETA: 0s - loss: 0.6747 - accuracy: 0.56 - ETA: 0s - loss: 0.6723 - accuracy: 0.57 - ETA: 0s - loss: 0.6684 - accuracy: 0.57 - ETA: 0s - loss: 0.6657 - accuracy: 0.58 - ETA: 0s - loss: 0.6649 - accuracy: 0.58 - ETA: 0s - loss: 0.6637 - accuracy: 0.58 - ETA: 0s - loss: 0.6632 - accuracy: 0.59 - ETA: 0s - loss: 0.6610 - accuracy: 0.59 - ETA: 0s - loss: 0.6603 - accuracy: 0.59 - ETA: 0s - loss: 0.6590 - accuracy: 0.60 - ETA: 0s - loss: 0.6578 - accuracy: 0.60 - ETA: 0s - loss: 0.6569 - accuracy: 0.60 - 1s 114us/step - loss: 0.6569 - accuracy: 0.6067 - val_loss: 0.6112 - val_accuracy: 0.7145\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:25 - loss: 0.6955 - accuracy: 0.50 - ETA: 3s - loss: 0.6933 - accuracy: 0.5308 - ETA: 2s - loss: 0.6922 - accuracy: 0.53 - ETA: 1s - loss: 0.6915 - accuracy: 0.53 - ETA: 1s - loss: 0.6888 - accuracy: 0.54 - ETA: 1s - loss: 0.6852 - accuracy: 0.55 - ETA: 1s - loss: 0.6821 - accuracy: 0.56 - ETA: 1s - loss: 0.6784 - accuracy: 0.57 - ETA: 1s - loss: 0.6759 - accuracy: 0.58 - ETA: 1s - loss: 0.6721 - accuracy: 0.58 - ETA: 0s - loss: 0.6708 - accuracy: 0.59 - ETA: 0s - loss: 0.6695 - accuracy: 0.59 - ETA: 0s - loss: 0.6683 - accuracy: 0.59 - ETA: 0s - loss: 0.6674 - accuracy: 0.60 - ETA: 0s - loss: 0.6640 - accuracy: 0.60 - ETA: 0s - loss: 0.6612 - accuracy: 0.61 - ETA: 0s - loss: 0.6582 - accuracy: 0.61 - ETA: 0s - loss: 0.6552 - accuracy: 0.62 - ETA: 0s - loss: 0.6529 - accuracy: 0.62 - ETA: 0s - loss: 0.6522 - accuracy: 0.62 - ETA: 0s - loss: 0.6507 - accuracy: 0.62 - ETA: 0s - loss: 0.6483 - accuracy: 0.63 - ETA: 0s - loss: 0.6475 - accuracy: 0.63 - ETA: 0s - loss: 0.6456 - accuracy: 0.63 - ETA: 0s - loss: 0.6438 - accuracy: 0.64 - ETA: 0s - loss: 0.6422 - accuracy: 0.64 - 2s 120us/step - loss: 0.6409 - accuracy: 0.6436 - val_loss: 0.5814 - val_accuracy: 0.7209\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:26 - loss: 0.6942 - accuracy: 0.40 - ETA: 4s - loss: 0.6946 - accuracy: 0.5080 - ETA: 2s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6877 - accuracy: 0.54 - ETA: 1s - loss: 0.6860 - accuracy: 0.55 - ETA: 1s - loss: 0.6824 - accuracy: 0.56 - ETA: 1s - loss: 0.6789 - accuracy: 0.57 - ETA: 1s - loss: 0.6772 - accuracy: 0.57 - ETA: 1s - loss: 0.6738 - accuracy: 0.58 - ETA: 0s - loss: 0.6718 - accuracy: 0.58 - ETA: 0s - loss: 0.6685 - accuracy: 0.59 - ETA: 0s - loss: 0.6665 - accuracy: 0.59 - ETA: 0s - loss: 0.6642 - accuracy: 0.59 - ETA: 0s - loss: 0.6617 - accuracy: 0.60 - ETA: 0s - loss: 0.6607 - accuracy: 0.60 - ETA: 0s - loss: 0.6577 - accuracy: 0.61 - ETA: 0s - loss: 0.6549 - accuracy: 0.61 - ETA: 0s - loss: 0.6507 - accuracy: 0.62 - ETA: 0s - loss: 0.6498 - accuracy: 0.62 - ETA: 0s - loss: 0.6471 - accuracy: 0.62 - ETA: 0s - loss: 0.6461 - accuracy: 0.63 - ETA: 0s - loss: 0.6454 - accuracy: 0.63 - ETA: 0s - loss: 0.6439 - accuracy: 0.63 - ETA: 0s - loss: 0.6410 - accuracy: 0.63 - ETA: 0s - loss: 0.6390 - accuracy: 0.64 - 1s 118us/step - loss: 0.6390 - accuracy: 0.6410 - val_loss: 0.5817 - val_accuracy: 0.7188\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:25 - loss: 0.6960 - accuracy: 0.50 - ETA: 3s - loss: 0.6954 - accuracy: 0.4451 - ETA: 2s - loss: 0.6927 - accuracy: 0.49 - ETA: 1s - loss: 0.6927 - accuracy: 0.49 - ETA: 1s - loss: 0.6902 - accuracy: 0.51 - ETA: 1s - loss: 0.6894 - accuracy: 0.52 - ETA: 1s - loss: 0.6883 - accuracy: 0.53 - ETA: 1s - loss: 0.6863 - accuracy: 0.54 - ETA: 1s - loss: 0.6847 - accuracy: 0.55 - ETA: 1s - loss: 0.6828 - accuracy: 0.55 - ETA: 0s - loss: 0.6792 - accuracy: 0.56 - ETA: 0s - loss: 0.6777 - accuracy: 0.57 - ETA: 0s - loss: 0.6759 - accuracy: 0.57 - ETA: 0s - loss: 0.6727 - accuracy: 0.58 - ETA: 0s - loss: 0.6701 - accuracy: 0.58 - ETA: 0s - loss: 0.6671 - accuracy: 0.59 - ETA: 0s - loss: 0.6649 - accuracy: 0.59 - ETA: 0s - loss: 0.6622 - accuracy: 0.60 - ETA: 0s - loss: 0.6603 - accuracy: 0.60 - ETA: 0s - loss: 0.6581 - accuracy: 0.60 - ETA: 0s - loss: 0.6555 - accuracy: 0.61 - ETA: 0s - loss: 0.6536 - accuracy: 0.61 - ETA: 0s - loss: 0.6523 - accuracy: 0.61 - ETA: 0s - loss: 0.6506 - accuracy: 0.62 - ETA: 0s - loss: 0.6492 - accuracy: 0.62 - ETA: 0s - loss: 0.6489 - accuracy: 0.62 - 2s 120us/step - loss: 0.6483 - accuracy: 0.6283 - val_loss: 0.5959 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:34 - loss: 0.6833 - accuracy: 0.60 - ETA: 4s - loss: 0.6901 - accuracy: 0.5588 - ETA: 2s - loss: 0.6923 - accuracy: 0.53 - ETA: 2s - loss: 0.6910 - accuracy: 0.54 - ETA: 1s - loss: 0.6882 - accuracy: 0.55 - ETA: 1s - loss: 0.6874 - accuracy: 0.55 - ETA: 1s - loss: 0.6833 - accuracy: 0.57 - ETA: 1s - loss: 0.6805 - accuracy: 0.57 - ETA: 1s - loss: 0.6766 - accuracy: 0.58 - ETA: 1s - loss: 0.6714 - accuracy: 0.59 - ETA: 0s - loss: 0.6668 - accuracy: 0.60 - ETA: 0s - loss: 0.6642 - accuracy: 0.60 - ETA: 0s - loss: 0.6629 - accuracy: 0.61 - ETA: 0s - loss: 0.6592 - accuracy: 0.61 - ETA: 0s - loss: 0.6573 - accuracy: 0.61 - ETA: 0s - loss: 0.6551 - accuracy: 0.62 - ETA: 0s - loss: 0.6531 - accuracy: 0.62 - ETA: 0s - loss: 0.6528 - accuracy: 0.62 - ETA: 0s - loss: 0.6501 - accuracy: 0.62 - ETA: 0s - loss: 0.6483 - accuracy: 0.63 - ETA: 0s - loss: 0.6469 - accuracy: 0.63 - ETA: 0s - loss: 0.6454 - accuracy: 0.63 - ETA: 0s - loss: 0.6439 - accuracy: 0.63 - ETA: 0s - loss: 0.6412 - accuracy: 0.64 - ETA: 0s - loss: 0.6385 - accuracy: 0.64 - ETA: 0s - loss: 0.6357 - accuracy: 0.64 - ETA: 0s - loss: 0.6357 - accuracy: 0.64 - 2s 122us/step - loss: 0.6355 - accuracy: 0.6493 - val_loss: 0.5865 - val_accuracy: 0.7173\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 46us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:40 - loss: 0.6963 - accuracy: 0.30 - ETA: 4s - loss: 0.6940 - accuracy: 0.5255 - ETA: 2s - loss: 0.6935 - accuracy: 0.53 - ETA: 2s - loss: 0.6922 - accuracy: 0.54 - ETA: 1s - loss: 0.6898 - accuracy: 0.55 - ETA: 1s - loss: 0.6881 - accuracy: 0.56 - ETA: 1s - loss: 0.6833 - accuracy: 0.57 - ETA: 1s - loss: 0.6805 - accuracy: 0.57 - ETA: 1s - loss: 0.6779 - accuracy: 0.58 - ETA: 1s - loss: 0.6749 - accuracy: 0.59 - ETA: 1s - loss: 0.6702 - accuracy: 0.60 - ETA: 0s - loss: 0.6669 - accuracy: 0.60 - ETA: 0s - loss: 0.6628 - accuracy: 0.61 - ETA: 0s - loss: 0.6598 - accuracy: 0.61 - ETA: 0s - loss: 0.6573 - accuracy: 0.62 - ETA: 0s - loss: 0.6557 - accuracy: 0.62 - ETA: 0s - loss: 0.6542 - accuracy: 0.62 - ETA: 0s - loss: 0.6522 - accuracy: 0.62 - ETA: 0s - loss: 0.6502 - accuracy: 0.63 - ETA: 0s - loss: 0.6490 - accuracy: 0.63 - ETA: 0s - loss: 0.6469 - accuracy: 0.63 - ETA: 0s - loss: 0.6457 - accuracy: 0.63 - ETA: 0s - loss: 0.6438 - accuracy: 0.64 - ETA: 0s - loss: 0.6414 - accuracy: 0.64 - ETA: 0s - loss: 0.6384 - accuracy: 0.64 - ETA: 0s - loss: 0.6381 - accuracy: 0.64 - ETA: 0s - loss: 0.6365 - accuracy: 0.65 - 2s 122us/step - loss: 0.6359 - accuracy: 0.6535 - val_loss: 0.5795 - val_accuracy: 0.7152\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:35 - loss: 0.6944 - accuracy: 0.50 - ETA: 4s - loss: 0.6932 - accuracy: 0.5196 - ETA: 2s - loss: 0.6928 - accuracy: 0.52 - ETA: 2s - loss: 0.6909 - accuracy: 0.53 - ETA: 1s - loss: 0.6894 - accuracy: 0.54 - ETA: 1s - loss: 0.6869 - accuracy: 0.54 - ETA: 1s - loss: 0.6844 - accuracy: 0.55 - ETA: 1s - loss: 0.6794 - accuracy: 0.56 - ETA: 1s - loss: 0.6784 - accuracy: 0.56 - ETA: 1s - loss: 0.6756 - accuracy: 0.57 - ETA: 1s - loss: 0.6732 - accuracy: 0.58 - ETA: 0s - loss: 0.6701 - accuracy: 0.59 - ETA: 0s - loss: 0.6681 - accuracy: 0.59 - ETA: 0s - loss: 0.6645 - accuracy: 0.60 - ETA: 0s - loss: 0.6609 - accuracy: 0.61 - ETA: 0s - loss: 0.6590 - accuracy: 0.61 - ETA: 0s - loss: 0.6579 - accuracy: 0.61 - ETA: 0s - loss: 0.6556 - accuracy: 0.62 - ETA: 0s - loss: 0.6534 - accuracy: 0.62 - ETA: 0s - loss: 0.6514 - accuracy: 0.62 - ETA: 0s - loss: 0.6492 - accuracy: 0.62 - ETA: 0s - loss: 0.6469 - accuracy: 0.63 - ETA: 0s - loss: 0.6457 - accuracy: 0.63 - ETA: 0s - loss: 0.6455 - accuracy: 0.63 - ETA: 0s - loss: 0.6433 - accuracy: 0.64 - ETA: 0s - loss: 0.6424 - accuracy: 0.64 - ETA: 0s - loss: 0.6403 - accuracy: 0.64 - 2s 122us/step - loss: 0.6404 - accuracy: 0.6454 - val_loss: 0.5963 - val_accuracy: 0.6996\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:11 - loss: 0.6919 - accuracy: 0.50 - ETA: 3s - loss: 0.6935 - accuracy: 0.5038 - ETA: 2s - loss: 0.6930 - accuracy: 0.53 - ETA: 1s - loss: 0.6927 - accuracy: 0.53 - ETA: 1s - loss: 0.6923 - accuracy: 0.54 - ETA: 1s - loss: 0.6917 - accuracy: 0.54 - ETA: 1s - loss: 0.6915 - accuracy: 0.54 - ETA: 1s - loss: 0.6910 - accuracy: 0.55 - ETA: 1s - loss: 0.6906 - accuracy: 0.55 - ETA: 1s - loss: 0.6902 - accuracy: 0.55 - ETA: 0s - loss: 0.6900 - accuracy: 0.55 - ETA: 0s - loss: 0.6890 - accuracy: 0.56 - ETA: 0s - loss: 0.6881 - accuracy: 0.56 - ETA: 0s - loss: 0.6870 - accuracy: 0.56 - ETA: 0s - loss: 0.6863 - accuracy: 0.57 - ETA: 0s - loss: 0.6854 - accuracy: 0.57 - ETA: 0s - loss: 0.6844 - accuracy: 0.57 - ETA: 0s - loss: 0.6833 - accuracy: 0.57 - ETA: 0s - loss: 0.6823 - accuracy: 0.58 - ETA: 0s - loss: 0.6807 - accuracy: 0.58 - ETA: 0s - loss: 0.6800 - accuracy: 0.58 - ETA: 0s - loss: 0.6787 - accuracy: 0.58 - ETA: 0s - loss: 0.6781 - accuracy: 0.58 - ETA: 0s - loss: 0.6766 - accuracy: 0.59 - ETA: 0s - loss: 0.6759 - accuracy: 0.59 - 1s 115us/step - loss: 0.6752 - accuracy: 0.5944 - val_loss: 0.6395 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:09 - loss: 0.6881 - accuracy: 0.40 - ETA: 3s - loss: 0.6917 - accuracy: 0.5500 - ETA: 2s - loss: 0.6912 - accuracy: 0.52 - ETA: 1s - loss: 0.6908 - accuracy: 0.52 - ETA: 1s - loss: 0.6908 - accuracy: 0.51 - ETA: 1s - loss: 0.6908 - accuracy: 0.51 - ETA: 1s - loss: 0.6904 - accuracy: 0.51 - ETA: 1s - loss: 0.6897 - accuracy: 0.52 - ETA: 1s - loss: 0.6890 - accuracy: 0.52 - ETA: 0s - loss: 0.6878 - accuracy: 0.52 - ETA: 0s - loss: 0.6867 - accuracy: 0.52 - ETA: 0s - loss: 0.6864 - accuracy: 0.52 - ETA: 0s - loss: 0.6852 - accuracy: 0.53 - ETA: 0s - loss: 0.6847 - accuracy: 0.53 - ETA: 0s - loss: 0.6841 - accuracy: 0.53 - ETA: 0s - loss: 0.6834 - accuracy: 0.54 - ETA: 0s - loss: 0.6829 - accuracy: 0.55 - ETA: 0s - loss: 0.6824 - accuracy: 0.55 - ETA: 0s - loss: 0.6810 - accuracy: 0.56 - ETA: 0s - loss: 0.6799 - accuracy: 0.56 - ETA: 0s - loss: 0.6789 - accuracy: 0.57 - ETA: 0s - loss: 0.6773 - accuracy: 0.57 - ETA: 0s - loss: 0.6765 - accuracy: 0.58 - ETA: 0s - loss: 0.6751 - accuracy: 0.58 - ETA: 0s - loss: 0.6741 - accuracy: 0.59 - 1s 115us/step - loss: 0.6734 - accuracy: 0.5932 - val_loss: 0.6365 - val_accuracy: 0.6811\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:11 - loss: 0.6905 - accuracy: 0.60 - ETA: 3s - loss: 0.6925 - accuracy: 0.5071 - ETA: 2s - loss: 0.6920 - accuracy: 0.50 - ETA: 1s - loss: 0.6920 - accuracy: 0.51 - ETA: 1s - loss: 0.6910 - accuracy: 0.51 - ETA: 1s - loss: 0.6906 - accuracy: 0.51 - ETA: 1s - loss: 0.6900 - accuracy: 0.51 - ETA: 1s - loss: 0.6894 - accuracy: 0.51 - ETA: 1s - loss: 0.6892 - accuracy: 0.51 - ETA: 0s - loss: 0.6885 - accuracy: 0.51 - ETA: 0s - loss: 0.6882 - accuracy: 0.51 - ETA: 0s - loss: 0.6876 - accuracy: 0.52 - ETA: 0s - loss: 0.6869 - accuracy: 0.52 - ETA: 0s - loss: 0.6861 - accuracy: 0.52 - ETA: 0s - loss: 0.6856 - accuracy: 0.52 - ETA: 0s - loss: 0.6848 - accuracy: 0.53 - ETA: 0s - loss: 0.6843 - accuracy: 0.53 - ETA: 0s - loss: 0.6836 - accuracy: 0.53 - ETA: 0s - loss: 0.6829 - accuracy: 0.54 - ETA: 0s - loss: 0.6825 - accuracy: 0.54 - ETA: 0s - loss: 0.6816 - accuracy: 0.54 - ETA: 0s - loss: 0.6808 - accuracy: 0.55 - ETA: 0s - loss: 0.6806 - accuracy: 0.55 - ETA: 0s - loss: 0.6802 - accuracy: 0.55 - ETA: 0s - loss: 0.6790 - accuracy: 0.55 - 1s 114us/step - loss: 0.6791 - accuracy: 0.5605 - val_loss: 0.6534 - val_accuracy: 0.7060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:09 - loss: 0.6880 - accuracy: 0.50 - ETA: 3s - loss: 0.6895 - accuracy: 0.5660 - ETA: 2s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6909 - accuracy: 0.53 - ETA: 1s - loss: 0.6891 - accuracy: 0.53 - ETA: 1s - loss: 0.6885 - accuracy: 0.53 - ETA: 1s - loss: 0.6887 - accuracy: 0.52 - ETA: 1s - loss: 0.6885 - accuracy: 0.53 - ETA: 1s - loss: 0.6876 - accuracy: 0.53 - ETA: 0s - loss: 0.6872 - accuracy: 0.54 - ETA: 0s - loss: 0.6859 - accuracy: 0.54 - ETA: 0s - loss: 0.6849 - accuracy: 0.54 - ETA: 0s - loss: 0.6839 - accuracy: 0.54 - ETA: 0s - loss: 0.6830 - accuracy: 0.55 - ETA: 0s - loss: 0.6816 - accuracy: 0.55 - ETA: 0s - loss: 0.6810 - accuracy: 0.55 - ETA: 0s - loss: 0.6799 - accuracy: 0.55 - ETA: 0s - loss: 0.6788 - accuracy: 0.56 - ETA: 0s - loss: 0.6778 - accuracy: 0.56 - ETA: 0s - loss: 0.6764 - accuracy: 0.56 - ETA: 0s - loss: 0.6749 - accuracy: 0.56 - ETA: 0s - loss: 0.6741 - accuracy: 0.57 - ETA: 0s - loss: 0.6736 - accuracy: 0.57 - ETA: 0s - loss: 0.6725 - accuracy: 0.57 - ETA: 0s - loss: 0.6717 - accuracy: 0.57 - 1s 112us/step - loss: 0.6716 - accuracy: 0.5754 - val_loss: 0.6397 - val_accuracy: 0.7159\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 44us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:09 - loss: 0.6941 - accuracy: 0.30 - ETA: 3s - loss: 0.6934 - accuracy: 0.5218 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6919 - accuracy: 0.51 - ETA: 1s - loss: 0.6914 - accuracy: 0.51 - ETA: 0s - loss: 0.6916 - accuracy: 0.51 - ETA: 0s - loss: 0.6917 - accuracy: 0.51 - ETA: 0s - loss: 0.6916 - accuracy: 0.51 - ETA: 0s - loss: 0.6914 - accuracy: 0.51 - ETA: 0s - loss: 0.6914 - accuracy: 0.51 - ETA: 0s - loss: 0.6912 - accuracy: 0.51 - ETA: 0s - loss: 0.6909 - accuracy: 0.51 - ETA: 0s - loss: 0.6908 - accuracy: 0.51 - ETA: 0s - loss: 0.6907 - accuracy: 0.51 - ETA: 0s - loss: 0.6906 - accuracy: 0.51 - ETA: 0s - loss: 0.6905 - accuracy: 0.51 - ETA: 0s - loss: 0.6906 - accuracy: 0.51 - ETA: 0s - loss: 0.6905 - accuracy: 0.51 - ETA: 0s - loss: 0.6901 - accuracy: 0.51 - ETA: 0s - loss: 0.6900 - accuracy: 0.51 - ETA: 0s - loss: 0.6894 - accuracy: 0.51 - ETA: 0s - loss: 0.6890 - accuracy: 0.51 - ETA: 0s - loss: 0.6884 - accuracy: 0.52 - ETA: 0s - loss: 0.6881 - accuracy: 0.52 - ETA: 0s - loss: 0.6880 - accuracy: 0.52 - 2s 131us/step - loss: 0.6877 - accuracy: 0.5229 - val_loss: 0.6731 - val_accuracy: 0.6200\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - 0s 58us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:14 - loss: 0.6897 - accuracy: 0.70 - ETA: 3s - loss: 0.6935 - accuracy: 0.5189 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.50 - ETA: 1s - loss: 0.6918 - accuracy: 0.51 - ETA: 1s - loss: 0.6914 - accuracy: 0.51 - ETA: 1s - loss: 0.6909 - accuracy: 0.52 - ETA: 1s - loss: 0.6904 - accuracy: 0.53 - ETA: 1s - loss: 0.6896 - accuracy: 0.55 - ETA: 0s - loss: 0.6887 - accuracy: 0.56 - ETA: 0s - loss: 0.6880 - accuracy: 0.56 - ETA: 0s - loss: 0.6871 - accuracy: 0.57 - ETA: 0s - loss: 0.6865 - accuracy: 0.57 - ETA: 0s - loss: 0.6857 - accuracy: 0.58 - ETA: 0s - loss: 0.6847 - accuracy: 0.58 - ETA: 0s - loss: 0.6836 - accuracy: 0.59 - ETA: 0s - loss: 0.6824 - accuracy: 0.59 - ETA: 0s - loss: 0.6810 - accuracy: 0.60 - ETA: 0s - loss: 0.6804 - accuracy: 0.60 - ETA: 0s - loss: 0.6794 - accuracy: 0.60 - ETA: 0s - loss: 0.6784 - accuracy: 0.60 - ETA: 0s - loss: 0.6773 - accuracy: 0.60 - ETA: 0s - loss: 0.6762 - accuracy: 0.61 - ETA: 0s - loss: 0.6756 - accuracy: 0.61 - ETA: 0s - loss: 0.6742 - accuracy: 0.61 - 1s 115us/step - loss: 0.6737 - accuracy: 0.6189 - val_loss: 0.6421 - val_accuracy: 0.6875\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 46us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:07 - loss: 0.6964 - accuracy: 0.30 - ETA: 3s - loss: 0.6940 - accuracy: 0.5057 - ETA: 2s - loss: 0.6933 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.53 - ETA: 1s - loss: 0.6918 - accuracy: 0.53 - ETA: 1s - loss: 0.6910 - accuracy: 0.54 - ETA: 1s - loss: 0.6902 - accuracy: 0.54 - ETA: 1s - loss: 0.6889 - accuracy: 0.55 - ETA: 1s - loss: 0.6874 - accuracy: 0.56 - ETA: 0s - loss: 0.6859 - accuracy: 0.56 - ETA: 0s - loss: 0.6844 - accuracy: 0.57 - ETA: 0s - loss: 0.6825 - accuracy: 0.58 - ETA: 0s - loss: 0.6808 - accuracy: 0.58 - ETA: 0s - loss: 0.6789 - accuracy: 0.58 - ETA: 0s - loss: 0.6781 - accuracy: 0.59 - ETA: 0s - loss: 0.6759 - accuracy: 0.59 - ETA: 0s - loss: 0.6739 - accuracy: 0.59 - ETA: 0s - loss: 0.6716 - accuracy: 0.60 - ETA: 0s - loss: 0.6696 - accuracy: 0.60 - ETA: 0s - loss: 0.6686 - accuracy: 0.60 - ETA: 0s - loss: 0.6670 - accuracy: 0.61 - ETA: 0s - loss: 0.6651 - accuracy: 0.61 - ETA: 0s - loss: 0.6638 - accuracy: 0.61 - ETA: 0s - loss: 0.6629 - accuracy: 0.61 - ETA: 0s - loss: 0.6619 - accuracy: 0.61 - 1s 114us/step - loss: 0.6608 - accuracy: 0.6177 - val_loss: 0.6079 - val_accuracy: 0.7060\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 47us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:07 - loss: 0.6958 - accuracy: 0.20 - ETA: 3s - loss: 0.6939 - accuracy: 0.4943 - ETA: 2s - loss: 0.6934 - accuracy: 0.49 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6900 - accuracy: 0.54 - ETA: 1s - loss: 0.6891 - accuracy: 0.54 - ETA: 1s - loss: 0.6885 - accuracy: 0.55 - ETA: 1s - loss: 0.6868 - accuracy: 0.56 - ETA: 0s - loss: 0.6855 - accuracy: 0.56 - ETA: 0s - loss: 0.6840 - accuracy: 0.57 - ETA: 0s - loss: 0.6824 - accuracy: 0.57 - ETA: 0s - loss: 0.6814 - accuracy: 0.58 - ETA: 0s - loss: 0.6793 - accuracy: 0.58 - ETA: 0s - loss: 0.6781 - accuracy: 0.58 - ETA: 0s - loss: 0.6764 - accuracy: 0.59 - ETA: 0s - loss: 0.6753 - accuracy: 0.59 - ETA: 0s - loss: 0.6734 - accuracy: 0.59 - ETA: 0s - loss: 0.6724 - accuracy: 0.59 - ETA: 0s - loss: 0.6700 - accuracy: 0.60 - ETA: 0s - loss: 0.6684 - accuracy: 0.60 - ETA: 0s - loss: 0.6660 - accuracy: 0.60 - ETA: 0s - loss: 0.6655 - accuracy: 0.60 - ETA: 0s - loss: 0.6641 - accuracy: 0.61 - ETA: 0s - loss: 0.6632 - accuracy: 0.61 - 2s 118us/step - loss: 0.6626 - accuracy: 0.6123 - val_loss: 0.6103 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 47us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:07 - loss: 0.6965 - accuracy: 0.20 - ETA: 3s - loss: 0.6934 - accuracy: 0.5019 - ETA: 2s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6909 - accuracy: 0.52 - ETA: 1s - loss: 0.6900 - accuracy: 0.53 - ETA: 1s - loss: 0.6892 - accuracy: 0.54 - ETA: 1s - loss: 0.6881 - accuracy: 0.54 - ETA: 1s - loss: 0.6869 - accuracy: 0.55 - ETA: 1s - loss: 0.6854 - accuracy: 0.56 - ETA: 0s - loss: 0.6835 - accuracy: 0.57 - ETA: 0s - loss: 0.6815 - accuracy: 0.57 - ETA: 0s - loss: 0.6791 - accuracy: 0.58 - ETA: 0s - loss: 0.6774 - accuracy: 0.58 - ETA: 0s - loss: 0.6755 - accuracy: 0.59 - ETA: 0s - loss: 0.6730 - accuracy: 0.59 - ETA: 0s - loss: 0.6711 - accuracy: 0.59 - ETA: 0s - loss: 0.6693 - accuracy: 0.59 - ETA: 0s - loss: 0.6680 - accuracy: 0.60 - ETA: 0s - loss: 0.6660 - accuracy: 0.60 - ETA: 0s - loss: 0.6642 - accuracy: 0.60 - ETA: 0s - loss: 0.6629 - accuracy: 0.61 - ETA: 0s - loss: 0.6612 - accuracy: 0.61 - ETA: 0s - loss: 0.6600 - accuracy: 0.61 - ETA: 0s - loss: 0.6583 - accuracy: 0.61 - ETA: 0s - loss: 0.6568 - accuracy: 0.62 - 1s 113us/step - loss: 0.6564 - accuracy: 0.6229 - val_loss: 0.6053 - val_accuracy: 0.7209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 46us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:10 - loss: 0.6962 - accuracy: 0.40 - ETA: 3s - loss: 0.6931 - accuracy: 0.5111 - ETA: 2s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6911 - accuracy: 0.54 - ETA: 1s - loss: 0.6897 - accuracy: 0.54 - ETA: 1s - loss: 0.6884 - accuracy: 0.55 - ETA: 1s - loss: 0.6868 - accuracy: 0.56 - ETA: 1s - loss: 0.6857 - accuracy: 0.56 - ETA: 1s - loss: 0.6845 - accuracy: 0.57 - ETA: 0s - loss: 0.6832 - accuracy: 0.57 - ETA: 0s - loss: 0.6813 - accuracy: 0.58 - ETA: 0s - loss: 0.6795 - accuracy: 0.58 - ETA: 0s - loss: 0.6779 - accuracy: 0.59 - ETA: 0s - loss: 0.6771 - accuracy: 0.59 - ETA: 0s - loss: 0.6754 - accuracy: 0.59 - ETA: 0s - loss: 0.6746 - accuracy: 0.60 - ETA: 0s - loss: 0.6734 - accuracy: 0.60 - ETA: 0s - loss: 0.6715 - accuracy: 0.60 - ETA: 0s - loss: 0.6701 - accuracy: 0.61 - ETA: 0s - loss: 0.6683 - accuracy: 0.61 - ETA: 0s - loss: 0.6667 - accuracy: 0.61 - ETA: 0s - loss: 0.6651 - accuracy: 0.62 - ETA: 0s - loss: 0.6631 - accuracy: 0.62 - ETA: 0s - loss: 0.6614 - accuracy: 0.62 - ETA: 0s - loss: 0.6591 - accuracy: 0.62 - ETA: 0s - loss: 0.6584 - accuracy: 0.62 - 1s 117us/step - loss: 0.6576 - accuracy: 0.6300 - val_loss: 0.6061 - val_accuracy: 0.7053\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 44us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:06 - loss: 0.6931 - accuracy: 0.50 - ETA: 3s - loss: 0.6937 - accuracy: 0.5091 - ETA: 2s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6924 - accuracy: 0.51 - ETA: 1s - loss: 0.6922 - accuracy: 0.52 - ETA: 1s - loss: 0.6914 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.52 - ETA: 1s - loss: 0.6899 - accuracy: 0.52 - ETA: 1s - loss: 0.6889 - accuracy: 0.52 - ETA: 0s - loss: 0.6882 - accuracy: 0.53 - ETA: 0s - loss: 0.6867 - accuracy: 0.53 - ETA: 0s - loss: 0.6850 - accuracy: 0.54 - ETA: 0s - loss: 0.6832 - accuracy: 0.54 - ETA: 0s - loss: 0.6825 - accuracy: 0.55 - ETA: 0s - loss: 0.6813 - accuracy: 0.55 - ETA: 0s - loss: 0.6805 - accuracy: 0.56 - ETA: 0s - loss: 0.6789 - accuracy: 0.56 - ETA: 0s - loss: 0.6776 - accuracy: 0.57 - ETA: 0s - loss: 0.6763 - accuracy: 0.57 - ETA: 0s - loss: 0.6748 - accuracy: 0.57 - ETA: 0s - loss: 0.6734 - accuracy: 0.58 - ETA: 0s - loss: 0.6717 - accuracy: 0.58 - ETA: 0s - loss: 0.6701 - accuracy: 0.58 - ETA: 0s - loss: 0.6690 - accuracy: 0.59 - ETA: 0s - loss: 0.6672 - accuracy: 0.59 - 1s 114us/step - loss: 0.6664 - accuracy: 0.5991 - val_loss: 0.6200 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 44us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:14 - loss: 0.6910 - accuracy: 0.80 - ETA: 3s - loss: 0.6931 - accuracy: 0.5340 - ETA: 2s - loss: 0.6929 - accuracy: 0.52 - ETA: 1s - loss: 0.6924 - accuracy: 0.53 - ETA: 1s - loss: 0.6912 - accuracy: 0.54 - ETA: 1s - loss: 0.6905 - accuracy: 0.55 - ETA: 1s - loss: 0.6888 - accuracy: 0.55 - ETA: 1s - loss: 0.6875 - accuracy: 0.56 - ETA: 1s - loss: 0.6862 - accuracy: 0.57 - ETA: 0s - loss: 0.6839 - accuracy: 0.58 - ETA: 0s - loss: 0.6823 - accuracy: 0.58 - ETA: 0s - loss: 0.6799 - accuracy: 0.59 - ETA: 0s - loss: 0.6779 - accuracy: 0.59 - ETA: 0s - loss: 0.6762 - accuracy: 0.60 - ETA: 0s - loss: 0.6744 - accuracy: 0.60 - ETA: 0s - loss: 0.6719 - accuracy: 0.61 - ETA: 0s - loss: 0.6693 - accuracy: 0.61 - ETA: 0s - loss: 0.6676 - accuracy: 0.62 - ETA: 0s - loss: 0.6660 - accuracy: 0.62 - ETA: 0s - loss: 0.6639 - accuracy: 0.62 - ETA: 0s - loss: 0.6628 - accuracy: 0.62 - ETA: 0s - loss: 0.6616 - accuracy: 0.62 - ETA: 0s - loss: 0.6601 - accuracy: 0.62 - ETA: 0s - loss: 0.6588 - accuracy: 0.62 - ETA: 0s - loss: 0.6571 - accuracy: 0.63 - 1s 116us/step - loss: 0.6558 - accuracy: 0.6337 - val_loss: 0.6059 - val_accuracy: 0.7145\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 44us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:11 - loss: 0.6957 - accuracy: 0.40 - ETA: 3s - loss: 0.6946 - accuracy: 0.5113 - ETA: 2s - loss: 0.6941 - accuracy: 0.51 - ETA: 1s - loss: 0.6924 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.54 - ETA: 1s - loss: 0.6894 - accuracy: 0.56 - ETA: 1s - loss: 0.6877 - accuracy: 0.57 - ETA: 1s - loss: 0.6856 - accuracy: 0.57 - ETA: 1s - loss: 0.6832 - accuracy: 0.58 - ETA: 0s - loss: 0.6810 - accuracy: 0.58 - ETA: 0s - loss: 0.6785 - accuracy: 0.59 - ETA: 0s - loss: 0.6758 - accuracy: 0.60 - ETA: 0s - loss: 0.6748 - accuracy: 0.60 - ETA: 0s - loss: 0.6728 - accuracy: 0.60 - ETA: 0s - loss: 0.6706 - accuracy: 0.61 - ETA: 0s - loss: 0.6691 - accuracy: 0.61 - ETA: 0s - loss: 0.6673 - accuracy: 0.61 - ETA: 0s - loss: 0.6657 - accuracy: 0.62 - ETA: 0s - loss: 0.6645 - accuracy: 0.62 - ETA: 0s - loss: 0.6624 - accuracy: 0.62 - ETA: 0s - loss: 0.6592 - accuracy: 0.63 - ETA: 0s - loss: 0.6579 - accuracy: 0.63 - ETA: 0s - loss: 0.6557 - accuracy: 0.63 - ETA: 0s - loss: 0.6540 - accuracy: 0.63 - ETA: 0s - loss: 0.6537 - accuracy: 0.63 - ETA: 0s - loss: 0.6519 - accuracy: 0.64 - 1s 116us/step - loss: 0.6518 - accuracy: 0.6417 - val_loss: 0.5951 - val_accuracy: 0.7166\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 44us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:14 - loss: 0.6922 - accuracy: 0.50 - ETA: 3s - loss: 0.6949 - accuracy: 0.4863 - ETA: 2s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6908 - accuracy: 0.52 - ETA: 1s - loss: 0.6898 - accuracy: 0.52 - ETA: 1s - loss: 0.6882 - accuracy: 0.53 - ETA: 1s - loss: 0.6876 - accuracy: 0.53 - ETA: 1s - loss: 0.6868 - accuracy: 0.54 - ETA: 1s - loss: 0.6854 - accuracy: 0.54 - ETA: 1s - loss: 0.6835 - accuracy: 0.55 - ETA: 1s - loss: 0.6826 - accuracy: 0.55 - ETA: 1s - loss: 0.6816 - accuracy: 0.56 - ETA: 1s - loss: 0.6794 - accuracy: 0.56 - ETA: 0s - loss: 0.6781 - accuracy: 0.57 - ETA: 0s - loss: 0.6769 - accuracy: 0.57 - ETA: 0s - loss: 0.6753 - accuracy: 0.57 - ETA: 0s - loss: 0.6744 - accuracy: 0.58 - ETA: 0s - loss: 0.6735 - accuracy: 0.58 - ETA: 0s - loss: 0.6712 - accuracy: 0.59 - ETA: 0s - loss: 0.6695 - accuracy: 0.59 - ETA: 0s - loss: 0.6675 - accuracy: 0.59 - ETA: 0s - loss: 0.6662 - accuracy: 0.60 - ETA: 0s - loss: 0.6641 - accuracy: 0.60 - ETA: 0s - loss: 0.6624 - accuracy: 0.61 - ETA: 0s - loss: 0.6619 - accuracy: 0.61 - ETA: 0s - loss: 0.6603 - accuracy: 0.61 - ETA: 0s - loss: 0.6589 - accuracy: 0.61 - ETA: 0s - loss: 0.6583 - accuracy: 0.61 - ETA: 0s - loss: 0.6570 - accuracy: 0.61 - ETA: 0s - loss: 0.6562 - accuracy: 0.62 - ETA: 0s - loss: 0.6544 - accuracy: 0.62 - ETA: 0s - loss: 0.6523 - accuracy: 0.62 - 2s 142us/step - loss: 0.6518 - accuracy: 0.6270 - val_loss: 0.6026 - val_accuracy: 0.6960\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 48us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:10 - loss: 0.6977 - accuracy: 0.50 - ETA: 3s - loss: 0.6940 - accuracy: 0.5145 - ETA: 2s - loss: 0.6925 - accuracy: 0.53 - ETA: 1s - loss: 0.6917 - accuracy: 0.53 - ETA: 1s - loss: 0.6902 - accuracy: 0.54 - ETA: 1s - loss: 0.6882 - accuracy: 0.56 - ETA: 1s - loss: 0.6860 - accuracy: 0.56 - ETA: 1s - loss: 0.6842 - accuracy: 0.57 - ETA: 0s - loss: 0.6825 - accuracy: 0.58 - ETA: 0s - loss: 0.6801 - accuracy: 0.58 - ETA: 0s - loss: 0.6783 - accuracy: 0.59 - ETA: 0s - loss: 0.6759 - accuracy: 0.59 - ETA: 0s - loss: 0.6744 - accuracy: 0.59 - ETA: 0s - loss: 0.6708 - accuracy: 0.60 - ETA: 0s - loss: 0.6693 - accuracy: 0.60 - ETA: 0s - loss: 0.6671 - accuracy: 0.61 - ETA: 0s - loss: 0.6643 - accuracy: 0.61 - ETA: 0s - loss: 0.6618 - accuracy: 0.62 - ETA: 0s - loss: 0.6587 - accuracy: 0.62 - ETA: 0s - loss: 0.6572 - accuracy: 0.62 - ETA: 0s - loss: 0.6556 - accuracy: 0.62 - ETA: 0s - loss: 0.6538 - accuracy: 0.63 - ETA: 0s - loss: 0.6530 - accuracy: 0.63 - ETA: 0s - loss: 0.6525 - accuracy: 0.63 - 1s 112us/step - loss: 0.6504 - accuracy: 0.6383 - val_loss: 0.5951 - val_accuracy: 0.7209\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:09 - loss: 0.6932 - accuracy: 0.40 - ETA: 3s - loss: 0.6930 - accuracy: 0.5481 - ETA: 2s - loss: 0.6918 - accuracy: 0.55 - ETA: 1s - loss: 0.6907 - accuracy: 0.54 - ETA: 1s - loss: 0.6890 - accuracy: 0.54 - ETA: 1s - loss: 0.6881 - accuracy: 0.55 - ETA: 1s - loss: 0.6861 - accuracy: 0.56 - ETA: 1s - loss: 0.6840 - accuracy: 0.57 - ETA: 0s - loss: 0.6817 - accuracy: 0.58 - ETA: 0s - loss: 0.6798 - accuracy: 0.58 - ETA: 0s - loss: 0.6775 - accuracy: 0.59 - ETA: 0s - loss: 0.6751 - accuracy: 0.60 - ETA: 0s - loss: 0.6737 - accuracy: 0.60 - ETA: 0s - loss: 0.6716 - accuracy: 0.61 - ETA: 0s - loss: 0.6696 - accuracy: 0.61 - ETA: 0s - loss: 0.6667 - accuracy: 0.62 - ETA: 0s - loss: 0.6643 - accuracy: 0.62 - ETA: 0s - loss: 0.6618 - accuracy: 0.62 - ETA: 0s - loss: 0.6594 - accuracy: 0.62 - ETA: 0s - loss: 0.6572 - accuracy: 0.63 - ETA: 0s - loss: 0.6558 - accuracy: 0.63 - ETA: 0s - loss: 0.6538 - accuracy: 0.63 - ETA: 0s - loss: 0.6524 - accuracy: 0.63 - ETA: 0s - loss: 0.6500 - accuracy: 0.64 - 1s 111us/step - loss: 0.6490 - accuracy: 0.6430 - val_loss: 0.5897 - val_accuracy: 0.7152\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:11 - loss: 0.6978 - accuracy: 0.40 - ETA: 3s - loss: 0.6942 - accuracy: 0.5113 - ETA: 2s - loss: 0.6929 - accuracy: 0.53 - ETA: 1s - loss: 0.6917 - accuracy: 0.54 - ETA: 1s - loss: 0.6898 - accuracy: 0.56 - ETA: 1s - loss: 0.6883 - accuracy: 0.56 - ETA: 1s - loss: 0.6874 - accuracy: 0.56 - ETA: 1s - loss: 0.6861 - accuracy: 0.57 - ETA: 1s - loss: 0.6839 - accuracy: 0.58 - ETA: 0s - loss: 0.6815 - accuracy: 0.59 - ETA: 0s - loss: 0.6792 - accuracy: 0.59 - ETA: 0s - loss: 0.6759 - accuracy: 0.60 - ETA: 0s - loss: 0.6730 - accuracy: 0.60 - ETA: 0s - loss: 0.6697 - accuracy: 0.61 - ETA: 0s - loss: 0.6664 - accuracy: 0.61 - ETA: 0s - loss: 0.6653 - accuracy: 0.62 - ETA: 0s - loss: 0.6636 - accuracy: 0.62 - ETA: 0s - loss: 0.6614 - accuracy: 0.62 - ETA: 0s - loss: 0.6593 - accuracy: 0.63 - ETA: 0s - loss: 0.6573 - accuracy: 0.63 - ETA: 0s - loss: 0.6555 - accuracy: 0.63 - ETA: 0s - loss: 0.6534 - accuracy: 0.63 - ETA: 0s - loss: 0.6512 - accuracy: 0.64 - ETA: 0s - loss: 0.6494 - accuracy: 0.64 - 1s 111us/step - loss: 0.6481 - accuracy: 0.6434 - val_loss: 0.5884 - val_accuracy: 0.7131\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:10 - loss: 0.6914 - accuracy: 0.50 - ETA: 3s - loss: 0.6913 - accuracy: 0.5868 - ETA: 2s - loss: 0.6902 - accuracy: 0.57 - ETA: 1s - loss: 0.6883 - accuracy: 0.57 - ETA: 1s - loss: 0.6878 - accuracy: 0.56 - ETA: 1s - loss: 0.6862 - accuracy: 0.57 - ETA: 1s - loss: 0.6851 - accuracy: 0.57 - ETA: 1s - loss: 0.6834 - accuracy: 0.58 - ETA: 1s - loss: 0.6812 - accuracy: 0.59 - ETA: 0s - loss: 0.6790 - accuracy: 0.59 - ETA: 0s - loss: 0.6780 - accuracy: 0.59 - ETA: 0s - loss: 0.6751 - accuracy: 0.60 - ETA: 0s - loss: 0.6725 - accuracy: 0.61 - ETA: 0s - loss: 0.6707 - accuracy: 0.61 - ETA: 0s - loss: 0.6693 - accuracy: 0.61 - ETA: 0s - loss: 0.6671 - accuracy: 0.62 - ETA: 0s - loss: 0.6651 - accuracy: 0.62 - ETA: 0s - loss: 0.6624 - accuracy: 0.62 - ETA: 0s - loss: 0.6606 - accuracy: 0.63 - ETA: 0s - loss: 0.6580 - accuracy: 0.63 - ETA: 0s - loss: 0.6563 - accuracy: 0.63 - ETA: 0s - loss: 0.6553 - accuracy: 0.63 - ETA: 0s - loss: 0.6537 - accuracy: 0.63 - ETA: 0s - loss: 0.6519 - accuracy: 0.64 - ETA: 0s - loss: 0.6501 - accuracy: 0.64 - 1s 113us/step - loss: 0.6497 - accuracy: 0.6445 - val_loss: 0.5998 - val_accuracy: 0.6982\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:53 - loss: 0.6919 - accuracy: 0.80 - ETA: 3s - loss: 0.6928 - accuracy: 0.5400 - ETA: 2s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - 1s 105us/step - loss: 0.6921 - accuracy: 0.5140 - val_loss: 0.6894 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 49us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:06 - loss: 0.6957 - accuracy: 0.20 - ETA: 3s - loss: 0.6935 - accuracy: 0.5164 - ETA: 2s - loss: 0.6935 - accuracy: 0.52 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - 1s 112us/step - loss: 0.6932 - accuracy: 0.5146 - val_loss: 0.6920 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 47us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 1:55 - loss: 0.6959 - accuracy: 0.30 - ETA: 3s - loss: 0.6936 - accuracy: 0.4875 - ETA: 2s - loss: 0.6936 - accuracy: 0.49 - ETA: 1s - loss: 0.6935 - accuracy: 0.49 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6934 - accuracy: 0.50 - ETA: 1s - loss: 0.6934 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - 1s 113us/step - loss: 0.6931 - accuracy: 0.5126 - val_loss: 0.6917 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 47us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:55 - loss: 0.6949 - accuracy: 0.50 - ETA: 2s - loss: 0.6939 - accuracy: 0.4825 - ETA: 1s - loss: 0.6937 - accuracy: 0.49 - ETA: 1s - loss: 0.6935 - accuracy: 0.49 - ETA: 1s - loss: 0.6931 - accuracy: 0.50 - ETA: 1s - loss: 0.6929 - accuracy: 0.50 - ETA: 1s - loss: 0.6931 - accuracy: 0.50 - ETA: 1s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6929 - accuracy: 0.50 - ETA: 0s - loss: 0.6928 - accuracy: 0.50 - ETA: 0s - loss: 0.6926 - accuracy: 0.50 - ETA: 0s - loss: 0.6926 - accuracy: 0.50 - ETA: 0s - loss: 0.6923 - accuracy: 0.50 - ETA: 0s - loss: 0.6922 - accuracy: 0.50 - ETA: 0s - loss: 0.6921 - accuracy: 0.50 - ETA: 0s - loss: 0.6919 - accuracy: 0.51 - ETA: 0s - loss: 0.6915 - accuracy: 0.51 - ETA: 0s - loss: 0.6913 - accuracy: 0.51 - ETA: 0s - loss: 0.6912 - accuracy: 0.51 - ETA: 0s - loss: 0.6910 - accuracy: 0.51 - ETA: 0s - loss: 0.6908 - accuracy: 0.51 - ETA: 0s - loss: 0.6904 - accuracy: 0.51 - ETA: 0s - loss: 0.6904 - accuracy: 0.51 - 1s 109us/step - loss: 0.6905 - accuracy: 0.5129 - val_loss: 0.6846 - val_accuracy: 0.5433\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:53 - loss: 0.6937 - accuracy: 0.50 - ETA: 3s - loss: 0.6934 - accuracy: 0.5091 - ETA: 2s - loss: 0.6937 - accuracy: 0.49 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - 1s 113us/step - loss: 0.6933 - accuracy: 0.5127 - val_loss: 0.6924 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 44us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:01 - loss: 0.6934 - accuracy: 0.50 - ETA: 3s - loss: 0.6935 - accuracy: 0.5056 - ETA: 2s - loss: 0.6933 - accuracy: 0.50 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.50 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - 1s 113us/step - loss: 0.6927 - accuracy: 0.5145 - val_loss: 0.6911 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 47us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:56 - loss: 0.6951 - accuracy: 0.40 - ETA: 3s - loss: 0.6947 - accuracy: 0.4556 - ETA: 2s - loss: 0.6945 - accuracy: 0.47 - ETA: 1s - loss: 0.6945 - accuracy: 0.48 - ETA: 1s - loss: 0.6943 - accuracy: 0.49 - ETA: 1s - loss: 0.6942 - accuracy: 0.50 - ETA: 1s - loss: 0.6940 - accuracy: 0.50 - ETA: 1s - loss: 0.6940 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - 1s 108us/step - loss: 0.6931 - accuracy: 0.5122 - val_loss: 0.6900 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:51 - loss: 0.6826 - accuracy: 0.90 - ETA: 3s - loss: 0.6950 - accuracy: 0.4926 - ETA: 2s - loss: 0.6945 - accuracy: 0.50 - ETA: 1s - loss: 0.6943 - accuracy: 0.51 - ETA: 1s - loss: 0.6939 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.52 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.52 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.53 - ETA: 0s - loss: 0.6924 - accuracy: 0.53 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.53 - ETA: 0s - loss: 0.6922 - accuracy: 0.53 - ETA: 0s - loss: 0.6919 - accuracy: 0.53 - ETA: 0s - loss: 0.6918 - accuracy: 0.53 - ETA: 0s - loss: 0.6917 - accuracy: 0.53 - 1s 106us/step - loss: 0.6913 - accuracy: 0.5402 - val_loss: 0.6857 - val_accuracy: 0.5852\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 44us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:52 - loss: 0.6954 - accuracy: 0.30 - ETA: 3s - loss: 0.6949 - accuracy: 0.4891 - ETA: 2s - loss: 0.6939 - accuracy: 0.50 - ETA: 1s - loss: 0.6942 - accuracy: 0.50 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - 1s 108us/step - loss: 0.6928 - accuracy: 0.5115 - val_loss: 0.6895 - val_accuracy: 0.5526\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:51 - loss: 0.6948 - accuracy: 0.50 - ETA: 2s - loss: 0.6937 - accuracy: 0.5321 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 1s - loss: 0.6935 - accuracy: 0.52 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.53 - ETA: 0s - loss: 0.6931 - accuracy: 0.53 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.53 - ETA: 0s - loss: 0.6926 - accuracy: 0.53 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - 1s 108us/step - loss: 0.6924 - accuracy: 0.5308 - val_loss: 0.6904 - val_accuracy: 0.5874\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:51 - loss: 0.6947 - accuracy: 0.20 - ETA: 2s - loss: 0.6946 - accuracy: 0.4857 - ETA: 2s - loss: 0.6947 - accuracy: 0.48 - ETA: 1s - loss: 0.6944 - accuracy: 0.50 - ETA: 1s - loss: 0.6944 - accuracy: 0.50 - ETA: 1s - loss: 0.6944 - accuracy: 0.50 - ETA: 1s - loss: 0.6944 - accuracy: 0.50 - ETA: 1s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - 1s 108us/step - loss: 0.6936 - accuracy: 0.5112 - val_loss: 0.6916 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:52 - loss: 0.6923 - accuracy: 0.80 - ETA: 3s - loss: 0.6943 - accuracy: 0.5164 - ETA: 2s - loss: 0.6938 - accuracy: 0.52 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6932 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - 1s 107us/step - loss: 0.6921 - accuracy: 0.5199 - val_loss: 0.6885 - val_accuracy: 0.5433\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:53 - loss: 0.6907 - accuracy: 0.60 - ETA: 3s - loss: 0.6953 - accuracy: 0.4745 - ETA: 2s - loss: 0.6951 - accuracy: 0.49 - ETA: 1s - loss: 0.6952 - accuracy: 0.49 - ETA: 1s - loss: 0.6950 - accuracy: 0.49 - ETA: 1s - loss: 0.6947 - accuracy: 0.50 - ETA: 1s - loss: 0.6946 - accuracy: 0.50 - ETA: 1s - loss: 0.6944 - accuracy: 0.50 - ETA: 0s - loss: 0.6944 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - 1s 105us/step - loss: 0.6923 - accuracy: 0.5223 - val_loss: 0.6863 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 44us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:52 - loss: 0.6943 - accuracy: 0.60 - ETA: 3s - loss: 0.6942 - accuracy: 0.5145 - ETA: 2s - loss: 0.6947 - accuracy: 0.50 - ETA: 1s - loss: 0.6946 - accuracy: 0.50 - ETA: 1s - loss: 0.6946 - accuracy: 0.50 - ETA: 1s - loss: 0.6945 - accuracy: 0.50 - ETA: 1s - loss: 0.6944 - accuracy: 0.51 - ETA: 1s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - 1s 107us/step - loss: 0.6920 - accuracy: 0.5187 - val_loss: 0.6863 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 1:52 - loss: 0.6959 - accuracy: 0.40 - ETA: 2s - loss: 0.6949 - accuracy: 0.4825 - ETA: 1s - loss: 0.6948 - accuracy: 0.49 - ETA: 1s - loss: 0.6948 - accuracy: 0.50 - ETA: 1s - loss: 0.6948 - accuracy: 0.50 - ETA: 1s - loss: 0.6943 - accuracy: 0.51 - ETA: 1s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6919 - accuracy: 0.52 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - ETA: 0s - loss: 0.6917 - accuracy: 0.52 - 1s 107us/step - loss: 0.6916 - accuracy: 0.5266 - val_loss: 0.6851 - val_accuracy: 0.5334\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:55 - loss: 0.6902 - accuracy: 0.60 - ETA: 3s - loss: 0.6938 - accuracy: 0.5382 - ETA: 2s - loss: 0.6940 - accuracy: 0.52 - ETA: 1s - loss: 0.6942 - accuracy: 0.52 - ETA: 1s - loss: 0.6940 - accuracy: 0.52 - ETA: 1s - loss: 0.6940 - accuracy: 0.52 - ETA: 1s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - 1s 105us/step - loss: 0.6925 - accuracy: 0.5205 - val_loss: 0.6882 - val_accuracy: 0.5526\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:59 - loss: 0.6986 - accuracy: 0.40 - ETA: 3s - loss: 0.6956 - accuracy: 0.5054 - ETA: 2s - loss: 0.6949 - accuracy: 0.51 - ETA: 1s - loss: 0.6952 - accuracy: 0.49 - ETA: 1s - loss: 0.6950 - accuracy: 0.50 - ETA: 1s - loss: 0.6948 - accuracy: 0.51 - ETA: 1s - loss: 0.6947 - accuracy: 0.51 - ETA: 1s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - 1s 113us/step - loss: 0.6929 - accuracy: 0.5210 - val_loss: 0.6883 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 46us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:56 - loss: 0.6963 - accuracy: 0.50 - ETA: 3s - loss: 0.6954 - accuracy: 0.4868 - ETA: 2s - loss: 0.6950 - accuracy: 0.49 - ETA: 1s - loss: 0.6951 - accuracy: 0.49 - ETA: 1s - loss: 0.6951 - accuracy: 0.49 - ETA: 1s - loss: 0.6951 - accuracy: 0.49 - ETA: 1s - loss: 0.6949 - accuracy: 0.50 - ETA: 1s - loss: 0.6949 - accuracy: 0.50 - ETA: 1s - loss: 0.6949 - accuracy: 0.50 - ETA: 0s - loss: 0.6948 - accuracy: 0.50 - ETA: 0s - loss: 0.6946 - accuracy: 0.50 - ETA: 0s - loss: 0.6946 - accuracy: 0.50 - ETA: 0s - loss: 0.6946 - accuracy: 0.50 - ETA: 0s - loss: 0.6946 - accuracy: 0.50 - ETA: 0s - loss: 0.6946 - accuracy: 0.50 - ETA: 0s - loss: 0.6945 - accuracy: 0.50 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - 1s 115us/step - loss: 0.6938 - accuracy: 0.5168 - val_loss: 0.6917 - val_accuracy: 0.5114\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 44us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:30 - loss: 0.6861 - accuracy: 0.60 - ETA: 3s - loss: 0.6935 - accuracy: 0.5038 - ETA: 2s - loss: 0.6928 - accuracy: 0.51 - ETA: 1s - loss: 0.6912 - accuracy: 0.52 - ETA: 1s - loss: 0.6911 - accuracy: 0.51 - ETA: 1s - loss: 0.6908 - accuracy: 0.51 - ETA: 1s - loss: 0.6907 - accuracy: 0.51 - ETA: 1s - loss: 0.6898 - accuracy: 0.52 - ETA: 1s - loss: 0.6891 - accuracy: 0.52 - ETA: 1s - loss: 0.6879 - accuracy: 0.52 - ETA: 0s - loss: 0.6876 - accuracy: 0.52 - ETA: 0s - loss: 0.6864 - accuracy: 0.52 - ETA: 0s - loss: 0.6849 - accuracy: 0.53 - ETA: 0s - loss: 0.6838 - accuracy: 0.53 - ETA: 0s - loss: 0.6828 - accuracy: 0.53 - ETA: 0s - loss: 0.6819 - accuracy: 0.54 - ETA: 0s - loss: 0.6815 - accuracy: 0.55 - ETA: 0s - loss: 0.6805 - accuracy: 0.55 - ETA: 0s - loss: 0.6796 - accuracy: 0.56 - ETA: 0s - loss: 0.6786 - accuracy: 0.56 - ETA: 0s - loss: 0.6776 - accuracy: 0.57 - ETA: 0s - loss: 0.6769 - accuracy: 0.57 - ETA: 0s - loss: 0.6762 - accuracy: 0.57 - ETA: 0s - loss: 0.6751 - accuracy: 0.58 - ETA: 0s - loss: 0.6749 - accuracy: 0.58 - ETA: 0s - loss: 0.6738 - accuracy: 0.58 - 1s 118us/step - loss: 0.6737 - accuracy: 0.5889 - val_loss: 0.6417 - val_accuracy: 0.7060\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 46us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:29 - loss: 0.6939 - accuracy: 0.50 - ETA: 4s - loss: 0.6937 - accuracy: 0.4920 - ETA: 2s - loss: 0.6937 - accuracy: 0.47 - ETA: 2s - loss: 0.6935 - accuracy: 0.48 - ETA: 1s - loss: 0.6931 - accuracy: 0.49 - ETA: 1s - loss: 0.6928 - accuracy: 0.49 - ETA: 1s - loss: 0.6914 - accuracy: 0.50 - ETA: 1s - loss: 0.6912 - accuracy: 0.50 - ETA: 1s - loss: 0.6901 - accuracy: 0.50 - ETA: 1s - loss: 0.6895 - accuracy: 0.50 - ETA: 1s - loss: 0.6884 - accuracy: 0.50 - ETA: 0s - loss: 0.6876 - accuracy: 0.51 - ETA: 0s - loss: 0.6863 - accuracy: 0.51 - ETA: 0s - loss: 0.6857 - accuracy: 0.52 - ETA: 0s - loss: 0.6844 - accuracy: 0.52 - ETA: 0s - loss: 0.6834 - accuracy: 0.53 - ETA: 0s - loss: 0.6831 - accuracy: 0.54 - ETA: 0s - loss: 0.6823 - accuracy: 0.54 - ETA: 0s - loss: 0.6812 - accuracy: 0.54 - ETA: 0s - loss: 0.6801 - accuracy: 0.55 - ETA: 0s - loss: 0.6794 - accuracy: 0.55 - ETA: 0s - loss: 0.6784 - accuracy: 0.56 - ETA: 0s - loss: 0.6772 - accuracy: 0.56 - ETA: 0s - loss: 0.6760 - accuracy: 0.57 - ETA: 0s - loss: 0.6750 - accuracy: 0.57 - ETA: 0s - loss: 0.6739 - accuracy: 0.57 - ETA: 0s - loss: 0.6729 - accuracy: 0.58 - ETA: 0s - loss: 0.6720 - accuracy: 0.58 - 2s 127us/step - loss: 0.6717 - accuracy: 0.5850 - val_loss: 0.6355 - val_accuracy: 0.7067\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:37 - loss: 0.6962 - accuracy: 0.30 - ETA: 4s - loss: 0.6938 - accuracy: 0.5019 - ETA: 2s - loss: 0.6936 - accuracy: 0.50 - ETA: 2s - loss: 0.6933 - accuracy: 0.50 - ETA: 1s - loss: 0.6932 - accuracy: 0.50 - ETA: 1s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6915 - accuracy: 0.51 - ETA: 1s - loss: 0.6917 - accuracy: 0.51 - ETA: 1s - loss: 0.6912 - accuracy: 0.51 - ETA: 1s - loss: 0.6903 - accuracy: 0.52 - ETA: 0s - loss: 0.6895 - accuracy: 0.52 - ETA: 0s - loss: 0.6884 - accuracy: 0.53 - ETA: 0s - loss: 0.6871 - accuracy: 0.53 - ETA: 0s - loss: 0.6864 - accuracy: 0.53 - ETA: 0s - loss: 0.6853 - accuracy: 0.54 - ETA: 0s - loss: 0.6840 - accuracy: 0.54 - ETA: 0s - loss: 0.6827 - accuracy: 0.54 - ETA: 0s - loss: 0.6813 - accuracy: 0.55 - ETA: 0s - loss: 0.6796 - accuracy: 0.55 - ETA: 0s - loss: 0.6790 - accuracy: 0.55 - ETA: 0s - loss: 0.6775 - accuracy: 0.56 - ETA: 0s - loss: 0.6764 - accuracy: 0.56 - ETA: 0s - loss: 0.6745 - accuracy: 0.56 - ETA: 0s - loss: 0.6731 - accuracy: 0.56 - ETA: 0s - loss: 0.6721 - accuracy: 0.56 - ETA: 0s - loss: 0.6710 - accuracy: 0.57 - 1s 118us/step - loss: 0.6709 - accuracy: 0.5711 - val_loss: 0.6276 - val_accuracy: 0.7017\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:29 - loss: 0.6937 - accuracy: 0.80 - ETA: 3s - loss: 0.6933 - accuracy: 0.5269 - ETA: 2s - loss: 0.6922 - accuracy: 0.53 - ETA: 1s - loss: 0.6921 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6914 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.52 - ETA: 1s - loss: 0.6891 - accuracy: 0.52 - ETA: 1s - loss: 0.6893 - accuracy: 0.52 - ETA: 1s - loss: 0.6880 - accuracy: 0.52 - ETA: 0s - loss: 0.6878 - accuracy: 0.52 - ETA: 0s - loss: 0.6874 - accuracy: 0.52 - ETA: 0s - loss: 0.6861 - accuracy: 0.53 - ETA: 0s - loss: 0.6854 - accuracy: 0.53 - ETA: 0s - loss: 0.6844 - accuracy: 0.53 - ETA: 0s - loss: 0.6834 - accuracy: 0.54 - ETA: 0s - loss: 0.6826 - accuracy: 0.54 - ETA: 0s - loss: 0.6812 - accuracy: 0.55 - ETA: 0s - loss: 0.6805 - accuracy: 0.55 - ETA: 0s - loss: 0.6794 - accuracy: 0.55 - ETA: 0s - loss: 0.6782 - accuracy: 0.56 - ETA: 0s - loss: 0.6768 - accuracy: 0.56 - ETA: 0s - loss: 0.6754 - accuracy: 0.57 - ETA: 0s - loss: 0.6753 - accuracy: 0.57 - ETA: 0s - loss: 0.6746 - accuracy: 0.57 - ETA: 0s - loss: 0.6733 - accuracy: 0.58 - 1s 118us/step - loss: 0.6730 - accuracy: 0.5845 - val_loss: 0.6373 - val_accuracy: 0.6967\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 46us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:33 - loss: 0.6927 - accuracy: 0.30 - ETA: 4s - loss: 0.6937 - accuracy: 0.5059 - ETA: 2s - loss: 0.6938 - accuracy: 0.48 - ETA: 2s - loss: 0.6936 - accuracy: 0.48 - ETA: 1s - loss: 0.6934 - accuracy: 0.49 - ETA: 1s - loss: 0.6930 - accuracy: 0.49 - ETA: 1s - loss: 0.6924 - accuracy: 0.50 - ETA: 1s - loss: 0.6921 - accuracy: 0.50 - ETA: 1s - loss: 0.6915 - accuracy: 0.51 - ETA: 1s - loss: 0.6909 - accuracy: 0.52 - ETA: 0s - loss: 0.6902 - accuracy: 0.53 - ETA: 0s - loss: 0.6893 - accuracy: 0.54 - ETA: 0s - loss: 0.6884 - accuracy: 0.55 - ETA: 0s - loss: 0.6873 - accuracy: 0.55 - ETA: 0s - loss: 0.6863 - accuracy: 0.55 - ETA: 0s - loss: 0.6852 - accuracy: 0.56 - ETA: 0s - loss: 0.6844 - accuracy: 0.56 - ETA: 0s - loss: 0.6836 - accuracy: 0.57 - ETA: 0s - loss: 0.6828 - accuracy: 0.57 - ETA: 0s - loss: 0.6812 - accuracy: 0.58 - ETA: 0s - loss: 0.6806 - accuracy: 0.58 - ETA: 0s - loss: 0.6792 - accuracy: 0.59 - ETA: 0s - loss: 0.6783 - accuracy: 0.59 - ETA: 0s - loss: 0.6772 - accuracy: 0.59 - ETA: 0s - loss: 0.6763 - accuracy: 0.60 - ETA: 0s - loss: 0.6750 - accuracy: 0.60 - ETA: 0s - loss: 0.6739 - accuracy: 0.60 - 2s 122us/step - loss: 0.6737 - accuracy: 0.6080 - val_loss: 0.6338 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:33 - loss: 0.6940 - accuracy: 0.20 - ETA: 4s - loss: 0.6937 - accuracy: 0.4638 - ETA: 2s - loss: 0.6934 - accuracy: 0.48 - ETA: 2s - loss: 0.6930 - accuracy: 0.49 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6905 - accuracy: 0.52 - ETA: 1s - loss: 0.6891 - accuracy: 0.53 - ETA: 1s - loss: 0.6883 - accuracy: 0.54 - ETA: 0s - loss: 0.6871 - accuracy: 0.54 - ETA: 0s - loss: 0.6864 - accuracy: 0.54 - ETA: 0s - loss: 0.6855 - accuracy: 0.54 - ETA: 0s - loss: 0.6844 - accuracy: 0.54 - ETA: 0s - loss: 0.6839 - accuracy: 0.54 - ETA: 0s - loss: 0.6821 - accuracy: 0.55 - ETA: 0s - loss: 0.6810 - accuracy: 0.55 - ETA: 0s - loss: 0.6800 - accuracy: 0.55 - ETA: 0s - loss: 0.6795 - accuracy: 0.55 - ETA: 0s - loss: 0.6788 - accuracy: 0.55 - ETA: 0s - loss: 0.6778 - accuracy: 0.55 - ETA: 0s - loss: 0.6777 - accuracy: 0.55 - ETA: 0s - loss: 0.6766 - accuracy: 0.56 - ETA: 0s - loss: 0.6762 - accuracy: 0.56 - ETA: 0s - loss: 0.6752 - accuracy: 0.56 - ETA: 0s - loss: 0.6742 - accuracy: 0.56 - 1s 118us/step - loss: 0.6739 - accuracy: 0.5642 - val_loss: 0.6384 - val_accuracy: 0.7145\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:37 - loss: 0.6939 - accuracy: 0.40 - ETA: 4s - loss: 0.6936 - accuracy: 0.5040 - ETA: 2s - loss: 0.6927 - accuracy: 0.52 - ETA: 2s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.52 - ETA: 1s - loss: 0.6890 - accuracy: 0.53 - ETA: 1s - loss: 0.6882 - accuracy: 0.53 - ETA: 1s - loss: 0.6866 - accuracy: 0.54 - ETA: 1s - loss: 0.6845 - accuracy: 0.55 - ETA: 1s - loss: 0.6817 - accuracy: 0.56 - ETA: 0s - loss: 0.6803 - accuracy: 0.56 - ETA: 0s - loss: 0.6793 - accuracy: 0.57 - ETA: 0s - loss: 0.6779 - accuracy: 0.57 - ETA: 0s - loss: 0.6761 - accuracy: 0.57 - ETA: 0s - loss: 0.6744 - accuracy: 0.58 - ETA: 0s - loss: 0.6730 - accuracy: 0.58 - ETA: 0s - loss: 0.6712 - accuracy: 0.59 - ETA: 0s - loss: 0.6697 - accuracy: 0.59 - ETA: 0s - loss: 0.6683 - accuracy: 0.59 - ETA: 0s - loss: 0.6664 - accuracy: 0.60 - ETA: 0s - loss: 0.6647 - accuracy: 0.60 - ETA: 0s - loss: 0.6637 - accuracy: 0.60 - ETA: 0s - loss: 0.6631 - accuracy: 0.60 - ETA: 0s - loss: 0.6610 - accuracy: 0.61 - ETA: 0s - loss: 0.6591 - accuracy: 0.61 - ETA: 0s - loss: 0.6580 - accuracy: 0.61 - ETA: 0s - loss: 0.6573 - accuracy: 0.61 - 2s 128us/step - loss: 0.6565 - accuracy: 0.6183 - val_loss: 0.6044 - val_accuracy: 0.7081\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 44us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:25 - loss: 0.6946 - accuracy: 0.40 - ETA: 3s - loss: 0.6944 - accuracy: 0.4765 - ETA: 2s - loss: 0.6929 - accuracy: 0.50 - ETA: 1s - loss: 0.6917 - accuracy: 0.50 - ETA: 1s - loss: 0.6910 - accuracy: 0.51 - ETA: 1s - loss: 0.6884 - accuracy: 0.52 - ETA: 1s - loss: 0.6869 - accuracy: 0.53 - ETA: 1s - loss: 0.6849 - accuracy: 0.54 - ETA: 1s - loss: 0.6835 - accuracy: 0.54 - ETA: 1s - loss: 0.6823 - accuracy: 0.55 - ETA: 0s - loss: 0.6796 - accuracy: 0.56 - ETA: 0s - loss: 0.6770 - accuracy: 0.57 - ETA: 0s - loss: 0.6753 - accuracy: 0.58 - ETA: 0s - loss: 0.6738 - accuracy: 0.58 - ETA: 0s - loss: 0.6714 - accuracy: 0.59 - ETA: 0s - loss: 0.6689 - accuracy: 0.59 - ETA: 0s - loss: 0.6674 - accuracy: 0.60 - ETA: 0s - loss: 0.6641 - accuracy: 0.60 - ETA: 0s - loss: 0.6627 - accuracy: 0.61 - ETA: 0s - loss: 0.6613 - accuracy: 0.61 - ETA: 0s - loss: 0.6597 - accuracy: 0.61 - ETA: 0s - loss: 0.6573 - accuracy: 0.61 - ETA: 0s - loss: 0.6555 - accuracy: 0.62 - ETA: 0s - loss: 0.6549 - accuracy: 0.62 - ETA: 0s - loss: 0.6536 - accuracy: 0.62 - ETA: 0s - loss: 0.6519 - accuracy: 0.63 - 2s 120us/step - loss: 0.6513 - accuracy: 0.6307 - val_loss: 0.6010 - val_accuracy: 0.7088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 45us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:37 - loss: 0.6943 - accuracy: 0.60 - ETA: 4s - loss: 0.6938 - accuracy: 0.5000 - ETA: 2s - loss: 0.6936 - accuracy: 0.50 - ETA: 2s - loss: 0.6931 - accuracy: 0.50 - ETA: 1s - loss: 0.6924 - accuracy: 0.51 - ETA: 1s - loss: 0.6906 - accuracy: 0.51 - ETA: 1s - loss: 0.6891 - accuracy: 0.51 - ETA: 1s - loss: 0.6873 - accuracy: 0.52 - ETA: 1s - loss: 0.6859 - accuracy: 0.52 - ETA: 1s - loss: 0.6845 - accuracy: 0.53 - ETA: 1s - loss: 0.6836 - accuracy: 0.54 - ETA: 0s - loss: 0.6812 - accuracy: 0.55 - ETA: 0s - loss: 0.6789 - accuracy: 0.56 - ETA: 0s - loss: 0.6775 - accuracy: 0.56 - ETA: 0s - loss: 0.6758 - accuracy: 0.57 - ETA: 0s - loss: 0.6743 - accuracy: 0.58 - ETA: 0s - loss: 0.6725 - accuracy: 0.58 - ETA: 0s - loss: 0.6704 - accuracy: 0.59 - ETA: 0s - loss: 0.6690 - accuracy: 0.59 - ETA: 0s - loss: 0.6672 - accuracy: 0.60 - ETA: 0s - loss: 0.6658 - accuracy: 0.60 - ETA: 0s - loss: 0.6641 - accuracy: 0.60 - ETA: 0s - loss: 0.6625 - accuracy: 0.61 - ETA: 0s - loss: 0.6607 - accuracy: 0.61 - ETA: 0s - loss: 0.6595 - accuracy: 0.61 - ETA: 0s - loss: 0.6581 - accuracy: 0.61 - ETA: 0s - loss: 0.6572 - accuracy: 0.62 - 2s 124us/step - loss: 0.6564 - accuracy: 0.6234 - val_loss: 0.6045 - val_accuracy: 0.7067\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:28 - loss: 0.6935 - accuracy: 0.40 - ETA: 3s - loss: 0.6942 - accuracy: 0.4942 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6919 - accuracy: 0.54 - ETA: 1s - loss: 0.6906 - accuracy: 0.54 - ETA: 1s - loss: 0.6896 - accuracy: 0.55 - ETA: 1s - loss: 0.6875 - accuracy: 0.56 - ETA: 1s - loss: 0.6858 - accuracy: 0.56 - ETA: 1s - loss: 0.6827 - accuracy: 0.57 - ETA: 1s - loss: 0.6811 - accuracy: 0.57 - ETA: 0s - loss: 0.6784 - accuracy: 0.58 - ETA: 0s - loss: 0.6762 - accuracy: 0.59 - ETA: 0s - loss: 0.6739 - accuracy: 0.59 - ETA: 0s - loss: 0.6722 - accuracy: 0.59 - ETA: 0s - loss: 0.6695 - accuracy: 0.60 - ETA: 0s - loss: 0.6666 - accuracy: 0.60 - ETA: 0s - loss: 0.6650 - accuracy: 0.60 - ETA: 0s - loss: 0.6634 - accuracy: 0.61 - ETA: 0s - loss: 0.6608 - accuracy: 0.61 - ETA: 0s - loss: 0.6599 - accuracy: 0.61 - ETA: 0s - loss: 0.6571 - accuracy: 0.62 - ETA: 0s - loss: 0.6564 - accuracy: 0.62 - ETA: 0s - loss: 0.6544 - accuracy: 0.62 - ETA: 0s - loss: 0.6532 - accuracy: 0.62 - ETA: 0s - loss: 0.6520 - accuracy: 0.62 - ETA: 0s - loss: 0.6504 - accuracy: 0.63 - 2s 119us/step - loss: 0.6503 - accuracy: 0.6328 - val_loss: 0.5997 - val_accuracy: 0.7166\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 47us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:25 - loss: 0.6928 - accuracy: 0.60 - ETA: 3s - loss: 0.6929 - accuracy: 0.5148 - ETA: 2s - loss: 0.6919 - accuracy: 0.53 - ETA: 1s - loss: 0.6909 - accuracy: 0.52 - ETA: 1s - loss: 0.6887 - accuracy: 0.53 - ETA: 1s - loss: 0.6880 - accuracy: 0.53 - ETA: 1s - loss: 0.6873 - accuracy: 0.53 - ETA: 1s - loss: 0.6862 - accuracy: 0.54 - ETA: 1s - loss: 0.6850 - accuracy: 0.55 - ETA: 0s - loss: 0.6835 - accuracy: 0.55 - ETA: 0s - loss: 0.6814 - accuracy: 0.56 - ETA: 0s - loss: 0.6806 - accuracy: 0.56 - ETA: 0s - loss: 0.6787 - accuracy: 0.57 - ETA: 0s - loss: 0.6772 - accuracy: 0.57 - ETA: 0s - loss: 0.6751 - accuracy: 0.57 - ETA: 0s - loss: 0.6731 - accuracy: 0.58 - ETA: 0s - loss: 0.6713 - accuracy: 0.58 - ETA: 0s - loss: 0.6698 - accuracy: 0.59 - ETA: 0s - loss: 0.6690 - accuracy: 0.59 - ETA: 0s - loss: 0.6665 - accuracy: 0.60 - ETA: 0s - loss: 0.6646 - accuracy: 0.60 - ETA: 0s - loss: 0.6632 - accuracy: 0.60 - ETA: 0s - loss: 0.6614 - accuracy: 0.61 - ETA: 0s - loss: 0.6601 - accuracy: 0.61 - ETA: 0s - loss: 0.6592 - accuracy: 0.61 - 1s 116us/step - loss: 0.6590 - accuracy: 0.6167 - val_loss: 0.6146 - val_accuracy: 0.7180\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 44us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:30 - loss: 0.6962 - accuracy: 0.30 - ETA: 4s - loss: 0.6939 - accuracy: 0.4843 - ETA: 2s - loss: 0.6936 - accuracy: 0.49 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6920 - accuracy: 0.53 - ETA: 1s - loss: 0.6907 - accuracy: 0.54 - ETA: 1s - loss: 0.6893 - accuracy: 0.55 - ETA: 1s - loss: 0.6874 - accuracy: 0.56 - ETA: 1s - loss: 0.6858 - accuracy: 0.57 - ETA: 0s - loss: 0.6835 - accuracy: 0.57 - ETA: 0s - loss: 0.6805 - accuracy: 0.58 - ETA: 0s - loss: 0.6774 - accuracy: 0.59 - ETA: 0s - loss: 0.6752 - accuracy: 0.59 - ETA: 0s - loss: 0.6738 - accuracy: 0.60 - ETA: 0s - loss: 0.6719 - accuracy: 0.60 - ETA: 0s - loss: 0.6692 - accuracy: 0.61 - ETA: 0s - loss: 0.6677 - accuracy: 0.61 - ETA: 0s - loss: 0.6658 - accuracy: 0.61 - ETA: 0s - loss: 0.6643 - accuracy: 0.61 - ETA: 0s - loss: 0.6623 - accuracy: 0.61 - ETA: 0s - loss: 0.6616 - accuracy: 0.61 - ETA: 0s - loss: 0.6589 - accuracy: 0.62 - ETA: 0s - loss: 0.6569 - accuracy: 0.62 - ETA: 0s - loss: 0.6547 - accuracy: 0.62 - ETA: 0s - loss: 0.6528 - accuracy: 0.63 - ETA: 0s - loss: 0.6516 - accuracy: 0.63 - 2s 121us/step - loss: 0.6515 - accuracy: 0.6318 - val_loss: 0.6011 - val_accuracy: 0.6974\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 44us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:24 - loss: 0.7030 - accuracy: 0.30 - ETA: 3s - loss: 0.6918 - accuracy: 0.5423 - ETA: 2s - loss: 0.6904 - accuracy: 0.53 - ETA: 1s - loss: 0.6890 - accuracy: 0.53 - ETA: 1s - loss: 0.6874 - accuracy: 0.54 - ETA: 1s - loss: 0.6867 - accuracy: 0.54 - ETA: 1s - loss: 0.6845 - accuracy: 0.55 - ETA: 1s - loss: 0.6838 - accuracy: 0.56 - ETA: 1s - loss: 0.6826 - accuracy: 0.56 - ETA: 1s - loss: 0.6809 - accuracy: 0.57 - ETA: 0s - loss: 0.6786 - accuracy: 0.58 - ETA: 0s - loss: 0.6758 - accuracy: 0.59 - ETA: 0s - loss: 0.6740 - accuracy: 0.59 - ETA: 0s - loss: 0.6711 - accuracy: 0.60 - ETA: 0s - loss: 0.6674 - accuracy: 0.61 - ETA: 0s - loss: 0.6661 - accuracy: 0.61 - ETA: 0s - loss: 0.6639 - accuracy: 0.61 - ETA: 0s - loss: 0.6614 - accuracy: 0.61 - ETA: 0s - loss: 0.6603 - accuracy: 0.61 - ETA: 0s - loss: 0.6573 - accuracy: 0.62 - ETA: 0s - loss: 0.6570 - accuracy: 0.62 - ETA: 0s - loss: 0.6548 - accuracy: 0.62 - ETA: 0s - loss: 0.6530 - accuracy: 0.63 - ETA: 0s - loss: 0.6514 - accuracy: 0.63 - ETA: 0s - loss: 0.6501 - accuracy: 0.63 - ETA: 0s - loss: 0.6482 - accuracy: 0.63 - ETA: 0s - loss: 0.6469 - accuracy: 0.64 - 2s 121us/step - loss: 0.6470 - accuracy: 0.6413 - val_loss: 0.5931 - val_accuracy: 0.7259\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 46us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:30 - loss: 0.6931 - accuracy: 0.80 - ETA: 4s - loss: 0.6938 - accuracy: 0.5275 - ETA: 2s - loss: 0.6937 - accuracy: 0.50 - ETA: 2s - loss: 0.6928 - accuracy: 0.52 - ETA: 1s - loss: 0.6917 - accuracy: 0.53 - ETA: 1s - loss: 0.6904 - accuracy: 0.55 - ETA: 1s - loss: 0.6879 - accuracy: 0.56 - ETA: 1s - loss: 0.6849 - accuracy: 0.58 - ETA: 1s - loss: 0.6829 - accuracy: 0.58 - ETA: 1s - loss: 0.6799 - accuracy: 0.59 - ETA: 0s - loss: 0.6762 - accuracy: 0.60 - ETA: 0s - loss: 0.6740 - accuracy: 0.60 - ETA: 0s - loss: 0.6723 - accuracy: 0.60 - ETA: 0s - loss: 0.6710 - accuracy: 0.60 - ETA: 0s - loss: 0.6684 - accuracy: 0.61 - ETA: 0s - loss: 0.6656 - accuracy: 0.61 - ETA: 0s - loss: 0.6632 - accuracy: 0.62 - ETA: 0s - loss: 0.6596 - accuracy: 0.62 - ETA: 0s - loss: 0.6572 - accuracy: 0.63 - ETA: 0s - loss: 0.6549 - accuracy: 0.63 - ETA: 0s - loss: 0.6531 - accuracy: 0.63 - ETA: 0s - loss: 0.6520 - accuracy: 0.63 - ETA: 0s - loss: 0.6511 - accuracy: 0.63 - ETA: 0s - loss: 0.6486 - accuracy: 0.64 - ETA: 0s - loss: 0.6477 - accuracy: 0.64 - ETA: 0s - loss: 0.6458 - accuracy: 0.64 - 2s 118us/step - loss: 0.6449 - accuracy: 0.6469 - val_loss: 0.5879 - val_accuracy: 0.7188\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:26 - loss: 0.6898 - accuracy: 0.60 - ETA: 3s - loss: 0.6917 - accuracy: 0.4981 - ETA: 2s - loss: 0.6894 - accuracy: 0.52 - ETA: 1s - loss: 0.6904 - accuracy: 0.51 - ETA: 1s - loss: 0.6893 - accuracy: 0.52 - ETA: 1s - loss: 0.6882 - accuracy: 0.54 - ETA: 1s - loss: 0.6864 - accuracy: 0.55 - ETA: 1s - loss: 0.6842 - accuracy: 0.56 - ETA: 1s - loss: 0.6827 - accuracy: 0.56 - ETA: 1s - loss: 0.6805 - accuracy: 0.57 - ETA: 0s - loss: 0.6787 - accuracy: 0.58 - ETA: 0s - loss: 0.6758 - accuracy: 0.59 - ETA: 0s - loss: 0.6733 - accuracy: 0.59 - ETA: 0s - loss: 0.6714 - accuracy: 0.60 - ETA: 0s - loss: 0.6687 - accuracy: 0.60 - ETA: 0s - loss: 0.6650 - accuracy: 0.61 - ETA: 0s - loss: 0.6618 - accuracy: 0.61 - ETA: 0s - loss: 0.6597 - accuracy: 0.62 - ETA: 0s - loss: 0.6577 - accuracy: 0.62 - ETA: 0s - loss: 0.6551 - accuracy: 0.62 - ETA: 0s - loss: 0.6531 - accuracy: 0.62 - ETA: 0s - loss: 0.6515 - accuracy: 0.63 - ETA: 0s - loss: 0.6497 - accuracy: 0.63 - ETA: 0s - loss: 0.6475 - accuracy: 0.63 - ETA: 0s - loss: 0.6453 - accuracy: 0.63 - ETA: 0s - loss: 0.6440 - accuracy: 0.64 - 1s 118us/step - loss: 0.6434 - accuracy: 0.6423 - val_loss: 0.5866 - val_accuracy: 0.7180\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:29 - loss: 0.6971 - accuracy: 0.30 - ETA: 3s - loss: 0.6946 - accuracy: 0.4885 - ETA: 2s - loss: 0.6926 - accuracy: 0.55 - ETA: 1s - loss: 0.6914 - accuracy: 0.55 - ETA: 1s - loss: 0.6890 - accuracy: 0.57 - ETA: 1s - loss: 0.6864 - accuracy: 0.58 - ETA: 1s - loss: 0.6839 - accuracy: 0.58 - ETA: 1s - loss: 0.6821 - accuracy: 0.58 - ETA: 1s - loss: 0.6792 - accuracy: 0.59 - ETA: 1s - loss: 0.6768 - accuracy: 0.59 - ETA: 0s - loss: 0.6747 - accuracy: 0.60 - ETA: 0s - loss: 0.6709 - accuracy: 0.60 - ETA: 0s - loss: 0.6687 - accuracy: 0.61 - ETA: 0s - loss: 0.6653 - accuracy: 0.61 - ETA: 0s - loss: 0.6620 - accuracy: 0.62 - ETA: 0s - loss: 0.6605 - accuracy: 0.62 - ETA: 0s - loss: 0.6582 - accuracy: 0.62 - ETA: 0s - loss: 0.6555 - accuracy: 0.63 - ETA: 0s - loss: 0.6540 - accuracy: 0.63 - ETA: 0s - loss: 0.6519 - accuracy: 0.63 - ETA: 0s - loss: 0.6496 - accuracy: 0.63 - ETA: 0s - loss: 0.6471 - accuracy: 0.64 - ETA: 0s - loss: 0.6455 - accuracy: 0.64 - ETA: 0s - loss: 0.6434 - accuracy: 0.64 - ETA: 0s - loss: 0.6429 - accuracy: 0.64 - ETA: 0s - loss: 0.6417 - accuracy: 0.64 - ETA: 0s - loss: 0.6404 - accuracy: 0.64 - 2s 121us/step - loss: 0.6404 - accuracy: 0.6497 - val_loss: 0.5872 - val_accuracy: 0.7124\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:30 - loss: 0.6941 - accuracy: 0.50 - ETA: 3s - loss: 0.6942 - accuracy: 0.5075 - ETA: 2s - loss: 0.6922 - accuracy: 0.51 - ETA: 1s - loss: 0.6903 - accuracy: 0.53 - ETA: 1s - loss: 0.6877 - accuracy: 0.55 - ETA: 1s - loss: 0.6864 - accuracy: 0.55 - ETA: 1s - loss: 0.6834 - accuracy: 0.57 - ETA: 1s - loss: 0.6825 - accuracy: 0.57 - ETA: 1s - loss: 0.6808 - accuracy: 0.57 - ETA: 0s - loss: 0.6790 - accuracy: 0.58 - ETA: 0s - loss: 0.6768 - accuracy: 0.59 - ETA: 0s - loss: 0.6740 - accuracy: 0.60 - ETA: 0s - loss: 0.6713 - accuracy: 0.60 - ETA: 0s - loss: 0.6704 - accuracy: 0.60 - ETA: 0s - loss: 0.6678 - accuracy: 0.61 - ETA: 0s - loss: 0.6641 - accuracy: 0.62 - ETA: 0s - loss: 0.6631 - accuracy: 0.62 - ETA: 0s - loss: 0.6604 - accuracy: 0.62 - ETA: 0s - loss: 0.6588 - accuracy: 0.62 - ETA: 0s - loss: 0.6574 - accuracy: 0.62 - ETA: 0s - loss: 0.6553 - accuracy: 0.63 - ETA: 0s - loss: 0.6539 - accuracy: 0.63 - ETA: 0s - loss: 0.6525 - accuracy: 0.63 - ETA: 0s - loss: 0.6513 - accuracy: 0.63 - ETA: 0s - loss: 0.6490 - accuracy: 0.64 - 1s 116us/step - loss: 0.6474 - accuracy: 0.6425 - val_loss: 0.5898 - val_accuracy: 0.7166\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:26 - loss: 0.6956 - accuracy: 0.50 - ETA: 3s - loss: 0.6940 - accuracy: 0.5385 - ETA: 2s - loss: 0.6931 - accuracy: 0.54 - ETA: 2s - loss: 0.6916 - accuracy: 0.55 - ETA: 1s - loss: 0.6909 - accuracy: 0.54 - ETA: 1s - loss: 0.6897 - accuracy: 0.55 - ETA: 1s - loss: 0.6882 - accuracy: 0.56 - ETA: 1s - loss: 0.6855 - accuracy: 0.57 - ETA: 1s - loss: 0.6836 - accuracy: 0.58 - ETA: 1s - loss: 0.6809 - accuracy: 0.58 - ETA: 1s - loss: 0.6787 - accuracy: 0.59 - ETA: 0s - loss: 0.6746 - accuracy: 0.60 - ETA: 0s - loss: 0.6716 - accuracy: 0.60 - ETA: 0s - loss: 0.6684 - accuracy: 0.60 - ETA: 0s - loss: 0.6662 - accuracy: 0.61 - ETA: 0s - loss: 0.6642 - accuracy: 0.61 - ETA: 0s - loss: 0.6617 - accuracy: 0.61 - ETA: 0s - loss: 0.6597 - accuracy: 0.62 - ETA: 0s - loss: 0.6578 - accuracy: 0.62 - ETA: 0s - loss: 0.6559 - accuracy: 0.62 - ETA: 0s - loss: 0.6543 - accuracy: 0.63 - ETA: 0s - loss: 0.6521 - accuracy: 0.63 - ETA: 0s - loss: 0.6492 - accuracy: 0.63 - ETA: 0s - loss: 0.6473 - accuracy: 0.64 - ETA: 0s - loss: 0.6455 - accuracy: 0.64 - ETA: 0s - loss: 0.6440 - accuracy: 0.64 - 1s 118us/step - loss: 0.6436 - accuracy: 0.6460 - val_loss: 0.5951 - val_accuracy: 0.7102\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:02 - loss: 0.6914 - accuracy: 0.50 - ETA: 3s - loss: 0.6931 - accuracy: 0.5389 - ETA: 2s - loss: 0.6924 - accuracy: 0.55 - ETA: 1s - loss: 0.6904 - accuracy: 0.57 - ETA: 1s - loss: 0.6898 - accuracy: 0.57 - ETA: 1s - loss: 0.6900 - accuracy: 0.56 - ETA: 1s - loss: 0.6890 - accuracy: 0.57 - ETA: 1s - loss: 0.6885 - accuracy: 0.57 - ETA: 0s - loss: 0.6880 - accuracy: 0.57 - ETA: 0s - loss: 0.6874 - accuracy: 0.57 - ETA: 0s - loss: 0.6867 - accuracy: 0.58 - ETA: 0s - loss: 0.6856 - accuracy: 0.58 - ETA: 0s - loss: 0.6851 - accuracy: 0.58 - ETA: 0s - loss: 0.6843 - accuracy: 0.58 - ETA: 0s - loss: 0.6835 - accuracy: 0.59 - ETA: 0s - loss: 0.6833 - accuracy: 0.58 - ETA: 0s - loss: 0.6824 - accuracy: 0.59 - ETA: 0s - loss: 0.6818 - accuracy: 0.59 - ETA: 0s - loss: 0.6813 - accuracy: 0.59 - ETA: 0s - loss: 0.6805 - accuracy: 0.59 - ETA: 0s - loss: 0.6798 - accuracy: 0.59 - ETA: 0s - loss: 0.6794 - accuracy: 0.59 - ETA: 0s - loss: 0.6792 - accuracy: 0.59 - ETA: 0s - loss: 0.6787 - accuracy: 0.59 - 1s 109us/step - loss: 0.6784 - accuracy: 0.5968 - val_loss: 0.6592 - val_accuracy: 0.7109\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 2:02 - loss: 0.6945 - accuracy: 0.60 - ETA: 3s - loss: 0.6931 - accuracy: 0.5089 - ETA: 2s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6924 - accuracy: 0.53 - ETA: 1s - loss: 0.6914 - accuracy: 0.54 - ETA: 1s - loss: 0.6909 - accuracy: 0.55 - ETA: 1s - loss: 0.6897 - accuracy: 0.55 - ETA: 1s - loss: 0.6891 - accuracy: 0.55 - ETA: 0s - loss: 0.6887 - accuracy: 0.55 - ETA: 0s - loss: 0.6884 - accuracy: 0.55 - ETA: 0s - loss: 0.6875 - accuracy: 0.55 - ETA: 0s - loss: 0.6869 - accuracy: 0.55 - ETA: 0s - loss: 0.6864 - accuracy: 0.55 - ETA: 0s - loss: 0.6861 - accuracy: 0.55 - ETA: 0s - loss: 0.6853 - accuracy: 0.55 - ETA: 0s - loss: 0.6849 - accuracy: 0.55 - ETA: 0s - loss: 0.6844 - accuracy: 0.56 - ETA: 0s - loss: 0.6839 - accuracy: 0.56 - ETA: 0s - loss: 0.6832 - accuracy: 0.56 - ETA: 0s - loss: 0.6823 - accuracy: 0.56 - ETA: 0s - loss: 0.6815 - accuracy: 0.56 - ETA: 0s - loss: 0.6810 - accuracy: 0.56 - ETA: 0s - loss: 0.6802 - accuracy: 0.56 - ETA: 0s - loss: 0.6799 - accuracy: 0.57 - 1s 109us/step - loss: 0.6796 - accuracy: 0.5711 - val_loss: 0.6620 - val_accuracy: 0.6932\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:04 - loss: 0.6949 - accuracy: 0.50 - ETA: 3s - loss: 0.6903 - accuracy: 0.5400 - ETA: 2s - loss: 0.6901 - accuracy: 0.53 - ETA: 1s - loss: 0.6877 - accuracy: 0.54 - ETA: 1s - loss: 0.6897 - accuracy: 0.52 - ETA: 1s - loss: 0.6881 - accuracy: 0.53 - ETA: 1s - loss: 0.6883 - accuracy: 0.53 - ETA: 1s - loss: 0.6883 - accuracy: 0.52 - ETA: 0s - loss: 0.6887 - accuracy: 0.51 - ETA: 0s - loss: 0.6882 - accuracy: 0.52 - ETA: 0s - loss: 0.6884 - accuracy: 0.52 - ETA: 0s - loss: 0.6882 - accuracy: 0.51 - ETA: 0s - loss: 0.6875 - accuracy: 0.52 - ETA: 0s - loss: 0.6873 - accuracy: 0.51 - ETA: 0s - loss: 0.6868 - accuracy: 0.51 - ETA: 0s - loss: 0.6867 - accuracy: 0.51 - ETA: 0s - loss: 0.6862 - accuracy: 0.51 - ETA: 0s - loss: 0.6860 - accuracy: 0.51 - ETA: 0s - loss: 0.6855 - accuracy: 0.52 - ETA: 0s - loss: 0.6854 - accuracy: 0.52 - ETA: 0s - loss: 0.6852 - accuracy: 0.52 - ETA: 0s - loss: 0.6850 - accuracy: 0.52 - ETA: 0s - loss: 0.6848 - accuracy: 0.52 - ETA: 0s - loss: 0.6846 - accuracy: 0.52 - 1s 108us/step - loss: 0.6845 - accuracy: 0.5303 - val_loss: 0.6739 - val_accuracy: 0.6108\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:04 - loss: 0.6938 - accuracy: 0.40 - ETA: 3s - loss: 0.6932 - accuracy: 0.5148 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6921 - accuracy: 0.52 - ETA: 1s - loss: 0.6917 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 0s - loss: 0.6902 - accuracy: 0.54 - ETA: 0s - loss: 0.6897 - accuracy: 0.54 - ETA: 0s - loss: 0.6891 - accuracy: 0.55 - ETA: 0s - loss: 0.6884 - accuracy: 0.56 - ETA: 0s - loss: 0.6879 - accuracy: 0.55 - ETA: 0s - loss: 0.6874 - accuracy: 0.56 - ETA: 0s - loss: 0.6864 - accuracy: 0.56 - ETA: 0s - loss: 0.6856 - accuracy: 0.56 - ETA: 0s - loss: 0.6849 - accuracy: 0.56 - ETA: 0s - loss: 0.6839 - accuracy: 0.56 - ETA: 0s - loss: 0.6835 - accuracy: 0.56 - ETA: 0s - loss: 0.6829 - accuracy: 0.56 - ETA: 0s - loss: 0.6823 - accuracy: 0.56 - ETA: 0s - loss: 0.6818 - accuracy: 0.56 - ETA: 0s - loss: 0.6814 - accuracy: 0.56 - ETA: 0s - loss: 0.6810 - accuracy: 0.57 - 1s 109us/step - loss: 0.6809 - accuracy: 0.5702 - val_loss: 0.6642 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:59 - loss: 0.6950 - accuracy: 0.30 - ETA: 3s - loss: 0.6936 - accuracy: 0.5036 - ETA: 2s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6919 - accuracy: 0.51 - ETA: 0s - loss: 0.6915 - accuracy: 0.51 - ETA: 0s - loss: 0.6909 - accuracy: 0.51 - ETA: 0s - loss: 0.6900 - accuracy: 0.51 - ETA: 0s - loss: 0.6899 - accuracy: 0.51 - ETA: 0s - loss: 0.6894 - accuracy: 0.51 - ETA: 0s - loss: 0.6889 - accuracy: 0.51 - ETA: 0s - loss: 0.6889 - accuracy: 0.51 - ETA: 0s - loss: 0.6881 - accuracy: 0.51 - ETA: 0s - loss: 0.6878 - accuracy: 0.51 - ETA: 0s - loss: 0.6875 - accuracy: 0.51 - ETA: 0s - loss: 0.6872 - accuracy: 0.51 - ETA: 0s - loss: 0.6869 - accuracy: 0.51 - ETA: 0s - loss: 0.6863 - accuracy: 0.51 - ETA: 0s - loss: 0.6858 - accuracy: 0.51 - ETA: 0s - loss: 0.6855 - accuracy: 0.51 - 1s 107us/step - loss: 0.6854 - accuracy: 0.5185 - val_loss: 0.6710 - val_accuracy: 0.5859\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:02 - loss: 0.6931 - accuracy: 0.80 - ETA: 3s - loss: 0.6908 - accuracy: 0.5370 - ETA: 2s - loss: 0.6901 - accuracy: 0.53 - ETA: 1s - loss: 0.6887 - accuracy: 0.54 - ETA: 1s - loss: 0.6884 - accuracy: 0.55 - ETA: 1s - loss: 0.6884 - accuracy: 0.54 - ETA: 1s - loss: 0.6874 - accuracy: 0.55 - ETA: 1s - loss: 0.6859 - accuracy: 0.55 - ETA: 0s - loss: 0.6853 - accuracy: 0.55 - ETA: 0s - loss: 0.6846 - accuracy: 0.56 - ETA: 0s - loss: 0.6836 - accuracy: 0.56 - ETA: 0s - loss: 0.6833 - accuracy: 0.56 - ETA: 0s - loss: 0.6828 - accuracy: 0.56 - ETA: 0s - loss: 0.6819 - accuracy: 0.57 - ETA: 0s - loss: 0.6812 - accuracy: 0.57 - ETA: 0s - loss: 0.6805 - accuracy: 0.57 - ETA: 0s - loss: 0.6799 - accuracy: 0.57 - ETA: 0s - loss: 0.6792 - accuracy: 0.57 - ETA: 0s - loss: 0.6788 - accuracy: 0.57 - ETA: 0s - loss: 0.6781 - accuracy: 0.58 - ETA: 0s - loss: 0.6775 - accuracy: 0.58 - ETA: 0s - loss: 0.6774 - accuracy: 0.58 - ETA: 0s - loss: 0.6769 - accuracy: 0.58 - ETA: 0s - loss: 0.6767 - accuracy: 0.58 - 1s 108us/step - loss: 0.6767 - accuracy: 0.5837 - val_loss: 0.6605 - val_accuracy: 0.6939\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:00 - loss: 0.6878 - accuracy: 0.90 - ETA: 3s - loss: 0.6917 - accuracy: 0.5491 - ETA: 2s - loss: 0.6921 - accuracy: 0.52 - ETA: 1s - loss: 0.6909 - accuracy: 0.54 - ETA: 1s - loss: 0.6888 - accuracy: 0.55 - ETA: 1s - loss: 0.6869 - accuracy: 0.57 - ETA: 1s - loss: 0.6862 - accuracy: 0.57 - ETA: 1s - loss: 0.6841 - accuracy: 0.58 - ETA: 0s - loss: 0.6831 - accuracy: 0.58 - ETA: 0s - loss: 0.6822 - accuracy: 0.58 - ETA: 0s - loss: 0.6806 - accuracy: 0.59 - ETA: 0s - loss: 0.6797 - accuracy: 0.59 - ETA: 0s - loss: 0.6786 - accuracy: 0.59 - ETA: 0s - loss: 0.6779 - accuracy: 0.60 - ETA: 0s - loss: 0.6767 - accuracy: 0.60 - ETA: 0s - loss: 0.6758 - accuracy: 0.60 - ETA: 0s - loss: 0.6743 - accuracy: 0.61 - ETA: 0s - loss: 0.6731 - accuracy: 0.61 - ETA: 0s - loss: 0.6722 - accuracy: 0.61 - ETA: 0s - loss: 0.6711 - accuracy: 0.61 - ETA: 0s - loss: 0.6699 - accuracy: 0.62 - ETA: 0s - loss: 0.6688 - accuracy: 0.62 - ETA: 0s - loss: 0.6683 - accuracy: 0.62 - ETA: 0s - loss: 0.6671 - accuracy: 0.62 - 1s 109us/step - loss: 0.6666 - accuracy: 0.6237 - val_loss: 0.6335 - val_accuracy: 0.7088\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:59 - loss: 0.6939 - accuracy: 0.60 - ETA: 3s - loss: 0.6923 - accuracy: 0.5222 - ETA: 2s - loss: 0.6887 - accuracy: 0.55 - ETA: 1s - loss: 0.6880 - accuracy: 0.55 - ETA: 1s - loss: 0.6862 - accuracy: 0.56 - ETA: 1s - loss: 0.6856 - accuracy: 0.56 - ETA: 1s - loss: 0.6849 - accuracy: 0.56 - ETA: 1s - loss: 0.6827 - accuracy: 0.57 - ETA: 0s - loss: 0.6804 - accuracy: 0.58 - ETA: 0s - loss: 0.6797 - accuracy: 0.58 - ETA: 0s - loss: 0.6785 - accuracy: 0.58 - ETA: 0s - loss: 0.6771 - accuracy: 0.59 - ETA: 0s - loss: 0.6756 - accuracy: 0.60 - ETA: 0s - loss: 0.6744 - accuracy: 0.60 - ETA: 0s - loss: 0.6735 - accuracy: 0.60 - ETA: 0s - loss: 0.6725 - accuracy: 0.61 - ETA: 0s - loss: 0.6715 - accuracy: 0.61 - ETA: 0s - loss: 0.6708 - accuracy: 0.61 - ETA: 0s - loss: 0.6701 - accuracy: 0.61 - ETA: 0s - loss: 0.6689 - accuracy: 0.61 - ETA: 0s - loss: 0.6678 - accuracy: 0.61 - ETA: 0s - loss: 0.6668 - accuracy: 0.62 - ETA: 0s - loss: 0.6657 - accuracy: 0.62 - ETA: 0s - loss: 0.6646 - accuracy: 0.62 - 1s 108us/step - loss: 0.6646 - accuracy: 0.6254 - val_loss: 0.6333 - val_accuracy: 0.7053\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:00 - loss: 0.6990 - accuracy: 0.30 - ETA: 3s - loss: 0.6936 - accuracy: 0.5224 - ETA: 2s - loss: 0.6906 - accuracy: 0.54 - ETA: 1s - loss: 0.6901 - accuracy: 0.55 - ETA: 1s - loss: 0.6889 - accuracy: 0.55 - ETA: 1s - loss: 0.6873 - accuracy: 0.56 - ETA: 1s - loss: 0.6861 - accuracy: 0.57 - ETA: 1s - loss: 0.6839 - accuracy: 0.58 - ETA: 0s - loss: 0.6818 - accuracy: 0.59 - ETA: 0s - loss: 0.6812 - accuracy: 0.59 - ETA: 0s - loss: 0.6798 - accuracy: 0.59 - ETA: 0s - loss: 0.6784 - accuracy: 0.60 - ETA: 0s - loss: 0.6770 - accuracy: 0.60 - ETA: 0s - loss: 0.6758 - accuracy: 0.60 - ETA: 0s - loss: 0.6748 - accuracy: 0.60 - ETA: 0s - loss: 0.6736 - accuracy: 0.60 - ETA: 0s - loss: 0.6726 - accuracy: 0.61 - ETA: 0s - loss: 0.6717 - accuracy: 0.61 - ETA: 0s - loss: 0.6704 - accuracy: 0.61 - ETA: 0s - loss: 0.6694 - accuracy: 0.61 - ETA: 0s - loss: 0.6688 - accuracy: 0.61 - ETA: 0s - loss: 0.6681 - accuracy: 0.61 - ETA: 0s - loss: 0.6667 - accuracy: 0.61 - ETA: 0s - loss: 0.6655 - accuracy: 0.62 - ETA: 0s - loss: 0.6646 - accuracy: 0.62 - 1s 112us/step - loss: 0.6644 - accuracy: 0.6235 - val_loss: 0.6320 - val_accuracy: 0.7102\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:01 - loss: 0.6938 - accuracy: 0.70 - ETA: 3s - loss: 0.6921 - accuracy: 0.5214 - ETA: 2s - loss: 0.6909 - accuracy: 0.53 - ETA: 1s - loss: 0.6902 - accuracy: 0.54 - ETA: 1s - loss: 0.6907 - accuracy: 0.53 - ETA: 1s - loss: 0.6900 - accuracy: 0.53 - ETA: 1s - loss: 0.6895 - accuracy: 0.54 - ETA: 1s - loss: 0.6882 - accuracy: 0.54 - ETA: 0s - loss: 0.6879 - accuracy: 0.55 - ETA: 0s - loss: 0.6872 - accuracy: 0.55 - ETA: 0s - loss: 0.6865 - accuracy: 0.55 - ETA: 0s - loss: 0.6863 - accuracy: 0.55 - ETA: 0s - loss: 0.6855 - accuracy: 0.56 - ETA: 0s - loss: 0.6846 - accuracy: 0.56 - ETA: 0s - loss: 0.6839 - accuracy: 0.57 - ETA: 0s - loss: 0.6833 - accuracy: 0.57 - ETA: 0s - loss: 0.6826 - accuracy: 0.57 - ETA: 0s - loss: 0.6823 - accuracy: 0.57 - ETA: 0s - loss: 0.6814 - accuracy: 0.57 - ETA: 0s - loss: 0.6807 - accuracy: 0.57 - ETA: 0s - loss: 0.6804 - accuracy: 0.57 - ETA: 0s - loss: 0.6800 - accuracy: 0.57 - ETA: 0s - loss: 0.6796 - accuracy: 0.57 - ETA: 0s - loss: 0.6788 - accuracy: 0.58 - 1s 109us/step - loss: 0.6788 - accuracy: 0.5817 - val_loss: 0.6617 - val_accuracy: 0.6982\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:02 - loss: 0.6927 - accuracy: 0.50 - ETA: 3s - loss: 0.6895 - accuracy: 0.5491 - ETA: 2s - loss: 0.6879 - accuracy: 0.55 - ETA: 1s - loss: 0.6861 - accuracy: 0.56 - ETA: 1s - loss: 0.6844 - accuracy: 0.56 - ETA: 1s - loss: 0.6821 - accuracy: 0.57 - ETA: 1s - loss: 0.6802 - accuracy: 0.57 - ETA: 1s - loss: 0.6787 - accuracy: 0.58 - ETA: 0s - loss: 0.6777 - accuracy: 0.58 - ETA: 0s - loss: 0.6761 - accuracy: 0.59 - ETA: 0s - loss: 0.6751 - accuracy: 0.59 - ETA: 0s - loss: 0.6742 - accuracy: 0.59 - ETA: 0s - loss: 0.6729 - accuracy: 0.60 - ETA: 0s - loss: 0.6718 - accuracy: 0.60 - ETA: 0s - loss: 0.6707 - accuracy: 0.60 - ETA: 0s - loss: 0.6700 - accuracy: 0.60 - ETA: 0s - loss: 0.6695 - accuracy: 0.60 - ETA: 0s - loss: 0.6677 - accuracy: 0.61 - ETA: 0s - loss: 0.6670 - accuracy: 0.61 - ETA: 0s - loss: 0.6654 - accuracy: 0.61 - ETA: 0s - loss: 0.6642 - accuracy: 0.61 - ETA: 0s - loss: 0.6630 - accuracy: 0.62 - ETA: 0s - loss: 0.6614 - accuracy: 0.62 - ETA: 0s - loss: 0.6607 - accuracy: 0.62 - 1s 109us/step - loss: 0.6603 - accuracy: 0.6255 - val_loss: 0.6267 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:04 - loss: 0.6936 - accuracy: 0.40 - ETA: 3s - loss: 0.6912 - accuracy: 0.5286 - ETA: 2s - loss: 0.6887 - accuracy: 0.53 - ETA: 1s - loss: 0.6870 - accuracy: 0.54 - ETA: 1s - loss: 0.6863 - accuracy: 0.55 - ETA: 1s - loss: 0.6852 - accuracy: 0.56 - ETA: 1s - loss: 0.6839 - accuracy: 0.57 - ETA: 1s - loss: 0.6821 - accuracy: 0.58 - ETA: 0s - loss: 0.6803 - accuracy: 0.58 - ETA: 0s - loss: 0.6788 - accuracy: 0.59 - ETA: 0s - loss: 0.6777 - accuracy: 0.59 - ETA: 0s - loss: 0.6762 - accuracy: 0.59 - ETA: 0s - loss: 0.6753 - accuracy: 0.60 - ETA: 0s - loss: 0.6745 - accuracy: 0.60 - ETA: 0s - loss: 0.6724 - accuracy: 0.60 - ETA: 0s - loss: 0.6715 - accuracy: 0.60 - ETA: 0s - loss: 0.6704 - accuracy: 0.60 - ETA: 0s - loss: 0.6695 - accuracy: 0.61 - ETA: 0s - loss: 0.6685 - accuracy: 0.61 - ETA: 0s - loss: 0.6672 - accuracy: 0.61 - ETA: 0s - loss: 0.6662 - accuracy: 0.61 - ETA: 0s - loss: 0.6655 - accuracy: 0.61 - ETA: 0s - loss: 0.6644 - accuracy: 0.62 - ETA: 0s - loss: 0.6636 - accuracy: 0.62 - 1s 110us/step - loss: 0.6631 - accuracy: 0.6222 - val_loss: 0.6379 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:01 - loss: 0.6935 - accuracy: 0.50 - ETA: 3s - loss: 0.6899 - accuracy: 0.5214 - ETA: 2s - loss: 0.6901 - accuracy: 0.51 - ETA: 1s - loss: 0.6845 - accuracy: 0.54 - ETA: 1s - loss: 0.6840 - accuracy: 0.53 - ETA: 1s - loss: 0.6833 - accuracy: 0.54 - ETA: 1s - loss: 0.6829 - accuracy: 0.55 - ETA: 1s - loss: 0.6808 - accuracy: 0.56 - ETA: 0s - loss: 0.6791 - accuracy: 0.57 - ETA: 0s - loss: 0.6775 - accuracy: 0.58 - ETA: 0s - loss: 0.6762 - accuracy: 0.58 - ETA: 0s - loss: 0.6748 - accuracy: 0.59 - ETA: 0s - loss: 0.6734 - accuracy: 0.59 - ETA: 0s - loss: 0.6719 - accuracy: 0.60 - ETA: 0s - loss: 0.6710 - accuracy: 0.60 - ETA: 0s - loss: 0.6702 - accuracy: 0.60 - ETA: 0s - loss: 0.6690 - accuracy: 0.61 - ETA: 0s - loss: 0.6675 - accuracy: 0.61 - ETA: 0s - loss: 0.6659 - accuracy: 0.61 - ETA: 0s - loss: 0.6645 - accuracy: 0.61 - ETA: 0s - loss: 0.6636 - accuracy: 0.62 - ETA: 0s - loss: 0.6626 - accuracy: 0.62 - ETA: 0s - loss: 0.6614 - accuracy: 0.62 - ETA: 0s - loss: 0.6605 - accuracy: 0.62 - 1s 109us/step - loss: 0.6600 - accuracy: 0.6287 - val_loss: 0.6243 - val_accuracy: 0.7095\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 2:02 - loss: 0.6973 - accuracy: 0.40 - ETA: 3s - loss: 0.6920 - accuracy: 0.5111 - ETA: 2s - loss: 0.6901 - accuracy: 0.53 - ETA: 1s - loss: 0.6890 - accuracy: 0.54 - ETA: 1s - loss: 0.6859 - accuracy: 0.55 - ETA: 1s - loss: 0.6845 - accuracy: 0.56 - ETA: 1s - loss: 0.6835 - accuracy: 0.56 - ETA: 1s - loss: 0.6824 - accuracy: 0.57 - ETA: 0s - loss: 0.6816 - accuracy: 0.57 - ETA: 0s - loss: 0.6804 - accuracy: 0.58 - ETA: 0s - loss: 0.6797 - accuracy: 0.58 - ETA: 0s - loss: 0.6782 - accuracy: 0.58 - ETA: 0s - loss: 0.6770 - accuracy: 0.59 - ETA: 0s - loss: 0.6754 - accuracy: 0.59 - ETA: 0s - loss: 0.6737 - accuracy: 0.60 - ETA: 0s - loss: 0.6722 - accuracy: 0.60 - ETA: 0s - loss: 0.6719 - accuracy: 0.60 - ETA: 0s - loss: 0.6712 - accuracy: 0.60 - ETA: 0s - loss: 0.6693 - accuracy: 0.61 - ETA: 0s - loss: 0.6679 - accuracy: 0.61 - ETA: 0s - loss: 0.6663 - accuracy: 0.61 - ETA: 0s - loss: 0.6646 - accuracy: 0.62 - ETA: 0s - loss: 0.6638 - accuracy: 0.62 - ETA: 0s - loss: 0.6627 - accuracy: 0.62 - 1s 109us/step - loss: 0.6627 - accuracy: 0.6276 - val_loss: 0.6294 - val_accuracy: 0.7088\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:01 - loss: 0.6895 - accuracy: 0.50 - ETA: 3s - loss: 0.6912 - accuracy: 0.5407 - ETA: 2s - loss: 0.6879 - accuracy: 0.55 - ETA: 1s - loss: 0.6846 - accuracy: 0.56 - ETA: 1s - loss: 0.6824 - accuracy: 0.59 - ETA: 1s - loss: 0.6788 - accuracy: 0.60 - ETA: 1s - loss: 0.6750 - accuracy: 0.61 - ETA: 1s - loss: 0.6717 - accuracy: 0.62 - ETA: 0s - loss: 0.6696 - accuracy: 0.62 - ETA: 0s - loss: 0.6677 - accuracy: 0.62 - ETA: 0s - loss: 0.6659 - accuracy: 0.63 - ETA: 0s - loss: 0.6633 - accuracy: 0.63 - ETA: 0s - loss: 0.6612 - accuracy: 0.63 - ETA: 0s - loss: 0.6597 - accuracy: 0.64 - ETA: 0s - loss: 0.6585 - accuracy: 0.64 - ETA: 0s - loss: 0.6575 - accuracy: 0.64 - ETA: 0s - loss: 0.6558 - accuracy: 0.64 - ETA: 0s - loss: 0.6548 - accuracy: 0.64 - ETA: 0s - loss: 0.6524 - accuracy: 0.65 - ETA: 0s - loss: 0.6512 - accuracy: 0.65 - ETA: 0s - loss: 0.6505 - accuracy: 0.65 - ETA: 0s - loss: 0.6499 - accuracy: 0.65 - ETA: 0s - loss: 0.6484 - accuracy: 0.65 - 1s 106us/step - loss: 0.6474 - accuracy: 0.6537 - val_loss: 0.6093 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:01 - loss: 0.6956 - accuracy: 0.60 - ETA: 3s - loss: 0.6920 - accuracy: 0.5296 - ETA: 2s - loss: 0.6868 - accuracy: 0.56 - ETA: 1s - loss: 0.6857 - accuracy: 0.56 - ETA: 1s - loss: 0.6839 - accuracy: 0.56 - ETA: 1s - loss: 0.6810 - accuracy: 0.57 - ETA: 1s - loss: 0.6779 - accuracy: 0.58 - ETA: 1s - loss: 0.6757 - accuracy: 0.59 - ETA: 0s - loss: 0.6731 - accuracy: 0.60 - ETA: 0s - loss: 0.6709 - accuracy: 0.60 - ETA: 0s - loss: 0.6697 - accuracy: 0.61 - ETA: 0s - loss: 0.6666 - accuracy: 0.61 - ETA: 0s - loss: 0.6644 - accuracy: 0.62 - ETA: 0s - loss: 0.6625 - accuracy: 0.62 - ETA: 0s - loss: 0.6604 - accuracy: 0.63 - ETA: 0s - loss: 0.6582 - accuracy: 0.63 - ETA: 0s - loss: 0.6558 - accuracy: 0.63 - ETA: 0s - loss: 0.6548 - accuracy: 0.63 - ETA: 0s - loss: 0.6536 - accuracy: 0.63 - ETA: 0s - loss: 0.6518 - accuracy: 0.64 - ETA: 0s - loss: 0.6497 - accuracy: 0.64 - ETA: 0s - loss: 0.6492 - accuracy: 0.64 - ETA: 0s - loss: 0.6490 - accuracy: 0.64 - ETA: 0s - loss: 0.6478 - accuracy: 0.64 - 1s 109us/step - loss: 0.6478 - accuracy: 0.6479 - val_loss: 0.6118 - val_accuracy: 0.7138\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:00 - loss: 0.6979 - accuracy: 0.30 - ETA: 3s - loss: 0.6910 - accuracy: 0.5370 - ETA: 2s - loss: 0.6891 - accuracy: 0.55 - ETA: 1s - loss: 0.6868 - accuracy: 0.57 - ETA: 1s - loss: 0.6846 - accuracy: 0.58 - ETA: 1s - loss: 0.6832 - accuracy: 0.58 - ETA: 1s - loss: 0.6808 - accuracy: 0.59 - ETA: 1s - loss: 0.6791 - accuracy: 0.60 - ETA: 0s - loss: 0.6763 - accuracy: 0.60 - ETA: 0s - loss: 0.6746 - accuracy: 0.61 - ETA: 0s - loss: 0.6723 - accuracy: 0.61 - ETA: 0s - loss: 0.6705 - accuracy: 0.62 - ETA: 0s - loss: 0.6686 - accuracy: 0.62 - ETA: 0s - loss: 0.6679 - accuracy: 0.62 - ETA: 0s - loss: 0.6668 - accuracy: 0.62 - ETA: 0s - loss: 0.6650 - accuracy: 0.62 - ETA: 0s - loss: 0.6633 - accuracy: 0.63 - ETA: 0s - loss: 0.6628 - accuracy: 0.63 - ETA: 0s - loss: 0.6613 - accuracy: 0.63 - ETA: 0s - loss: 0.6597 - accuracy: 0.63 - ETA: 0s - loss: 0.6589 - accuracy: 0.63 - ETA: 0s - loss: 0.6589 - accuracy: 0.63 - ETA: 0s - loss: 0.6587 - accuracy: 0.63 - ETA: 0s - loss: 0.6580 - accuracy: 0.63 - 1s 110us/step - loss: 0.6572 - accuracy: 0.6362 - val_loss: 0.6234 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 44us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:00 - loss: 0.6937 - accuracy: 0.60 - ETA: 3s - loss: 0.6915 - accuracy: 0.5175 - ETA: 2s - loss: 0.6905 - accuracy: 0.51 - ETA: 1s - loss: 0.6896 - accuracy: 0.53 - ETA: 1s - loss: 0.6873 - accuracy: 0.54 - ETA: 1s - loss: 0.6854 - accuracy: 0.54 - ETA: 1s - loss: 0.6831 - accuracy: 0.56 - ETA: 1s - loss: 0.6810 - accuracy: 0.56 - ETA: 0s - loss: 0.6788 - accuracy: 0.57 - ETA: 0s - loss: 0.6760 - accuracy: 0.58 - ETA: 0s - loss: 0.6753 - accuracy: 0.58 - ETA: 0s - loss: 0.6736 - accuracy: 0.59 - ETA: 0s - loss: 0.6719 - accuracy: 0.60 - ETA: 0s - loss: 0.6703 - accuracy: 0.60 - ETA: 0s - loss: 0.6689 - accuracy: 0.61 - ETA: 0s - loss: 0.6675 - accuracy: 0.61 - ETA: 0s - loss: 0.6662 - accuracy: 0.61 - ETA: 0s - loss: 0.6643 - accuracy: 0.62 - ETA: 0s - loss: 0.6627 - accuracy: 0.62 - ETA: 0s - loss: 0.6615 - accuracy: 0.62 - ETA: 0s - loss: 0.6595 - accuracy: 0.62 - ETA: 0s - loss: 0.6589 - accuracy: 0.63 - ETA: 0s - loss: 0.6577 - accuracy: 0.63 - ETA: 0s - loss: 0.6561 - accuracy: 0.63 - 1s 108us/step - loss: 0.6558 - accuracy: 0.6369 - val_loss: 0.6250 - val_accuracy: 0.7067\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:24 - loss: 0.6937 - accuracy: 0.60 - ETA: 3s - loss: 0.6938 - accuracy: 0.4698 - ETA: 2s - loss: 0.6937 - accuracy: 0.47 - ETA: 1s - loss: 0.6937 - accuracy: 0.48 - ETA: 1s - loss: 0.6935 - accuracy: 0.49 - ETA: 1s - loss: 0.6930 - accuracy: 0.50 - ETA: 1s - loss: 0.6928 - accuracy: 0.50 - ETA: 1s - loss: 0.6924 - accuracy: 0.50 - ETA: 1s - loss: 0.6915 - accuracy: 0.50 - ETA: 0s - loss: 0.6906 - accuracy: 0.50 - ETA: 0s - loss: 0.6890 - accuracy: 0.51 - ETA: 0s - loss: 0.6875 - accuracy: 0.51 - ETA: 0s - loss: 0.6852 - accuracy: 0.52 - ETA: 0s - loss: 0.6820 - accuracy: 0.53 - ETA: 0s - loss: 0.6805 - accuracy: 0.53 - ETA: 0s - loss: 0.6786 - accuracy: 0.53 - ETA: 0s - loss: 0.6778 - accuracy: 0.54 - ETA: 0s - loss: 0.6760 - accuracy: 0.54 - ETA: 0s - loss: 0.6746 - accuracy: 0.54 - ETA: 0s - loss: 0.6725 - accuracy: 0.54 - ETA: 0s - loss: 0.6714 - accuracy: 0.55 - ETA: 0s - loss: 0.6694 - accuracy: 0.55 - ETA: 0s - loss: 0.6671 - accuracy: 0.55 - ETA: 0s - loss: 0.6655 - accuracy: 0.56 - ETA: 0s - loss: 0.6651 - accuracy: 0.56 - ETA: 0s - loss: 0.6632 - accuracy: 0.56 - 1s 117us/step - loss: 0.6630 - accuracy: 0.5652 - val_loss: 0.6021 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:24 - loss: 0.6928 - accuracy: 0.40 - ETA: 3s - loss: 0.6937 - accuracy: 0.4636 - ETA: 2s - loss: 0.6935 - accuracy: 0.49 - ETA: 1s - loss: 0.6933 - accuracy: 0.50 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6914 - accuracy: 0.53 - ETA: 1s - loss: 0.6901 - accuracy: 0.54 - ETA: 1s - loss: 0.6886 - accuracy: 0.54 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6859 - accuracy: 0.55 - ETA: 0s - loss: 0.6845 - accuracy: 0.54 - ETA: 0s - loss: 0.6830 - accuracy: 0.55 - ETA: 0s - loss: 0.6808 - accuracy: 0.55 - ETA: 0s - loss: 0.6796 - accuracy: 0.55 - ETA: 0s - loss: 0.6781 - accuracy: 0.56 - ETA: 0s - loss: 0.6773 - accuracy: 0.56 - ETA: 0s - loss: 0.6759 - accuracy: 0.56 - ETA: 0s - loss: 0.6744 - accuracy: 0.57 - ETA: 0s - loss: 0.6735 - accuracy: 0.57 - ETA: 0s - loss: 0.6723 - accuracy: 0.57 - ETA: 0s - loss: 0.6708 - accuracy: 0.58 - ETA: 0s - loss: 0.6684 - accuracy: 0.58 - ETA: 0s - loss: 0.6669 - accuracy: 0.58 - ETA: 0s - loss: 0.6647 - accuracy: 0.59 - 1s 114us/step - loss: 0.6647 - accuracy: 0.5934 - val_loss: 0.6143 - val_accuracy: 0.7251\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 47us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:25 - loss: 0.6879 - accuracy: 0.70 - ETA: 3s - loss: 0.6935 - accuracy: 0.5000 - ETA: 2s - loss: 0.6931 - accuracy: 0.50 - ETA: 1s - loss: 0.6926 - accuracy: 0.50 - ETA: 1s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6913 - accuracy: 0.51 - ETA: 1s - loss: 0.6903 - accuracy: 0.52 - ETA: 1s - loss: 0.6895 - accuracy: 0.53 - ETA: 1s - loss: 0.6886 - accuracy: 0.53 - ETA: 1s - loss: 0.6874 - accuracy: 0.53 - ETA: 0s - loss: 0.6859 - accuracy: 0.53 - ETA: 0s - loss: 0.6841 - accuracy: 0.54 - ETA: 0s - loss: 0.6827 - accuracy: 0.54 - ETA: 0s - loss: 0.6811 - accuracy: 0.55 - ETA: 0s - loss: 0.6792 - accuracy: 0.55 - ETA: 0s - loss: 0.6777 - accuracy: 0.55 - ETA: 0s - loss: 0.6769 - accuracy: 0.55 - ETA: 0s - loss: 0.6750 - accuracy: 0.55 - ETA: 0s - loss: 0.6737 - accuracy: 0.56 - ETA: 0s - loss: 0.6732 - accuracy: 0.56 - ETA: 0s - loss: 0.6726 - accuracy: 0.56 - ETA: 0s - loss: 0.6705 - accuracy: 0.56 - ETA: 0s - loss: 0.6696 - accuracy: 0.57 - ETA: 0s - loss: 0.6688 - accuracy: 0.57 - ETA: 0s - loss: 0.6681 - accuracy: 0.57 - ETA: 0s - loss: 0.6668 - accuracy: 0.57 - 1s 118us/step - loss: 0.6660 - accuracy: 0.5798 - val_loss: 0.6165 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:24 - loss: 0.6926 - accuracy: 0.80 - ETA: 3s - loss: 0.6938 - accuracy: 0.5481 - ETA: 2s - loss: 0.6932 - accuracy: 0.52 - ETA: 1s - loss: 0.6916 - accuracy: 0.53 - ETA: 1s - loss: 0.6915 - accuracy: 0.53 - ETA: 1s - loss: 0.6885 - accuracy: 0.54 - ETA: 1s - loss: 0.6850 - accuracy: 0.55 - ETA: 1s - loss: 0.6815 - accuracy: 0.55 - ETA: 1s - loss: 0.6771 - accuracy: 0.56 - ETA: 0s - loss: 0.6737 - accuracy: 0.56 - ETA: 0s - loss: 0.6714 - accuracy: 0.56 - ETA: 0s - loss: 0.6710 - accuracy: 0.56 - ETA: 0s - loss: 0.6699 - accuracy: 0.56 - ETA: 0s - loss: 0.6697 - accuracy: 0.57 - ETA: 0s - loss: 0.6681 - accuracy: 0.57 - ETA: 0s - loss: 0.6651 - accuracy: 0.57 - ETA: 0s - loss: 0.6629 - accuracy: 0.58 - ETA: 0s - loss: 0.6625 - accuracy: 0.58 - ETA: 0s - loss: 0.6615 - accuracy: 0.58 - ETA: 0s - loss: 0.6599 - accuracy: 0.58 - ETA: 0s - loss: 0.6593 - accuracy: 0.58 - ETA: 0s - loss: 0.6580 - accuracy: 0.58 - ETA: 0s - loss: 0.6571 - accuracy: 0.59 - ETA: 0s - loss: 0.6547 - accuracy: 0.59 - ETA: 0s - loss: 0.6537 - accuracy: 0.59 - 1s 115us/step - loss: 0.6532 - accuracy: 0.5968 - val_loss: 0.6027 - val_accuracy: 0.7159\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 45us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:25 - loss: 0.6936 - accuracy: 0.60 - ETA: 3s - loss: 0.6933 - accuracy: 0.5269 - ETA: 2s - loss: 0.6933 - accuracy: 0.53 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6915 - accuracy: 0.53 - ETA: 1s - loss: 0.6905 - accuracy: 0.53 - ETA: 1s - loss: 0.6891 - accuracy: 0.53 - ETA: 0s - loss: 0.6880 - accuracy: 0.54 - ETA: 0s - loss: 0.6853 - accuracy: 0.54 - ETA: 0s - loss: 0.6826 - accuracy: 0.55 - ETA: 0s - loss: 0.6811 - accuracy: 0.55 - ETA: 0s - loss: 0.6788 - accuracy: 0.56 - ETA: 0s - loss: 0.6773 - accuracy: 0.56 - ETA: 0s - loss: 0.6761 - accuracy: 0.56 - ETA: 0s - loss: 0.6746 - accuracy: 0.57 - ETA: 0s - loss: 0.6722 - accuracy: 0.57 - ETA: 0s - loss: 0.6699 - accuracy: 0.57 - ETA: 0s - loss: 0.6681 - accuracy: 0.58 - ETA: 0s - loss: 0.6664 - accuracy: 0.58 - ETA: 0s - loss: 0.6649 - accuracy: 0.58 - ETA: 0s - loss: 0.6636 - accuracy: 0.59 - ETA: 0s - loss: 0.6621 - accuracy: 0.59 - ETA: 0s - loss: 0.6612 - accuracy: 0.59 - 2s 119us/step - loss: 0.6606 - accuracy: 0.5954 - val_loss: 0.6095 - val_accuracy: 0.7173\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:23 - loss: 0.6894 - accuracy: 0.70 - ETA: 3s - loss: 0.6940 - accuracy: 0.4885 - ETA: 2s - loss: 0.6936 - accuracy: 0.49 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6928 - accuracy: 0.51 - ETA: 1s - loss: 0.6920 - accuracy: 0.52 - ETA: 1s - loss: 0.6890 - accuracy: 0.53 - ETA: 1s - loss: 0.6882 - accuracy: 0.54 - ETA: 1s - loss: 0.6870 - accuracy: 0.54 - ETA: 1s - loss: 0.6851 - accuracy: 0.55 - ETA: 0s - loss: 0.6829 - accuracy: 0.55 - ETA: 0s - loss: 0.6807 - accuracy: 0.56 - ETA: 0s - loss: 0.6792 - accuracy: 0.56 - ETA: 0s - loss: 0.6770 - accuracy: 0.56 - ETA: 0s - loss: 0.6759 - accuracy: 0.57 - ETA: 0s - loss: 0.6749 - accuracy: 0.57 - ETA: 0s - loss: 0.6734 - accuracy: 0.57 - ETA: 0s - loss: 0.6727 - accuracy: 0.58 - ETA: 0s - loss: 0.6715 - accuracy: 0.58 - ETA: 0s - loss: 0.6699 - accuracy: 0.58 - ETA: 0s - loss: 0.6681 - accuracy: 0.58 - ETA: 0s - loss: 0.6677 - accuracy: 0.59 - ETA: 0s - loss: 0.6666 - accuracy: 0.59 - ETA: 0s - loss: 0.6651 - accuracy: 0.59 - ETA: 0s - loss: 0.6643 - accuracy: 0.59 - ETA: 0s - loss: 0.6635 - accuracy: 0.59 - 1s 117us/step - loss: 0.6634 - accuracy: 0.5979 - val_loss: 0.6205 - val_accuracy: 0.7180\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:24 - loss: 0.6949 - accuracy: 0.20 - ETA: 3s - loss: 0.6936 - accuracy: 0.5208 - ETA: 2s - loss: 0.6938 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.54 - ETA: 1s - loss: 0.6925 - accuracy: 0.54 - ETA: 1s - loss: 0.6922 - accuracy: 0.54 - ETA: 1s - loss: 0.6907 - accuracy: 0.55 - ETA: 1s - loss: 0.6881 - accuracy: 0.56 - ETA: 1s - loss: 0.6856 - accuracy: 0.57 - ETA: 0s - loss: 0.6840 - accuracy: 0.57 - ETA: 0s - loss: 0.6825 - accuracy: 0.57 - ETA: 0s - loss: 0.6796 - accuracy: 0.58 - ETA: 0s - loss: 0.6779 - accuracy: 0.58 - ETA: 0s - loss: 0.6759 - accuracy: 0.59 - ETA: 0s - loss: 0.6739 - accuracy: 0.59 - ETA: 0s - loss: 0.6723 - accuracy: 0.59 - ETA: 0s - loss: 0.6702 - accuracy: 0.60 - ETA: 0s - loss: 0.6687 - accuracy: 0.60 - ETA: 0s - loss: 0.6675 - accuracy: 0.60 - ETA: 0s - loss: 0.6659 - accuracy: 0.61 - ETA: 0s - loss: 0.6641 - accuracy: 0.61 - ETA: 0s - loss: 0.6623 - accuracy: 0.61 - ETA: 0s - loss: 0.6614 - accuracy: 0.62 - ETA: 0s - loss: 0.6600 - accuracy: 0.62 - ETA: 0s - loss: 0.6584 - accuracy: 0.62 - 1s 115us/step - loss: 0.6578 - accuracy: 0.6284 - val_loss: 0.6034 - val_accuracy: 0.7173\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 43us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 2:26 - loss: 0.6938 - accuracy: 0.60 - ETA: 3s - loss: 0.6938 - accuracy: 0.4887 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.51 - ETA: 1s - loss: 0.6920 - accuracy: 0.53 - ETA: 1s - loss: 0.6909 - accuracy: 0.53 - ETA: 1s - loss: 0.6894 - accuracy: 0.54 - ETA: 1s - loss: 0.6867 - accuracy: 0.55 - ETA: 1s - loss: 0.6839 - accuracy: 0.55 - ETA: 0s - loss: 0.6805 - accuracy: 0.56 - ETA: 0s - loss: 0.6787 - accuracy: 0.56 - ETA: 0s - loss: 0.6757 - accuracy: 0.57 - ETA: 0s - loss: 0.6729 - accuracy: 0.57 - ETA: 0s - loss: 0.6708 - accuracy: 0.58 - ETA: 0s - loss: 0.6687 - accuracy: 0.58 - ETA: 0s - loss: 0.6664 - accuracy: 0.59 - ETA: 0s - loss: 0.6640 - accuracy: 0.59 - ETA: 0s - loss: 0.6625 - accuracy: 0.60 - ETA: 0s - loss: 0.6596 - accuracy: 0.60 - ETA: 0s - loss: 0.6586 - accuracy: 0.60 - ETA: 0s - loss: 0.6573 - accuracy: 0.60 - ETA: 0s - loss: 0.6557 - accuracy: 0.61 - ETA: 0s - loss: 0.6544 - accuracy: 0.61 - ETA: 0s - loss: 0.6533 - accuracy: 0.61 - ETA: 0s - loss: 0.6517 - accuracy: 0.61 - 1s 116us/step - loss: 0.6512 - accuracy: 0.6209 - val_loss: 0.5920 - val_accuracy: 0.7216\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 46us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:25 - loss: 0.6956 - accuracy: 0.40 - ETA: 3s - loss: 0.6928 - accuracy: 0.5321 - ETA: 2s - loss: 0.6925 - accuracy: 0.53 - ETA: 1s - loss: 0.6916 - accuracy: 0.54 - ETA: 1s - loss: 0.6903 - accuracy: 0.54 - ETA: 1s - loss: 0.6881 - accuracy: 0.55 - ETA: 1s - loss: 0.6866 - accuracy: 0.55 - ETA: 1s - loss: 0.6847 - accuracy: 0.56 - ETA: 1s - loss: 0.6822 - accuracy: 0.56 - ETA: 0s - loss: 0.6780 - accuracy: 0.57 - ETA: 0s - loss: 0.6746 - accuracy: 0.58 - ETA: 0s - loss: 0.6705 - accuracy: 0.58 - ETA: 0s - loss: 0.6695 - accuracy: 0.58 - ETA: 0s - loss: 0.6660 - accuracy: 0.59 - ETA: 0s - loss: 0.6641 - accuracy: 0.59 - ETA: 0s - loss: 0.6615 - accuracy: 0.60 - ETA: 0s - loss: 0.6593 - accuracy: 0.60 - ETA: 0s - loss: 0.6574 - accuracy: 0.60 - ETA: 0s - loss: 0.6555 - accuracy: 0.61 - ETA: 0s - loss: 0.6538 - accuracy: 0.61 - ETA: 0s - loss: 0.6542 - accuracy: 0.61 - ETA: 0s - loss: 0.6527 - accuracy: 0.61 - ETA: 0s - loss: 0.6520 - accuracy: 0.61 - ETA: 0s - loss: 0.6506 - accuracy: 0.62 - ETA: 0s - loss: 0.6491 - accuracy: 0.62 - 1s 116us/step - loss: 0.6478 - accuracy: 0.6256 - val_loss: 0.5984 - val_accuracy: 0.7202\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:26 - loss: 0.6823 - accuracy: 0.90 - ETA: 3s - loss: 0.6935 - accuracy: 0.5019 - ETA: 2s - loss: 0.6928 - accuracy: 0.51 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6913 - accuracy: 0.52 - ETA: 1s - loss: 0.6896 - accuracy: 0.52 - ETA: 1s - loss: 0.6863 - accuracy: 0.53 - ETA: 1s - loss: 0.6841 - accuracy: 0.53 - ETA: 1s - loss: 0.6818 - accuracy: 0.54 - ETA: 1s - loss: 0.6801 - accuracy: 0.55 - ETA: 1s - loss: 0.6787 - accuracy: 0.55 - ETA: 0s - loss: 0.6784 - accuracy: 0.55 - ETA: 0s - loss: 0.6769 - accuracy: 0.56 - ETA: 0s - loss: 0.6767 - accuracy: 0.57 - ETA: 0s - loss: 0.6760 - accuracy: 0.57 - ETA: 0s - loss: 0.6750 - accuracy: 0.58 - ETA: 0s - loss: 0.6731 - accuracy: 0.58 - ETA: 0s - loss: 0.6717 - accuracy: 0.59 - ETA: 0s - loss: 0.6702 - accuracy: 0.59 - ETA: 0s - loss: 0.6695 - accuracy: 0.59 - ETA: 0s - loss: 0.6673 - accuracy: 0.60 - ETA: 0s - loss: 0.6658 - accuracy: 0.60 - ETA: 0s - loss: 0.6648 - accuracy: 0.60 - ETA: 0s - loss: 0.6630 - accuracy: 0.61 - ETA: 0s - loss: 0.6615 - accuracy: 0.61 - ETA: 0s - loss: 0.6605 - accuracy: 0.61 - ETA: 0s - loss: 0.6589 - accuracy: 0.62 - ETA: 0s - loss: 0.6571 - accuracy: 0.62 - ETA: 0s - loss: 0.6562 - accuracy: 0.62 - ETA: 0s - loss: 0.6551 - accuracy: 0.62 - ETA: 0s - loss: 0.6545 - accuracy: 0.63 - 2s 140us/step - loss: 0.6538 - accuracy: 0.6316 - val_loss: 0.5992 - val_accuracy: 0.7145\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:26 - loss: 0.6980 - accuracy: 0.40 - ETA: 3s - loss: 0.6937 - accuracy: 0.5075 - ETA: 2s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6922 - accuracy: 0.53 - ETA: 1s - loss: 0.6916 - accuracy: 0.53 - ETA: 1s - loss: 0.6889 - accuracy: 0.55 - ETA: 1s - loss: 0.6869 - accuracy: 0.55 - ETA: 1s - loss: 0.6833 - accuracy: 0.56 - ETA: 0s - loss: 0.6828 - accuracy: 0.57 - ETA: 0s - loss: 0.6800 - accuracy: 0.57 - ETA: 0s - loss: 0.6779 - accuracy: 0.58 - ETA: 0s - loss: 0.6754 - accuracy: 0.58 - ETA: 0s - loss: 0.6734 - accuracy: 0.58 - ETA: 0s - loss: 0.6717 - accuracy: 0.59 - ETA: 0s - loss: 0.6695 - accuracy: 0.59 - ETA: 0s - loss: 0.6667 - accuracy: 0.60 - ETA: 0s - loss: 0.6656 - accuracy: 0.60 - ETA: 0s - loss: 0.6636 - accuracy: 0.60 - ETA: 0s - loss: 0.6616 - accuracy: 0.60 - ETA: 0s - loss: 0.6606 - accuracy: 0.61 - ETA: 0s - loss: 0.6586 - accuracy: 0.61 - ETA: 0s - loss: 0.6578 - accuracy: 0.61 - ETA: 0s - loss: 0.6574 - accuracy: 0.61 - ETA: 0s - loss: 0.6564 - accuracy: 0.61 - 1s 116us/step - loss: 0.6561 - accuracy: 0.6157 - val_loss: 0.6078 - val_accuracy: 0.7180\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:29 - loss: 0.6920 - accuracy: 0.50 - ETA: 4s - loss: 0.6930 - accuracy: 0.5137 - ETA: 2s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6906 - accuracy: 0.53 - ETA: 1s - loss: 0.6892 - accuracy: 0.54 - ETA: 1s - loss: 0.6867 - accuracy: 0.55 - ETA: 1s - loss: 0.6845 - accuracy: 0.56 - ETA: 1s - loss: 0.6820 - accuracy: 0.56 - ETA: 1s - loss: 0.6795 - accuracy: 0.56 - ETA: 1s - loss: 0.6782 - accuracy: 0.56 - ETA: 0s - loss: 0.6748 - accuracy: 0.57 - ETA: 0s - loss: 0.6716 - accuracy: 0.58 - ETA: 0s - loss: 0.6680 - accuracy: 0.58 - ETA: 0s - loss: 0.6650 - accuracy: 0.58 - ETA: 0s - loss: 0.6638 - accuracy: 0.58 - ETA: 0s - loss: 0.6620 - accuracy: 0.59 - ETA: 0s - loss: 0.6588 - accuracy: 0.59 - ETA: 0s - loss: 0.6564 - accuracy: 0.59 - ETA: 0s - loss: 0.6549 - accuracy: 0.60 - ETA: 0s - loss: 0.6533 - accuracy: 0.60 - ETA: 0s - loss: 0.6518 - accuracy: 0.60 - ETA: 0s - loss: 0.6505 - accuracy: 0.61 - ETA: 0s - loss: 0.6496 - accuracy: 0.61 - ETA: 0s - loss: 0.6473 - accuracy: 0.61 - ETA: 0s - loss: 0.6455 - accuracy: 0.61 - ETA: 0s - loss: 0.6448 - accuracy: 0.62 - 2s 119us/step - loss: 0.6439 - accuracy: 0.6217 - val_loss: 0.5949 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:30 - loss: 0.6962 - accuracy: 0.30 - ETA: 3s - loss: 0.6941 - accuracy: 0.5135 - ETA: 2s - loss: 0.6934 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.53 - ETA: 1s - loss: 0.6894 - accuracy: 0.55 - ETA: 1s - loss: 0.6857 - accuracy: 0.56 - ETA: 1s - loss: 0.6819 - accuracy: 0.57 - ETA: 1s - loss: 0.6792 - accuracy: 0.57 - ETA: 1s - loss: 0.6751 - accuracy: 0.59 - ETA: 1s - loss: 0.6721 - accuracy: 0.59 - ETA: 0s - loss: 0.6678 - accuracy: 0.60 - ETA: 0s - loss: 0.6660 - accuracy: 0.60 - ETA: 0s - loss: 0.6638 - accuracy: 0.60 - ETA: 0s - loss: 0.6620 - accuracy: 0.61 - ETA: 0s - loss: 0.6593 - accuracy: 0.61 - ETA: 0s - loss: 0.6562 - accuracy: 0.61 - ETA: 0s - loss: 0.6539 - accuracy: 0.62 - ETA: 0s - loss: 0.6540 - accuracy: 0.62 - ETA: 0s - loss: 0.6524 - accuracy: 0.62 - ETA: 0s - loss: 0.6500 - accuracy: 0.62 - ETA: 0s - loss: 0.6481 - accuracy: 0.63 - ETA: 0s - loss: 0.6474 - accuracy: 0.63 - ETA: 0s - loss: 0.6456 - accuracy: 0.63 - ETA: 0s - loss: 0.6439 - accuracy: 0.63 - ETA: 0s - loss: 0.6433 - accuracy: 0.63 - ETA: 0s - loss: 0.6418 - accuracy: 0.63 - 2s 119us/step - loss: 0.6413 - accuracy: 0.6402 - val_loss: 0.5855 - val_accuracy: 0.7244\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:25 - loss: 0.6883 - accuracy: 0.70 - ETA: 3s - loss: 0.6951 - accuracy: 0.4827 - ETA: 2s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6895 - accuracy: 0.54 - ETA: 1s - loss: 0.6869 - accuracy: 0.54 - ETA: 1s - loss: 0.6849 - accuracy: 0.54 - ETA: 1s - loss: 0.6826 - accuracy: 0.56 - ETA: 1s - loss: 0.6792 - accuracy: 0.57 - ETA: 1s - loss: 0.6746 - accuracy: 0.58 - ETA: 1s - loss: 0.6712 - accuracy: 0.58 - ETA: 0s - loss: 0.6672 - accuracy: 0.59 - ETA: 0s - loss: 0.6631 - accuracy: 0.60 - ETA: 0s - loss: 0.6607 - accuracy: 0.60 - ETA: 0s - loss: 0.6584 - accuracy: 0.60 - ETA: 0s - loss: 0.6562 - accuracy: 0.61 - ETA: 0s - loss: 0.6557 - accuracy: 0.61 - ETA: 0s - loss: 0.6535 - accuracy: 0.61 - ETA: 0s - loss: 0.6503 - accuracy: 0.62 - ETA: 0s - loss: 0.6494 - accuracy: 0.62 - ETA: 0s - loss: 0.6468 - accuracy: 0.62 - ETA: 0s - loss: 0.6450 - accuracy: 0.63 - ETA: 0s - loss: 0.6429 - accuracy: 0.63 - ETA: 0s - loss: 0.6417 - accuracy: 0.63 - ETA: 0s - loss: 0.6403 - accuracy: 0.64 - ETA: 0s - loss: 0.6383 - accuracy: 0.64 - ETA: 0s - loss: 0.6361 - accuracy: 0.64 - 1s 117us/step - loss: 0.6360 - accuracy: 0.6466 - val_loss: 0.5772 - val_accuracy: 0.7223\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:30 - loss: 0.6899 - accuracy: 0.90 - ETA: 4s - loss: 0.6940 - accuracy: 0.5373 - ETA: 2s - loss: 0.6916 - accuracy: 0.54 - ETA: 1s - loss: 0.6901 - accuracy: 0.53 - ETA: 1s - loss: 0.6859 - accuracy: 0.54 - ETA: 1s - loss: 0.6830 - accuracy: 0.55 - ETA: 1s - loss: 0.6809 - accuracy: 0.56 - ETA: 1s - loss: 0.6770 - accuracy: 0.57 - ETA: 1s - loss: 0.6750 - accuracy: 0.58 - ETA: 1s - loss: 0.6718 - accuracy: 0.59 - ETA: 0s - loss: 0.6676 - accuracy: 0.60 - ETA: 0s - loss: 0.6650 - accuracy: 0.60 - ETA: 0s - loss: 0.6614 - accuracy: 0.61 - ETA: 0s - loss: 0.6588 - accuracy: 0.62 - ETA: 0s - loss: 0.6546 - accuracy: 0.62 - ETA: 0s - loss: 0.6524 - accuracy: 0.62 - ETA: 0s - loss: 0.6525 - accuracy: 0.63 - ETA: 0s - loss: 0.6496 - accuracy: 0.63 - ETA: 0s - loss: 0.6483 - accuracy: 0.63 - ETA: 0s - loss: 0.6467 - accuracy: 0.63 - ETA: 0s - loss: 0.6445 - accuracy: 0.64 - ETA: 0s - loss: 0.6436 - accuracy: 0.64 - ETA: 0s - loss: 0.6422 - accuracy: 0.64 - ETA: 0s - loss: 0.6403 - accuracy: 0.65 - ETA: 0s - loss: 0.6386 - accuracy: 0.65 - 1s 116us/step - loss: 0.6372 - accuracy: 0.6547 - val_loss: 0.5815 - val_accuracy: 0.7095\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:28 - loss: 0.6995 - accuracy: 0.40 - ETA: 3s - loss: 0.6927 - accuracy: 0.5608 - ETA: 2s - loss: 0.6900 - accuracy: 0.55 - ETA: 1s - loss: 0.6896 - accuracy: 0.54 - ETA: 1s - loss: 0.6874 - accuracy: 0.55 - ETA: 1s - loss: 0.6834 - accuracy: 0.56 - ETA: 1s - loss: 0.6796 - accuracy: 0.57 - ETA: 1s - loss: 0.6750 - accuracy: 0.58 - ETA: 1s - loss: 0.6712 - accuracy: 0.59 - ETA: 1s - loss: 0.6680 - accuracy: 0.60 - ETA: 0s - loss: 0.6640 - accuracy: 0.60 - ETA: 0s - loss: 0.6576 - accuracy: 0.61 - ETA: 0s - loss: 0.6551 - accuracy: 0.62 - ETA: 0s - loss: 0.6496 - accuracy: 0.62 - ETA: 0s - loss: 0.6460 - accuracy: 0.63 - ETA: 0s - loss: 0.6436 - accuracy: 0.63 - ETA: 0s - loss: 0.6423 - accuracy: 0.63 - ETA: 0s - loss: 0.6415 - accuracy: 0.64 - ETA: 0s - loss: 0.6411 - accuracy: 0.64 - ETA: 0s - loss: 0.6391 - accuracy: 0.64 - ETA: 0s - loss: 0.6371 - accuracy: 0.64 - ETA: 0s - loss: 0.6361 - accuracy: 0.64 - ETA: 0s - loss: 0.6340 - accuracy: 0.65 - ETA: 0s - loss: 0.6327 - accuracy: 0.65 - ETA: 0s - loss: 0.6309 - accuracy: 0.65 - ETA: 0s - loss: 0.6301 - accuracy: 0.65 - 2s 119us/step - loss: 0.6291 - accuracy: 0.6586 - val_loss: 0.5878 - val_accuracy: 0.7138\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 41us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:28 - loss: 0.6964 - accuracy: 0.40 - ETA: 3s - loss: 0.6941 - accuracy: 0.5192 - ETA: 2s - loss: 0.6925 - accuracy: 0.53 - ETA: 1s - loss: 0.6905 - accuracy: 0.54 - ETA: 1s - loss: 0.6862 - accuracy: 0.55 - ETA: 1s - loss: 0.6831 - accuracy: 0.56 - ETA: 1s - loss: 0.6812 - accuracy: 0.56 - ETA: 1s - loss: 0.6773 - accuracy: 0.58 - ETA: 1s - loss: 0.6734 - accuracy: 0.59 - ETA: 1s - loss: 0.6699 - accuracy: 0.60 - ETA: 0s - loss: 0.6670 - accuracy: 0.60 - ETA: 0s - loss: 0.6633 - accuracy: 0.60 - ETA: 0s - loss: 0.6601 - accuracy: 0.61 - ETA: 0s - loss: 0.6575 - accuracy: 0.61 - ETA: 0s - loss: 0.6546 - accuracy: 0.62 - ETA: 0s - loss: 0.6526 - accuracy: 0.62 - ETA: 0s - loss: 0.6506 - accuracy: 0.63 - ETA: 0s - loss: 0.6468 - accuracy: 0.63 - ETA: 0s - loss: 0.6447 - accuracy: 0.63 - ETA: 0s - loss: 0.6411 - accuracy: 0.64 - ETA: 0s - loss: 0.6394 - accuracy: 0.64 - ETA: 0s - loss: 0.6381 - accuracy: 0.64 - ETA: 0s - loss: 0.6358 - accuracy: 0.65 - ETA: 0s - loss: 0.6342 - accuracy: 0.65 - ETA: 0s - loss: 0.6331 - accuracy: 0.65 - ETA: 0s - loss: 0.6328 - accuracy: 0.65 - 2s 119us/step - loss: 0.6321 - accuracy: 0.6561 - val_loss: 0.5820 - val_accuracy: 0.7195\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 42us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:28 - loss: 0.6929 - accuracy: 0.70 - ETA: 3s - loss: 0.6945 - accuracy: 0.5208 - ETA: 2s - loss: 0.6947 - accuracy: 0.51 - ETA: 1s - loss: 0.6934 - accuracy: 0.53 - ETA: 1s - loss: 0.6918 - accuracy: 0.54 - ETA: 1s - loss: 0.6878 - accuracy: 0.55 - ETA: 1s - loss: 0.6839 - accuracy: 0.56 - ETA: 1s - loss: 0.6815 - accuracy: 0.57 - ETA: 1s - loss: 0.6774 - accuracy: 0.58 - ETA: 1s - loss: 0.6732 - accuracy: 0.59 - ETA: 0s - loss: 0.6699 - accuracy: 0.59 - ETA: 0s - loss: 0.6664 - accuracy: 0.60 - ETA: 0s - loss: 0.6626 - accuracy: 0.60 - ETA: 0s - loss: 0.6596 - accuracy: 0.61 - ETA: 0s - loss: 0.6593 - accuracy: 0.61 - ETA: 0s - loss: 0.6562 - accuracy: 0.62 - ETA: 0s - loss: 0.6526 - accuracy: 0.62 - ETA: 0s - loss: 0.6508 - accuracy: 0.62 - ETA: 0s - loss: 0.6490 - accuracy: 0.63 - ETA: 0s - loss: 0.6476 - accuracy: 0.63 - ETA: 0s - loss: 0.6447 - accuracy: 0.63 - ETA: 0s - loss: 0.6436 - accuracy: 0.63 - ETA: 0s - loss: 0.6422 - accuracy: 0.64 - ETA: 0s - loss: 0.6421 - accuracy: 0.64 - ETA: 0s - loss: 0.6407 - accuracy: 0.64 - ETA: 0s - loss: 0.6389 - accuracy: 0.64 - 1s 118us/step - loss: 0.6387 - accuracy: 0.6469 - val_loss: 0.5887 - val_accuracy: 0.7180\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - ETA:  - 0s 40us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:19 - loss: 0.6927 - accuracy: 0.56 - ETA: 2s - loss: 0.6928 - accuracy: 0.5370 - ETA: 1s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6918 - accuracy: 0.53 - ETA: 0s - loss: 0.6909 - accuracy: 0.54 - ETA: 0s - loss: 0.6899 - accuracy: 0.54 - ETA: 0s - loss: 0.6892 - accuracy: 0.55 - ETA: 0s - loss: 0.6885 - accuracy: 0.55 - ETA: 0s - loss: 0.6876 - accuracy: 0.56 - ETA: 0s - loss: 0.6866 - accuracy: 0.56 - ETA: 0s - loss: 0.6857 - accuracy: 0.56 - ETA: 0s - loss: 0.6847 - accuracy: 0.57 - ETA: 0s - loss: 0.6840 - accuracy: 0.57 - ETA: 0s - loss: 0.6829 - accuracy: 0.57 - ETA: 0s - loss: 0.6824 - accuracy: 0.57 - 1s 74us/step - loss: 0.6823 - accuracy: 0.5775 - val_loss: 0.6587 - val_accuracy: 0.7074\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 1:19 - loss: 0.6918 - accuracy: 0.43 - ETA: 2s - loss: 0.6936 - accuracy: 0.4741 - ETA: 1s - loss: 0.6935 - accuracy: 0.48 - ETA: 0s - loss: 0.6932 - accuracy: 0.50 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.53 - ETA: 0s - loss: 0.6921 - accuracy: 0.54 - ETA: 0s - loss: 0.6916 - accuracy: 0.55 - ETA: 0s - loss: 0.6910 - accuracy: 0.56 - ETA: 0s - loss: 0.6904 - accuracy: 0.56 - ETA: 0s - loss: 0.6897 - accuracy: 0.57 - ETA: 0s - loss: 0.6890 - accuracy: 0.57 - ETA: 0s - loss: 0.6883 - accuracy: 0.57 - ETA: 0s - loss: 0.6874 - accuracy: 0.58 - ETA: 0s - loss: 0.6865 - accuracy: 0.58 - 1s 73us/step - loss: 0.6858 - accuracy: 0.5877 - val_loss: 0.6684 - val_accuracy: 0.6946\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:20 - loss: 0.6934 - accuracy: 0.56 - ETA: 2s - loss: 0.6933 - accuracy: 0.5301 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.53 - ETA: 0s - loss: 0.6922 - accuracy: 0.54 - ETA: 0s - loss: 0.6916 - accuracy: 0.55 - ETA: 0s - loss: 0.6908 - accuracy: 0.56 - ETA: 0s - loss: 0.6898 - accuracy: 0.57 - ETA: 0s - loss: 0.6886 - accuracy: 0.57 - ETA: 0s - loss: 0.6875 - accuracy: 0.58 - ETA: 0s - loss: 0.6860 - accuracy: 0.58 - ETA: 0s - loss: 0.6849 - accuracy: 0.59 - ETA: 0s - loss: 0.6837 - accuracy: 0.59 - ETA: 0s - loss: 0.6824 - accuracy: 0.60 - ETA: 0s - loss: 0.6805 - accuracy: 0.60 - 1s 74us/step - loss: 0.6804 - accuracy: 0.6057 - val_loss: 0.6553 - val_accuracy: 0.6655\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:19 - loss: 0.6936 - accuracy: 0.56 - ETA: 2s - loss: 0.6929 - accuracy: 0.5035 - ETA: 1s - loss: 0.6927 - accuracy: 0.49 - ETA: 1s - loss: 0.6915 - accuracy: 0.52 - ETA: 0s - loss: 0.6901 - accuracy: 0.52 - ETA: 0s - loss: 0.6887 - accuracy: 0.53 - ETA: 0s - loss: 0.6881 - accuracy: 0.53 - ETA: 0s - loss: 0.6862 - accuracy: 0.55 - ETA: 0s - loss: 0.6849 - accuracy: 0.56 - ETA: 0s - loss: 0.6835 - accuracy: 0.56 - ETA: 0s - loss: 0.6820 - accuracy: 0.57 - ETA: 0s - loss: 0.6795 - accuracy: 0.57 - ETA: 0s - loss: 0.6777 - accuracy: 0.58 - ETA: 0s - loss: 0.6760 - accuracy: 0.58 - ETA: 0s - loss: 0.6748 - accuracy: 0.58 - ETA: 0s - loss: 0.6734 - accuracy: 0.59 - 1s 74us/step - loss: 0.6732 - accuracy: 0.5921 - val_loss: 0.6414 - val_accuracy: 0.7095\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:21 - loss: 0.6921 - accuracy: 0.56 - ETA: 2s - loss: 0.6933 - accuracy: 0.5204 - ETA: 1s - loss: 0.6927 - accuracy: 0.54 - ETA: 1s - loss: 0.6924 - accuracy: 0.54 - ETA: 0s - loss: 0.6914 - accuracy: 0.56 - ETA: 0s - loss: 0.6909 - accuracy: 0.56 - ETA: 0s - loss: 0.6903 - accuracy: 0.57 - ETA: 0s - loss: 0.6896 - accuracy: 0.57 - ETA: 0s - loss: 0.6884 - accuracy: 0.58 - ETA: 0s - loss: 0.6873 - accuracy: 0.58 - ETA: 0s - loss: 0.6859 - accuracy: 0.59 - ETA: 0s - loss: 0.6847 - accuracy: 0.59 - ETA: 0s - loss: 0.6825 - accuracy: 0.59 - ETA: 0s - loss: 0.6811 - accuracy: 0.59 - ETA: 0s - loss: 0.6801 - accuracy: 0.60 - ETA: 0s - loss: 0.6787 - accuracy: 0.60 - 1s 76us/step - loss: 0.6779 - accuracy: 0.6028 - val_loss: 0.6491 - val_accuracy: 0.7109\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:19 - loss: 0.6920 - accuracy: 0.62 - ETA: 2s - loss: 0.6937 - accuracy: 0.4942 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.53 - ETA: 0s - loss: 0.6913 - accuracy: 0.54 - ETA: 0s - loss: 0.6907 - accuracy: 0.54 - ETA: 0s - loss: 0.6897 - accuracy: 0.55 - ETA: 0s - loss: 0.6887 - accuracy: 0.55 - ETA: 0s - loss: 0.6880 - accuracy: 0.55 - ETA: 0s - loss: 0.6871 - accuracy: 0.56 - ETA: 0s - loss: 0.6862 - accuracy: 0.56 - ETA: 0s - loss: 0.6849 - accuracy: 0.57 - ETA: 0s - loss: 0.6840 - accuracy: 0.57 - ETA: 0s - loss: 0.6825 - accuracy: 0.57 - 1s 75us/step - loss: 0.6819 - accuracy: 0.5754 - val_loss: 0.6576 - val_accuracy: 0.6896\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:19 - loss: 0.6949 - accuracy: 0.43 - ETA: 2s - loss: 0.6938 - accuracy: 0.5046 - ETA: 1s - loss: 0.6931 - accuracy: 0.53 - ETA: 1s - loss: 0.6924 - accuracy: 0.54 - ETA: 0s - loss: 0.6914 - accuracy: 0.54 - ETA: 0s - loss: 0.6905 - accuracy: 0.55 - ETA: 0s - loss: 0.6897 - accuracy: 0.55 - ETA: 0s - loss: 0.6887 - accuracy: 0.56 - ETA: 0s - loss: 0.6874 - accuracy: 0.56 - ETA: 0s - loss: 0.6860 - accuracy: 0.57 - ETA: 0s - loss: 0.6849 - accuracy: 0.57 - ETA: 0s - loss: 0.6840 - accuracy: 0.57 - ETA: 0s - loss: 0.6830 - accuracy: 0.57 - ETA: 0s - loss: 0.6817 - accuracy: 0.57 - ETA: 0s - loss: 0.6798 - accuracy: 0.58 - 1s 73us/step - loss: 0.6776 - accuracy: 0.5858 - val_loss: 0.6425 - val_accuracy: 0.7095\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:18 - loss: 0.6948 - accuracy: 0.56 - ETA: 2s - loss: 0.6935 - accuracy: 0.5382 - ETA: 1s - loss: 0.6922 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6908 - accuracy: 0.53 - ETA: 0s - loss: 0.6901 - accuracy: 0.53 - ETA: 0s - loss: 0.6888 - accuracy: 0.54 - ETA: 0s - loss: 0.6872 - accuracy: 0.55 - ETA: 0s - loss: 0.6855 - accuracy: 0.56 - ETA: 0s - loss: 0.6841 - accuracy: 0.57 - ETA: 0s - loss: 0.6827 - accuracy: 0.57 - ETA: 0s - loss: 0.6803 - accuracy: 0.58 - ETA: 0s - loss: 0.6784 - accuracy: 0.58 - ETA: 0s - loss: 0.6760 - accuracy: 0.59 - ETA: 0s - loss: 0.6743 - accuracy: 0.59 - ETA: 0s - loss: 0.6725 - accuracy: 0.59 - 1s 75us/step - loss: 0.6720 - accuracy: 0.5982 - val_loss: 0.6309 - val_accuracy: 0.7102\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:19 - loss: 0.6905 - accuracy: 0.62 - ETA: 2s - loss: 0.6930 - accuracy: 0.5200 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6902 - accuracy: 0.54 - ETA: 0s - loss: 0.6881 - accuracy: 0.55 - ETA: 0s - loss: 0.6862 - accuracy: 0.55 - ETA: 0s - loss: 0.6836 - accuracy: 0.57 - ETA: 0s - loss: 0.6808 - accuracy: 0.58 - ETA: 0s - loss: 0.6788 - accuracy: 0.58 - ETA: 0s - loss: 0.6768 - accuracy: 0.59 - ETA: 0s - loss: 0.6735 - accuracy: 0.60 - ETA: 0s - loss: 0.6716 - accuracy: 0.60 - ETA: 0s - loss: 0.6690 - accuracy: 0.60 - ETA: 0s - loss: 0.6671 - accuracy: 0.61 - ETA: 0s - loss: 0.6646 - accuracy: 0.61 - ETA: 0s - loss: 0.6617 - accuracy: 0.61 - 1s 75us/step - loss: 0.6617 - accuracy: 0.6193 - val_loss: 0.6160 - val_accuracy: 0.7131\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:21 - loss: 0.6937 - accuracy: 0.62 - ETA: 2s - loss: 0.6927 - accuracy: 0.5477 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.53 - ETA: 0s - loss: 0.6907 - accuracy: 0.54 - ETA: 0s - loss: 0.6894 - accuracy: 0.55 - ETA: 0s - loss: 0.6870 - accuracy: 0.55 - ETA: 0s - loss: 0.6862 - accuracy: 0.55 - ETA: 0s - loss: 0.6838 - accuracy: 0.56 - ETA: 0s - loss: 0.6818 - accuracy: 0.57 - ETA: 0s - loss: 0.6797 - accuracy: 0.57 - ETA: 0s - loss: 0.6770 - accuracy: 0.58 - ETA: 0s - loss: 0.6741 - accuracy: 0.59 - ETA: 0s - loss: 0.6717 - accuracy: 0.59 - ETA: 0s - loss: 0.6694 - accuracy: 0.60 - ETA: 0s - loss: 0.6667 - accuracy: 0.60 - 1s 75us/step - loss: 0.6660 - accuracy: 0.6070 - val_loss: 0.6228 - val_accuracy: 0.7152\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:19 - loss: 0.6978 - accuracy: 0.43 - ETA: 2s - loss: 0.6930 - accuracy: 0.5413 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6903 - accuracy: 0.53 - ETA: 0s - loss: 0.6901 - accuracy: 0.53 - ETA: 0s - loss: 0.6889 - accuracy: 0.54 - ETA: 0s - loss: 0.6869 - accuracy: 0.55 - ETA: 0s - loss: 0.6847 - accuracy: 0.56 - ETA: 0s - loss: 0.6832 - accuracy: 0.56 - ETA: 0s - loss: 0.6816 - accuracy: 0.57 - ETA: 0s - loss: 0.6789 - accuracy: 0.58 - ETA: 0s - loss: 0.6766 - accuracy: 0.59 - ETA: 0s - loss: 0.6746 - accuracy: 0.59 - ETA: 0s - loss: 0.6728 - accuracy: 0.59 - ETA: 0s - loss: 0.6710 - accuracy: 0.60 - ETA: 0s - loss: 0.6696 - accuracy: 0.60 - 1s 74us/step - loss: 0.6693 - accuracy: 0.6038 - val_loss: 0.6233 - val_accuracy: 0.7109\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 30us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:19 - loss: 0.6955 - accuracy: 0.50 - ETA: 2s - loss: 0.6934 - accuracy: 0.5200 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 0s - loss: 0.6902 - accuracy: 0.54 - ETA: 0s - loss: 0.6889 - accuracy: 0.54 - ETA: 0s - loss: 0.6872 - accuracy: 0.56 - ETA: 0s - loss: 0.6855 - accuracy: 0.57 - ETA: 0s - loss: 0.6834 - accuracy: 0.58 - ETA: 0s - loss: 0.6811 - accuracy: 0.58 - ETA: 0s - loss: 0.6786 - accuracy: 0.59 - ETA: 0s - loss: 0.6765 - accuracy: 0.60 - ETA: 0s - loss: 0.6744 - accuracy: 0.60 - ETA: 0s - loss: 0.6709 - accuracy: 0.61 - ETA: 0s - loss: 0.6688 - accuracy: 0.61 - ETA: 0s - loss: 0.6660 - accuracy: 0.61 - 1s 75us/step - loss: 0.6646 - accuracy: 0.6197 - val_loss: 0.6219 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:21 - loss: 0.6953 - accuracy: 0.43 - ETA: 2s - loss: 0.6941 - accuracy: 0.4811 - ETA: 1s - loss: 0.6932 - accuracy: 0.50 - ETA: 1s - loss: 0.6922 - accuracy: 0.52 - ETA: 0s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6895 - accuracy: 0.54 - ETA: 0s - loss: 0.6875 - accuracy: 0.56 - ETA: 0s - loss: 0.6850 - accuracy: 0.57 - ETA: 0s - loss: 0.6819 - accuracy: 0.58 - ETA: 0s - loss: 0.6789 - accuracy: 0.59 - ETA: 0s - loss: 0.6755 - accuracy: 0.59 - ETA: 0s - loss: 0.6732 - accuracy: 0.60 - ETA: 0s - loss: 0.6704 - accuracy: 0.60 - ETA: 0s - loss: 0.6691 - accuracy: 0.60 - ETA: 0s - loss: 0.6670 - accuracy: 0.61 - ETA: 0s - loss: 0.6639 - accuracy: 0.61 - 1s 76us/step - loss: 0.6626 - accuracy: 0.6182 - val_loss: 0.6089 - val_accuracy: 0.7124\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:26 - loss: 0.6934 - accuracy: 0.43 - ETA: 2s - loss: 0.6913 - accuracy: 0.5370 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6899 - accuracy: 0.53 - ETA: 0s - loss: 0.6887 - accuracy: 0.54 - ETA: 0s - loss: 0.6855 - accuracy: 0.56 - ETA: 0s - loss: 0.6838 - accuracy: 0.56 - ETA: 0s - loss: 0.6811 - accuracy: 0.57 - ETA: 0s - loss: 0.6799 - accuracy: 0.58 - ETA: 0s - loss: 0.6776 - accuracy: 0.59 - ETA: 0s - loss: 0.6743 - accuracy: 0.59 - ETA: 0s - loss: 0.6713 - accuracy: 0.60 - ETA: 0s - loss: 0.6671 - accuracy: 0.61 - ETA: 0s - loss: 0.6653 - accuracy: 0.61 - ETA: 0s - loss: 0.6626 - accuracy: 0.61 - ETA: 0s - loss: 0.6602 - accuracy: 0.62 - 1s 77us/step - loss: 0.6584 - accuracy: 0.6256 - val_loss: 0.6068 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:23 - loss: 0.6903 - accuracy: 0.56 - ETA: 2s - loss: 0.6944 - accuracy: 0.5125 - ETA: 1s - loss: 0.6919 - accuracy: 0.54 - ETA: 1s - loss: 0.6901 - accuracy: 0.56 - ETA: 0s - loss: 0.6888 - accuracy: 0.57 - ETA: 0s - loss: 0.6866 - accuracy: 0.57 - ETA: 0s - loss: 0.6841 - accuracy: 0.59 - ETA: 0s - loss: 0.6822 - accuracy: 0.59 - ETA: 0s - loss: 0.6793 - accuracy: 0.60 - ETA: 0s - loss: 0.6754 - accuracy: 0.61 - ETA: 0s - loss: 0.6719 - accuracy: 0.62 - ETA: 0s - loss: 0.6692 - accuracy: 0.62 - ETA: 0s - loss: 0.6665 - accuracy: 0.62 - ETA: 0s - loss: 0.6637 - accuracy: 0.62 - ETA: 0s - loss: 0.6620 - accuracy: 0.63 - ETA: 0s - loss: 0.6593 - accuracy: 0.63 - ETA: 0s - loss: 0.6570 - accuracy: 0.63 - 1s 79us/step - loss: 0.6566 - accuracy: 0.6388 - val_loss: 0.6054 - val_accuracy: 0.7109\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:18 - loss: 0.6941 - accuracy: 0.56 - ETA: 2s - loss: 0.6936 - accuracy: 0.5212 - ETA: 1s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6906 - accuracy: 0.54 - ETA: 0s - loss: 0.6895 - accuracy: 0.54 - ETA: 0s - loss: 0.6880 - accuracy: 0.55 - ETA: 0s - loss: 0.6862 - accuracy: 0.56 - ETA: 0s - loss: 0.6832 - accuracy: 0.57 - ETA: 0s - loss: 0.6811 - accuracy: 0.58 - ETA: 0s - loss: 0.6779 - accuracy: 0.59 - ETA: 0s - loss: 0.6758 - accuracy: 0.60 - ETA: 0s - loss: 0.6738 - accuracy: 0.60 - ETA: 0s - loss: 0.6706 - accuracy: 0.61 - ETA: 0s - loss: 0.6681 - accuracy: 0.61 - ETA: 0s - loss: 0.6662 - accuracy: 0.61 - ETA: 0s - loss: 0.6642 - accuracy: 0.62 - ETA: 0s - loss: 0.6619 - accuracy: 0.62 - 1s 79us/step - loss: 0.6608 - accuracy: 0.6247 - val_loss: 0.6085 - val_accuracy: 0.7159\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:21 - loss: 0.6982 - accuracy: 0.25 - ETA: 2s - loss: 0.6940 - accuracy: 0.5200 - ETA: 1s - loss: 0.6921 - accuracy: 0.53 - ETA: 1s - loss: 0.6906 - accuracy: 0.54 - ETA: 0s - loss: 0.6888 - accuracy: 0.56 - ETA: 0s - loss: 0.6873 - accuracy: 0.57 - ETA: 0s - loss: 0.6852 - accuracy: 0.58 - ETA: 0s - loss: 0.6824 - accuracy: 0.59 - ETA: 0s - loss: 0.6800 - accuracy: 0.59 - ETA: 0s - loss: 0.6781 - accuracy: 0.60 - ETA: 0s - loss: 0.6765 - accuracy: 0.60 - ETA: 0s - loss: 0.6734 - accuracy: 0.60 - ETA: 0s - loss: 0.6707 - accuracy: 0.61 - ETA: 0s - loss: 0.6683 - accuracy: 0.61 - ETA: 0s - loss: 0.6660 - accuracy: 0.62 - ETA: 0s - loss: 0.6633 - accuracy: 0.62 - ETA: 0s - loss: 0.6610 - accuracy: 0.62 - 1s 79us/step - loss: 0.6605 - accuracy: 0.6292 - val_loss: 0.6087 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 1:23 - loss: 0.6952 - accuracy: 0.50 - ETA: 2s - loss: 0.6934 - accuracy: 0.5578 - ETA: 1s - loss: 0.6914 - accuracy: 0.55 - ETA: 1s - loss: 0.6916 - accuracy: 0.54 - ETA: 0s - loss: 0.6901 - accuracy: 0.55 - ETA: 0s - loss: 0.6889 - accuracy: 0.56 - ETA: 0s - loss: 0.6860 - accuracy: 0.58 - ETA: 0s - loss: 0.6841 - accuracy: 0.58 - ETA: 0s - loss: 0.6809 - accuracy: 0.59 - ETA: 0s - loss: 0.6776 - accuracy: 0.60 - ETA: 0s - loss: 0.6737 - accuracy: 0.61 - ETA: 0s - loss: 0.6702 - accuracy: 0.61 - ETA: 0s - loss: 0.6667 - accuracy: 0.62 - ETA: 0s - loss: 0.6636 - accuracy: 0.62 - ETA: 0s - loss: 0.6607 - accuracy: 0.63 - ETA: 0s - loss: 0.6578 - accuracy: 0.63 - ETA: 0s - loss: 0.6560 - accuracy: 0.63 - 1s 79us/step - loss: 0.6554 - accuracy: 0.6381 - val_loss: 0.6106 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 25us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:10 - loss: 0.6934 - accuracy: 0.68 - ETA: 1s - loss: 0.6936 - accuracy: 0.5212 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - 1s 72us/step - loss: 0.6933 - accuracy: 0.5141 - val_loss: 0.6922 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 31us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:11 - loss: 0.6921 - accuracy: 0.75 - ETA: 1s - loss: 0.6937 - accuracy: 0.5046 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - 1s 75us/step - loss: 0.6934 - accuracy: 0.5134 - val_loss: 0.6923 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:08 - loss: 0.6926 - accuracy: 0.62 - ETA: 1s - loss: 0.6939 - accuracy: 0.5197 - ETA: 1s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.53 - ETA: 0s - loss: 0.6935 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - 1s 72us/step - loss: 0.6936 - accuracy: 0.5137 - val_loss: 0.6930 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:10 - loss: 0.6925 - accuracy: 0.50 - ETA: 1s - loss: 0.6937 - accuracy: 0.4670 - ETA: 1s - loss: 0.6936 - accuracy: 0.48 - ETA: 0s - loss: 0.6936 - accuracy: 0.48 - ETA: 0s - loss: 0.6936 - accuracy: 0.49 - ETA: 0s - loss: 0.6935 - accuracy: 0.49 - ETA: 0s - loss: 0.6936 - accuracy: 0.49 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - 1s 72us/step - loss: 0.6931 - accuracy: 0.5124 - val_loss: 0.6923 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:11 - loss: 0.6931 - accuracy: 0.50 - ETA: 1s - loss: 0.6941 - accuracy: 0.4777 - ETA: 1s - loss: 0.6939 - accuracy: 0.49 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - 1s 71us/step - loss: 0.6934 - accuracy: 0.5123 - val_loss: 0.6926 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:09 - loss: 0.6917 - accuracy: 0.62 - ETA: 1s - loss: 0.6935 - accuracy: 0.5156 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - 1s 72us/step - loss: 0.6933 - accuracy: 0.5136 - val_loss: 0.6926 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:10 - loss: 0.6988 - accuracy: 0.31 - ETA: 1s - loss: 0.6946 - accuracy: 0.4850 - ETA: 1s - loss: 0.6944 - accuracy: 0.48 - ETA: 0s - loss: 0.6944 - accuracy: 0.49 - ETA: 0s - loss: 0.6941 - accuracy: 0.49 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - 1s 74us/step - loss: 0.6933 - accuracy: 0.5094 - val_loss: 0.6911 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:11 - loss: 0.6940 - accuracy: 0.50 - ETA: 1s - loss: 0.6940 - accuracy: 0.5068 - ETA: 1s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.52 - ETA: 0s - loss: 0.6939 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.53 - ETA: 0s - loss: 0.6936 - accuracy: 0.53 - ETA: 0s - loss: 0.6934 - accuracy: 0.53 - ETA: 0s - loss: 0.6932 - accuracy: 0.53 - ETA: 0s - loss: 0.6930 - accuracy: 0.53 - ETA: 0s - loss: 0.6930 - accuracy: 0.53 - ETA: 0s - loss: 0.6929 - accuracy: 0.53 - ETA: 0s - loss: 0.6928 - accuracy: 0.53 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - 1s 76us/step - loss: 0.6927 - accuracy: 0.5287 - val_loss: 0.6902 - val_accuracy: 0.5270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:10 - loss: 0.6942 - accuracy: 0.75 - ETA: 1s - loss: 0.6941 - accuracy: 0.4964 - ETA: 1s - loss: 0.6941 - accuracy: 0.49 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - 1s 74us/step - loss: 0.6936 - accuracy: 0.5133 - val_loss: 0.6922 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:09 - loss: 0.6947 - accuracy: 0.43 - ETA: 1s - loss: 0.6940 - accuracy: 0.5301 - ETA: 1s - loss: 0.6941 - accuracy: 0.52 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.52 - ETA: 0s - loss: 0.6936 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6936 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - 1s 70us/step - loss: 0.6939 - accuracy: 0.5153 - val_loss: 0.6932 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:10 - loss: 0.7020 - accuracy: 0.31 - ETA: 1s - loss: 0.6952 - accuracy: 0.5045 - ETA: 1s - loss: 0.6947 - accuracy: 0.50 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - 1s 70us/step - loss: 0.6932 - accuracy: 0.5144 - val_loss: 0.6906 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:10 - loss: 0.6969 - accuracy: 0.43 - ETA: 1s - loss: 0.6945 - accuracy: 0.5068 - ETA: 1s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - 1s 70us/step - loss: 0.6938 - accuracy: 0.5175 - val_loss: 0.6930 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:11 - loss: 0.6907 - accuracy: 0.62 - ETA: 1s - loss: 0.6941 - accuracy: 0.5156 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - 1s 71us/step - loss: 0.6937 - accuracy: 0.5126 - val_loss: 0.6912 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:10 - loss: 0.6941 - accuracy: 0.56 - ETA: 1s - loss: 0.6951 - accuracy: 0.4766 - ETA: 1s - loss: 0.6948 - accuracy: 0.49 - ETA: 0s - loss: 0.6948 - accuracy: 0.50 - ETA: 0s - loss: 0.6947 - accuracy: 0.50 - ETA: 0s - loss: 0.6947 - accuracy: 0.50 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.52 - 1s 71us/step - loss: 0.6936 - accuracy: 0.5235 - val_loss: 0.6912 - val_accuracy: 0.5277\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:10 - loss: 0.6934 - accuracy: 0.56 - ETA: 1s - loss: 0.6954 - accuracy: 0.4920 - ETA: 1s - loss: 0.6952 - accuracy: 0.50 - ETA: 0s - loss: 0.6950 - accuracy: 0.50 - ETA: 0s - loss: 0.6948 - accuracy: 0.51 - ETA: 0s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - 1s 70us/step - loss: 0.6940 - accuracy: 0.5168 - val_loss: 0.6919 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:10 - loss: 0.6988 - accuracy: 0.25 - ETA: 1s - loss: 0.6947 - accuracy: 0.5125 - ETA: 1s - loss: 0.6945 - accuracy: 0.52 - ETA: 0s - loss: 0.6946 - accuracy: 0.52 - ETA: 0s - loss: 0.6945 - accuracy: 0.52 - ETA: 0s - loss: 0.6944 - accuracy: 0.52 - ETA: 0s - loss: 0.6942 - accuracy: 0.52 - ETA: 0s - loss: 0.6943 - accuracy: 0.52 - ETA: 0s - loss: 0.6941 - accuracy: 0.52 - ETA: 0s - loss: 0.6940 - accuracy: 0.52 - ETA: 0s - loss: 0.6939 - accuracy: 0.52 - ETA: 0s - loss: 0.6939 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6939 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - 1s 70us/step - loss: 0.6939 - accuracy: 0.5215 - val_loss: 0.6928 - val_accuracy: 0.5305\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 25us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:10 - loss: 0.6938 - accuracy: 0.56 - ETA: 1s - loss: 0.6944 - accuracy: 0.5000 - ETA: 1s - loss: 0.6944 - accuracy: 0.50 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.52 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.52 - ETA: 0s - loss: 0.6940 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.52 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - 1s 70us/step - loss: 0.6934 - accuracy: 0.5284 - val_loss: 0.6908 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:10 - loss: 0.6986 - accuracy: 0.37 - ETA: 1s - loss: 0.6947 - accuracy: 0.4911 - ETA: 1s - loss: 0.6950 - accuracy: 0.49 - ETA: 0s - loss: 0.6944 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - 1s 70us/step - loss: 0.6930 - accuracy: 0.5159 - val_loss: 0.6912 - val_accuracy: 0.5284\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:32 - loss: 0.6938 - accuracy: 0.56 - ETA: 2s - loss: 0.6934 - accuracy: 0.5189 - ETA: 1s - loss: 0.6928 - accuracy: 0.53 - ETA: 1s - loss: 0.6919 - accuracy: 0.54 - ETA: 0s - loss: 0.6915 - accuracy: 0.55 - ETA: 0s - loss: 0.6908 - accuracy: 0.55 - ETA: 0s - loss: 0.6897 - accuracy: 0.55 - ETA: 0s - loss: 0.6884 - accuracy: 0.55 - ETA: 0s - loss: 0.6872 - accuracy: 0.55 - ETA: 0s - loss: 0.6857 - accuracy: 0.56 - ETA: 0s - loss: 0.6843 - accuracy: 0.56 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6820 - accuracy: 0.56 - ETA: 0s - loss: 0.6813 - accuracy: 0.56 - ETA: 0s - loss: 0.6807 - accuracy: 0.56 - ETA: 0s - loss: 0.6798 - accuracy: 0.56 - 1s 77us/step - loss: 0.6788 - accuracy: 0.5684 - val_loss: 0.6483 - val_accuracy: 0.7045\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:32 - loss: 0.6979 - accuracy: 0.43 - ETA: 2s - loss: 0.6931 - accuracy: 0.5233 - ETA: 1s - loss: 0.6925 - accuracy: 0.53 - ETA: 1s - loss: 0.6916 - accuracy: 0.54 - ETA: 0s - loss: 0.6899 - accuracy: 0.55 - ETA: 0s - loss: 0.6882 - accuracy: 0.55 - ETA: 0s - loss: 0.6867 - accuracy: 0.56 - ETA: 0s - loss: 0.6851 - accuracy: 0.56 - ETA: 0s - loss: 0.6841 - accuracy: 0.57 - ETA: 0s - loss: 0.6827 - accuracy: 0.57 - ETA: 0s - loss: 0.6805 - accuracy: 0.57 - ETA: 0s - loss: 0.6789 - accuracy: 0.58 - ETA: 0s - loss: 0.6769 - accuracy: 0.58 - ETA: 0s - loss: 0.6751 - accuracy: 0.58 - ETA: 0s - loss: 0.6727 - accuracy: 0.59 - ETA: 0s - loss: 0.6711 - accuracy: 0.59 - 1s 77us/step - loss: 0.6710 - accuracy: 0.5962 - val_loss: 0.6371 - val_accuracy: 0.7010\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:30 - loss: 0.6942 - accuracy: 0.43 - ETA: 2s - loss: 0.6937 - accuracy: 0.5123 - ETA: 1s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6915 - accuracy: 0.53 - ETA: 0s - loss: 0.6915 - accuracy: 0.53 - ETA: 0s - loss: 0.6910 - accuracy: 0.53 - ETA: 0s - loss: 0.6900 - accuracy: 0.54 - ETA: 0s - loss: 0.6880 - accuracy: 0.55 - ETA: 0s - loss: 0.6856 - accuracy: 0.55 - ETA: 0s - loss: 0.6842 - accuracy: 0.56 - ETA: 0s - loss: 0.6833 - accuracy: 0.56 - ETA: 0s - loss: 0.6818 - accuracy: 0.56 - ETA: 0s - loss: 0.6802 - accuracy: 0.56 - ETA: 0s - loss: 0.6787 - accuracy: 0.56 - ETA: 0s - loss: 0.6772 - accuracy: 0.56 - ETA: 0s - loss: 0.6756 - accuracy: 0.57 - 1s 78us/step - loss: 0.6741 - accuracy: 0.5736 - val_loss: 0.6333 - val_accuracy: 0.7088\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:31 - loss: 0.6922 - accuracy: 0.81 - ETA: 2s - loss: 0.6930 - accuracy: 0.5221 - ETA: 1s - loss: 0.6926 - accuracy: 0.52 - ETA: 1s - loss: 0.6920 - accuracy: 0.52 - ETA: 0s - loss: 0.6914 - accuracy: 0.52 - ETA: 0s - loss: 0.6900 - accuracy: 0.52 - ETA: 0s - loss: 0.6886 - accuracy: 0.52 - ETA: 0s - loss: 0.6872 - accuracy: 0.52 - ETA: 0s - loss: 0.6860 - accuracy: 0.52 - ETA: 0s - loss: 0.6848 - accuracy: 0.52 - ETA: 0s - loss: 0.6835 - accuracy: 0.52 - ETA: 0s - loss: 0.6826 - accuracy: 0.53 - ETA: 0s - loss: 0.6821 - accuracy: 0.53 - ETA: 0s - loss: 0.6805 - accuracy: 0.54 - ETA: 0s - loss: 0.6801 - accuracy: 0.55 - ETA: 0s - loss: 0.6788 - accuracy: 0.55 - ETA: 0s - loss: 0.6776 - accuracy: 0.56 - 1s 80us/step - loss: 0.6774 - accuracy: 0.5654 - val_loss: 0.6479 - val_accuracy: 0.6882\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:31 - loss: 0.6934 - accuracy: 0.50 - ETA: 2s - loss: 0.6928 - accuracy: 0.5275 - ETA: 1s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6926 - accuracy: 0.50 - ETA: 0s - loss: 0.6918 - accuracy: 0.52 - ETA: 0s - loss: 0.6910 - accuracy: 0.52 - ETA: 0s - loss: 0.6897 - accuracy: 0.53 - ETA: 0s - loss: 0.6881 - accuracy: 0.54 - ETA: 0s - loss: 0.6863 - accuracy: 0.54 - ETA: 0s - loss: 0.6845 - accuracy: 0.55 - ETA: 0s - loss: 0.6828 - accuracy: 0.55 - ETA: 0s - loss: 0.6807 - accuracy: 0.56 - ETA: 0s - loss: 0.6788 - accuracy: 0.56 - ETA: 0s - loss: 0.6771 - accuracy: 0.56 - ETA: 0s - loss: 0.6755 - accuracy: 0.56 - ETA: 0s - loss: 0.6740 - accuracy: 0.57 - 1s 79us/step - loss: 0.6727 - accuracy: 0.5738 - val_loss: 0.6323 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:31 - loss: 0.6945 - accuracy: 0.50 - ETA: 2s - loss: 0.6936 - accuracy: 0.4856 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 1s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6912 - accuracy: 0.53 - ETA: 0s - loss: 0.6898 - accuracy: 0.54 - ETA: 0s - loss: 0.6892 - accuracy: 0.54 - ETA: 0s - loss: 0.6881 - accuracy: 0.55 - ETA: 0s - loss: 0.6864 - accuracy: 0.55 - ETA: 0s - loss: 0.6849 - accuracy: 0.56 - ETA: 0s - loss: 0.6836 - accuracy: 0.56 - ETA: 0s - loss: 0.6824 - accuracy: 0.57 - ETA: 0s - loss: 0.6800 - accuracy: 0.57 - ETA: 0s - loss: 0.6785 - accuracy: 0.58 - ETA: 0s - loss: 0.6770 - accuracy: 0.58 - ETA: 0s - loss: 0.6759 - accuracy: 0.58 - ETA: 0s - loss: 0.6747 - accuracy: 0.58 - 1s 80us/step - loss: 0.6744 - accuracy: 0.5901 - val_loss: 0.6404 - val_accuracy: 0.7102\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:31 - loss: 0.6945 - accuracy: 0.43 - ETA: 2s - loss: 0.6910 - accuracy: 0.5377 - ETA: 1s - loss: 0.6903 - accuracy: 0.53 - ETA: 1s - loss: 0.6887 - accuracy: 0.53 - ETA: 0s - loss: 0.6882 - accuracy: 0.53 - ETA: 0s - loss: 0.6875 - accuracy: 0.54 - ETA: 0s - loss: 0.6860 - accuracy: 0.55 - ETA: 0s - loss: 0.6841 - accuracy: 0.55 - ETA: 0s - loss: 0.6817 - accuracy: 0.56 - ETA: 0s - loss: 0.6799 - accuracy: 0.56 - ETA: 0s - loss: 0.6789 - accuracy: 0.57 - ETA: 0s - loss: 0.6767 - accuracy: 0.57 - ETA: 0s - loss: 0.6747 - accuracy: 0.58 - ETA: 0s - loss: 0.6717 - accuracy: 0.59 - ETA: 0s - loss: 0.6700 - accuracy: 0.59 - ETA: 0s - loss: 0.6700 - accuracy: 0.59 - 1s 78us/step - loss: 0.6679 - accuracy: 0.5953 - val_loss: 0.6235 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 1:30 - loss: 0.6959 - accuracy: 0.50 - ETA: 2s - loss: 0.6916 - accuracy: 0.5370 - ETA: 1s - loss: 0.6909 - accuracy: 0.53 - ETA: 1s - loss: 0.6900 - accuracy: 0.52 - ETA: 0s - loss: 0.6888 - accuracy: 0.52 - ETA: 0s - loss: 0.6868 - accuracy: 0.53 - ETA: 0s - loss: 0.6846 - accuracy: 0.54 - ETA: 0s - loss: 0.6822 - accuracy: 0.56 - ETA: 0s - loss: 0.6796 - accuracy: 0.57 - ETA: 0s - loss: 0.6776 - accuracy: 0.57 - ETA: 0s - loss: 0.6760 - accuracy: 0.58 - ETA: 0s - loss: 0.6727 - accuracy: 0.58 - ETA: 0s - loss: 0.6687 - accuracy: 0.59 - ETA: 0s - loss: 0.6659 - accuracy: 0.60 - ETA: 0s - loss: 0.6628 - accuracy: 0.60 - ETA: 0s - loss: 0.6613 - accuracy: 0.61 - 1s 78us/step - loss: 0.6596 - accuracy: 0.6154 - val_loss: 0.6160 - val_accuracy: 0.7223\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:33 - loss: 0.6930 - accuracy: 0.56 - ETA: 2s - loss: 0.6933 - accuracy: 0.5252 - ETA: 1s - loss: 0.6924 - accuracy: 0.54 - ETA: 1s - loss: 0.6912 - accuracy: 0.55 - ETA: 0s - loss: 0.6902 - accuracy: 0.55 - ETA: 0s - loss: 0.6893 - accuracy: 0.55 - ETA: 0s - loss: 0.6884 - accuracy: 0.55 - ETA: 0s - loss: 0.6867 - accuracy: 0.56 - ETA: 0s - loss: 0.6849 - accuracy: 0.56 - ETA: 0s - loss: 0.6824 - accuracy: 0.57 - ETA: 0s - loss: 0.6802 - accuracy: 0.57 - ETA: 0s - loss: 0.6785 - accuracy: 0.58 - ETA: 0s - loss: 0.6766 - accuracy: 0.58 - ETA: 0s - loss: 0.6741 - accuracy: 0.59 - ETA: 0s - loss: 0.6718 - accuracy: 0.59 - ETA: 0s - loss: 0.6694 - accuracy: 0.59 - 1s 78us/step - loss: 0.6666 - accuracy: 0.6002 - val_loss: 0.6185 - val_accuracy: 0.7131\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:30 - loss: 0.6955 - accuracy: 0.31 - ETA: 2s - loss: 0.6939 - accuracy: 0.5053 - ETA: 1s - loss: 0.6926 - accuracy: 0.54 - ETA: 1s - loss: 0.6912 - accuracy: 0.55 - ETA: 0s - loss: 0.6907 - accuracy: 0.54 - ETA: 0s - loss: 0.6892 - accuracy: 0.55 - ETA: 0s - loss: 0.6875 - accuracy: 0.56 - ETA: 0s - loss: 0.6848 - accuracy: 0.57 - ETA: 0s - loss: 0.6811 - accuracy: 0.58 - ETA: 0s - loss: 0.6787 - accuracy: 0.58 - ETA: 0s - loss: 0.6754 - accuracy: 0.59 - ETA: 0s - loss: 0.6723 - accuracy: 0.60 - ETA: 0s - loss: 0.6690 - accuracy: 0.60 - ETA: 0s - loss: 0.6662 - accuracy: 0.60 - ETA: 0s - loss: 0.6640 - accuracy: 0.61 - ETA: 0s - loss: 0.6617 - accuracy: 0.61 - ETA: 0s - loss: 0.6604 - accuracy: 0.61 - 1s 79us/step - loss: 0.6601 - accuracy: 0.6197 - val_loss: 0.6102 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:30 - loss: 0.6949 - accuracy: 0.50 - ETA: 2s - loss: 0.6938 - accuracy: 0.5135 - ETA: 1s - loss: 0.6932 - accuracy: 0.54 - ETA: 1s - loss: 0.6928 - accuracy: 0.54 - ETA: 0s - loss: 0.6921 - accuracy: 0.54 - ETA: 0s - loss: 0.6907 - accuracy: 0.56 - ETA: 0s - loss: 0.6896 - accuracy: 0.56 - ETA: 0s - loss: 0.6883 - accuracy: 0.56 - ETA: 0s - loss: 0.6860 - accuracy: 0.57 - ETA: 0s - loss: 0.6839 - accuracy: 0.58 - ETA: 0s - loss: 0.6822 - accuracy: 0.58 - ETA: 0s - loss: 0.6794 - accuracy: 0.59 - ETA: 0s - loss: 0.6779 - accuracy: 0.59 - ETA: 0s - loss: 0.6754 - accuracy: 0.60 - ETA: 0s - loss: 0.6733 - accuracy: 0.60 - ETA: 0s - loss: 0.6712 - accuracy: 0.61 - 1s 78us/step - loss: 0.6699 - accuracy: 0.6128 - val_loss: 0.6219 - val_accuracy: 0.7102\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 25us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:31 - loss: 0.6986 - accuracy: 0.25 - ETA: 2s - loss: 0.6934 - accuracy: 0.5127 - ETA: 1s - loss: 0.6923 - accuracy: 0.51 - ETA: 1s - loss: 0.6896 - accuracy: 0.55 - ETA: 0s - loss: 0.6891 - accuracy: 0.54 - ETA: 0s - loss: 0.6871 - accuracy: 0.55 - ETA: 0s - loss: 0.6845 - accuracy: 0.57 - ETA: 0s - loss: 0.6812 - accuracy: 0.58 - ETA: 0s - loss: 0.6791 - accuracy: 0.58 - ETA: 0s - loss: 0.6767 - accuracy: 0.59 - ETA: 0s - loss: 0.6742 - accuracy: 0.59 - ETA: 0s - loss: 0.6730 - accuracy: 0.59 - ETA: 0s - loss: 0.6706 - accuracy: 0.60 - ETA: 0s - loss: 0.6681 - accuracy: 0.60 - ETA: 0s - loss: 0.6645 - accuracy: 0.61 - ETA: 0s - loss: 0.6613 - accuracy: 0.61 - ETA: 0s - loss: 0.6590 - accuracy: 0.62 - 1s 79us/step - loss: 0.6587 - accuracy: 0.6213 - val_loss: 0.6143 - val_accuracy: 0.7188\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:30 - loss: 0.6964 - accuracy: 0.43 - ETA: 2s - loss: 0.6932 - accuracy: 0.5368 - ETA: 1s - loss: 0.6915 - accuracy: 0.54 - ETA: 1s - loss: 0.6897 - accuracy: 0.55 - ETA: 0s - loss: 0.6879 - accuracy: 0.56 - ETA: 0s - loss: 0.6859 - accuracy: 0.56 - ETA: 0s - loss: 0.6830 - accuracy: 0.57 - ETA: 0s - loss: 0.6820 - accuracy: 0.57 - ETA: 0s - loss: 0.6789 - accuracy: 0.58 - ETA: 0s - loss: 0.6770 - accuracy: 0.59 - ETA: 0s - loss: 0.6748 - accuracy: 0.59 - ETA: 0s - loss: 0.6716 - accuracy: 0.60 - ETA: 0s - loss: 0.6674 - accuracy: 0.61 - ETA: 0s - loss: 0.6653 - accuracy: 0.61 - ETA: 0s - loss: 0.6632 - accuracy: 0.62 - ETA: 0s - loss: 0.6606 - accuracy: 0.62 - 1s 78us/step - loss: 0.6588 - accuracy: 0.6267 - val_loss: 0.6033 - val_accuracy: 0.7145\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:30 - loss: 0.6898 - accuracy: 0.68 - ETA: 2s - loss: 0.6933 - accuracy: 0.5204 - ETA: 1s - loss: 0.6923 - accuracy: 0.53 - ETA: 1s - loss: 0.6902 - accuracy: 0.55 - ETA: 0s - loss: 0.6889 - accuracy: 0.55 - ETA: 0s - loss: 0.6866 - accuracy: 0.56 - ETA: 0s - loss: 0.6820 - accuracy: 0.58 - ETA: 0s - loss: 0.6779 - accuracy: 0.59 - ETA: 0s - loss: 0.6737 - accuracy: 0.59 - ETA: 0s - loss: 0.6697 - accuracy: 0.60 - ETA: 0s - loss: 0.6662 - accuracy: 0.61 - ETA: 0s - loss: 0.6624 - accuracy: 0.61 - ETA: 0s - loss: 0.6597 - accuracy: 0.62 - ETA: 0s - loss: 0.6566 - accuracy: 0.62 - ETA: 0s - loss: 0.6532 - accuracy: 0.63 - ETA: 0s - loss: 0.6508 - accuracy: 0.63 - ETA: 0s - loss: 0.6479 - accuracy: 0.64 - 1s 79us/step - loss: 0.6478 - accuracy: 0.6399 - val_loss: 0.5919 - val_accuracy: 0.7209\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:30 - loss: 0.6925 - accuracy: 0.62 - ETA: 2s - loss: 0.6941 - accuracy: 0.5325 - ETA: 1s - loss: 0.6926 - accuracy: 0.53 - ETA: 1s - loss: 0.6916 - accuracy: 0.53 - ETA: 0s - loss: 0.6897 - accuracy: 0.55 - ETA: 0s - loss: 0.6885 - accuracy: 0.56 - ETA: 0s - loss: 0.6851 - accuracy: 0.57 - ETA: 0s - loss: 0.6826 - accuracy: 0.58 - ETA: 0s - loss: 0.6789 - accuracy: 0.59 - ETA: 0s - loss: 0.6745 - accuracy: 0.60 - ETA: 0s - loss: 0.6709 - accuracy: 0.60 - ETA: 0s - loss: 0.6684 - accuracy: 0.61 - ETA: 0s - loss: 0.6659 - accuracy: 0.61 - ETA: 0s - loss: 0.6616 - accuracy: 0.62 - ETA: 0s - loss: 0.6595 - accuracy: 0.62 - ETA: 0s - loss: 0.6566 - accuracy: 0.63 - ETA: 0s - loss: 0.6540 - accuracy: 0.63 - 1s 81us/step - loss: 0.6526 - accuracy: 0.6389 - val_loss: 0.5950 - val_accuracy: 0.7102\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:30 - loss: 0.6941 - accuracy: 0.68 - ETA: 2s - loss: 0.6935 - accuracy: 0.5307 - ETA: 1s - loss: 0.6927 - accuracy: 0.53 - ETA: 1s - loss: 0.6909 - accuracy: 0.54 - ETA: 0s - loss: 0.6892 - accuracy: 0.54 - ETA: 0s - loss: 0.6856 - accuracy: 0.56 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6800 - accuracy: 0.58 - ETA: 0s - loss: 0.6780 - accuracy: 0.58 - ETA: 0s - loss: 0.6751 - accuracy: 0.59 - ETA: 0s - loss: 0.6725 - accuracy: 0.59 - ETA: 0s - loss: 0.6689 - accuracy: 0.60 - ETA: 0s - loss: 0.6656 - accuracy: 0.60 - ETA: 0s - loss: 0.6624 - accuracy: 0.61 - ETA: 0s - loss: 0.6593 - accuracy: 0.61 - ETA: 0s - loss: 0.6575 - accuracy: 0.62 - 1s 77us/step - loss: 0.6557 - accuracy: 0.6235 - val_loss: 0.6015 - val_accuracy: 0.7102\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:31 - loss: 0.6971 - accuracy: 0.50 - ETA: 2s - loss: 0.6947 - accuracy: 0.4904 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6908 - accuracy: 0.53 - ETA: 0s - loss: 0.6886 - accuracy: 0.54 - ETA: 0s - loss: 0.6854 - accuracy: 0.55 - ETA: 0s - loss: 0.6817 - accuracy: 0.56 - ETA: 0s - loss: 0.6786 - accuracy: 0.57 - ETA: 0s - loss: 0.6740 - accuracy: 0.58 - ETA: 0s - loss: 0.6707 - accuracy: 0.59 - ETA: 0s - loss: 0.6675 - accuracy: 0.60 - ETA: 0s - loss: 0.6647 - accuracy: 0.60 - ETA: 0s - loss: 0.6627 - accuracy: 0.61 - ETA: 0s - loss: 0.6618 - accuracy: 0.61 - ETA: 0s - loss: 0.6592 - accuracy: 0.61 - ETA: 0s - loss: 0.6563 - accuracy: 0.62 - 1s 80us/step - loss: 0.6548 - accuracy: 0.6248 - val_loss: 0.5966 - val_accuracy: 0.7209\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:30 - loss: 0.7012 - accuracy: 0.25 - ETA: 2s - loss: 0.6942 - accuracy: 0.5140 - ETA: 1s - loss: 0.6921 - accuracy: 0.55 - ETA: 1s - loss: 0.6892 - accuracy: 0.58 - ETA: 0s - loss: 0.6866 - accuracy: 0.58 - ETA: 0s - loss: 0.6840 - accuracy: 0.59 - ETA: 0s - loss: 0.6807 - accuracy: 0.60 - ETA: 0s - loss: 0.6767 - accuracy: 0.61 - ETA: 0s - loss: 0.6721 - accuracy: 0.61 - ETA: 0s - loss: 0.6682 - accuracy: 0.62 - ETA: 0s - loss: 0.6644 - accuracy: 0.62 - ETA: 0s - loss: 0.6617 - accuracy: 0.63 - ETA: 0s - loss: 0.6590 - accuracy: 0.63 - ETA: 0s - loss: 0.6564 - accuracy: 0.63 - ETA: 0s - loss: 0.6536 - accuracy: 0.64 - ETA: 0s - loss: 0.6512 - accuracy: 0.64 - ETA: 0s - loss: 0.6482 - accuracy: 0.64 - 1s 79us/step - loss: 0.6483 - accuracy: 0.6456 - val_loss: 0.5987 - val_accuracy: 0.7145\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:14 - loss: 0.6934 - accuracy: 0.50 - ETA: 1s - loss: 0.6920 - accuracy: 0.5068 - ETA: 1s - loss: 0.6909 - accuracy: 0.52 - ETA: 0s - loss: 0.6900 - accuracy: 0.53 - ETA: 0s - loss: 0.6884 - accuracy: 0.53 - ETA: 0s - loss: 0.6875 - accuracy: 0.55 - ETA: 0s - loss: 0.6871 - accuracy: 0.55 - ETA: 0s - loss: 0.6861 - accuracy: 0.55 - ETA: 0s - loss: 0.6851 - accuracy: 0.56 - ETA: 0s - loss: 0.6844 - accuracy: 0.56 - ETA: 0s - loss: 0.6838 - accuracy: 0.56 - ETA: 0s - loss: 0.6828 - accuracy: 0.56 - ETA: 0s - loss: 0.6815 - accuracy: 0.56 - ETA: 0s - loss: 0.6810 - accuracy: 0.57 - ETA: 0s - loss: 0.6804 - accuracy: 0.57 - 1s 72us/step - loss: 0.6798 - accuracy: 0.5745 - val_loss: 0.6615 - val_accuracy: 0.7074\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:13 - loss: 0.6934 - accuracy: 0.62 - ETA: 1s - loss: 0.6909 - accuracy: 0.5428 - ETA: 1s - loss: 0.6889 - accuracy: 0.55 - ETA: 0s - loss: 0.6882 - accuracy: 0.55 - ETA: 0s - loss: 0.6886 - accuracy: 0.55 - ETA: 0s - loss: 0.6885 - accuracy: 0.55 - ETA: 0s - loss: 0.6879 - accuracy: 0.55 - ETA: 0s - loss: 0.6870 - accuracy: 0.55 - ETA: 0s - loss: 0.6864 - accuracy: 0.55 - ETA: 0s - loss: 0.6851 - accuracy: 0.55 - ETA: 0s - loss: 0.6845 - accuracy: 0.56 - ETA: 0s - loss: 0.6836 - accuracy: 0.56 - ETA: 0s - loss: 0.6829 - accuracy: 0.56 - ETA: 0s - loss: 0.6820 - accuracy: 0.56 - ETA: 0s - loss: 0.6815 - accuracy: 0.56 - 1s 72us/step - loss: 0.6809 - accuracy: 0.5654 - val_loss: 0.6639 - val_accuracy: 0.7031\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:15 - loss: 0.6968 - accuracy: 0.43 - ETA: 1s - loss: 0.6931 - accuracy: 0.5045 - ETA: 1s - loss: 0.6917 - accuracy: 0.53 - ETA: 0s - loss: 0.6901 - accuracy: 0.55 - ETA: 0s - loss: 0.6886 - accuracy: 0.55 - ETA: 0s - loss: 0.6885 - accuracy: 0.55 - ETA: 0s - loss: 0.6874 - accuracy: 0.55 - ETA: 0s - loss: 0.6863 - accuracy: 0.56 - ETA: 0s - loss: 0.6849 - accuracy: 0.57 - ETA: 0s - loss: 0.6838 - accuracy: 0.57 - ETA: 0s - loss: 0.6825 - accuracy: 0.58 - ETA: 0s - loss: 0.6814 - accuracy: 0.58 - ETA: 0s - loss: 0.6800 - accuracy: 0.58 - ETA: 0s - loss: 0.6789 - accuracy: 0.58 - ETA: 0s - loss: 0.6779 - accuracy: 0.58 - 1s 72us/step - loss: 0.6772 - accuracy: 0.5907 - val_loss: 0.6572 - val_accuracy: 0.7067\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:14 - loss: 0.6937 - accuracy: 0.56 - ETA: 1s - loss: 0.6917 - accuracy: 0.5159 - ETA: 1s - loss: 0.6914 - accuracy: 0.54 - ETA: 0s - loss: 0.6898 - accuracy: 0.57 - ETA: 0s - loss: 0.6889 - accuracy: 0.59 - ETA: 0s - loss: 0.6877 - accuracy: 0.59 - ETA: 0s - loss: 0.6871 - accuracy: 0.59 - ETA: 0s - loss: 0.6862 - accuracy: 0.60 - ETA: 0s - loss: 0.6848 - accuracy: 0.60 - ETA: 0s - loss: 0.6835 - accuracy: 0.61 - ETA: 0s - loss: 0.6827 - accuracy: 0.61 - ETA: 0s - loss: 0.6818 - accuracy: 0.62 - ETA: 0s - loss: 0.6808 - accuracy: 0.62 - ETA: 0s - loss: 0.6800 - accuracy: 0.62 - ETA: 0s - loss: 0.6795 - accuracy: 0.62 - 1s 72us/step - loss: 0.6790 - accuracy: 0.6283 - val_loss: 0.6648 - val_accuracy: 0.7131\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:16 - loss: 0.6937 - accuracy: 0.43 - ETA: 1s - loss: 0.6929 - accuracy: 0.5080 - ETA: 1s - loss: 0.6907 - accuracy: 0.53 - ETA: 0s - loss: 0.6898 - accuracy: 0.52 - ETA: 0s - loss: 0.6887 - accuracy: 0.52 - ETA: 0s - loss: 0.6875 - accuracy: 0.53 - ETA: 0s - loss: 0.6863 - accuracy: 0.53 - ETA: 0s - loss: 0.6844 - accuracy: 0.53 - ETA: 0s - loss: 0.6827 - accuracy: 0.54 - ETA: 0s - loss: 0.6823 - accuracy: 0.54 - ETA: 0s - loss: 0.6814 - accuracy: 0.54 - ETA: 0s - loss: 0.6808 - accuracy: 0.54 - ETA: 0s - loss: 0.6805 - accuracy: 0.55 - ETA: 0s - loss: 0.6798 - accuracy: 0.55 - ETA: 0s - loss: 0.6787 - accuracy: 0.55 - ETA: 0s - loss: 0.6777 - accuracy: 0.56 - 1s 74us/step - loss: 0.6774 - accuracy: 0.5640 - val_loss: 0.6572 - val_accuracy: 0.6882\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:15 - loss: 0.6967 - accuracy: 0.25 - ETA: 1s - loss: 0.6929 - accuracy: 0.5231 - ETA: 1s - loss: 0.6916 - accuracy: 0.53 - ETA: 0s - loss: 0.6902 - accuracy: 0.54 - ETA: 0s - loss: 0.6894 - accuracy: 0.54 - ETA: 0s - loss: 0.6884 - accuracy: 0.55 - ETA: 0s - loss: 0.6869 - accuracy: 0.55 - ETA: 0s - loss: 0.6861 - accuracy: 0.56 - ETA: 0s - loss: 0.6847 - accuracy: 0.56 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6825 - accuracy: 0.56 - ETA: 0s - loss: 0.6810 - accuracy: 0.57 - ETA: 0s - loss: 0.6805 - accuracy: 0.57 - ETA: 0s - loss: 0.6797 - accuracy: 0.57 - ETA: 0s - loss: 0.6788 - accuracy: 0.57 - 1s 73us/step - loss: 0.6782 - accuracy: 0.5776 - val_loss: 0.6613 - val_accuracy: 0.6946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:16 - loss: 0.6923 - accuracy: 0.43 - ETA: 2s - loss: 0.6919 - accuracy: 0.5278 - ETA: 1s - loss: 0.6905 - accuracy: 0.54 - ETA: 0s - loss: 0.6876 - accuracy: 0.57 - ETA: 0s - loss: 0.6853 - accuracy: 0.58 - ETA: 0s - loss: 0.6833 - accuracy: 0.58 - ETA: 0s - loss: 0.6814 - accuracy: 0.58 - ETA: 0s - loss: 0.6781 - accuracy: 0.59 - ETA: 0s - loss: 0.6761 - accuracy: 0.60 - ETA: 0s - loss: 0.6739 - accuracy: 0.60 - ETA: 0s - loss: 0.6723 - accuracy: 0.61 - ETA: 0s - loss: 0.6699 - accuracy: 0.61 - ETA: 0s - loss: 0.6679 - accuracy: 0.62 - ETA: 0s - loss: 0.6659 - accuracy: 0.62 - ETA: 0s - loss: 0.6644 - accuracy: 0.62 - 1s 72us/step - loss: 0.6634 - accuracy: 0.6273 - val_loss: 0.6297 - val_accuracy: 0.7202\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:16 - loss: 0.6938 - accuracy: 0.56 - ETA: 1s - loss: 0.6930 - accuracy: 0.5148 - ETA: 1s - loss: 0.6911 - accuracy: 0.52 - ETA: 0s - loss: 0.6891 - accuracy: 0.54 - ETA: 0s - loss: 0.6883 - accuracy: 0.55 - ETA: 0s - loss: 0.6862 - accuracy: 0.57 - ETA: 0s - loss: 0.6844 - accuracy: 0.57 - ETA: 0s - loss: 0.6816 - accuracy: 0.59 - ETA: 0s - loss: 0.6790 - accuracy: 0.59 - ETA: 0s - loss: 0.6773 - accuracy: 0.60 - ETA: 0s - loss: 0.6753 - accuracy: 0.60 - ETA: 0s - loss: 0.6734 - accuracy: 0.61 - ETA: 0s - loss: 0.6714 - accuracy: 0.61 - ETA: 0s - loss: 0.6701 - accuracy: 0.61 - ETA: 0s - loss: 0.6689 - accuracy: 0.62 - 1s 72us/step - loss: 0.6678 - accuracy: 0.6229 - val_loss: 0.6359 - val_accuracy: 0.7088\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:15 - loss: 0.6942 - accuracy: 0.50 - ETA: 2s - loss: 0.6939 - accuracy: 0.4976 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6920 - accuracy: 0.53 - ETA: 0s - loss: 0.6909 - accuracy: 0.53 - ETA: 0s - loss: 0.6895 - accuracy: 0.53 - ETA: 0s - loss: 0.6884 - accuracy: 0.54 - ETA: 0s - loss: 0.6872 - accuracy: 0.55 - ETA: 0s - loss: 0.6854 - accuracy: 0.55 - ETA: 0s - loss: 0.6838 - accuracy: 0.56 - ETA: 0s - loss: 0.6831 - accuracy: 0.56 - ETA: 0s - loss: 0.6814 - accuracy: 0.57 - ETA: 0s - loss: 0.6801 - accuracy: 0.57 - ETA: 0s - loss: 0.6787 - accuracy: 0.57 - ETA: 0s - loss: 0.6776 - accuracy: 0.58 - 1s 72us/step - loss: 0.6769 - accuracy: 0.5837 - val_loss: 0.6525 - val_accuracy: 0.7088\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:14 - loss: 0.6937 - accuracy: 0.62 - ETA: 1s - loss: 0.6904 - accuracy: 0.5301 - ETA: 1s - loss: 0.6884 - accuracy: 0.53 - ETA: 0s - loss: 0.6884 - accuracy: 0.52 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6858 - accuracy: 0.55 - ETA: 0s - loss: 0.6837 - accuracy: 0.56 - ETA: 0s - loss: 0.6822 - accuracy: 0.57 - ETA: 0s - loss: 0.6805 - accuracy: 0.57 - ETA: 0s - loss: 0.6786 - accuracy: 0.58 - ETA: 0s - loss: 0.6768 - accuracy: 0.59 - ETA: 0s - loss: 0.6756 - accuracy: 0.59 - ETA: 0s - loss: 0.6739 - accuracy: 0.59 - ETA: 0s - loss: 0.6721 - accuracy: 0.60 - ETA: 0s - loss: 0.6710 - accuracy: 0.60 - 1s 72us/step - loss: 0.6701 - accuracy: 0.6059 - val_loss: 0.6411 - val_accuracy: 0.7180\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:16 - loss: 0.6912 - accuracy: 0.62 - ETA: 2s - loss: 0.6916 - accuracy: 0.5336 - ETA: 1s - loss: 0.6918 - accuracy: 0.51 - ETA: 0s - loss: 0.6907 - accuracy: 0.52 - ETA: 0s - loss: 0.6900 - accuracy: 0.52 - ETA: 0s - loss: 0.6890 - accuracy: 0.53 - ETA: 0s - loss: 0.6872 - accuracy: 0.54 - ETA: 0s - loss: 0.6856 - accuracy: 0.55 - ETA: 0s - loss: 0.6840 - accuracy: 0.56 - ETA: 0s - loss: 0.6816 - accuracy: 0.56 - ETA: 0s - loss: 0.6795 - accuracy: 0.57 - ETA: 0s - loss: 0.6773 - accuracy: 0.58 - ETA: 0s - loss: 0.6755 - accuracy: 0.58 - ETA: 0s - loss: 0.6743 - accuracy: 0.59 - ETA: 0s - loss: 0.6730 - accuracy: 0.59 - 1s 71us/step - loss: 0.6718 - accuracy: 0.5983 - val_loss: 0.6400 - val_accuracy: 0.7060\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:15 - loss: 0.6923 - accuracy: 0.56 - ETA: 1s - loss: 0.6934 - accuracy: 0.5033 - ETA: 1s - loss: 0.6901 - accuracy: 0.51 - ETA: 0s - loss: 0.6881 - accuracy: 0.52 - ETA: 0s - loss: 0.6863 - accuracy: 0.53 - ETA: 0s - loss: 0.6853 - accuracy: 0.54 - ETA: 0s - loss: 0.6836 - accuracy: 0.55 - ETA: 0s - loss: 0.6821 - accuracy: 0.56 - ETA: 0s - loss: 0.6807 - accuracy: 0.56 - ETA: 0s - loss: 0.6797 - accuracy: 0.57 - ETA: 0s - loss: 0.6785 - accuracy: 0.57 - ETA: 0s - loss: 0.6777 - accuracy: 0.58 - ETA: 0s - loss: 0.6763 - accuracy: 0.58 - ETA: 0s - loss: 0.6747 - accuracy: 0.59 - ETA: 0s - loss: 0.6737 - accuracy: 0.59 - ETA: 0s - loss: 0.6722 - accuracy: 0.59 - 1s 75us/step - loss: 0.6712 - accuracy: 0.6004 - val_loss: 0.6498 - val_accuracy: 0.6889\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:17 - loss: 0.6951 - accuracy: 0.43 - ETA: 2s - loss: 0.6879 - accuracy: 0.5521 - ETA: 1s - loss: 0.6857 - accuracy: 0.55 - ETA: 0s - loss: 0.6842 - accuracy: 0.57 - ETA: 0s - loss: 0.6813 - accuracy: 0.58 - ETA: 0s - loss: 0.6797 - accuracy: 0.59 - ETA: 0s - loss: 0.6755 - accuracy: 0.61 - ETA: 0s - loss: 0.6730 - accuracy: 0.61 - ETA: 0s - loss: 0.6712 - accuracy: 0.61 - ETA: 0s - loss: 0.6693 - accuracy: 0.62 - ETA: 0s - loss: 0.6670 - accuracy: 0.62 - ETA: 0s - loss: 0.6657 - accuracy: 0.62 - ETA: 0s - loss: 0.6630 - accuracy: 0.63 - ETA: 0s - loss: 0.6608 - accuracy: 0.63 - ETA: 0s - loss: 0.6588 - accuracy: 0.63 - 1s 73us/step - loss: 0.6571 - accuracy: 0.6405 - val_loss: 0.6184 - val_accuracy: 0.7180\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 25us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:16 - loss: 0.6936 - accuracy: 0.50 - ETA: 2s - loss: 0.6923 - accuracy: 0.5336 - ETA: 1s - loss: 0.6880 - accuracy: 0.55 - ETA: 0s - loss: 0.6868 - accuracy: 0.55 - ETA: 0s - loss: 0.6838 - accuracy: 0.57 - ETA: 0s - loss: 0.6806 - accuracy: 0.58 - ETA: 0s - loss: 0.6780 - accuracy: 0.59 - ETA: 0s - loss: 0.6761 - accuracy: 0.60 - ETA: 0s - loss: 0.6740 - accuracy: 0.60 - ETA: 0s - loss: 0.6717 - accuracy: 0.61 - ETA: 0s - loss: 0.6687 - accuracy: 0.61 - ETA: 0s - loss: 0.6655 - accuracy: 0.62 - ETA: 0s - loss: 0.6634 - accuracy: 0.62 - ETA: 0s - loss: 0.6614 - accuracy: 0.63 - ETA: 0s - loss: 0.6594 - accuracy: 0.63 - 1s 73us/step - loss: 0.6582 - accuracy: 0.6374 - val_loss: 0.6217 - val_accuracy: 0.7095\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:16 - loss: 0.6962 - accuracy: 0.31 - ETA: 2s - loss: 0.6917 - accuracy: 0.5093 - ETA: 1s - loss: 0.6899 - accuracy: 0.52 - ETA: 0s - loss: 0.6877 - accuracy: 0.54 - ETA: 0s - loss: 0.6861 - accuracy: 0.55 - ETA: 0s - loss: 0.6824 - accuracy: 0.56 - ETA: 0s - loss: 0.6792 - accuracy: 0.57 - ETA: 0s - loss: 0.6760 - accuracy: 0.58 - ETA: 0s - loss: 0.6743 - accuracy: 0.59 - ETA: 0s - loss: 0.6711 - accuracy: 0.60 - ETA: 0s - loss: 0.6687 - accuracy: 0.60 - ETA: 0s - loss: 0.6669 - accuracy: 0.61 - ETA: 0s - loss: 0.6646 - accuracy: 0.61 - ETA: 0s - loss: 0.6626 - accuracy: 0.62 - ETA: 0s - loss: 0.6604 - accuracy: 0.62 - 1s 72us/step - loss: 0.6592 - accuracy: 0.6279 - val_loss: 0.6221 - val_accuracy: 0.7088\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:17 - loss: 0.6945 - accuracy: 0.50 - ETA: 2s - loss: 0.6931 - accuracy: 0.5185 - ETA: 1s - loss: 0.6914 - accuracy: 0.53 - ETA: 0s - loss: 0.6884 - accuracy: 0.55 - ETA: 0s - loss: 0.6851 - accuracy: 0.56 - ETA: 0s - loss: 0.6826 - accuracy: 0.57 - ETA: 0s - loss: 0.6812 - accuracy: 0.58 - ETA: 0s - loss: 0.6792 - accuracy: 0.59 - ETA: 0s - loss: 0.6764 - accuracy: 0.59 - ETA: 0s - loss: 0.6740 - accuracy: 0.60 - ETA: 0s - loss: 0.6715 - accuracy: 0.60 - ETA: 0s - loss: 0.6695 - accuracy: 0.61 - ETA: 0s - loss: 0.6676 - accuracy: 0.61 - ETA: 0s - loss: 0.6664 - accuracy: 0.61 - ETA: 0s - loss: 0.6642 - accuracy: 0.62 - 1s 71us/step - loss: 0.6634 - accuracy: 0.6238 - val_loss: 0.6285 - val_accuracy: 0.7081\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:15 - loss: 0.6947 - accuracy: 0.56 - ETA: 1s - loss: 0.6904 - accuracy: 0.5451 - ETA: 1s - loss: 0.6884 - accuracy: 0.55 - ETA: 0s - loss: 0.6855 - accuracy: 0.57 - ETA: 0s - loss: 0.6829 - accuracy: 0.59 - ETA: 0s - loss: 0.6788 - accuracy: 0.60 - ETA: 0s - loss: 0.6760 - accuracy: 0.61 - ETA: 0s - loss: 0.6738 - accuracy: 0.61 - ETA: 0s - loss: 0.6710 - accuracy: 0.62 - ETA: 0s - loss: 0.6681 - accuracy: 0.62 - ETA: 0s - loss: 0.6657 - accuracy: 0.63 - ETA: 0s - loss: 0.6634 - accuracy: 0.63 - ETA: 0s - loss: 0.6617 - accuracy: 0.63 - ETA: 0s - loss: 0.6597 - accuracy: 0.64 - ETA: 0s - loss: 0.6575 - accuracy: 0.64 - 1s 72us/step - loss: 0.6553 - accuracy: 0.6480 - val_loss: 0.6169 - val_accuracy: 0.7081\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:14 - loss: 0.6906 - accuracy: 0.68 - ETA: 1s - loss: 0.6917 - accuracy: 0.5114 - ETA: 1s - loss: 0.6890 - accuracy: 0.54 - ETA: 0s - loss: 0.6866 - accuracy: 0.56 - ETA: 0s - loss: 0.6847 - accuracy: 0.57 - ETA: 0s - loss: 0.6821 - accuracy: 0.58 - ETA: 0s - loss: 0.6798 - accuracy: 0.59 - ETA: 0s - loss: 0.6768 - accuracy: 0.60 - ETA: 0s - loss: 0.6741 - accuracy: 0.61 - ETA: 0s - loss: 0.6719 - accuracy: 0.61 - ETA: 0s - loss: 0.6702 - accuracy: 0.62 - ETA: 0s - loss: 0.6685 - accuracy: 0.62 - ETA: 0s - loss: 0.6667 - accuracy: 0.62 - ETA: 0s - loss: 0.6650 - accuracy: 0.62 - ETA: 0s - loss: 0.6634 - accuracy: 0.63 - 1s 72us/step - loss: 0.6621 - accuracy: 0.6328 - val_loss: 0.6314 - val_accuracy: 0.6797\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:31 - loss: 0.6955 - accuracy: 0.43 - ETA: 2s - loss: 0.6935 - accuracy: 0.5142 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6892 - accuracy: 0.53 - ETA: 0s - loss: 0.6867 - accuracy: 0.54 - ETA: 0s - loss: 0.6844 - accuracy: 0.54 - ETA: 0s - loss: 0.6795 - accuracy: 0.55 - ETA: 0s - loss: 0.6773 - accuracy: 0.55 - ETA: 0s - loss: 0.6754 - accuracy: 0.55 - ETA: 0s - loss: 0.6733 - accuracy: 0.56 - ETA: 0s - loss: 0.6705 - accuracy: 0.56 - ETA: 0s - loss: 0.6686 - accuracy: 0.56 - ETA: 0s - loss: 0.6666 - accuracy: 0.56 - ETA: 0s - loss: 0.6652 - accuracy: 0.57 - ETA: 0s - loss: 0.6640 - accuracy: 0.57 - 1s 79us/step - loss: 0.6639 - accuracy: 0.5726 - val_loss: 0.6180 - val_accuracy: 0.7053\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:30 - loss: 0.6973 - accuracy: 0.37 - ETA: 2s - loss: 0.6927 - accuracy: 0.5142 - ETA: 1s - loss: 0.6917 - accuracy: 0.52 - ETA: 1s - loss: 0.6908 - accuracy: 0.52 - ETA: 0s - loss: 0.6895 - accuracy: 0.53 - ETA: 0s - loss: 0.6878 - accuracy: 0.54 - ETA: 0s - loss: 0.6855 - accuracy: 0.56 - ETA: 0s - loss: 0.6840 - accuracy: 0.57 - ETA: 0s - loss: 0.6813 - accuracy: 0.58 - ETA: 0s - loss: 0.6798 - accuracy: 0.58 - ETA: 0s - loss: 0.6768 - accuracy: 0.59 - ETA: 0s - loss: 0.6747 - accuracy: 0.59 - ETA: 0s - loss: 0.6715 - accuracy: 0.60 - ETA: 0s - loss: 0.6691 - accuracy: 0.61 - ETA: 0s - loss: 0.6676 - accuracy: 0.61 - ETA: 0s - loss: 0.6658 - accuracy: 0.61 - 1s 78us/step - loss: 0.6645 - accuracy: 0.6230 - val_loss: 0.6275 - val_accuracy: 0.7131\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:31 - loss: 0.6942 - accuracy: 0.43 - ETA: 2s - loss: 0.6933 - accuracy: 0.5110 - ETA: 1s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.50 - ETA: 0s - loss: 0.6909 - accuracy: 0.50 - ETA: 0s - loss: 0.6891 - accuracy: 0.50 - ETA: 0s - loss: 0.6877 - accuracy: 0.51 - ETA: 0s - loss: 0.6858 - accuracy: 0.52 - ETA: 0s - loss: 0.6836 - accuracy: 0.52 - ETA: 0s - loss: 0.6817 - accuracy: 0.53 - ETA: 0s - loss: 0.6803 - accuracy: 0.54 - ETA: 0s - loss: 0.6792 - accuracy: 0.55 - ETA: 0s - loss: 0.6776 - accuracy: 0.56 - ETA: 0s - loss: 0.6769 - accuracy: 0.56 - ETA: 0s - loss: 0.6754 - accuracy: 0.57 - ETA: 0s - loss: 0.6752 - accuracy: 0.57 - 1s 78us/step - loss: 0.6736 - accuracy: 0.5800 - val_loss: 0.6383 - val_accuracy: 0.7017\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:30 - loss: 0.6945 - accuracy: 0.37 - ETA: 2s - loss: 0.6932 - accuracy: 0.5096 - ETA: 1s - loss: 0.6929 - accuracy: 0.50 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 0s - loss: 0.6913 - accuracy: 0.52 - ETA: 0s - loss: 0.6884 - accuracy: 0.52 - ETA: 0s - loss: 0.6861 - accuracy: 0.53 - ETA: 0s - loss: 0.6829 - accuracy: 0.54 - ETA: 0s - loss: 0.6803 - accuracy: 0.55 - ETA: 0s - loss: 0.6780 - accuracy: 0.55 - ETA: 0s - loss: 0.6745 - accuracy: 0.56 - ETA: 0s - loss: 0.6703 - accuracy: 0.57 - ETA: 0s - loss: 0.6660 - accuracy: 0.57 - ETA: 0s - loss: 0.6626 - accuracy: 0.58 - ETA: 0s - loss: 0.6597 - accuracy: 0.58 - ETA: 0s - loss: 0.6580 - accuracy: 0.59 - 1s 77us/step - loss: 0.6560 - accuracy: 0.5933 - val_loss: 0.5987 - val_accuracy: 0.7152\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:30 - loss: 0.6937 - accuracy: 0.56 - ETA: 2s - loss: 0.6939 - accuracy: 0.4964 - ETA: 1s - loss: 0.6930 - accuracy: 0.53 - ETA: 1s - loss: 0.6926 - accuracy: 0.54 - ETA: 0s - loss: 0.6919 - accuracy: 0.54 - ETA: 0s - loss: 0.6911 - accuracy: 0.55 - ETA: 0s - loss: 0.6904 - accuracy: 0.56 - ETA: 0s - loss: 0.6895 - accuracy: 0.56 - ETA: 0s - loss: 0.6875 - accuracy: 0.57 - ETA: 0s - loss: 0.6856 - accuracy: 0.57 - ETA: 0s - loss: 0.6841 - accuracy: 0.57 - ETA: 0s - loss: 0.6828 - accuracy: 0.58 - ETA: 0s - loss: 0.6816 - accuracy: 0.58 - ETA: 0s - loss: 0.6798 - accuracy: 0.58 - ETA: 0s - loss: 0.6785 - accuracy: 0.59 - ETA: 0s - loss: 0.6777 - accuracy: 0.59 - 1s 78us/step - loss: 0.6765 - accuracy: 0.5966 - val_loss: 0.6448 - val_accuracy: 0.7131\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 1:33 - loss: 0.6930 - accuracy: 0.56 - ETA: 2s - loss: 0.6932 - accuracy: 0.5096 - ETA: 1s - loss: 0.6928 - accuracy: 0.51 - ETA: 1s - loss: 0.6907 - accuracy: 0.51 - ETA: 0s - loss: 0.6883 - accuracy: 0.52 - ETA: 0s - loss: 0.6874 - accuracy: 0.52 - ETA: 0s - loss: 0.6847 - accuracy: 0.52 - ETA: 0s - loss: 0.6816 - accuracy: 0.54 - ETA: 0s - loss: 0.6803 - accuracy: 0.54 - ETA: 0s - loss: 0.6775 - accuracy: 0.55 - ETA: 0s - loss: 0.6750 - accuracy: 0.55 - ETA: 0s - loss: 0.6718 - accuracy: 0.56 - ETA: 0s - loss: 0.6685 - accuracy: 0.57 - ETA: 0s - loss: 0.6662 - accuracy: 0.57 - ETA: 0s - loss: 0.6638 - accuracy: 0.58 - ETA: 0s - loss: 0.6629 - accuracy: 0.58 - 1s 77us/step - loss: 0.6620 - accuracy: 0.5862 - val_loss: 0.6157 - val_accuracy: 0.7180\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 25us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:31 - loss: 0.6905 - accuracy: 0.75 - ETA: 2s - loss: 0.6932 - accuracy: 0.5392 - ETA: 1s - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6904 - accuracy: 0.54 - ETA: 0s - loss: 0.6871 - accuracy: 0.56 - ETA: 0s - loss: 0.6843 - accuracy: 0.56 - ETA: 0s - loss: 0.6790 - accuracy: 0.58 - ETA: 0s - loss: 0.6746 - accuracy: 0.59 - ETA: 0s - loss: 0.6712 - accuracy: 0.59 - ETA: 0s - loss: 0.6673 - accuracy: 0.60 - ETA: 0s - loss: 0.6641 - accuracy: 0.61 - ETA: 0s - loss: 0.6597 - accuracy: 0.61 - ETA: 0s - loss: 0.6574 - accuracy: 0.62 - ETA: 0s - loss: 0.6554 - accuracy: 0.62 - ETA: 0s - loss: 0.6536 - accuracy: 0.63 - 1s 77us/step - loss: 0.6522 - accuracy: 0.6333 - val_loss: 0.5977 - val_accuracy: 0.7180\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:29 - loss: 0.6927 - accuracy: 0.62 - ETA: 2s - loss: 0.6934 - accuracy: 0.5159 - ETA: 1s - loss: 0.6937 - accuracy: 0.51 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6914 - accuracy: 0.54 - ETA: 0s - loss: 0.6894 - accuracy: 0.54 - ETA: 0s - loss: 0.6872 - accuracy: 0.55 - ETA: 0s - loss: 0.6843 - accuracy: 0.55 - ETA: 0s - loss: 0.6802 - accuracy: 0.56 - ETA: 0s - loss: 0.6771 - accuracy: 0.57 - ETA: 0s - loss: 0.6731 - accuracy: 0.57 - ETA: 0s - loss: 0.6704 - accuracy: 0.58 - ETA: 0s - loss: 0.6676 - accuracy: 0.58 - ETA: 0s - loss: 0.6646 - accuracy: 0.59 - ETA: 0s - loss: 0.6625 - accuracy: 0.59 - ETA: 0s - loss: 0.6597 - accuracy: 0.60 - ETA: 0s - loss: 0.6582 - accuracy: 0.60 - 1s 79us/step - loss: 0.6577 - accuracy: 0.6073 - val_loss: 0.6058 - val_accuracy: 0.7195\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:31 - loss: 0.6939 - accuracy: 0.62 - ETA: 2s - loss: 0.6940 - accuracy: 0.5130 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6907 - accuracy: 0.53 - ETA: 0s - loss: 0.6869 - accuracy: 0.54 - ETA: 0s - loss: 0.6832 - accuracy: 0.55 - ETA: 0s - loss: 0.6793 - accuracy: 0.56 - ETA: 0s - loss: 0.6739 - accuracy: 0.57 - ETA: 0s - loss: 0.6696 - accuracy: 0.58 - ETA: 0s - loss: 0.6647 - accuracy: 0.58 - ETA: 0s - loss: 0.6635 - accuracy: 0.59 - ETA: 0s - loss: 0.6600 - accuracy: 0.60 - ETA: 0s - loss: 0.6577 - accuracy: 0.60 - ETA: 0s - loss: 0.6551 - accuracy: 0.60 - ETA: 0s - loss: 0.6526 - accuracy: 0.61 - ETA: 0s - loss: 0.6502 - accuracy: 0.61 - 1s 78us/step - loss: 0.6481 - accuracy: 0.6229 - val_loss: 0.5862 - val_accuracy: 0.7308\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:30 - loss: 0.6943 - accuracy: 0.31 - ETA: 2s - loss: 0.6938 - accuracy: 0.5373 - ETA: 1s - loss: 0.6934 - accuracy: 0.54 - ETA: 1s - loss: 0.6931 - accuracy: 0.54 - ETA: 0s - loss: 0.6921 - accuracy: 0.54 - ETA: 0s - loss: 0.6906 - accuracy: 0.55 - ETA: 0s - loss: 0.6885 - accuracy: 0.55 - ETA: 0s - loss: 0.6852 - accuracy: 0.56 - ETA: 0s - loss: 0.6835 - accuracy: 0.56 - ETA: 0s - loss: 0.6808 - accuracy: 0.56 - ETA: 0s - loss: 0.6778 - accuracy: 0.56 - ETA: 0s - loss: 0.6759 - accuracy: 0.57 - ETA: 0s - loss: 0.6729 - accuracy: 0.57 - ETA: 0s - loss: 0.6709 - accuracy: 0.58 - ETA: 0s - loss: 0.6680 - accuracy: 0.58 - ETA: 0s - loss: 0.6655 - accuracy: 0.58 - ETA: 0s - loss: 0.6641 - accuracy: 0.59 - 1s 79us/step - loss: 0.6641 - accuracy: 0.5920 - val_loss: 0.6146 - val_accuracy: 0.7195\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:34 - loss: 0.6942 - accuracy: 0.50 - ETA: 2s - loss: 0.6940 - accuracy: 0.5072 - ETA: 1s - loss: 0.6925 - accuracy: 0.53 - ETA: 1s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6896 - accuracy: 0.53 - ETA: 0s - loss: 0.6876 - accuracy: 0.54 - ETA: 0s - loss: 0.6840 - accuracy: 0.55 - ETA: 0s - loss: 0.6808 - accuracy: 0.56 - ETA: 0s - loss: 0.6768 - accuracy: 0.56 - ETA: 0s - loss: 0.6714 - accuracy: 0.57 - ETA: 0s - loss: 0.6692 - accuracy: 0.58 - ETA: 0s - loss: 0.6674 - accuracy: 0.58 - ETA: 0s - loss: 0.6640 - accuracy: 0.58 - ETA: 0s - loss: 0.6613 - accuracy: 0.59 - ETA: 0s - loss: 0.6585 - accuracy: 0.59 - ETA: 0s - loss: 0.6572 - accuracy: 0.59 - 1s 78us/step - loss: 0.6552 - accuracy: 0.6013 - val_loss: 0.5971 - val_accuracy: 0.7237\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:31 - loss: 0.6931 - accuracy: 0.50 - ETA: 2s - loss: 0.6936 - accuracy: 0.5051 - ETA: 1s - loss: 0.6906 - accuracy: 0.51 - ETA: 1s - loss: 0.6895 - accuracy: 0.51 - ETA: 0s - loss: 0.6876 - accuracy: 0.51 - ETA: 0s - loss: 0.6846 - accuracy: 0.52 - ETA: 0s - loss: 0.6815 - accuracy: 0.53 - ETA: 0s - loss: 0.6782 - accuracy: 0.54 - ETA: 0s - loss: 0.6755 - accuracy: 0.54 - ETA: 0s - loss: 0.6732 - accuracy: 0.55 - ETA: 0s - loss: 0.6709 - accuracy: 0.55 - ETA: 0s - loss: 0.6678 - accuracy: 0.56 - ETA: 0s - loss: 0.6636 - accuracy: 0.56 - ETA: 0s - loss: 0.6613 - accuracy: 0.57 - ETA: 0s - loss: 0.6585 - accuracy: 0.57 - ETA: 0s - loss: 0.6574 - accuracy: 0.58 - ETA: 0s - loss: 0.6570 - accuracy: 0.58 - 1s 79us/step - loss: 0.6566 - accuracy: 0.5861 - val_loss: 0.6051 - val_accuracy: 0.7138\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 25us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:31 - loss: 0.6942 - accuracy: 0.62 - ETA: 2s - loss: 0.6944 - accuracy: 0.5153 - ETA: 1s - loss: 0.6927 - accuracy: 0.54 - ETA: 1s - loss: 0.6902 - accuracy: 0.55 - ETA: 0s - loss: 0.6891 - accuracy: 0.55 - ETA: 0s - loss: 0.6853 - accuracy: 0.56 - ETA: 0s - loss: 0.6821 - accuracy: 0.57 - ETA: 0s - loss: 0.6788 - accuracy: 0.58 - ETA: 0s - loss: 0.6743 - accuracy: 0.59 - ETA: 0s - loss: 0.6701 - accuracy: 0.59 - ETA: 0s - loss: 0.6658 - accuracy: 0.60 - ETA: 0s - loss: 0.6621 - accuracy: 0.61 - ETA: 0s - loss: 0.6585 - accuracy: 0.61 - ETA: 0s - loss: 0.6553 - accuracy: 0.62 - ETA: 0s - loss: 0.6523 - accuracy: 0.62 - ETA: 0s - loss: 0.6494 - accuracy: 0.62 - ETA: 0s - loss: 0.6470 - accuracy: 0.63 - 1s 79us/step - loss: 0.6466 - accuracy: 0.6334 - val_loss: 0.5877 - val_accuracy: 0.7223\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:28 - loss: 0.6950 - accuracy: 0.43 - ETA: 2s - loss: 0.6914 - accuracy: 0.5571 - ETA: 1s - loss: 0.6877 - accuracy: 0.55 - ETA: 1s - loss: 0.6825 - accuracy: 0.57 - ETA: 1s - loss: 0.6786 - accuracy: 0.58 - ETA: 0s - loss: 0.6717 - accuracy: 0.59 - ETA: 0s - loss: 0.6671 - accuracy: 0.60 - ETA: 0s - loss: 0.6635 - accuracy: 0.61 - ETA: 0s - loss: 0.6576 - accuracy: 0.62 - ETA: 0s - loss: 0.6534 - accuracy: 0.62 - ETA: 0s - loss: 0.6509 - accuracy: 0.63 - ETA: 0s - loss: 0.6488 - accuracy: 0.63 - ETA: 0s - loss: 0.6472 - accuracy: 0.63 - ETA: 0s - loss: 0.6445 - accuracy: 0.63 - ETA: 0s - loss: 0.6447 - accuracy: 0.63 - ETA: 0s - loss: 0.6421 - accuracy: 0.64 - 1s 77us/step - loss: 0.6413 - accuracy: 0.6455 - val_loss: 0.5872 - val_accuracy: 0.7188\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:30 - loss: 0.6935 - accuracy: 0.75 - ETA: 2s - loss: 0.6932 - accuracy: 0.5108 - ETA: 1s - loss: 0.6900 - accuracy: 0.52 - ETA: 1s - loss: 0.6864 - accuracy: 0.54 - ETA: 0s - loss: 0.6823 - accuracy: 0.56 - ETA: 0s - loss: 0.6776 - accuracy: 0.58 - ETA: 0s - loss: 0.6730 - accuracy: 0.59 - ETA: 0s - loss: 0.6691 - accuracy: 0.60 - ETA: 0s - loss: 0.6638 - accuracy: 0.60 - ETA: 0s - loss: 0.6617 - accuracy: 0.61 - ETA: 0s - loss: 0.6569 - accuracy: 0.62 - ETA: 0s - loss: 0.6534 - accuracy: 0.62 - ETA: 0s - loss: 0.6509 - accuracy: 0.62 - ETA: 0s - loss: 0.6487 - accuracy: 0.63 - ETA: 0s - loss: 0.6464 - accuracy: 0.63 - ETA: 0s - loss: 0.6433 - accuracy: 0.64 - 1s 78us/step - loss: 0.6412 - accuracy: 0.6436 - val_loss: 0.5852 - val_accuracy: 0.7237\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:30 - loss: 0.6928 - accuracy: 0.62 - ETA: 2s - loss: 0.6928 - accuracy: 0.5283 - ETA: 1s - loss: 0.6913 - accuracy: 0.53 - ETA: 1s - loss: 0.6894 - accuracy: 0.54 - ETA: 0s - loss: 0.6866 - accuracy: 0.55 - ETA: 0s - loss: 0.6825 - accuracy: 0.57 - ETA: 0s - loss: 0.6773 - accuracy: 0.58 - ETA: 0s - loss: 0.6742 - accuracy: 0.59 - ETA: 0s - loss: 0.6698 - accuracy: 0.60 - ETA: 0s - loss: 0.6650 - accuracy: 0.60 - ETA: 0s - loss: 0.6605 - accuracy: 0.61 - ETA: 0s - loss: 0.6547 - accuracy: 0.62 - ETA: 0s - loss: 0.6506 - accuracy: 0.62 - ETA: 0s - loss: 0.6476 - accuracy: 0.63 - ETA: 0s - loss: 0.6462 - accuracy: 0.63 - ETA: 0s - loss: 0.6421 - accuracy: 0.64 - 1s 78us/step - loss: 0.6393 - accuracy: 0.6438 - val_loss: 0.5811 - val_accuracy: 0.7230\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:30 - loss: 0.6981 - accuracy: 0.31 - ETA: 2s - loss: 0.6921 - accuracy: 0.5649 - ETA: 1s - loss: 0.6907 - accuracy: 0.56 - ETA: 1s - loss: 0.6879 - accuracy: 0.56 - ETA: 0s - loss: 0.6845 - accuracy: 0.58 - ETA: 0s - loss: 0.6792 - accuracy: 0.59 - ETA: 0s - loss: 0.6733 - accuracy: 0.59 - ETA: 0s - loss: 0.6661 - accuracy: 0.61 - ETA: 0s - loss: 0.6595 - accuracy: 0.61 - ETA: 0s - loss: 0.6547 - accuracy: 0.62 - ETA: 0s - loss: 0.6513 - accuracy: 0.63 - ETA: 0s - loss: 0.6498 - accuracy: 0.63 - ETA: 0s - loss: 0.6455 - accuracy: 0.63 - ETA: 0s - loss: 0.6439 - accuracy: 0.64 - ETA: 0s - loss: 0.6408 - accuracy: 0.64 - ETA: 0s - loss: 0.6380 - accuracy: 0.64 - ETA: 0s - loss: 0.6361 - accuracy: 0.65 - 1s 79us/step - loss: 0.6360 - accuracy: 0.6505 - val_loss: 0.5792 - val_accuracy: 0.7216\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:30 - loss: 0.6958 - accuracy: 0.56 - ETA: 2s - loss: 0.6944 - accuracy: 0.5084 - ETA: 1s - loss: 0.6932 - accuracy: 0.52 - ETA: 1s - loss: 0.6903 - accuracy: 0.52 - ETA: 0s - loss: 0.6876 - accuracy: 0.54 - ETA: 0s - loss: 0.6820 - accuracy: 0.56 - ETA: 0s - loss: 0.6758 - accuracy: 0.57 - ETA: 0s - loss: 0.6728 - accuracy: 0.58 - ETA: 0s - loss: 0.6694 - accuracy: 0.59 - ETA: 0s - loss: 0.6660 - accuracy: 0.59 - ETA: 0s - loss: 0.6629 - accuracy: 0.60 - ETA: 0s - loss: 0.6580 - accuracy: 0.61 - ETA: 0s - loss: 0.6559 - accuracy: 0.61 - ETA: 0s - loss: 0.6524 - accuracy: 0.62 - ETA: 0s - loss: 0.6510 - accuracy: 0.62 - ETA: 0s - loss: 0.6493 - accuracy: 0.62 - ETA: 0s - loss: 0.6474 - accuracy: 0.63 - 1s 81us/step - loss: 0.6465 - accuracy: 0.6334 - val_loss: 0.6000 - val_accuracy: 0.7060\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:19 - loss: 0.6936 - accuracy: 0.62 - ETA: 2s - loss: 0.6933 - accuracy: 0.5071 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 0s - loss: 0.6909 - accuracy: 0.53 - ETA: 0s - loss: 0.6899 - accuracy: 0.53 - ETA: 0s - loss: 0.6887 - accuracy: 0.54 - ETA: 0s - loss: 0.6875 - accuracy: 0.55 - ETA: 0s - loss: 0.6861 - accuracy: 0.56 - ETA: 0s - loss: 0.6846 - accuracy: 0.56 - ETA: 0s - loss: 0.6836 - accuracy: 0.56 - ETA: 0s - loss: 0.6806 - accuracy: 0.57 - ETA: 0s - loss: 0.6793 - accuracy: 0.57 - ETA: 0s - loss: 0.6777 - accuracy: 0.57 - ETA: 0s - loss: 0.6762 - accuracy: 0.58 - ETA: 0s - loss: 0.6748 - accuracy: 0.58 - 1s 74us/step - loss: 0.6748 - accuracy: 0.5831 - val_loss: 0.6403 - val_accuracy: 0.7067\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:21 - loss: 0.6945 - accuracy: 0.37 - ETA: 2s - loss: 0.6933 - accuracy: 0.5243 - ETA: 1s - loss: 0.6931 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.53 - ETA: 0s - loss: 0.6922 - accuracy: 0.53 - ETA: 0s - loss: 0.6917 - accuracy: 0.53 - ETA: 0s - loss: 0.6909 - accuracy: 0.54 - ETA: 0s - loss: 0.6903 - accuracy: 0.55 - ETA: 0s - loss: 0.6896 - accuracy: 0.55 - ETA: 0s - loss: 0.6882 - accuracy: 0.56 - ETA: 0s - loss: 0.6866 - accuracy: 0.57 - ETA: 0s - loss: 0.6854 - accuracy: 0.57 - ETA: 0s - loss: 0.6842 - accuracy: 0.58 - ETA: 0s - loss: 0.6832 - accuracy: 0.58 - ETA: 0s - loss: 0.6818 - accuracy: 0.58 - ETA: 0s - loss: 0.6804 - accuracy: 0.59 - ETA: 0s - loss: 0.6795 - accuracy: 0.59 - ETA: 0s - loss: 0.6786 - accuracy: 0.59 - ETA: 0s - loss: 0.6772 - accuracy: 0.59 - 1s 88us/step - loss: 0.6766 - accuracy: 0.5975 - val_loss: 0.6441 - val_accuracy: 0.7131\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:21 - loss: 0.6939 - accuracy: 0.43 - ETA: 2s - loss: 0.6935 - accuracy: 0.4850 - ETA: 1s - loss: 0.6932 - accuracy: 0.50 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6917 - accuracy: 0.53 - ETA: 0s - loss: 0.6904 - accuracy: 0.54 - ETA: 0s - loss: 0.6891 - accuracy: 0.55 - ETA: 0s - loss: 0.6873 - accuracy: 0.55 - ETA: 0s - loss: 0.6864 - accuracy: 0.55 - ETA: 0s - loss: 0.6851 - accuracy: 0.56 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6819 - accuracy: 0.57 - ETA: 0s - loss: 0.6803 - accuracy: 0.57 - ETA: 0s - loss: 0.6790 - accuracy: 0.57 - ETA: 0s - loss: 0.6771 - accuracy: 0.58 - ETA: 0s - loss: 0.6756 - accuracy: 0.58 - 1s 75us/step - loss: 0.6752 - accuracy: 0.5871 - val_loss: 0.6397 - val_accuracy: 0.7067\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 1:21 - loss: 0.6894 - accuracy: 0.87 - ETA: 2s - loss: 0.6936 - accuracy: 0.5318 - ETA: 1s - loss: 0.6929 - accuracy: 0.54 - ETA: 0s - loss: 0.6925 - accuracy: 0.54 - ETA: 0s - loss: 0.6915 - accuracy: 0.54 - ETA: 0s - loss: 0.6911 - accuracy: 0.54 - ETA: 0s - loss: 0.6904 - accuracy: 0.54 - ETA: 0s - loss: 0.6895 - accuracy: 0.55 - ETA: 0s - loss: 0.6885 - accuracy: 0.55 - ETA: 0s - loss: 0.6875 - accuracy: 0.55 - ETA: 0s - loss: 0.6866 - accuracy: 0.56 - ETA: 0s - loss: 0.6852 - accuracy: 0.56 - ETA: 0s - loss: 0.6838 - accuracy: 0.56 - ETA: 0s - loss: 0.6823 - accuracy: 0.56 - ETA: 0s - loss: 0.6807 - accuracy: 0.56 - ETA: 0s - loss: 0.6798 - accuracy: 0.56 - 1s 74us/step - loss: 0.6797 - accuracy: 0.5692 - val_loss: 0.6519 - val_accuracy: 0.6953\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:21 - loss: 0.6936 - accuracy: 0.50 - ETA: 2s - loss: 0.6929 - accuracy: 0.5259 - ETA: 1s - loss: 0.6924 - accuracy: 0.52 - ETA: 1s - loss: 0.6917 - accuracy: 0.52 - ETA: 0s - loss: 0.6910 - accuracy: 0.53 - ETA: 0s - loss: 0.6901 - accuracy: 0.54 - ETA: 0s - loss: 0.6891 - accuracy: 0.55 - ETA: 0s - loss: 0.6877 - accuracy: 0.55 - ETA: 0s - loss: 0.6864 - accuracy: 0.56 - ETA: 0s - loss: 0.6847 - accuracy: 0.57 - ETA: 0s - loss: 0.6826 - accuracy: 0.57 - ETA: 0s - loss: 0.6809 - accuracy: 0.58 - ETA: 0s - loss: 0.6792 - accuracy: 0.58 - ETA: 0s - loss: 0.6783 - accuracy: 0.58 - ETA: 0s - loss: 0.6767 - accuracy: 0.59 - ETA: 0s - loss: 0.6747 - accuracy: 0.59 - 1s 74us/step - loss: 0.6747 - accuracy: 0.5937 - val_loss: 0.6383 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 25us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:20 - loss: 0.6937 - accuracy: 0.50 - ETA: 2s - loss: 0.6928 - accuracy: 0.5385 - ETA: 1s - loss: 0.6912 - accuracy: 0.55 - ETA: 1s - loss: 0.6901 - accuracy: 0.56 - ETA: 0s - loss: 0.6889 - accuracy: 0.56 - ETA: 0s - loss: 0.6885 - accuracy: 0.56 - ETA: 0s - loss: 0.6868 - accuracy: 0.57 - ETA: 0s - loss: 0.6855 - accuracy: 0.57 - ETA: 0s - loss: 0.6840 - accuracy: 0.57 - ETA: 0s - loss: 0.6828 - accuracy: 0.57 - ETA: 0s - loss: 0.6819 - accuracy: 0.57 - ETA: 0s - loss: 0.6795 - accuracy: 0.58 - ETA: 0s - loss: 0.6785 - accuracy: 0.58 - ETA: 0s - loss: 0.6764 - accuracy: 0.58 - ETA: 0s - loss: 0.6746 - accuracy: 0.59 - ETA: 0s - loss: 0.6725 - accuracy: 0.59 - 1s 75us/step - loss: 0.6724 - accuracy: 0.5949 - val_loss: 0.6410 - val_accuracy: 0.7102\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:19 - loss: 0.6880 - accuracy: 0.68 - ETA: 2s - loss: 0.6911 - accuracy: 0.5177 - ETA: 1s - loss: 0.6911 - accuracy: 0.53 - ETA: 1s - loss: 0.6894 - accuracy: 0.54 - ETA: 0s - loss: 0.6882 - accuracy: 0.55 - ETA: 0s - loss: 0.6867 - accuracy: 0.56 - ETA: 0s - loss: 0.6847 - accuracy: 0.57 - ETA: 0s - loss: 0.6827 - accuracy: 0.57 - ETA: 0s - loss: 0.6806 - accuracy: 0.58 - ETA: 0s - loss: 0.6787 - accuracy: 0.59 - ETA: 0s - loss: 0.6759 - accuracy: 0.59 - ETA: 0s - loss: 0.6738 - accuracy: 0.60 - ETA: 0s - loss: 0.6712 - accuracy: 0.60 - ETA: 0s - loss: 0.6687 - accuracy: 0.61 - ETA: 0s - loss: 0.6656 - accuracy: 0.61 - ETA: 0s - loss: 0.6642 - accuracy: 0.61 - 1s 74us/step - loss: 0.6639 - accuracy: 0.6176 - val_loss: 0.6151 - val_accuracy: 0.7202\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:22 - loss: 0.6942 - accuracy: 0.56 - ETA: 2s - loss: 0.6936 - accuracy: 0.5136 - ETA: 1s - loss: 0.6932 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6916 - accuracy: 0.53 - ETA: 0s - loss: 0.6907 - accuracy: 0.54 - ETA: 0s - loss: 0.6894 - accuracy: 0.55 - ETA: 0s - loss: 0.6877 - accuracy: 0.55 - ETA: 0s - loss: 0.6858 - accuracy: 0.56 - ETA: 0s - loss: 0.6832 - accuracy: 0.57 - ETA: 0s - loss: 0.6816 - accuracy: 0.58 - ETA: 0s - loss: 0.6784 - accuracy: 0.59 - ETA: 0s - loss: 0.6758 - accuracy: 0.59 - ETA: 0s - loss: 0.6737 - accuracy: 0.60 - ETA: 0s - loss: 0.6714 - accuracy: 0.60 - ETA: 0s - loss: 0.6691 - accuracy: 0.60 - 1s 74us/step - loss: 0.6692 - accuracy: 0.6077 - val_loss: 0.6215 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:20 - loss: 0.6934 - accuracy: 0.62 - ETA: 2s - loss: 0.6934 - accuracy: 0.5118 - ETA: 1s - loss: 0.6914 - accuracy: 0.53 - ETA: 1s - loss: 0.6916 - accuracy: 0.51 - ETA: 0s - loss: 0.6905 - accuracy: 0.51 - ETA: 0s - loss: 0.6889 - accuracy: 0.52 - ETA: 0s - loss: 0.6868 - accuracy: 0.52 - ETA: 0s - loss: 0.6859 - accuracy: 0.53 - ETA: 0s - loss: 0.6838 - accuracy: 0.54 - ETA: 0s - loss: 0.6819 - accuracy: 0.55 - ETA: 0s - loss: 0.6804 - accuracy: 0.56 - ETA: 0s - loss: 0.6775 - accuracy: 0.57 - ETA: 0s - loss: 0.6760 - accuracy: 0.58 - ETA: 0s - loss: 0.6744 - accuracy: 0.58 - ETA: 0s - loss: 0.6721 - accuracy: 0.59 - ETA: 0s - loss: 0.6698 - accuracy: 0.59 - 1s 74us/step - loss: 0.6696 - accuracy: 0.5949 - val_loss: 0.6233 - val_accuracy: 0.7102\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:20 - loss: 0.6913 - accuracy: 0.62 - ETA: 2s - loss: 0.6909 - accuracy: 0.5694 - ETA: 1s - loss: 0.6903 - accuracy: 0.57 - ETA: 1s - loss: 0.6884 - accuracy: 0.58 - ETA: 0s - loss: 0.6875 - accuracy: 0.58 - ETA: 0s - loss: 0.6840 - accuracy: 0.58 - ETA: 0s - loss: 0.6827 - accuracy: 0.58 - ETA: 0s - loss: 0.6797 - accuracy: 0.59 - ETA: 0s - loss: 0.6773 - accuracy: 0.59 - ETA: 0s - loss: 0.6750 - accuracy: 0.60 - ETA: 0s - loss: 0.6722 - accuracy: 0.60 - ETA: 0s - loss: 0.6694 - accuracy: 0.61 - ETA: 0s - loss: 0.6671 - accuracy: 0.61 - ETA: 0s - loss: 0.6642 - accuracy: 0.61 - ETA: 0s - loss: 0.6612 - accuracy: 0.62 - ETA: 0s - loss: 0.6589 - accuracy: 0.62 - 1s 75us/step - loss: 0.6583 - accuracy: 0.6269 - val_loss: 0.6098 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:19 - loss: 0.6929 - accuracy: 0.37 - ETA: 2s - loss: 0.6930 - accuracy: 0.5162 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6907 - accuracy: 0.52 - ETA: 0s - loss: 0.6888 - accuracy: 0.54 - ETA: 0s - loss: 0.6854 - accuracy: 0.55 - ETA: 0s - loss: 0.6842 - accuracy: 0.55 - ETA: 0s - loss: 0.6826 - accuracy: 0.55 - ETA: 0s - loss: 0.6807 - accuracy: 0.56 - ETA: 0s - loss: 0.6797 - accuracy: 0.56 - ETA: 0s - loss: 0.6782 - accuracy: 0.57 - ETA: 0s - loss: 0.6764 - accuracy: 0.58 - ETA: 0s - loss: 0.6741 - accuracy: 0.58 - ETA: 0s - loss: 0.6718 - accuracy: 0.59 - ETA: 0s - loss: 0.6698 - accuracy: 0.59 - ETA: 0s - loss: 0.6691 - accuracy: 0.59 - 1s 74us/step - loss: 0.6689 - accuracy: 0.5984 - val_loss: 0.6270 - val_accuracy: 0.7145\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:19 - loss: 0.6937 - accuracy: 0.56 - ETA: 2s - loss: 0.6940 - accuracy: 0.4870 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6909 - accuracy: 0.52 - ETA: 0s - loss: 0.6894 - accuracy: 0.53 - ETA: 0s - loss: 0.6882 - accuracy: 0.54 - ETA: 0s - loss: 0.6875 - accuracy: 0.54 - ETA: 0s - loss: 0.6859 - accuracy: 0.56 - ETA: 0s - loss: 0.6843 - accuracy: 0.56 - ETA: 0s - loss: 0.6821 - accuracy: 0.57 - ETA: 0s - loss: 0.6806 - accuracy: 0.57 - ETA: 0s - loss: 0.6778 - accuracy: 0.58 - ETA: 0s - loss: 0.6748 - accuracy: 0.59 - ETA: 0s - loss: 0.6733 - accuracy: 0.59 - ETA: 0s - loss: 0.6713 - accuracy: 0.60 - ETA: 0s - loss: 0.6695 - accuracy: 0.60 - 1s 74us/step - loss: 0.6693 - accuracy: 0.6042 - val_loss: 0.6299 - val_accuracy: 0.7095\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:25 - loss: 0.6922 - accuracy: 0.56 - ETA: 2s - loss: 0.6937 - accuracy: 0.5318 - ETA: 1s - loss: 0.6920 - accuracy: 0.54 - ETA: 1s - loss: 0.6900 - accuracy: 0.55 - ETA: 0s - loss: 0.6881 - accuracy: 0.56 - ETA: 0s - loss: 0.6857 - accuracy: 0.56 - ETA: 0s - loss: 0.6837 - accuracy: 0.57 - ETA: 0s - loss: 0.6801 - accuracy: 0.58 - ETA: 0s - loss: 0.6777 - accuracy: 0.59 - ETA: 0s - loss: 0.6752 - accuracy: 0.60 - ETA: 0s - loss: 0.6724 - accuracy: 0.60 - ETA: 0s - loss: 0.6709 - accuracy: 0.61 - ETA: 0s - loss: 0.6680 - accuracy: 0.61 - ETA: 0s - loss: 0.6650 - accuracy: 0.62 - ETA: 0s - loss: 0.6626 - accuracy: 0.62 - ETA: 0s - loss: 0.6604 - accuracy: 0.62 - 1s 77us/step - loss: 0.6587 - accuracy: 0.6303 - val_loss: 0.6115 - val_accuracy: 0.7202\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:20 - loss: 0.7016 - accuracy: 0.25 - ETA: 2s - loss: 0.6925 - accuracy: 0.5024 - ETA: 1s - loss: 0.6898 - accuracy: 0.52 - ETA: 1s - loss: 0.6907 - accuracy: 0.51 - ETA: 0s - loss: 0.6895 - accuracy: 0.52 - ETA: 0s - loss: 0.6885 - accuracy: 0.53 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6858 - accuracy: 0.55 - ETA: 0s - loss: 0.6839 - accuracy: 0.56 - ETA: 0s - loss: 0.6815 - accuracy: 0.57 - ETA: 0s - loss: 0.6782 - accuracy: 0.58 - ETA: 0s - loss: 0.6756 - accuracy: 0.59 - ETA: 0s - loss: 0.6736 - accuracy: 0.59 - ETA: 0s - loss: 0.6711 - accuracy: 0.60 - ETA: 0s - loss: 0.6685 - accuracy: 0.60 - ETA: 0s - loss: 0.6668 - accuracy: 0.60 - 1s 75us/step - loss: 0.6659 - accuracy: 0.6095 - val_loss: 0.6167 - val_accuracy: 0.7131\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:23 - loss: 0.6898 - accuracy: 0.62 - ETA: 2s - loss: 0.6917 - accuracy: 0.5366 - ETA: 1s - loss: 0.6890 - accuracy: 0.53 - ETA: 1s - loss: 0.6878 - accuracy: 0.54 - ETA: 0s - loss: 0.6865 - accuracy: 0.55 - ETA: 0s - loss: 0.6848 - accuracy: 0.57 - ETA: 0s - loss: 0.6827 - accuracy: 0.58 - ETA: 0s - loss: 0.6796 - accuracy: 0.59 - ETA: 0s - loss: 0.6769 - accuracy: 0.59 - ETA: 0s - loss: 0.6743 - accuracy: 0.60 - ETA: 0s - loss: 0.6719 - accuracy: 0.60 - ETA: 0s - loss: 0.6686 - accuracy: 0.61 - ETA: 0s - loss: 0.6656 - accuracy: 0.61 - ETA: 0s - loss: 0.6626 - accuracy: 0.62 - ETA: 0s - loss: 0.6595 - accuracy: 0.62 - ETA: 0s - loss: 0.6579 - accuracy: 0.63 - 1s 75us/step - loss: 0.6579 - accuracy: 0.6315 - val_loss: 0.6078 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:19 - loss: 0.6995 - accuracy: 0.25 - ETA: 2s - loss: 0.6913 - accuracy: 0.5413 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 1s - loss: 0.6905 - accuracy: 0.53 - ETA: 0s - loss: 0.6884 - accuracy: 0.53 - ETA: 0s - loss: 0.6860 - accuracy: 0.54 - ETA: 0s - loss: 0.6832 - accuracy: 0.55 - ETA: 0s - loss: 0.6802 - accuracy: 0.56 - ETA: 0s - loss: 0.6784 - accuracy: 0.57 - ETA: 0s - loss: 0.6766 - accuracy: 0.57 - ETA: 0s - loss: 0.6745 - accuracy: 0.58 - ETA: 0s - loss: 0.6721 - accuracy: 0.59 - ETA: 0s - loss: 0.6684 - accuracy: 0.59 - ETA: 0s - loss: 0.6654 - accuracy: 0.60 - ETA: 0s - loss: 0.6628 - accuracy: 0.61 - ETA: 0s - loss: 0.6599 - accuracy: 0.61 - 1s 74us/step - loss: 0.6593 - accuracy: 0.6178 - val_loss: 0.6119 - val_accuracy: 0.7173\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:19 - loss: 0.6954 - accuracy: 0.43 - ETA: 2s - loss: 0.6942 - accuracy: 0.5000 - ETA: 1s - loss: 0.6940 - accuracy: 0.50 - ETA: 1s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6924 - accuracy: 0.54 - ETA: 0s - loss: 0.6908 - accuracy: 0.55 - ETA: 0s - loss: 0.6888 - accuracy: 0.57 - ETA: 0s - loss: 0.6873 - accuracy: 0.58 - ETA: 0s - loss: 0.6855 - accuracy: 0.58 - ETA: 0s - loss: 0.6828 - accuracy: 0.59 - ETA: 0s - loss: 0.6803 - accuracy: 0.59 - ETA: 0s - loss: 0.6779 - accuracy: 0.60 - ETA: 0s - loss: 0.6747 - accuracy: 0.60 - ETA: 0s - loss: 0.6726 - accuracy: 0.61 - ETA: 0s - loss: 0.6693 - accuracy: 0.61 - ETA: 0s - loss: 0.6667 - accuracy: 0.62 - 1s 76us/step - loss: 0.6648 - accuracy: 0.6248 - val_loss: 0.6118 - val_accuracy: 0.7109\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:19 - loss: 0.6939 - accuracy: 0.50 - ETA: 2s - loss: 0.6948 - accuracy: 0.4884 - ETA: 1s - loss: 0.6934 - accuracy: 0.50 - ETA: 1s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6912 - accuracy: 0.53 - ETA: 0s - loss: 0.6897 - accuracy: 0.54 - ETA: 0s - loss: 0.6878 - accuracy: 0.55 - ETA: 0s - loss: 0.6852 - accuracy: 0.57 - ETA: 0s - loss: 0.6826 - accuracy: 0.58 - ETA: 0s - loss: 0.6798 - accuracy: 0.59 - ETA: 0s - loss: 0.6771 - accuracy: 0.59 - ETA: 0s - loss: 0.6744 - accuracy: 0.60 - ETA: 0s - loss: 0.6720 - accuracy: 0.60 - ETA: 0s - loss: 0.6689 - accuracy: 0.61 - ETA: 0s - loss: 0.6661 - accuracy: 0.61 - ETA: 0s - loss: 0.6633 - accuracy: 0.62 - 1s 76us/step - loss: 0.6625 - accuracy: 0.6223 - val_loss: 0.6210 - val_accuracy: 0.7109\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:08 - loss: 0.6921 - accuracy: 0.62 - ETA: 1s - loss: 0.6941 - accuracy: 0.4989 - ETA: 1s - loss: 0.6940 - accuracy: 0.48 - ETA: 0s - loss: 0.6939 - accuracy: 0.49 - ETA: 0s - loss: 0.6939 - accuracy: 0.49 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - 1s 70us/step - loss: 0.6934 - accuracy: 0.5102 - val_loss: 0.6922 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 1:09 - loss: 0.6915 - accuracy: 0.43 - ETA: 1s - loss: 0.6940 - accuracy: 0.4634 - ETA: 1s - loss: 0.6939 - accuracy: 0.47 - ETA: 0s - loss: 0.6937 - accuracy: 0.48 - ETA: 0s - loss: 0.6936 - accuracy: 0.49 - ETA: 0s - loss: 0.6936 - accuracy: 0.49 - ETA: 0s - loss: 0.6935 - accuracy: 0.49 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.50 - 1s 70us/step - loss: 0.6931 - accuracy: 0.5097 - val_loss: 0.6915 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:09 - loss: 0.6963 - accuracy: 0.37 - ETA: 1s - loss: 0.6929 - accuracy: 0.5139 - ETA: 1s - loss: 0.6932 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - 1s 70us/step - loss: 0.6924 - accuracy: 0.5156 - val_loss: 0.6905 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:11 - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6943 - accuracy: 0.4621 - ETA: 1s - loss: 0.6942 - accuracy: 0.47 - ETA: 0s - loss: 0.6940 - accuracy: 0.48 - ETA: 0s - loss: 0.6938 - accuracy: 0.49 - ETA: 0s - loss: 0.6937 - accuracy: 0.49 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - 1s 70us/step - loss: 0.6931 - accuracy: 0.5113 - val_loss: 0.6913 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:13 - loss: 0.6938 - accuracy: 0.50 - ETA: 1s - loss: 0.6938 - accuracy: 0.4838 - ETA: 1s - loss: 0.6937 - accuracy: 0.48 - ETA: 0s - loss: 0.6936 - accuracy: 0.49 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - 1s 70us/step - loss: 0.6931 - accuracy: 0.5128 - val_loss: 0.6923 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:10 - loss: 0.6952 - accuracy: 0.37 - ETA: 1s - loss: 0.6938 - accuracy: 0.4857 - ETA: 1s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - 1s 71us/step - loss: 0.6935 - accuracy: 0.5078 - val_loss: 0.6932 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:11 - loss: 0.6937 - accuracy: 0.56 - ETA: 1s - loss: 0.6940 - accuracy: 0.4956 - ETA: 1s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6936 - accuracy: 0.52 - 1s 70us/step - loss: 0.6936 - accuracy: 0.5197 - val_loss: 0.6923 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:09 - loss: 0.6945 - accuracy: 0.56 - ETA: 1s - loss: 0.6943 - accuracy: 0.5033 - ETA: 1s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - 1s 68us/step - loss: 0.6936 - accuracy: 0.5131 - val_loss: 0.6920 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:11 - loss: 0.6906 - accuracy: 0.56 - ETA: 1s - loss: 0.6947 - accuracy: 0.5033 - ETA: 1s - loss: 0.6944 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - 1s 72us/step - loss: 0.6931 - accuracy: 0.5121 - val_loss: 0.6908 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:09 - loss: 0.6930 - accuracy: 0.56 - ETA: 1s - loss: 0.6941 - accuracy: 0.5197 - ETA: 1s - loss: 0.6941 - accuracy: 0.52 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - 1s 70us/step - loss: 0.6932 - accuracy: 0.5180 - val_loss: 0.6907 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:08 - loss: 0.6859 - accuracy: 0.68 - ETA: 1s - loss: 0.6938 - accuracy: 0.5208 - ETA: 1s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - 1s 70us/step - loss: 0.6926 - accuracy: 0.5257 - val_loss: 0.6898 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:09 - loss: 0.6939 - accuracy: 0.56 - ETA: 1s - loss: 0.6939 - accuracy: 0.5068 - ETA: 1s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - 1s 71us/step - loss: 0.6933 - accuracy: 0.5166 - val_loss: 0.6919 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:09 - loss: 0.6963 - accuracy: 0.56 - ETA: 1s - loss: 0.6965 - accuracy: 0.5045 - ETA: 1s - loss: 0.6956 - accuracy: 0.50 - ETA: 0s - loss: 0.6953 - accuracy: 0.50 - ETA: 0s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.52 - ETA: 0s - loss: 0.6939 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6936 - accuracy: 0.53 - 1s 72us/step - loss: 0.6935 - accuracy: 0.5314 - val_loss: 0.6914 - val_accuracy: 0.5661\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:10 - loss: 0.6959 - accuracy: 0.37 - ETA: 1s - loss: 0.6943 - accuracy: 0.4932 - ETA: 1s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.50 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - 1s 71us/step - loss: 0.6920 - accuracy: 0.5135 - val_loss: 0.6886 - val_accuracy: 0.5277\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:09 - loss: 0.6948 - accuracy: 0.62 - ETA: 1s - loss: 0.6954 - accuracy: 0.4978 - ETA: 1s - loss: 0.6951 - accuracy: 0.50 - ETA: 0s - loss: 0.6949 - accuracy: 0.51 - ETA: 0s - loss: 0.6948 - accuracy: 0.51 - ETA: 0s - loss: 0.6947 - accuracy: 0.51 - ETA: 0s - loss: 0.6947 - accuracy: 0.51 - ETA: 0s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - 1s 70us/step - loss: 0.6936 - accuracy: 0.5192 - val_loss: 0.6906 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:13 - loss: 0.6899 - accuracy: 0.56 - ETA: 1s - loss: 0.6949 - accuracy: 0.5012 - ETA: 1s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.52 - ETA: 0s - loss: 0.6936 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - 1s 72us/step - loss: 0.6934 - accuracy: 0.5184 - val_loss: 0.6909 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:09 - loss: 0.6970 - accuracy: 0.31 - ETA: 1s - loss: 0.6943 - accuracy: 0.5399 - ETA: 1s - loss: 0.6945 - accuracy: 0.52 - ETA: 0s - loss: 0.6945 - accuracy: 0.52 - ETA: 0s - loss: 0.6948 - accuracy: 0.51 - ETA: 0s - loss: 0.6948 - accuracy: 0.51 - ETA: 0s - loss: 0.6947 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.52 - ETA: 0s - loss: 0.6943 - accuracy: 0.52 - ETA: 0s - loss: 0.6944 - accuracy: 0.52 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - 1s 69us/step - loss: 0.6945 - accuracy: 0.5153 - val_loss: 0.6935 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:09 - loss: 0.6978 - accuracy: 0.25 - ETA: 1s - loss: 0.6950 - accuracy: 0.5011 - ETA: 1s - loss: 0.6948 - accuracy: 0.51 - ETA: 0s - loss: 0.6948 - accuracy: 0.50 - ETA: 0s - loss: 0.6947 - accuracy: 0.50 - ETA: 0s - loss: 0.6947 - accuracy: 0.50 - ETA: 0s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.52 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - 1s 71us/step - loss: 0.6937 - accuracy: 0.5165 - val_loss: 0.6924 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:31 - loss: 0.6932 - accuracy: 0.43 - ETA: 2s - loss: 0.6938 - accuracy: 0.4796 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6914 - accuracy: 0.52 - ETA: 0s - loss: 0.6907 - accuracy: 0.52 - ETA: 0s - loss: 0.6898 - accuracy: 0.53 - ETA: 0s - loss: 0.6886 - accuracy: 0.53 - ETA: 0s - loss: 0.6870 - accuracy: 0.54 - ETA: 0s - loss: 0.6851 - accuracy: 0.54 - ETA: 0s - loss: 0.6831 - accuracy: 0.55 - ETA: 0s - loss: 0.6820 - accuracy: 0.55 - ETA: 0s - loss: 0.6808 - accuracy: 0.56 - ETA: 0s - loss: 0.6791 - accuracy: 0.56 - ETA: 0s - loss: 0.6774 - accuracy: 0.56 - 1s 80us/step - loss: 0.6766 - accuracy: 0.5695 - val_loss: 0.6360 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:32 - loss: 0.6939 - accuracy: 0.50 - ETA: 2s - loss: 0.6934 - accuracy: 0.5084 - ETA: 1s - loss: 0.6923 - accuracy: 0.53 - ETA: 1s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6909 - accuracy: 0.53 - ETA: 0s - loss: 0.6901 - accuracy: 0.54 - ETA: 0s - loss: 0.6893 - accuracy: 0.55 - ETA: 0s - loss: 0.6882 - accuracy: 0.56 - ETA: 0s - loss: 0.6866 - accuracy: 0.57 - ETA: 0s - loss: 0.6854 - accuracy: 0.57 - ETA: 0s - loss: 0.6844 - accuracy: 0.58 - ETA: 0s - loss: 0.6826 - accuracy: 0.58 - ETA: 0s - loss: 0.6814 - accuracy: 0.59 - ETA: 0s - loss: 0.6805 - accuracy: 0.59 - ETA: 0s - loss: 0.6791 - accuracy: 0.59 - ETA: 0s - loss: 0.6779 - accuracy: 0.60 - ETA: 0s - loss: 0.6761 - accuracy: 0.60 - ETA: 0s - loss: 0.6741 - accuracy: 0.61 - 1s 83us/step - loss: 0.6738 - accuracy: 0.6152 - val_loss: 0.6425 - val_accuracy: 0.7024\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:30 - loss: 0.6944 - accuracy: 0.50 - ETA: 2s - loss: 0.6934 - accuracy: 0.5102 - ETA: 1s - loss: 0.6931 - accuracy: 0.50 - ETA: 1s - loss: 0.6923 - accuracy: 0.50 - ETA: 0s - loss: 0.6912 - accuracy: 0.51 - ETA: 0s - loss: 0.6906 - accuracy: 0.51 - ETA: 0s - loss: 0.6896 - accuracy: 0.52 - ETA: 0s - loss: 0.6884 - accuracy: 0.53 - ETA: 0s - loss: 0.6870 - accuracy: 0.53 - ETA: 0s - loss: 0.6855 - accuracy: 0.53 - ETA: 0s - loss: 0.6837 - accuracy: 0.54 - ETA: 0s - loss: 0.6815 - accuracy: 0.55 - ETA: 0s - loss: 0.6795 - accuracy: 0.55 - ETA: 0s - loss: 0.6777 - accuracy: 0.55 - ETA: 0s - loss: 0.6762 - accuracy: 0.56 - ETA: 0s - loss: 0.6736 - accuracy: 0.56 - 1s 76us/step - loss: 0.6728 - accuracy: 0.5700 - val_loss: 0.6246 - val_accuracy: 0.7152\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 26us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:30 - loss: 0.6934 - accuracy: 0.50 - ETA: 2s - loss: 0.6936 - accuracy: 0.4743 - ETA: 1s - loss: 0.6934 - accuracy: 0.49 - ETA: 1s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6925 - accuracy: 0.50 - ETA: 0s - loss: 0.6917 - accuracy: 0.50 - ETA: 0s - loss: 0.6904 - accuracy: 0.51 - ETA: 0s - loss: 0.6902 - accuracy: 0.51 - ETA: 0s - loss: 0.6899 - accuracy: 0.51 - ETA: 0s - loss: 0.6897 - accuracy: 0.51 - ETA: 0s - loss: 0.6895 - accuracy: 0.51 - ETA: 0s - loss: 0.6888 - accuracy: 0.51 - ETA: 0s - loss: 0.6882 - accuracy: 0.51 - ETA: 0s - loss: 0.6871 - accuracy: 0.51 - ETA: 0s - loss: 0.6863 - accuracy: 0.51 - ETA: 0s - loss: 0.6854 - accuracy: 0.52 - ETA: 0s - loss: 0.6850 - accuracy: 0.52 - 1s 81us/step - loss: 0.6847 - accuracy: 0.5239 - val_loss: 0.6687 - val_accuracy: 0.6847\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:28 - loss: 0.6934 - accuracy: 0.56 - ETA: 2s - loss: 0.6935 - accuracy: 0.5000 - ETA: 1s - loss: 0.6932 - accuracy: 0.50 - ETA: 1s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6918 - accuracy: 0.51 - ETA: 0s - loss: 0.6902 - accuracy: 0.52 - ETA: 0s - loss: 0.6893 - accuracy: 0.52 - ETA: 0s - loss: 0.6886 - accuracy: 0.51 - ETA: 0s - loss: 0.6867 - accuracy: 0.52 - ETA: 0s - loss: 0.6850 - accuracy: 0.52 - ETA: 0s - loss: 0.6842 - accuracy: 0.53 - ETA: 0s - loss: 0.6835 - accuracy: 0.54 - ETA: 0s - loss: 0.6826 - accuracy: 0.54 - ETA: 0s - loss: 0.6811 - accuracy: 0.55 - ETA: 0s - loss: 0.6800 - accuracy: 0.55 - ETA: 0s - loss: 0.6792 - accuracy: 0.56 - 1s 78us/step - loss: 0.6783 - accuracy: 0.5692 - val_loss: 0.6505 - val_accuracy: 0.6982\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:31 - loss: 0.6961 - accuracy: 0.37 - ETA: 2s - loss: 0.6935 - accuracy: 0.4853 - ETA: 1s - loss: 0.6932 - accuracy: 0.49 - ETA: 1s - loss: 0.6926 - accuracy: 0.50 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6911 - accuracy: 0.52 - ETA: 0s - loss: 0.6899 - accuracy: 0.53 - ETA: 0s - loss: 0.6887 - accuracy: 0.53 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6856 - accuracy: 0.55 - ETA: 0s - loss: 0.6846 - accuracy: 0.55 - ETA: 0s - loss: 0.6823 - accuracy: 0.56 - ETA: 0s - loss: 0.6809 - accuracy: 0.56 - ETA: 0s - loss: 0.6794 - accuracy: 0.56 - ETA: 0s - loss: 0.6778 - accuracy: 0.57 - ETA: 0s - loss: 0.6759 - accuracy: 0.57 - ETA: 0s - loss: 0.6745 - accuracy: 0.58 - 1s 80us/step - loss: 0.6745 - accuracy: 0.5812 - val_loss: 0.6400 - val_accuracy: 0.7053\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 30us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:34 - loss: 0.6897 - accuracy: 0.93 - ETA: 2s - loss: 0.6935 - accuracy: 0.5091 - ETA: 1s - loss: 0.6921 - accuracy: 0.52 - ETA: 1s - loss: 0.6916 - accuracy: 0.52 - ETA: 1s - loss: 0.6910 - accuracy: 0.52 - ETA: 0s - loss: 0.6905 - accuracy: 0.52 - ETA: 0s - loss: 0.6897 - accuracy: 0.52 - ETA: 0s - loss: 0.6886 - accuracy: 0.53 - ETA: 0s - loss: 0.6867 - accuracy: 0.54 - ETA: 0s - loss: 0.6855 - accuracy: 0.55 - ETA: 0s - loss: 0.6843 - accuracy: 0.55 - ETA: 0s - loss: 0.6827 - accuracy: 0.56 - ETA: 0s - loss: 0.6809 - accuracy: 0.56 - ETA: 0s - loss: 0.6792 - accuracy: 0.57 - ETA: 0s - loss: 0.6778 - accuracy: 0.57 - ETA: 0s - loss: 0.6759 - accuracy: 0.57 - ETA: 0s - loss: 0.6749 - accuracy: 0.58 - ETA: 0s - loss: 0.6721 - accuracy: 0.58 - ETA: 0s - loss: 0.6708 - accuracy: 0.59 - ETA: 0s - loss: 0.6697 - accuracy: 0.59 - ETA: 0s - loss: 0.6688 - accuracy: 0.59 - 1s 97us/step - loss: 0.6681 - accuracy: 0.5985 - val_loss: 0.6179 - val_accuracy: 0.7045\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:31 - loss: 0.6944 - accuracy: 0.50 - ETA: 2s - loss: 0.6940 - accuracy: 0.5131 - ETA: 1s - loss: 0.6936 - accuracy: 0.51 - ETA: 1s - loss: 0.6929 - accuracy: 0.52 - ETA: 1s - loss: 0.6913 - accuracy: 0.54 - ETA: 1s - loss: 0.6911 - accuracy: 0.54 - ETA: 0s - loss: 0.6897 - accuracy: 0.55 - ETA: 0s - loss: 0.6886 - accuracy: 0.56 - ETA: 0s - loss: 0.6868 - accuracy: 0.56 - ETA: 0s - loss: 0.6851 - accuracy: 0.57 - ETA: 0s - loss: 0.6823 - accuracy: 0.58 - ETA: 0s - loss: 0.6794 - accuracy: 0.59 - ETA: 0s - loss: 0.6776 - accuracy: 0.59 - ETA: 0s - loss: 0.6746 - accuracy: 0.59 - ETA: 0s - loss: 0.6732 - accuracy: 0.60 - ETA: 0s - loss: 0.6711 - accuracy: 0.60 - ETA: 0s - loss: 0.6680 - accuracy: 0.61 - ETA: 0s - loss: 0.6665 - accuracy: 0.61 - ETA: 0s - loss: 0.6638 - accuracy: 0.61 - ETA: 0s - loss: 0.6614 - accuracy: 0.62 - ETA: 0s - loss: 0.6587 - accuracy: 0.62 - 1s 95us/step - loss: 0.6587 - accuracy: 0.6258 - val_loss: 0.6042 - val_accuracy: 0.7188\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:30 - loss: 0.6944 - accuracy: 0.50 - ETA: 2s - loss: 0.6939 - accuracy: 0.5041 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6927 - accuracy: 0.53 - ETA: 0s - loss: 0.6912 - accuracy: 0.55 - ETA: 0s - loss: 0.6887 - accuracy: 0.56 - ETA: 0s - loss: 0.6860 - accuracy: 0.57 - ETA: 0s - loss: 0.6840 - accuracy: 0.57 - ETA: 0s - loss: 0.6820 - accuracy: 0.58 - ETA: 0s - loss: 0.6791 - accuracy: 0.58 - ETA: 0s - loss: 0.6764 - accuracy: 0.59 - ETA: 0s - loss: 0.6739 - accuracy: 0.59 - ETA: 0s - loss: 0.6715 - accuracy: 0.60 - ETA: 0s - loss: 0.6697 - accuracy: 0.60 - ETA: 0s - loss: 0.6671 - accuracy: 0.61 - ETA: 0s - loss: 0.6648 - accuracy: 0.61 - ETA: 0s - loss: 0.6623 - accuracy: 0.62 - ETA: 0s - loss: 0.6597 - accuracy: 0.62 - ETA: 0s - loss: 0.6576 - accuracy: 0.62 - 1s 95us/step - loss: 0.6558 - accuracy: 0.6296 - val_loss: 0.6008 - val_accuracy: 0.7138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:42 - loss: 0.6915 - accuracy: 0.56 - ETA: 2s - loss: 0.6934 - accuracy: 0.5195 - ETA: 1s - loss: 0.6928 - accuracy: 0.51 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6912 - accuracy: 0.54 - ETA: 0s - loss: 0.6904 - accuracy: 0.54 - ETA: 0s - loss: 0.6880 - accuracy: 0.56 - ETA: 0s - loss: 0.6847 - accuracy: 0.57 - ETA: 0s - loss: 0.6826 - accuracy: 0.57 - ETA: 0s - loss: 0.6801 - accuracy: 0.58 - ETA: 0s - loss: 0.6785 - accuracy: 0.58 - ETA: 0s - loss: 0.6762 - accuracy: 0.59 - ETA: 0s - loss: 0.6726 - accuracy: 0.60 - ETA: 0s - loss: 0.6707 - accuracy: 0.60 - ETA: 0s - loss: 0.6685 - accuracy: 0.61 - ETA: 0s - loss: 0.6657 - accuracy: 0.61 - ETA: 0s - loss: 0.6632 - accuracy: 0.62 - 1s 83us/step - loss: 0.6618 - accuracy: 0.6216 - val_loss: 0.6133 - val_accuracy: 0.7166\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:38 - loss: 0.6935 - accuracy: 0.62 - ETA: 2s - loss: 0.6938 - accuracy: 0.5242 - ETA: 1s - loss: 0.6931 - accuracy: 0.54 - ETA: 1s - loss: 0.6924 - accuracy: 0.55 - ETA: 1s - loss: 0.6913 - accuracy: 0.56 - ETA: 0s - loss: 0.6893 - accuracy: 0.57 - ETA: 0s - loss: 0.6870 - accuracy: 0.58 - ETA: 0s - loss: 0.6850 - accuracy: 0.58 - ETA: 0s - loss: 0.6826 - accuracy: 0.59 - ETA: 0s - loss: 0.6792 - accuracy: 0.60 - ETA: 0s - loss: 0.6763 - accuracy: 0.60 - ETA: 0s - loss: 0.6740 - accuracy: 0.60 - ETA: 0s - loss: 0.6706 - accuracy: 0.61 - ETA: 0s - loss: 0.6686 - accuracy: 0.61 - ETA: 0s - loss: 0.6666 - accuracy: 0.61 - ETA: 0s - loss: 0.6650 - accuracy: 0.62 - ETA: 0s - loss: 0.6642 - accuracy: 0.62 - 1s 80us/step - loss: 0.6642 - accuracy: 0.6221 - val_loss: 0.6194 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:34 - loss: 0.6869 - accuracy: 0.68 - ETA: 2s - loss: 0.6932 - accuracy: 0.5088 - ETA: 1s - loss: 0.6913 - accuracy: 0.53 - ETA: 1s - loss: 0.6900 - accuracy: 0.55 - ETA: 0s - loss: 0.6888 - accuracy: 0.55 - ETA: 0s - loss: 0.6870 - accuracy: 0.56 - ETA: 0s - loss: 0.6858 - accuracy: 0.56 - ETA: 0s - loss: 0.6842 - accuracy: 0.57 - ETA: 0s - loss: 0.6829 - accuracy: 0.57 - ETA: 0s - loss: 0.6811 - accuracy: 0.57 - ETA: 0s - loss: 0.6790 - accuracy: 0.58 - ETA: 0s - loss: 0.6763 - accuracy: 0.58 - ETA: 0s - loss: 0.6739 - accuracy: 0.59 - ETA: 0s - loss: 0.6714 - accuracy: 0.59 - ETA: 0s - loss: 0.6686 - accuracy: 0.60 - ETA: 0s - loss: 0.6665 - accuracy: 0.60 - ETA: 0s - loss: 0.6636 - accuracy: 0.61 - 1s 81us/step - loss: 0.6623 - accuracy: 0.6119 - val_loss: 0.6184 - val_accuracy: 0.7003\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:38 - loss: 0.6971 - accuracy: 0.43 - ETA: 2s - loss: 0.6947 - accuracy: 0.5000 - ETA: 1s - loss: 0.6934 - accuracy: 0.50 - ETA: 1s - loss: 0.6926 - accuracy: 0.50 - ETA: 1s - loss: 0.6908 - accuracy: 0.51 - ETA: 0s - loss: 0.6890 - accuracy: 0.51 - ETA: 0s - loss: 0.6872 - accuracy: 0.52 - ETA: 0s - loss: 0.6851 - accuracy: 0.53 - ETA: 0s - loss: 0.6820 - accuracy: 0.55 - ETA: 0s - loss: 0.6791 - accuracy: 0.56 - ETA: 0s - loss: 0.6764 - accuracy: 0.57 - ETA: 0s - loss: 0.6738 - accuracy: 0.58 - ETA: 0s - loss: 0.6711 - accuracy: 0.58 - ETA: 0s - loss: 0.6691 - accuracy: 0.59 - ETA: 0s - loss: 0.6674 - accuracy: 0.59 - ETA: 0s - loss: 0.6653 - accuracy: 0.60 - ETA: 0s - loss: 0.6617 - accuracy: 0.60 - ETA: 0s - loss: 0.6593 - accuracy: 0.61 - 1s 84us/step - loss: 0.6591 - accuracy: 0.6129 - val_loss: 0.6037 - val_accuracy: 0.7251\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:41 - loss: 0.6935 - accuracy: 0.50 - ETA: 2s - loss: 0.6941 - accuracy: 0.5213 - ETA: 1s - loss: 0.6925 - accuracy: 0.54 - ETA: 1s - loss: 0.6897 - accuracy: 0.56 - ETA: 1s - loss: 0.6873 - accuracy: 0.56 - ETA: 0s - loss: 0.6845 - accuracy: 0.57 - ETA: 0s - loss: 0.6820 - accuracy: 0.57 - ETA: 0s - loss: 0.6792 - accuracy: 0.58 - ETA: 0s - loss: 0.6768 - accuracy: 0.59 - ETA: 0s - loss: 0.6739 - accuracy: 0.59 - ETA: 0s - loss: 0.6700 - accuracy: 0.60 - ETA: 0s - loss: 0.6672 - accuracy: 0.60 - ETA: 0s - loss: 0.6643 - accuracy: 0.61 - ETA: 0s - loss: 0.6616 - accuracy: 0.61 - ETA: 0s - loss: 0.6586 - accuracy: 0.62 - ETA: 0s - loss: 0.6560 - accuracy: 0.62 - ETA: 0s - loss: 0.6532 - accuracy: 0.63 - 1s 81us/step - loss: 0.6520 - accuracy: 0.6328 - val_loss: 0.5932 - val_accuracy: 0.7188\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 30us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:30 - loss: 0.6925 - accuracy: 0.62 - ETA: 2s - loss: 0.6937 - accuracy: 0.5325 - ETA: 1s - loss: 0.6925 - accuracy: 0.55 - ETA: 1s - loss: 0.6892 - accuracy: 0.57 - ETA: 1s - loss: 0.6867 - accuracy: 0.57 - ETA: 0s - loss: 0.6834 - accuracy: 0.58 - ETA: 0s - loss: 0.6801 - accuracy: 0.59 - ETA: 0s - loss: 0.6750 - accuracy: 0.60 - ETA: 0s - loss: 0.6722 - accuracy: 0.60 - ETA: 0s - loss: 0.6679 - accuracy: 0.61 - ETA: 0s - loss: 0.6634 - accuracy: 0.62 - ETA: 0s - loss: 0.6592 - accuracy: 0.63 - ETA: 0s - loss: 0.6560 - accuracy: 0.63 - ETA: 0s - loss: 0.6536 - accuracy: 0.63 - ETA: 0s - loss: 0.6509 - accuracy: 0.64 - ETA: 0s - loss: 0.6485 - accuracy: 0.64 - ETA: 0s - loss: 0.6459 - accuracy: 0.64 - 1s 81us/step - loss: 0.6450 - accuracy: 0.6479 - val_loss: 0.5915 - val_accuracy: 0.7131\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:38 - loss: 0.7056 - accuracy: 0.37 - ETA: 2s - loss: 0.6944 - accuracy: 0.5204 - ETA: 1s - loss: 0.6931 - accuracy: 0.54 - ETA: 1s - loss: 0.6921 - accuracy: 0.54 - ETA: 1s - loss: 0.6901 - accuracy: 0.56 - ETA: 0s - loss: 0.6874 - accuracy: 0.57 - ETA: 0s - loss: 0.6840 - accuracy: 0.58 - ETA: 0s - loss: 0.6810 - accuracy: 0.58 - ETA: 0s - loss: 0.6783 - accuracy: 0.59 - ETA: 0s - loss: 0.6744 - accuracy: 0.60 - ETA: 0s - loss: 0.6710 - accuracy: 0.60 - ETA: 0s - loss: 0.6675 - accuracy: 0.61 - ETA: 0s - loss: 0.6649 - accuracy: 0.61 - ETA: 0s - loss: 0.6613 - accuracy: 0.62 - ETA: 0s - loss: 0.6591 - accuracy: 0.62 - ETA: 0s - loss: 0.6564 - accuracy: 0.62 - ETA: 0s - loss: 0.6538 - accuracy: 0.63 - 1s 81us/step - loss: 0.6525 - accuracy: 0.6358 - val_loss: 0.5982 - val_accuracy: 0.7195\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:40 - loss: 0.6974 - accuracy: 0.43 - ETA: 2s - loss: 0.6934 - accuracy: 0.5331 - ETA: 1s - loss: 0.6922 - accuracy: 0.54 - ETA: 1s - loss: 0.6907 - accuracy: 0.55 - ETA: 1s - loss: 0.6892 - accuracy: 0.55 - ETA: 0s - loss: 0.6869 - accuracy: 0.57 - ETA: 0s - loss: 0.6838 - accuracy: 0.58 - ETA: 0s - loss: 0.6807 - accuracy: 0.58 - ETA: 0s - loss: 0.6774 - accuracy: 0.59 - ETA: 0s - loss: 0.6739 - accuracy: 0.60 - ETA: 0s - loss: 0.6701 - accuracy: 0.60 - ETA: 0s - loss: 0.6670 - accuracy: 0.60 - ETA: 0s - loss: 0.6637 - accuracy: 0.61 - ETA: 0s - loss: 0.6605 - accuracy: 0.61 - ETA: 0s - loss: 0.6571 - accuracy: 0.62 - ETA: 0s - loss: 0.6552 - accuracy: 0.62 - ETA: 0s - loss: 0.6534 - accuracy: 0.63 - ETA: 0s - loss: 0.6513 - accuracy: 0.63 - 1s 85us/step - loss: 0.6509 - accuracy: 0.6355 - val_loss: 0.5960 - val_accuracy: 0.7145\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 30us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:38 - loss: 0.6929 - accuracy: 0.68 - ETA: 2s - loss: 0.6924 - accuracy: 0.5740 - ETA: 1s - loss: 0.6911 - accuracy: 0.57 - ETA: 1s - loss: 0.6886 - accuracy: 0.57 - ETA: 0s - loss: 0.6868 - accuracy: 0.57 - ETA: 0s - loss: 0.6832 - accuracy: 0.58 - ETA: 0s - loss: 0.6794 - accuracy: 0.59 - ETA: 0s - loss: 0.6770 - accuracy: 0.59 - ETA: 0s - loss: 0.6737 - accuracy: 0.60 - ETA: 0s - loss: 0.6707 - accuracy: 0.61 - ETA: 0s - loss: 0.6674 - accuracy: 0.61 - ETA: 0s - loss: 0.6637 - accuracy: 0.62 - ETA: 0s - loss: 0.6601 - accuracy: 0.62 - ETA: 0s - loss: 0.6567 - accuracy: 0.63 - ETA: 0s - loss: 0.6545 - accuracy: 0.63 - ETA: 0s - loss: 0.6520 - accuracy: 0.63 - ETA: 0s - loss: 0.6484 - accuracy: 0.64 - 1s 81us/step - loss: 0.6470 - accuracy: 0.6470 - val_loss: 0.5968 - val_accuracy: 0.7195\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 30us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:16 - loss: 0.6921 - accuracy: 0.75 - ETA: 2s - loss: 0.6930 - accuracy: 0.5248 - ETA: 1s - loss: 0.6917 - accuracy: 0.53 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 0s - loss: 0.6916 - accuracy: 0.52 - ETA: 0s - loss: 0.6913 - accuracy: 0.52 - ETA: 0s - loss: 0.6908 - accuracy: 0.53 - ETA: 0s - loss: 0.6900 - accuracy: 0.53 - ETA: 0s - loss: 0.6896 - accuracy: 0.54 - ETA: 0s - loss: 0.6889 - accuracy: 0.54 - ETA: 0s - loss: 0.6883 - accuracy: 0.54 - ETA: 0s - loss: 0.6876 - accuracy: 0.54 - ETA: 0s - loss: 0.6872 - accuracy: 0.54 - ETA: 0s - loss: 0.6867 - accuracy: 0.55 - ETA: 0s - loss: 0.6860 - accuracy: 0.55 - ETA: 0s - loss: 0.6853 - accuracy: 0.55 - 1s 75us/step - loss: 0.6852 - accuracy: 0.5533 - val_loss: 0.6721 - val_accuracy: 0.7024\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:18 - loss: 0.6931 - accuracy: 0.62 - ETA: 2s - loss: 0.6916 - accuracy: 0.5330 - ETA: 1s - loss: 0.6902 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.51 - ETA: 0s - loss: 0.6909 - accuracy: 0.51 - ETA: 0s - loss: 0.6906 - accuracy: 0.51 - ETA: 0s - loss: 0.6909 - accuracy: 0.51 - ETA: 0s - loss: 0.6906 - accuracy: 0.51 - ETA: 0s - loss: 0.6902 - accuracy: 0.51 - ETA: 0s - loss: 0.6902 - accuracy: 0.51 - ETA: 0s - loss: 0.6897 - accuracy: 0.51 - ETA: 0s - loss: 0.6896 - accuracy: 0.51 - ETA: 0s - loss: 0.6894 - accuracy: 0.51 - ETA: 0s - loss: 0.6892 - accuracy: 0.51 - ETA: 0s - loss: 0.6888 - accuracy: 0.51 - ETA: 0s - loss: 0.6884 - accuracy: 0.51 - 1s 76us/step - loss: 0.6882 - accuracy: 0.5146 - val_loss: 0.6806 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:19 - loss: 0.6938 - accuracy: 0.62 - ETA: 2s - loss: 0.6925 - accuracy: 0.5362 - ETA: 1s - loss: 0.6920 - accuracy: 0.51 - ETA: 1s - loss: 0.6919 - accuracy: 0.51 - ETA: 0s - loss: 0.6906 - accuracy: 0.51 - ETA: 0s - loss: 0.6891 - accuracy: 0.51 - ETA: 0s - loss: 0.6883 - accuracy: 0.52 - ETA: 0s - loss: 0.6870 - accuracy: 0.52 - ETA: 0s - loss: 0.6862 - accuracy: 0.51 - ETA: 0s - loss: 0.6852 - accuracy: 0.51 - ETA: 0s - loss: 0.6842 - accuracy: 0.51 - ETA: 0s - loss: 0.6837 - accuracy: 0.51 - ETA: 0s - loss: 0.6837 - accuracy: 0.51 - ETA: 0s - loss: 0.6831 - accuracy: 0.52 - ETA: 0s - loss: 0.6825 - accuracy: 0.52 - ETA: 0s - loss: 0.6819 - accuracy: 0.52 - 1s 75us/step - loss: 0.6817 - accuracy: 0.5298 - val_loss: 0.6675 - val_accuracy: 0.6101\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:17 - loss: 0.6941 - accuracy: 0.56 - ETA: 2s - loss: 0.6932 - accuracy: 0.5212 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 1s - loss: 0.6913 - accuracy: 0.54 - ETA: 0s - loss: 0.6903 - accuracy: 0.55 - ETA: 0s - loss: 0.6897 - accuracy: 0.55 - ETA: 0s - loss: 0.6889 - accuracy: 0.56 - ETA: 0s - loss: 0.6885 - accuracy: 0.55 - ETA: 0s - loss: 0.6878 - accuracy: 0.56 - ETA: 0s - loss: 0.6871 - accuracy: 0.56 - ETA: 0s - loss: 0.6862 - accuracy: 0.56 - ETA: 0s - loss: 0.6856 - accuracy: 0.56 - ETA: 0s - loss: 0.6848 - accuracy: 0.56 - ETA: 0s - loss: 0.6845 - accuracy: 0.56 - ETA: 0s - loss: 0.6836 - accuracy: 0.56 - 1s 76us/step - loss: 0.6831 - accuracy: 0.5691 - val_loss: 0.6694 - val_accuracy: 0.7024\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:18 - loss: 0.6917 - accuracy: 0.62 - ETA: 2s - loss: 0.6923 - accuracy: 0.5130 - ETA: 1s - loss: 0.6908 - accuracy: 0.51 - ETA: 1s - loss: 0.6895 - accuracy: 0.52 - ETA: 0s - loss: 0.6887 - accuracy: 0.53 - ETA: 0s - loss: 0.6865 - accuracy: 0.55 - ETA: 0s - loss: 0.6847 - accuracy: 0.56 - ETA: 0s - loss: 0.6833 - accuracy: 0.56 - ETA: 0s - loss: 0.6816 - accuracy: 0.57 - ETA: 0s - loss: 0.6801 - accuracy: 0.57 - ETA: 0s - loss: 0.6793 - accuracy: 0.58 - ETA: 0s - loss: 0.6781 - accuracy: 0.58 - ETA: 0s - loss: 0.6774 - accuracy: 0.58 - ETA: 0s - loss: 0.6760 - accuracy: 0.58 - ETA: 0s - loss: 0.6747 - accuracy: 0.59 - ETA: 0s - loss: 0.6740 - accuracy: 0.59 - 1s 74us/step - loss: 0.6740 - accuracy: 0.5936 - val_loss: 0.6502 - val_accuracy: 0.7102\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 30us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:27 - loss: 0.6937 - accuracy: 0.43 - ETA: 2s - loss: 0.6925 - accuracy: 0.5436 - ETA: 1s - loss: 0.6919 - accuracy: 0.54 - ETA: 1s - loss: 0.6912 - accuracy: 0.55 - ETA: 0s - loss: 0.6901 - accuracy: 0.56 - ETA: 0s - loss: 0.6893 - accuracy: 0.56 - ETA: 0s - loss: 0.6891 - accuracy: 0.56 - ETA: 0s - loss: 0.6885 - accuracy: 0.57 - ETA: 0s - loss: 0.6879 - accuracy: 0.57 - ETA: 0s - loss: 0.6870 - accuracy: 0.57 - ETA: 0s - loss: 0.6860 - accuracy: 0.57 - ETA: 0s - loss: 0.6851 - accuracy: 0.57 - ETA: 0s - loss: 0.6846 - accuracy: 0.57 - ETA: 0s - loss: 0.6840 - accuracy: 0.57 - ETA: 0s - loss: 0.6833 - accuracy: 0.57 - ETA: 0s - loss: 0.6826 - accuracy: 0.57 - 1s 76us/step - loss: 0.6825 - accuracy: 0.5748 - val_loss: 0.6700 - val_accuracy: 0.7017\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 30us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:19 - loss: 0.6948 - accuracy: 0.50 - ETA: 2s - loss: 0.6927 - accuracy: 0.5300 - ETA: 1s - loss: 0.6909 - accuracy: 0.56 - ETA: 1s - loss: 0.6891 - accuracy: 0.58 - ETA: 0s - loss: 0.6871 - accuracy: 0.59 - ETA: 0s - loss: 0.6852 - accuracy: 0.60 - ETA: 0s - loss: 0.6828 - accuracy: 0.60 - ETA: 0s - loss: 0.6809 - accuracy: 0.61 - ETA: 0s - loss: 0.6790 - accuracy: 0.61 - ETA: 0s - loss: 0.6760 - accuracy: 0.62 - ETA: 0s - loss: 0.6739 - accuracy: 0.62 - ETA: 0s - loss: 0.6724 - accuracy: 0.62 - ETA: 0s - loss: 0.6704 - accuracy: 0.63 - ETA: 0s - loss: 0.6694 - accuracy: 0.62 - ETA: 0s - loss: 0.6683 - accuracy: 0.63 - ETA: 0s - loss: 0.6669 - accuracy: 0.63 - ETA: 0s - loss: 0.6656 - accuracy: 0.63 - 1s 78us/step - loss: 0.6655 - accuracy: 0.6339 - val_loss: 0.6335 - val_accuracy: 0.7152\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 1:17 - loss: 0.6931 - accuracy: 0.56 - ETA: 2s - loss: 0.6903 - accuracy: 0.5289 - ETA: 1s - loss: 0.6881 - accuracy: 0.52 - ETA: 1s - loss: 0.6872 - accuracy: 0.53 - ETA: 0s - loss: 0.6849 - accuracy: 0.55 - ETA: 0s - loss: 0.6832 - accuracy: 0.56 - ETA: 0s - loss: 0.6817 - accuracy: 0.57 - ETA: 0s - loss: 0.6803 - accuracy: 0.58 - ETA: 0s - loss: 0.6781 - accuracy: 0.58 - ETA: 0s - loss: 0.6771 - accuracy: 0.59 - ETA: 0s - loss: 0.6750 - accuracy: 0.59 - ETA: 0s - loss: 0.6726 - accuracy: 0.60 - ETA: 0s - loss: 0.6701 - accuracy: 0.60 - ETA: 0s - loss: 0.6681 - accuracy: 0.61 - ETA: 0s - loss: 0.6669 - accuracy: 0.61 - ETA: 0s - loss: 0.6651 - accuracy: 0.61 - 1s 77us/step - loss: 0.6642 - accuracy: 0.6199 - val_loss: 0.6311 - val_accuracy: 0.7102\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:17 - loss: 0.6915 - accuracy: 0.68 - ETA: 2s - loss: 0.6903 - accuracy: 0.5478 - ETA: 1s - loss: 0.6879 - accuracy: 0.55 - ETA: 1s - loss: 0.6869 - accuracy: 0.55 - ETA: 0s - loss: 0.6857 - accuracy: 0.55 - ETA: 0s - loss: 0.6841 - accuracy: 0.55 - ETA: 0s - loss: 0.6821 - accuracy: 0.56 - ETA: 0s - loss: 0.6811 - accuracy: 0.56 - ETA: 0s - loss: 0.6798 - accuracy: 0.57 - ETA: 0s - loss: 0.6788 - accuracy: 0.57 - ETA: 0s - loss: 0.6777 - accuracy: 0.57 - ETA: 0s - loss: 0.6758 - accuracy: 0.57 - ETA: 0s - loss: 0.6747 - accuracy: 0.57 - ETA: 0s - loss: 0.6735 - accuracy: 0.58 - ETA: 0s - loss: 0.6720 - accuracy: 0.58 - ETA: 0s - loss: 0.6713 - accuracy: 0.58 - 1s 75us/step - loss: 0.6711 - accuracy: 0.5878 - val_loss: 0.6485 - val_accuracy: 0.7060\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:19 - loss: 0.6907 - accuracy: 0.56 - ETA: 2s - loss: 0.6918 - accuracy: 0.5295 - ETA: 1s - loss: 0.6897 - accuracy: 0.54 - ETA: 1s - loss: 0.6887 - accuracy: 0.53 - ETA: 0s - loss: 0.6873 - accuracy: 0.54 - ETA: 0s - loss: 0.6865 - accuracy: 0.55 - ETA: 0s - loss: 0.6838 - accuracy: 0.55 - ETA: 0s - loss: 0.6822 - accuracy: 0.56 - ETA: 0s - loss: 0.6804 - accuracy: 0.56 - ETA: 0s - loss: 0.6790 - accuracy: 0.57 - ETA: 0s - loss: 0.6779 - accuracy: 0.57 - ETA: 0s - loss: 0.6771 - accuracy: 0.58 - ETA: 0s - loss: 0.6750 - accuracy: 0.58 - ETA: 0s - loss: 0.6740 - accuracy: 0.58 - ETA: 0s - loss: 0.6728 - accuracy: 0.59 - ETA: 0s - loss: 0.6711 - accuracy: 0.59 - 1s 74us/step - loss: 0.6710 - accuracy: 0.6001 - val_loss: 0.6443 - val_accuracy: 0.6932\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:19 - loss: 0.6925 - accuracy: 0.62 - ETA: 2s - loss: 0.6939 - accuracy: 0.5035 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6907 - accuracy: 0.54 - ETA: 0s - loss: 0.6894 - accuracy: 0.55 - ETA: 0s - loss: 0.6887 - accuracy: 0.55 - ETA: 0s - loss: 0.6872 - accuracy: 0.56 - ETA: 0s - loss: 0.6861 - accuracy: 0.56 - ETA: 0s - loss: 0.6847 - accuracy: 0.57 - ETA: 0s - loss: 0.6829 - accuracy: 0.57 - ETA: 0s - loss: 0.6810 - accuracy: 0.58 - ETA: 0s - loss: 0.6799 - accuracy: 0.58 - ETA: 0s - loss: 0.6792 - accuracy: 0.58 - ETA: 0s - loss: 0.6778 - accuracy: 0.59 - ETA: 0s - loss: 0.6770 - accuracy: 0.59 - ETA: 0s - loss: 0.6763 - accuracy: 0.59 - 1s 75us/step - loss: 0.6760 - accuracy: 0.6002 - val_loss: 0.6533 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:17 - loss: 0.6924 - accuracy: 0.50 - ETA: 2s - loss: 0.6905 - accuracy: 0.5576 - ETA: 1s - loss: 0.6862 - accuracy: 0.57 - ETA: 1s - loss: 0.6831 - accuracy: 0.58 - ETA: 0s - loss: 0.6813 - accuracy: 0.59 - ETA: 0s - loss: 0.6794 - accuracy: 0.59 - ETA: 0s - loss: 0.6760 - accuracy: 0.60 - ETA: 0s - loss: 0.6737 - accuracy: 0.61 - ETA: 0s - loss: 0.6726 - accuracy: 0.61 - ETA: 0s - loss: 0.6711 - accuracy: 0.61 - ETA: 0s - loss: 0.6689 - accuracy: 0.61 - ETA: 0s - loss: 0.6659 - accuracy: 0.62 - ETA: 0s - loss: 0.6642 - accuracy: 0.62 - ETA: 0s - loss: 0.6628 - accuracy: 0.62 - ETA: 0s - loss: 0.6608 - accuracy: 0.63 - ETA: 0s - loss: 0.6599 - accuracy: 0.63 - 1s 75us/step - loss: 0.6592 - accuracy: 0.6336 - val_loss: 0.6310 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:16 - loss: 0.6950 - accuracy: 0.43 - ETA: 2s - loss: 0.6923 - accuracy: 0.5312 - ETA: 1s - loss: 0.6882 - accuracy: 0.56 - ETA: 1s - loss: 0.6861 - accuracy: 0.57 - ETA: 0s - loss: 0.6824 - accuracy: 0.59 - ETA: 0s - loss: 0.6815 - accuracy: 0.59 - ETA: 0s - loss: 0.6785 - accuracy: 0.60 - ETA: 0s - loss: 0.6764 - accuracy: 0.60 - ETA: 0s - loss: 0.6744 - accuracy: 0.61 - ETA: 0s - loss: 0.6724 - accuracy: 0.61 - ETA: 0s - loss: 0.6703 - accuracy: 0.62 - ETA: 0s - loss: 0.6676 - accuracy: 0.63 - ETA: 0s - loss: 0.6653 - accuracy: 0.63 - ETA: 0s - loss: 0.6642 - accuracy: 0.63 - ETA: 0s - loss: 0.6630 - accuracy: 0.63 - ETA: 0s - loss: 0.6621 - accuracy: 0.63 - 1s 76us/step - loss: 0.6608 - accuracy: 0.6372 - val_loss: 0.6263 - val_accuracy: 0.6989\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:19 - loss: 0.6944 - accuracy: 0.50 - ETA: 2s - loss: 0.6936 - accuracy: 0.4914 - ETA: 1s - loss: 0.6913 - accuracy: 0.53 - ETA: 1s - loss: 0.6892 - accuracy: 0.54 - ETA: 0s - loss: 0.6877 - accuracy: 0.55 - ETA: 0s - loss: 0.6842 - accuracy: 0.56 - ETA: 0s - loss: 0.6832 - accuracy: 0.57 - ETA: 0s - loss: 0.6817 - accuracy: 0.57 - ETA: 0s - loss: 0.6780 - accuracy: 0.59 - ETA: 0s - loss: 0.6759 - accuracy: 0.59 - ETA: 0s - loss: 0.6735 - accuracy: 0.60 - ETA: 0s - loss: 0.6706 - accuracy: 0.61 - ETA: 0s - loss: 0.6692 - accuracy: 0.61 - ETA: 0s - loss: 0.6670 - accuracy: 0.61 - ETA: 0s - loss: 0.6650 - accuracy: 0.62 - ETA: 0s - loss: 0.6631 - accuracy: 0.62 - 1s 75us/step - loss: 0.6623 - accuracy: 0.6302 - val_loss: 0.6272 - val_accuracy: 0.7166\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:18 - loss: 0.7017 - accuracy: 0.31 - ETA: 2s - loss: 0.6912 - accuracy: 0.5300 - ETA: 1s - loss: 0.6886 - accuracy: 0.53 - ETA: 1s - loss: 0.6869 - accuracy: 0.54 - ETA: 0s - loss: 0.6844 - accuracy: 0.56 - ETA: 0s - loss: 0.6818 - accuracy: 0.57 - ETA: 0s - loss: 0.6789 - accuracy: 0.58 - ETA: 0s - loss: 0.6760 - accuracy: 0.59 - ETA: 0s - loss: 0.6743 - accuracy: 0.60 - ETA: 0s - loss: 0.6721 - accuracy: 0.60 - ETA: 0s - loss: 0.6696 - accuracy: 0.61 - ETA: 0s - loss: 0.6677 - accuracy: 0.61 - ETA: 0s - loss: 0.6649 - accuracy: 0.62 - ETA: 0s - loss: 0.6624 - accuracy: 0.63 - ETA: 0s - loss: 0.6610 - accuracy: 0.63 - ETA: 0s - loss: 0.6587 - accuracy: 0.63 - 1s 75us/step - loss: 0.6576 - accuracy: 0.6386 - val_loss: 0.6201 - val_accuracy: 0.7095\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:17 - loss: 0.6955 - accuracy: 0.62 - ETA: 2s - loss: 0.6902 - accuracy: 0.5576 - ETA: 1s - loss: 0.6862 - accuracy: 0.58 - ETA: 1s - loss: 0.6835 - accuracy: 0.58 - ETA: 0s - loss: 0.6797 - accuracy: 0.59 - ETA: 0s - loss: 0.6770 - accuracy: 0.60 - ETA: 0s - loss: 0.6743 - accuracy: 0.60 - ETA: 0s - loss: 0.6713 - accuracy: 0.61 - ETA: 0s - loss: 0.6678 - accuracy: 0.62 - ETA: 0s - loss: 0.6654 - accuracy: 0.62 - ETA: 0s - loss: 0.6619 - accuracy: 0.63 - ETA: 0s - loss: 0.6600 - accuracy: 0.63 - ETA: 0s - loss: 0.6584 - accuracy: 0.63 - ETA: 0s - loss: 0.6557 - accuracy: 0.64 - ETA: 0s - loss: 0.6545 - accuracy: 0.64 - ETA: 0s - loss: 0.6525 - accuracy: 0.64 - 1s 76us/step - loss: 0.6514 - accuracy: 0.6486 - val_loss: 0.6138 - val_accuracy: 0.7109\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:25 - loss: 0.6958 - accuracy: 0.50 - ETA: 2s - loss: 0.6906 - accuracy: 0.5319 - ETA: 1s - loss: 0.6891 - accuracy: 0.52 - ETA: 1s - loss: 0.6878 - accuracy: 0.52 - ETA: 0s - loss: 0.6851 - accuracy: 0.53 - ETA: 0s - loss: 0.6822 - accuracy: 0.55 - ETA: 0s - loss: 0.6813 - accuracy: 0.55 - ETA: 0s - loss: 0.6793 - accuracy: 0.56 - ETA: 0s - loss: 0.6773 - accuracy: 0.58 - ETA: 0s - loss: 0.6756 - accuracy: 0.58 - ETA: 0s - loss: 0.6742 - accuracy: 0.59 - ETA: 0s - loss: 0.6720 - accuracy: 0.59 - ETA: 0s - loss: 0.6708 - accuracy: 0.60 - ETA: 0s - loss: 0.6695 - accuracy: 0.60 - ETA: 0s - loss: 0.6680 - accuracy: 0.61 - ETA: 0s - loss: 0.6668 - accuracy: 0.61 - 1s 77us/step - loss: 0.6654 - accuracy: 0.6182 - val_loss: 0.6313 - val_accuracy: 0.7053\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:19 - loss: 0.6984 - accuracy: 0.25 - ETA: 2s - loss: 0.6887 - accuracy: 0.5276 - ETA: 1s - loss: 0.6850 - accuracy: 0.55 - ETA: 1s - loss: 0.6844 - accuracy: 0.55 - ETA: 0s - loss: 0.6809 - accuracy: 0.57 - ETA: 0s - loss: 0.6799 - accuracy: 0.57 - ETA: 0s - loss: 0.6773 - accuracy: 0.58 - ETA: 0s - loss: 0.6743 - accuracy: 0.59 - ETA: 0s - loss: 0.6718 - accuracy: 0.60 - ETA: 0s - loss: 0.6707 - accuracy: 0.60 - ETA: 0s - loss: 0.6683 - accuracy: 0.61 - ETA: 0s - loss: 0.6667 - accuracy: 0.61 - ETA: 0s - loss: 0.6657 - accuracy: 0.61 - ETA: 0s - loss: 0.6631 - accuracy: 0.62 - ETA: 0s - loss: 0.6618 - accuracy: 0.62 - ETA: 0s - loss: 0.6607 - accuracy: 0.63 - 1s 76us/step - loss: 0.6605 - accuracy: 0.6311 - val_loss: 0.6318 - val_accuracy: 0.7102\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 2:16 - loss: 0.6923 - accuracy: 0.56 - ETA: 3s - loss: 0.6934 - accuracy: 0.5255 - ETA: 1s - loss: 0.6930 - accuracy: 0.52 - ETA: 1s - loss: 0.6919 - accuracy: 0.53 - ETA: 1s - loss: 0.6897 - accuracy: 0.53 - ETA: 0s - loss: 0.6883 - accuracy: 0.54 - ETA: 0s - loss: 0.6867 - accuracy: 0.54 - ETA: 0s - loss: 0.6848 - accuracy: 0.54 - ETA: 0s - loss: 0.6827 - accuracy: 0.54 - ETA: 0s - loss: 0.6819 - accuracy: 0.54 - ETA: 0s - loss: 0.6808 - accuracy: 0.54 - ETA: 0s - loss: 0.6788 - accuracy: 0.54 - ETA: 0s - loss: 0.6761 - accuracy: 0.55 - ETA: 0s - loss: 0.6754 - accuracy: 0.55 - ETA: 0s - loss: 0.6736 - accuracy: 0.55 - ETA: 0s - loss: 0.6720 - accuracy: 0.56 - ETA: 0s - loss: 0.6704 - accuracy: 0.56 - 1s 85us/step - loss: 0.6702 - accuracy: 0.5625 - val_loss: 0.6327 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:35 - loss: 0.6880 - accuracy: 0.68 - ETA: 2s - loss: 0.6936 - accuracy: 0.5038 - ETA: 1s - loss: 0.6928 - accuracy: 0.51 - ETA: 1s - loss: 0.6918 - accuracy: 0.51 - ETA: 1s - loss: 0.6912 - accuracy: 0.51 - ETA: 0s - loss: 0.6898 - accuracy: 0.51 - ETA: 0s - loss: 0.6888 - accuracy: 0.51 - ETA: 0s - loss: 0.6874 - accuracy: 0.52 - ETA: 0s - loss: 0.6865 - accuracy: 0.53 - ETA: 0s - loss: 0.6853 - accuracy: 0.54 - ETA: 0s - loss: 0.6839 - accuracy: 0.54 - ETA: 0s - loss: 0.6824 - accuracy: 0.54 - ETA: 0s - loss: 0.6807 - accuracy: 0.55 - ETA: 0s - loss: 0.6797 - accuracy: 0.56 - ETA: 0s - loss: 0.6783 - accuracy: 0.56 - ETA: 0s - loss: 0.6775 - accuracy: 0.57 - ETA: 0s - loss: 0.6769 - accuracy: 0.57 - 1s 82us/step - loss: 0.6762 - accuracy: 0.5754 - val_loss: 0.6350 - val_accuracy: 0.7180\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:36 - loss: 0.6918 - accuracy: 0.68 - ETA: 2s - loss: 0.6941 - accuracy: 0.4951 - ETA: 1s - loss: 0.6937 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.49 - ETA: 0s - loss: 0.6925 - accuracy: 0.50 - ETA: 0s - loss: 0.6920 - accuracy: 0.50 - ETA: 0s - loss: 0.6907 - accuracy: 0.52 - ETA: 0s - loss: 0.6886 - accuracy: 0.53 - ETA: 0s - loss: 0.6863 - accuracy: 0.53 - ETA: 0s - loss: 0.6829 - accuracy: 0.54 - ETA: 0s - loss: 0.6800 - accuracy: 0.55 - ETA: 0s - loss: 0.6770 - accuracy: 0.55 - ETA: 0s - loss: 0.6751 - accuracy: 0.56 - ETA: 0s - loss: 0.6723 - accuracy: 0.56 - ETA: 0s - loss: 0.6690 - accuracy: 0.57 - ETA: 0s - loss: 0.6663 - accuracy: 0.57 - ETA: 0s - loss: 0.6643 - accuracy: 0.58 - 1s 80us/step - loss: 0.6642 - accuracy: 0.5835 - val_loss: 0.6098 - val_accuracy: 0.7173\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:37 - loss: 0.6939 - accuracy: 0.37 - ETA: 2s - loss: 0.6943 - accuracy: 0.4812 - ETA: 1s - loss: 0.6936 - accuracy: 0.50 - ETA: 1s - loss: 0.6935 - accuracy: 0.50 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6907 - accuracy: 0.53 - ETA: 0s - loss: 0.6889 - accuracy: 0.54 - ETA: 0s - loss: 0.6872 - accuracy: 0.54 - ETA: 0s - loss: 0.6854 - accuracy: 0.55 - ETA: 0s - loss: 0.6836 - accuracy: 0.55 - ETA: 0s - loss: 0.6815 - accuracy: 0.56 - ETA: 0s - loss: 0.6794 - accuracy: 0.56 - ETA: 0s - loss: 0.6776 - accuracy: 0.56 - ETA: 0s - loss: 0.6749 - accuracy: 0.57 - ETA: 0s - loss: 0.6724 - accuracy: 0.57 - ETA: 0s - loss: 0.6703 - accuracy: 0.58 - 1s 81us/step - loss: 0.6696 - accuracy: 0.5829 - val_loss: 0.6125 - val_accuracy: 0.7202\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:36 - loss: 0.6937 - accuracy: 0.37 - ETA: 2s - loss: 0.6934 - accuracy: 0.5312 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 1s - loss: 0.6933 - accuracy: 0.52 - ETA: 1s - loss: 0.6929 - accuracy: 0.53 - ETA: 0s - loss: 0.6923 - accuracy: 0.54 - ETA: 0s - loss: 0.6914 - accuracy: 0.54 - ETA: 0s - loss: 0.6903 - accuracy: 0.54 - ETA: 0s - loss: 0.6888 - accuracy: 0.55 - ETA: 0s - loss: 0.6867 - accuracy: 0.55 - ETA: 0s - loss: 0.6848 - accuracy: 0.55 - ETA: 0s - loss: 0.6823 - accuracy: 0.56 - ETA: 0s - loss: 0.6802 - accuracy: 0.56 - ETA: 0s - loss: 0.6782 - accuracy: 0.56 - ETA: 0s - loss: 0.6760 - accuracy: 0.56 - ETA: 0s - loss: 0.6743 - accuracy: 0.56 - ETA: 0s - loss: 0.6726 - accuracy: 0.57 - 1s 80us/step - loss: 0.6726 - accuracy: 0.5720 - val_loss: 0.6242 - val_accuracy: 0.7202\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 1:36 - loss: 0.6937 - accuracy: 0.43 - ETA: 2s - loss: 0.6931 - accuracy: 0.5293 - ETA: 1s - loss: 0.6926 - accuracy: 0.53 - ETA: 1s - loss: 0.6920 - accuracy: 0.52 - ETA: 0s - loss: 0.6917 - accuracy: 0.52 - ETA: 0s - loss: 0.6908 - accuracy: 0.52 - ETA: 0s - loss: 0.6895 - accuracy: 0.53 - ETA: 0s - loss: 0.6877 - accuracy: 0.53 - ETA: 0s - loss: 0.6865 - accuracy: 0.53 - ETA: 0s - loss: 0.6845 - accuracy: 0.53 - ETA: 0s - loss: 0.6820 - accuracy: 0.54 - ETA: 0s - loss: 0.6798 - accuracy: 0.54 - ETA: 0s - loss: 0.6766 - accuracy: 0.55 - ETA: 0s - loss: 0.6752 - accuracy: 0.55 - ETA: 0s - loss: 0.6738 - accuracy: 0.56 - ETA: 0s - loss: 0.6728 - accuracy: 0.56 - ETA: 0s - loss: 0.6710 - accuracy: 0.56 - ETA: 0s - loss: 0.6690 - accuracy: 0.57 - 1s 85us/step - loss: 0.6673 - accuracy: 0.5740 - val_loss: 0.6204 - val_accuracy: 0.7024\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:35 - loss: 0.6848 - accuracy: 0.68 - ETA: 2s - loss: 0.6925 - accuracy: 0.5238 - ETA: 1s - loss: 0.6869 - accuracy: 0.53 - ETA: 1s - loss: 0.6854 - accuracy: 0.53 - ETA: 1s - loss: 0.6828 - accuracy: 0.54 - ETA: 0s - loss: 0.6813 - accuracy: 0.55 - ETA: 0s - loss: 0.6807 - accuracy: 0.56 - ETA: 0s - loss: 0.6784 - accuracy: 0.57 - ETA: 0s - loss: 0.6752 - accuracy: 0.58 - ETA: 0s - loss: 0.6720 - accuracy: 0.59 - ETA: 0s - loss: 0.6698 - accuracy: 0.59 - ETA: 0s - loss: 0.6668 - accuracy: 0.60 - ETA: 0s - loss: 0.6634 - accuracy: 0.61 - ETA: 0s - loss: 0.6615 - accuracy: 0.61 - ETA: 0s - loss: 0.6600 - accuracy: 0.61 - ETA: 0s - loss: 0.6583 - accuracy: 0.62 - ETA: 0s - loss: 0.6554 - accuracy: 0.62 - 1s 82us/step - loss: 0.6542 - accuracy: 0.6297 - val_loss: 0.5992 - val_accuracy: 0.7244\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 30us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:35 - loss: 0.6940 - accuracy: 0.56 - ETA: 2s - loss: 0.6939 - accuracy: 0.5294 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 1s - loss: 0.6920 - accuracy: 0.54 - ETA: 0s - loss: 0.6907 - accuracy: 0.54 - ETA: 0s - loss: 0.6871 - accuracy: 0.56 - ETA: 0s - loss: 0.6831 - accuracy: 0.57 - ETA: 0s - loss: 0.6799 - accuracy: 0.57 - ETA: 0s - loss: 0.6753 - accuracy: 0.58 - ETA: 0s - loss: 0.6703 - accuracy: 0.59 - ETA: 0s - loss: 0.6646 - accuracy: 0.60 - ETA: 0s - loss: 0.6610 - accuracy: 0.60 - ETA: 0s - loss: 0.6579 - accuracy: 0.61 - ETA: 0s - loss: 0.6535 - accuracy: 0.62 - ETA: 0s - loss: 0.6514 - accuracy: 0.62 - ETA: 0s - loss: 0.6479 - accuracy: 0.62 - ETA: 0s - loss: 0.6465 - accuracy: 0.63 - 1s 80us/step - loss: 0.6462 - accuracy: 0.6316 - val_loss: 0.5875 - val_accuracy: 0.7095\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:38 - loss: 0.6928 - accuracy: 0.43 - ETA: 2s - loss: 0.6931 - accuracy: 0.5213 - ETA: 1s - loss: 0.6912 - accuracy: 0.53 - ETA: 1s - loss: 0.6906 - accuracy: 0.54 - ETA: 0s - loss: 0.6884 - accuracy: 0.55 - ETA: 0s - loss: 0.6848 - accuracy: 0.56 - ETA: 0s - loss: 0.6810 - accuracy: 0.56 - ETA: 0s - loss: 0.6776 - accuracy: 0.57 - ETA: 0s - loss: 0.6713 - accuracy: 0.58 - ETA: 0s - loss: 0.6693 - accuracy: 0.58 - ETA: 0s - loss: 0.6652 - accuracy: 0.59 - ETA: 0s - loss: 0.6614 - accuracy: 0.59 - ETA: 0s - loss: 0.6600 - accuracy: 0.59 - ETA: 0s - loss: 0.6580 - accuracy: 0.60 - ETA: 0s - loss: 0.6547 - accuracy: 0.60 - ETA: 0s - loss: 0.6518 - accuracy: 0.61 - ETA: 0s - loss: 0.6493 - accuracy: 0.61 - 1s 82us/step - loss: 0.6478 - accuracy: 0.6206 - val_loss: 0.5873 - val_accuracy: 0.7081\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 32us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:35 - loss: 0.6924 - accuracy: 0.75 - ETA: 2s - loss: 0.6937 - accuracy: 0.5331 - ETA: 1s - loss: 0.6933 - accuracy: 0.52 - ETA: 1s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6906 - accuracy: 0.51 - ETA: 0s - loss: 0.6892 - accuracy: 0.52 - ETA: 0s - loss: 0.6870 - accuracy: 0.52 - ETA: 0s - loss: 0.6853 - accuracy: 0.53 - ETA: 0s - loss: 0.6824 - accuracy: 0.54 - ETA: 0s - loss: 0.6803 - accuracy: 0.54 - ETA: 0s - loss: 0.6773 - accuracy: 0.55 - ETA: 0s - loss: 0.6744 - accuracy: 0.56 - ETA: 0s - loss: 0.6721 - accuracy: 0.57 - ETA: 0s - loss: 0.6694 - accuracy: 0.57 - ETA: 0s - loss: 0.6667 - accuracy: 0.58 - ETA: 0s - loss: 0.6640 - accuracy: 0.58 - 1s 81us/step - loss: 0.6635 - accuracy: 0.5910 - val_loss: 0.6143 - val_accuracy: 0.7152\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:44 - loss: 0.6917 - accuracy: 0.56 - ETA: 2s - loss: 0.6943 - accuracy: 0.5051 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6907 - accuracy: 0.52 - ETA: 1s - loss: 0.6898 - accuracy: 0.52 - ETA: 0s - loss: 0.6873 - accuracy: 0.53 - ETA: 0s - loss: 0.6842 - accuracy: 0.54 - ETA: 0s - loss: 0.6800 - accuracy: 0.55 - ETA: 0s - loss: 0.6753 - accuracy: 0.57 - ETA: 0s - loss: 0.6715 - accuracy: 0.57 - ETA: 0s - loss: 0.6663 - accuracy: 0.58 - ETA: 0s - loss: 0.6626 - accuracy: 0.59 - ETA: 0s - loss: 0.6580 - accuracy: 0.60 - ETA: 0s - loss: 0.6566 - accuracy: 0.60 - ETA: 0s - loss: 0.6547 - accuracy: 0.60 - ETA: 0s - loss: 0.6518 - accuracy: 0.61 - ETA: 0s - loss: 0.6495 - accuracy: 0.61 - 1s 82us/step - loss: 0.6490 - accuracy: 0.6180 - val_loss: 0.5926 - val_accuracy: 0.7209\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:37 - loss: 0.6936 - accuracy: 0.56 - ETA: 2s - loss: 0.6945 - accuracy: 0.4987 - ETA: 1s - loss: 0.6940 - accuracy: 0.51 - ETA: 1s - loss: 0.6936 - accuracy: 0.52 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.53 - ETA: 0s - loss: 0.6927 - accuracy: 0.53 - ETA: 0s - loss: 0.6922 - accuracy: 0.54 - ETA: 0s - loss: 0.6913 - accuracy: 0.55 - ETA: 0s - loss: 0.6903 - accuracy: 0.55 - ETA: 0s - loss: 0.6895 - accuracy: 0.55 - ETA: 0s - loss: 0.6875 - accuracy: 0.55 - ETA: 0s - loss: 0.6857 - accuracy: 0.56 - ETA: 0s - loss: 0.6848 - accuracy: 0.56 - ETA: 0s - loss: 0.6828 - accuracy: 0.57 - ETA: 0s - loss: 0.6799 - accuracy: 0.58 - ETA: 0s - loss: 0.6780 - accuracy: 0.58 - 1s 82us/step - loss: 0.6762 - accuracy: 0.5920 - val_loss: 0.6370 - val_accuracy: 0.6683\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:33 - loss: 0.6950 - accuracy: 0.56 - ETA: 2s - loss: 0.6940 - accuracy: 0.5138 - ETA: 1s - loss: 0.6939 - accuracy: 0.50 - ETA: 1s - loss: 0.6930 - accuracy: 0.50 - ETA: 0s - loss: 0.6912 - accuracy: 0.51 - ETA: 0s - loss: 0.6891 - accuracy: 0.51 - ETA: 0s - loss: 0.6860 - accuracy: 0.53 - ETA: 0s - loss: 0.6823 - accuracy: 0.54 - ETA: 0s - loss: 0.6804 - accuracy: 0.55 - ETA: 0s - loss: 0.6786 - accuracy: 0.56 - ETA: 0s - loss: 0.6764 - accuracy: 0.57 - ETA: 0s - loss: 0.6740 - accuracy: 0.58 - ETA: 0s - loss: 0.6724 - accuracy: 0.58 - ETA: 0s - loss: 0.6711 - accuracy: 0.59 - ETA: 0s - loss: 0.6685 - accuracy: 0.60 - ETA: 0s - loss: 0.6664 - accuracy: 0.60 - ETA: 0s - loss: 0.6649 - accuracy: 0.61 - 1s 81us/step - loss: 0.6636 - accuracy: 0.6177 - val_loss: 0.6210 - val_accuracy: 0.6790\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:46 - loss: 0.6952 - accuracy: 0.56 - ETA: 2s - loss: 0.6941 - accuracy: 0.5548 - ETA: 1s - loss: 0.6923 - accuracy: 0.56 - ETA: 1s - loss: 0.6905 - accuracy: 0.56 - ETA: 1s - loss: 0.6876 - accuracy: 0.56 - ETA: 0s - loss: 0.6840 - accuracy: 0.57 - ETA: 0s - loss: 0.6802 - accuracy: 0.58 - ETA: 0s - loss: 0.6765 - accuracy: 0.59 - ETA: 0s - loss: 0.6723 - accuracy: 0.59 - ETA: 0s - loss: 0.6682 - accuracy: 0.60 - ETA: 0s - loss: 0.6654 - accuracy: 0.60 - ETA: 0s - loss: 0.6627 - accuracy: 0.61 - ETA: 0s - loss: 0.6589 - accuracy: 0.61 - ETA: 0s - loss: 0.6556 - accuracy: 0.62 - ETA: 0s - loss: 0.6533 - accuracy: 0.62 - ETA: 0s - loss: 0.6501 - accuracy: 0.62 - ETA: 0s - loss: 0.6483 - accuracy: 0.63 - ETA: 0s - loss: 0.6445 - accuracy: 0.63 - 1s 86us/step - loss: 0.6448 - accuracy: 0.6372 - val_loss: 0.5858 - val_accuracy: 0.7223\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:31 - loss: 0.6973 - accuracy: 0.50 - ETA: 2s - loss: 0.6937 - accuracy: 0.5288 - ETA: 1s - loss: 0.6918 - accuracy: 0.56 - ETA: 1s - loss: 0.6890 - accuracy: 0.57 - ETA: 1s - loss: 0.6849 - accuracy: 0.58 - ETA: 0s - loss: 0.6792 - accuracy: 0.59 - ETA: 0s - loss: 0.6739 - accuracy: 0.60 - ETA: 0s - loss: 0.6685 - accuracy: 0.61 - ETA: 0s - loss: 0.6627 - accuracy: 0.61 - ETA: 0s - loss: 0.6584 - accuracy: 0.62 - ETA: 0s - loss: 0.6561 - accuracy: 0.62 - ETA: 0s - loss: 0.6529 - accuracy: 0.63 - ETA: 0s - loss: 0.6486 - accuracy: 0.63 - ETA: 0s - loss: 0.6444 - accuracy: 0.64 - ETA: 0s - loss: 0.6431 - accuracy: 0.64 - ETA: 0s - loss: 0.6403 - accuracy: 0.65 - ETA: 0s - loss: 0.6399 - accuracy: 0.65 - 1s 82us/step - loss: 0.6381 - accuracy: 0.6546 - val_loss: 0.5810 - val_accuracy: 0.7216\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:32 - loss: 0.6994 - accuracy: 0.25 - ETA: 2s - loss: 0.6946 - accuracy: 0.4963 - ETA: 1s - loss: 0.6904 - accuracy: 0.52 - ETA: 1s - loss: 0.6898 - accuracy: 0.53 - ETA: 1s - loss: 0.6872 - accuracy: 0.54 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6773 - accuracy: 0.57 - ETA: 0s - loss: 0.6701 - accuracy: 0.58 - ETA: 0s - loss: 0.6650 - accuracy: 0.59 - ETA: 0s - loss: 0.6585 - accuracy: 0.60 - ETA: 0s - loss: 0.6562 - accuracy: 0.61 - ETA: 0s - loss: 0.6543 - accuracy: 0.61 - ETA: 0s - loss: 0.6525 - accuracy: 0.61 - ETA: 0s - loss: 0.6496 - accuracy: 0.62 - ETA: 0s - loss: 0.6468 - accuracy: 0.63 - ETA: 0s - loss: 0.6428 - accuracy: 0.63 - ETA: 0s - loss: 0.6390 - accuracy: 0.64 - ETA: 0s - loss: 0.6381 - accuracy: 0.64 - 1s 84us/step - loss: 0.6368 - accuracy: 0.6438 - val_loss: 0.5806 - val_accuracy: 0.7145\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:38 - loss: 0.6932 - accuracy: 0.62 - ETA: 3s - loss: 0.6943 - accuracy: 0.5213 - ETA: 1s - loss: 0.6930 - accuracy: 0.53 - ETA: 1s - loss: 0.6910 - accuracy: 0.54 - ETA: 1s - loss: 0.6882 - accuracy: 0.55 - ETA: 0s - loss: 0.6849 - accuracy: 0.56 - ETA: 0s - loss: 0.6827 - accuracy: 0.57 - ETA: 0s - loss: 0.6773 - accuracy: 0.57 - ETA: 0s - loss: 0.6755 - accuracy: 0.58 - ETA: 0s - loss: 0.6713 - accuracy: 0.59 - ETA: 0s - loss: 0.6670 - accuracy: 0.59 - ETA: 0s - loss: 0.6625 - accuracy: 0.60 - ETA: 0s - loss: 0.6576 - accuracy: 0.61 - ETA: 0s - loss: 0.6553 - accuracy: 0.61 - ETA: 0s - loss: 0.6539 - accuracy: 0.62 - ETA: 0s - loss: 0.6517 - accuracy: 0.62 - ETA: 0s - loss: 0.6495 - accuracy: 0.63 - 1s 81us/step - loss: 0.6486 - accuracy: 0.6334 - val_loss: 0.5870 - val_accuracy: 0.7237\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:50 - loss: 0.6938 - accuracy: 0.62 - ETA: 3s - loss: 0.6940 - accuracy: 0.5245 - ETA: 1s - loss: 0.6906 - accuracy: 0.53 - ETA: 1s - loss: 0.6869 - accuracy: 0.54 - ETA: 1s - loss: 0.6827 - accuracy: 0.55 - ETA: 0s - loss: 0.6803 - accuracy: 0.56 - ETA: 0s - loss: 0.6732 - accuracy: 0.58 - ETA: 0s - loss: 0.6673 - accuracy: 0.59 - ETA: 0s - loss: 0.6651 - accuracy: 0.59 - ETA: 0s - loss: 0.6595 - accuracy: 0.60 - ETA: 0s - loss: 0.6553 - accuracy: 0.61 - ETA: 0s - loss: 0.6517 - accuracy: 0.61 - ETA: 0s - loss: 0.6490 - accuracy: 0.62 - ETA: 0s - loss: 0.6464 - accuracy: 0.62 - ETA: 0s - loss: 0.6429 - accuracy: 0.63 - ETA: 0s - loss: 0.6398 - accuracy: 0.63 - ETA: 0s - loss: 0.6373 - accuracy: 0.63 - 1s 82us/step - loss: 0.6360 - accuracy: 0.6404 - val_loss: 0.5844 - val_accuracy: 0.7244\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:28 - loss: 0.6945 - accuracy: 0.50 - ETA: 2s - loss: 0.6934 - accuracy: 0.5179 - ETA: 1s - loss: 0.6922 - accuracy: 0.53 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 0s - loss: 0.6915 - accuracy: 0.53 - ETA: 0s - loss: 0.6904 - accuracy: 0.53 - ETA: 0s - loss: 0.6900 - accuracy: 0.53 - ETA: 0s - loss: 0.6890 - accuracy: 0.54 - ETA: 0s - loss: 0.6884 - accuracy: 0.55 - ETA: 0s - loss: 0.6877 - accuracy: 0.55 - ETA: 0s - loss: 0.6870 - accuracy: 0.55 - ETA: 0s - loss: 0.6858 - accuracy: 0.56 - ETA: 0s - loss: 0.6852 - accuracy: 0.56 - ETA: 0s - loss: 0.6842 - accuracy: 0.56 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6820 - accuracy: 0.56 - 1s 78us/step - loss: 0.6809 - accuracy: 0.5703 - val_loss: 0.6535 - val_accuracy: 0.7031\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:23 - loss: 0.6950 - accuracy: 0.50 - ETA: 2s - loss: 0.6928 - accuracy: 0.5307 - ETA: 1s - loss: 0.6927 - accuracy: 0.51 - ETA: 1s - loss: 0.6918 - accuracy: 0.52 - ETA: 0s - loss: 0.6917 - accuracy: 0.51 - ETA: 0s - loss: 0.6916 - accuracy: 0.51 - ETA: 0s - loss: 0.6910 - accuracy: 0.52 - ETA: 0s - loss: 0.6902 - accuracy: 0.52 - ETA: 0s - loss: 0.6892 - accuracy: 0.53 - ETA: 0s - loss: 0.6883 - accuracy: 0.54 - ETA: 0s - loss: 0.6876 - accuracy: 0.54 - ETA: 0s - loss: 0.6870 - accuracy: 0.54 - ETA: 0s - loss: 0.6860 - accuracy: 0.55 - ETA: 0s - loss: 0.6846 - accuracy: 0.55 - ETA: 0s - loss: 0.6827 - accuracy: 0.56 - ETA: 0s - loss: 0.6809 - accuracy: 0.56 - ETA: 0s - loss: 0.6794 - accuracy: 0.57 - 1s 79us/step - loss: 0.6792 - accuracy: 0.5715 - val_loss: 0.6463 - val_accuracy: 0.7131\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:31 - loss: 0.6936 - accuracy: 0.68 - ETA: 2s - loss: 0.6935 - accuracy: 0.5061 - ETA: 1s - loss: 0.6932 - accuracy: 0.52 - ETA: 1s - loss: 0.6931 - accuracy: 0.53 - ETA: 0s - loss: 0.6927 - accuracy: 0.54 - ETA: 0s - loss: 0.6924 - accuracy: 0.54 - ETA: 0s - loss: 0.6918 - accuracy: 0.55 - ETA: 0s - loss: 0.6909 - accuracy: 0.56 - ETA: 0s - loss: 0.6902 - accuracy: 0.56 - ETA: 0s - loss: 0.6892 - accuracy: 0.57 - ETA: 0s - loss: 0.6882 - accuracy: 0.57 - ETA: 0s - loss: 0.6869 - accuracy: 0.58 - ETA: 0s - loss: 0.6855 - accuracy: 0.58 - ETA: 0s - loss: 0.6848 - accuracy: 0.58 - ETA: 0s - loss: 0.6837 - accuracy: 0.58 - ETA: 0s - loss: 0.6822 - accuracy: 0.59 - ETA: 0s - loss: 0.6811 - accuracy: 0.59 - 1s 79us/step - loss: 0.6810 - accuracy: 0.5944 - val_loss: 0.6539 - val_accuracy: 0.7124\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 1:24 - loss: 0.6972 - accuracy: 0.50 - ETA: 2s - loss: 0.6934 - accuracy: 0.5125 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 1s - loss: 0.6922 - accuracy: 0.53 - ETA: 0s - loss: 0.6911 - accuracy: 0.55 - ETA: 0s - loss: 0.6899 - accuracy: 0.55 - ETA: 0s - loss: 0.6894 - accuracy: 0.55 - ETA: 0s - loss: 0.6881 - accuracy: 0.56 - ETA: 0s - loss: 0.6871 - accuracy: 0.57 - ETA: 0s - loss: 0.6861 - accuracy: 0.57 - ETA: 0s - loss: 0.6841 - accuracy: 0.58 - ETA: 0s - loss: 0.6830 - accuracy: 0.58 - ETA: 0s - loss: 0.6815 - accuracy: 0.59 - ETA: 0s - loss: 0.6805 - accuracy: 0.59 - ETA: 0s - loss: 0.6789 - accuracy: 0.59 - ETA: 0s - loss: 0.6769 - accuracy: 0.60 - 1s 77us/step - loss: 0.6758 - accuracy: 0.6028 - val_loss: 0.6432 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:20 - loss: 0.6905 - accuracy: 0.50 - ETA: 2s - loss: 0.6926 - accuracy: 0.5368 - ETA: 1s - loss: 0.6915 - accuracy: 0.55 - ETA: 1s - loss: 0.6907 - accuracy: 0.56 - ETA: 0s - loss: 0.6893 - accuracy: 0.56 - ETA: 0s - loss: 0.6883 - accuracy: 0.56 - ETA: 0s - loss: 0.6872 - accuracy: 0.57 - ETA: 0s - loss: 0.6856 - accuracy: 0.57 - ETA: 0s - loss: 0.6841 - accuracy: 0.57 - ETA: 0s - loss: 0.6831 - accuracy: 0.58 - ETA: 0s - loss: 0.6810 - accuracy: 0.58 - ETA: 0s - loss: 0.6798 - accuracy: 0.59 - ETA: 0s - loss: 0.6781 - accuracy: 0.59 - ETA: 0s - loss: 0.6759 - accuracy: 0.59 - ETA: 0s - loss: 0.6745 - accuracy: 0.59 - ETA: 0s - loss: 0.6729 - accuracy: 0.59 - 1s 77us/step - loss: 0.6717 - accuracy: 0.6021 - val_loss: 0.6363 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:27 - loss: 0.6946 - accuracy: 0.37 - ETA: 2s - loss: 0.6930 - accuracy: 0.5084 - ETA: 1s - loss: 0.6920 - accuracy: 0.51 - ETA: 1s - loss: 0.6916 - accuracy: 0.51 - ETA: 0s - loss: 0.6908 - accuracy: 0.51 - ETA: 0s - loss: 0.6902 - accuracy: 0.51 - ETA: 0s - loss: 0.6888 - accuracy: 0.51 - ETA: 0s - loss: 0.6881 - accuracy: 0.51 - ETA: 0s - loss: 0.6876 - accuracy: 0.52 - ETA: 0s - loss: 0.6868 - accuracy: 0.53 - ETA: 0s - loss: 0.6856 - accuracy: 0.54 - ETA: 0s - loss: 0.6839 - accuracy: 0.55 - ETA: 0s - loss: 0.6820 - accuracy: 0.55 - ETA: 0s - loss: 0.6805 - accuracy: 0.56 - ETA: 0s - loss: 0.6785 - accuracy: 0.57 - ETA: 0s - loss: 0.6772 - accuracy: 0.57 - 1s 77us/step - loss: 0.6769 - accuracy: 0.5781 - val_loss: 0.6535 - val_accuracy: 0.6825\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:24 - loss: 0.7012 - accuracy: 0.31 - ETA: 2s - loss: 0.6934 - accuracy: 0.5156 - ETA: 1s - loss: 0.6924 - accuracy: 0.54 - ETA: 1s - loss: 0.6913 - accuracy: 0.55 - ETA: 0s - loss: 0.6903 - accuracy: 0.56 - ETA: 0s - loss: 0.6893 - accuracy: 0.57 - ETA: 0s - loss: 0.6875 - accuracy: 0.58 - ETA: 0s - loss: 0.6866 - accuracy: 0.58 - ETA: 0s - loss: 0.6857 - accuracy: 0.58 - ETA: 0s - loss: 0.6840 - accuracy: 0.59 - ETA: 0s - loss: 0.6820 - accuracy: 0.60 - ETA: 0s - loss: 0.6794 - accuracy: 0.60 - ETA: 0s - loss: 0.6775 - accuracy: 0.61 - ETA: 0s - loss: 0.6762 - accuracy: 0.61 - ETA: 0s - loss: 0.6750 - accuracy: 0.61 - ETA: 0s - loss: 0.6734 - accuracy: 0.61 - 1s 77us/step - loss: 0.6716 - accuracy: 0.6177 - val_loss: 0.6337 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:24 - loss: 0.6966 - accuracy: 0.37 - ETA: 2s - loss: 0.6910 - accuracy: 0.5175 - ETA: 1s - loss: 0.6899 - accuracy: 0.52 - ETA: 1s - loss: 0.6887 - accuracy: 0.54 - ETA: 0s - loss: 0.6876 - accuracy: 0.54 - ETA: 0s - loss: 0.6863 - accuracy: 0.55 - ETA: 0s - loss: 0.6849 - accuracy: 0.56 - ETA: 0s - loss: 0.6828 - accuracy: 0.57 - ETA: 0s - loss: 0.6810 - accuracy: 0.58 - ETA: 0s - loss: 0.6793 - accuracy: 0.58 - ETA: 0s - loss: 0.6763 - accuracy: 0.59 - ETA: 0s - loss: 0.6743 - accuracy: 0.59 - ETA: 0s - loss: 0.6704 - accuracy: 0.60 - ETA: 0s - loss: 0.6684 - accuracy: 0.60 - ETA: 0s - loss: 0.6654 - accuracy: 0.61 - ETA: 0s - loss: 0.6640 - accuracy: 0.61 - ETA: 0s - loss: 0.6630 - accuracy: 0.61 - 1s 79us/step - loss: 0.6627 - accuracy: 0.6190 - val_loss: 0.6158 - val_accuracy: 0.7131\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 30us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:19 - loss: 0.6918 - accuracy: 0.56 - ETA: 2s - loss: 0.6917 - accuracy: 0.5221 - ETA: 1s - loss: 0.6907 - accuracy: 0.52 - ETA: 1s - loss: 0.6903 - accuracy: 0.53 - ETA: 0s - loss: 0.6895 - accuracy: 0.54 - ETA: 0s - loss: 0.6886 - accuracy: 0.54 - ETA: 0s - loss: 0.6872 - accuracy: 0.55 - ETA: 0s - loss: 0.6860 - accuracy: 0.55 - ETA: 0s - loss: 0.6845 - accuracy: 0.56 - ETA: 0s - loss: 0.6835 - accuracy: 0.57 - ETA: 0s - loss: 0.6827 - accuracy: 0.57 - ETA: 0s - loss: 0.6811 - accuracy: 0.58 - ETA: 0s - loss: 0.6792 - accuracy: 0.58 - ETA: 0s - loss: 0.6779 - accuracy: 0.58 - ETA: 0s - loss: 0.6754 - accuracy: 0.59 - ETA: 0s - loss: 0.6739 - accuracy: 0.59 - ETA: 0s - loss: 0.6726 - accuracy: 0.59 - ETA: 0s - loss: 0.6712 - accuracy: 0.59 - 1s 82us/step - loss: 0.6709 - accuracy: 0.6005 - val_loss: 0.6287 - val_accuracy: 0.7188\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 30us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:23 - loss: 0.6871 - accuracy: 0.68 - ETA: 2s - loss: 0.6946 - accuracy: 0.4860 - ETA: 1s - loss: 0.6939 - accuracy: 0.49 - ETA: 1s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6917 - accuracy: 0.53 - ETA: 0s - loss: 0.6903 - accuracy: 0.54 - ETA: 0s - loss: 0.6881 - accuracy: 0.55 - ETA: 0s - loss: 0.6858 - accuracy: 0.56 - ETA: 0s - loss: 0.6839 - accuracy: 0.57 - ETA: 0s - loss: 0.6815 - accuracy: 0.58 - ETA: 0s - loss: 0.6793 - accuracy: 0.59 - ETA: 0s - loss: 0.6772 - accuracy: 0.59 - ETA: 0s - loss: 0.6743 - accuracy: 0.60 - ETA: 0s - loss: 0.6712 - accuracy: 0.60 - ETA: 0s - loss: 0.6694 - accuracy: 0.61 - ETA: 0s - loss: 0.6666 - accuracy: 0.61 - ETA: 0s - loss: 0.6647 - accuracy: 0.61 - 1s 79us/step - loss: 0.6639 - accuracy: 0.6198 - val_loss: 0.6157 - val_accuracy: 0.7102\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 31us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 2:10 - loss: 0.6948 - accuracy: 0.43 - ETA: 3s - loss: 0.6934 - accuracy: 0.5373 - ETA: 1s - loss: 0.6931 - accuracy: 0.53 - ETA: 1s - loss: 0.6924 - accuracy: 0.54 - ETA: 1s - loss: 0.6912 - accuracy: 0.55 - ETA: 0s - loss: 0.6895 - accuracy: 0.56 - ETA: 0s - loss: 0.6878 - accuracy: 0.56 - ETA: 0s - loss: 0.6868 - accuracy: 0.56 - ETA: 0s - loss: 0.6852 - accuracy: 0.57 - ETA: 0s - loss: 0.6837 - accuracy: 0.58 - ETA: 0s - loss: 0.6813 - accuracy: 0.58 - ETA: 0s - loss: 0.6803 - accuracy: 0.59 - ETA: 0s - loss: 0.6792 - accuracy: 0.59 - ETA: 0s - loss: 0.6771 - accuracy: 0.60 - ETA: 0s - loss: 0.6753 - accuracy: 0.60 - ETA: 0s - loss: 0.6724 - accuracy: 0.60 - ETA: 0s - loss: 0.6708 - accuracy: 0.61 - ETA: 0s - loss: 0.6686 - accuracy: 0.61 - 1s 89us/step - loss: 0.6669 - accuracy: 0.6166 - val_loss: 0.6245 - val_accuracy: 0.7095\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:23 - loss: 0.6948 - accuracy: 0.56 - ETA: 2s - loss: 0.6936 - accuracy: 0.5184 - ETA: 1s - loss: 0.6932 - accuracy: 0.52 - ETA: 1s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6914 - accuracy: 0.53 - ETA: 0s - loss: 0.6909 - accuracy: 0.54 - ETA: 0s - loss: 0.6902 - accuracy: 0.54 - ETA: 0s - loss: 0.6897 - accuracy: 0.54 - ETA: 0s - loss: 0.6888 - accuracy: 0.55 - ETA: 0s - loss: 0.6878 - accuracy: 0.55 - ETA: 0s - loss: 0.6868 - accuracy: 0.56 - ETA: 0s - loss: 0.6849 - accuracy: 0.56 - ETA: 0s - loss: 0.6834 - accuracy: 0.57 - ETA: 0s - loss: 0.6818 - accuracy: 0.57 - ETA: 0s - loss: 0.6804 - accuracy: 0.57 - 1s 77us/step - loss: 0.6791 - accuracy: 0.5765 - val_loss: 0.6503 - val_accuracy: 0.7060\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:23 - loss: 0.6938 - accuracy: 0.62 - ETA: 2s - loss: 0.6933 - accuracy: 0.5385 - ETA: 1s - loss: 0.6918 - accuracy: 0.53 - ETA: 1s - loss: 0.6909 - accuracy: 0.53 - ETA: 0s - loss: 0.6905 - accuracy: 0.54 - ETA: 0s - loss: 0.6885 - accuracy: 0.56 - ETA: 0s - loss: 0.6862 - accuracy: 0.57 - ETA: 0s - loss: 0.6845 - accuracy: 0.57 - ETA: 0s - loss: 0.6828 - accuracy: 0.58 - ETA: 0s - loss: 0.6793 - accuracy: 0.59 - ETA: 0s - loss: 0.6773 - accuracy: 0.59 - ETA: 0s - loss: 0.6756 - accuracy: 0.59 - ETA: 0s - loss: 0.6738 - accuracy: 0.60 - ETA: 0s - loss: 0.6705 - accuracy: 0.60 - ETA: 0s - loss: 0.6679 - accuracy: 0.61 - ETA: 0s - loss: 0.6654 - accuracy: 0.61 - ETA: 0s - loss: 0.6627 - accuracy: 0.62 - 1s 78us/step - loss: 0.6625 - accuracy: 0.6218 - val_loss: 0.6083 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:23 - loss: 0.6937 - accuracy: 0.62 - ETA: 2s - loss: 0.6903 - accuracy: 0.5637 - ETA: 1s - loss: 0.6870 - accuracy: 0.55 - ETA: 1s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6863 - accuracy: 0.56 - ETA: 0s - loss: 0.6848 - accuracy: 0.57 - ETA: 0s - loss: 0.6818 - accuracy: 0.57 - ETA: 0s - loss: 0.6782 - accuracy: 0.58 - ETA: 0s - loss: 0.6757 - accuracy: 0.59 - ETA: 0s - loss: 0.6727 - accuracy: 0.60 - ETA: 0s - loss: 0.6702 - accuracy: 0.60 - ETA: 0s - loss: 0.6674 - accuracy: 0.61 - ETA: 0s - loss: 0.6652 - accuracy: 0.61 - ETA: 0s - loss: 0.6620 - accuracy: 0.62 - ETA: 0s - loss: 0.6610 - accuracy: 0.62 - ETA: 0s - loss: 0.6589 - accuracy: 0.62 - ETA: 0s - loss: 0.6582 - accuracy: 0.62 - 1s 79us/step - loss: 0.6575 - accuracy: 0.6306 - val_loss: 0.6128 - val_accuracy: 0.7251\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:21 - loss: 0.6955 - accuracy: 0.37 - ETA: 2s - loss: 0.6935 - accuracy: 0.5084 - ETA: 1s - loss: 0.6919 - accuracy: 0.54 - ETA: 1s - loss: 0.6908 - accuracy: 0.55 - ETA: 0s - loss: 0.6887 - accuracy: 0.57 - ETA: 0s - loss: 0.6865 - accuracy: 0.58 - ETA: 0s - loss: 0.6851 - accuracy: 0.58 - ETA: 0s - loss: 0.6822 - accuracy: 0.59 - ETA: 0s - loss: 0.6782 - accuracy: 0.60 - ETA: 0s - loss: 0.6761 - accuracy: 0.60 - ETA: 0s - loss: 0.6729 - accuracy: 0.61 - ETA: 0s - loss: 0.6699 - accuracy: 0.61 - ETA: 0s - loss: 0.6673 - accuracy: 0.62 - ETA: 0s - loss: 0.6637 - accuracy: 0.62 - ETA: 0s - loss: 0.6612 - accuracy: 0.63 - ETA: 0s - loss: 0.6592 - accuracy: 0.63 - ETA: 0s - loss: 0.6576 - accuracy: 0.63 - 1s 78us/step - loss: 0.6575 - accuracy: 0.6380 - val_loss: 0.6049 - val_accuracy: 0.7053\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:22 - loss: 0.6924 - accuracy: 0.50 - ETA: 2s - loss: 0.6913 - accuracy: 0.5319 - ETA: 1s - loss: 0.6897 - accuracy: 0.52 - ETA: 1s - loss: 0.6884 - accuracy: 0.53 - ETA: 0s - loss: 0.6861 - accuracy: 0.54 - ETA: 0s - loss: 0.6843 - accuracy: 0.55 - ETA: 0s - loss: 0.6825 - accuracy: 0.56 - ETA: 0s - loss: 0.6804 - accuracy: 0.57 - ETA: 0s - loss: 0.6782 - accuracy: 0.58 - ETA: 0s - loss: 0.6764 - accuracy: 0.58 - ETA: 0s - loss: 0.6741 - accuracy: 0.59 - ETA: 0s - loss: 0.6720 - accuracy: 0.60 - ETA: 0s - loss: 0.6696 - accuracy: 0.60 - ETA: 0s - loss: 0.6666 - accuracy: 0.61 - ETA: 0s - loss: 0.6636 - accuracy: 0.61 - ETA: 0s - loss: 0.6599 - accuracy: 0.62 - ETA: 0s - loss: 0.6578 - accuracy: 0.62 - 1s 78us/step - loss: 0.6576 - accuracy: 0.6287 - val_loss: 0.6088 - val_accuracy: 0.7067\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:21 - loss: 0.6911 - accuracy: 0.75 - ETA: 2s - loss: 0.6952 - accuracy: 0.4964 - ETA: 1s - loss: 0.6940 - accuracy: 0.52 - ETA: 1s - loss: 0.6929 - accuracy: 0.53 - ETA: 0s - loss: 0.6921 - accuracy: 0.54 - ETA: 0s - loss: 0.6910 - accuracy: 0.55 - ETA: 0s - loss: 0.6892 - accuracy: 0.57 - ETA: 0s - loss: 0.6868 - accuracy: 0.58 - ETA: 0s - loss: 0.6843 - accuracy: 0.59 - ETA: 0s - loss: 0.6826 - accuracy: 0.59 - ETA: 0s - loss: 0.6801 - accuracy: 0.59 - ETA: 0s - loss: 0.6781 - accuracy: 0.60 - ETA: 0s - loss: 0.6749 - accuracy: 0.60 - ETA: 0s - loss: 0.6723 - accuracy: 0.61 - ETA: 0s - loss: 0.6703 - accuracy: 0.61 - ETA: 0s - loss: 0.6672 - accuracy: 0.61 - ETA: 0s - loss: 0.6653 - accuracy: 0.62 - 1s 80us/step - loss: 0.6647 - accuracy: 0.6242 - val_loss: 0.6120 - val_accuracy: 0.7166\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:23 - loss: 0.6941 - accuracy: 0.62 - ETA: 2s - loss: 0.6933 - accuracy: 0.5343 - ETA: 1s - loss: 0.6926 - accuracy: 0.54 - ETA: 1s - loss: 0.6914 - accuracy: 0.54 - ETA: 1s - loss: 0.6900 - accuracy: 0.54 - ETA: 0s - loss: 0.6888 - accuracy: 0.55 - ETA: 0s - loss: 0.6866 - accuracy: 0.56 - ETA: 0s - loss: 0.6842 - accuracy: 0.57 - ETA: 0s - loss: 0.6805 - accuracy: 0.58 - ETA: 0s - loss: 0.6769 - accuracy: 0.59 - ETA: 0s - loss: 0.6739 - accuracy: 0.60 - ETA: 0s - loss: 0.6717 - accuracy: 0.60 - ETA: 0s - loss: 0.6687 - accuracy: 0.61 - ETA: 0s - loss: 0.6655 - accuracy: 0.61 - ETA: 0s - loss: 0.6634 - accuracy: 0.62 - ETA: 0s - loss: 0.6610 - accuracy: 0.62 - ETA: 0s - loss: 0.6586 - accuracy: 0.63 - 1s 80us/step - loss: 0.6583 - accuracy: 0.6300 - val_loss: 0.6169 - val_accuracy: 0.7109\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:13 - loss: 0.6936 - accuracy: 0.56 - ETA: 1s - loss: 0.6938 - accuracy: 0.5162 - ETA: 1s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - 1s 74us/step - loss: 0.6935 - accuracy: 0.5139 - val_loss: 0.6926 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 1:15 - loss: 0.6941 - accuracy: 0.43 - ETA: 2s - loss: 0.6940 - accuracy: 0.4965 - ETA: 1s - loss: 0.6939 - accuracy: 0.49 - ETA: 1s - loss: 0.6937 - accuracy: 0.48 - ETA: 0s - loss: 0.6935 - accuracy: 0.49 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6932 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - 1s 74us/step - loss: 0.6929 - accuracy: 0.5154 - val_loss: 0.6911 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:11 - loss: 0.6954 - accuracy: 0.43 - ETA: 1s - loss: 0.6936 - accuracy: 0.5227 - ETA: 1s - loss: 0.6935 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - 1s 73us/step - loss: 0.6929 - accuracy: 0.5137 - val_loss: 0.6910 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:12 - loss: 0.6942 - accuracy: 0.43 - ETA: 1s - loss: 0.6938 - accuracy: 0.4641 - ETA: 1s - loss: 0.6937 - accuracy: 0.49 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - 1s 72us/step - loss: 0.6934 - accuracy: 0.5124 - val_loss: 0.6924 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:15 - loss: 0.6941 - accuracy: 0.37 - ETA: 1s - loss: 0.6949 - accuracy: 0.4931 - ETA: 1s - loss: 0.6945 - accuracy: 0.49 - ETA: 0s - loss: 0.6943 - accuracy: 0.49 - ETA: 0s - loss: 0.6943 - accuracy: 0.49 - ETA: 0s - loss: 0.6942 - accuracy: 0.49 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - 1s 74us/step - loss: 0.6937 - accuracy: 0.5084 - val_loss: 0.6926 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:13 - loss: 0.6917 - accuracy: 0.43 - ETA: 1s - loss: 0.6933 - accuracy: 0.5220 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - 1s 74us/step - loss: 0.6932 - accuracy: 0.5210 - val_loss: 0.6927 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:11 - loss: 0.6927 - accuracy: 0.62 - ETA: 1s - loss: 0.6938 - accuracy: 0.5370 - ETA: 1s - loss: 0.6939 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - 1s 74us/step - loss: 0.6928 - accuracy: 0.5145 - val_loss: 0.6905 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:13 - loss: 0.6992 - accuracy: 0.37 - ETA: 1s - loss: 0.6944 - accuracy: 0.4988 - ETA: 1s - loss: 0.6944 - accuracy: 0.48 - ETA: 0s - loss: 0.6942 - accuracy: 0.49 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - 1s 75us/step - loss: 0.6930 - accuracy: 0.5167 - val_loss: 0.6912 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:11 - loss: 0.6938 - accuracy: 0.56 - ETA: 1s - loss: 0.6941 - accuracy: 0.5312 - ETA: 1s - loss: 0.6941 - accuracy: 0.52 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - 1s 74us/step - loss: 0.6935 - accuracy: 0.5186 - val_loss: 0.6919 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:15 - loss: 0.6959 - accuracy: 0.43 - ETA: 1s - loss: 0.6941 - accuracy: 0.4861 - ETA: 1s - loss: 0.6941 - accuracy: 0.48 - ETA: 0s - loss: 0.6940 - accuracy: 0.49 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6930 - accuracy: 0.50 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6919 - accuracy: 0.51 - 1s 75us/step - loss: 0.6918 - accuracy: 0.5141 - val_loss: 0.6882 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:15 - loss: 0.6945 - accuracy: 0.68 - ETA: 2s - loss: 0.6944 - accuracy: 0.4988 - ETA: 1s - loss: 0.6944 - accuracy: 0.50 - ETA: 0s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - 1s 75us/step - loss: 0.6938 - accuracy: 0.5135 - val_loss: 0.6926 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:15 - loss: 0.6919 - accuracy: 0.62 - ETA: 1s - loss: 0.6938 - accuracy: 0.5324 - ETA: 1s - loss: 0.6939 - accuracy: 0.52 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - 1s 78us/step - loss: 0.6933 - accuracy: 0.5081 - val_loss: 0.6917 - val_accuracy: 0.5128\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 31us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:11 - loss: 0.7005 - accuracy: 0.56 - ETA: 1s - loss: 0.6946 - accuracy: 0.5118 - ETA: 1s - loss: 0.6936 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - 1s 74us/step - loss: 0.6933 - accuracy: 0.5166 - val_loss: 0.6904 - val_accuracy: 0.5277\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:13 - loss: 0.6928 - accuracy: 0.56 - ETA: 2s - loss: 0.6956 - accuracy: 0.4766 - ETA: 1s - loss: 0.6951 - accuracy: 0.49 - ETA: 1s - loss: 0.6950 - accuracy: 0.49 - ETA: 0s - loss: 0.6949 - accuracy: 0.51 - ETA: 0s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.52 - ETA: 0s - loss: 0.6941 - accuracy: 0.52 - ETA: 0s - loss: 0.6939 - accuracy: 0.52 - ETA: 0s - loss: 0.6940 - accuracy: 0.52 - ETA: 0s - loss: 0.6939 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6936 - accuracy: 0.52 - 1s 77us/step - loss: 0.6936 - accuracy: 0.5257 - val_loss: 0.6914 - val_accuracy: 0.5341\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:14 - loss: 0.6942 - accuracy: 0.56 - ETA: 2s - loss: 0.6949 - accuracy: 0.5013 - ETA: 1s - loss: 0.6949 - accuracy: 0.50 - ETA: 1s - loss: 0.6949 - accuracy: 0.50 - ETA: 0s - loss: 0.6948 - accuracy: 0.50 - ETA: 0s - loss: 0.6947 - accuracy: 0.51 - ETA: 0s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.52 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.52 - ETA: 0s - loss: 0.6940 - accuracy: 0.52 - 1s 73us/step - loss: 0.6940 - accuracy: 0.5205 - val_loss: 0.6917 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:14 - loss: 0.6961 - accuracy: 0.31 - ETA: 1s - loss: 0.6950 - accuracy: 0.4757 - ETA: 1s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.52 - ETA: 0s - loss: 0.6939 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.53 - ETA: 0s - loss: 0.6936 - accuracy: 0.53 - ETA: 0s - loss: 0.6933 - accuracy: 0.53 - ETA: 0s - loss: 0.6933 - accuracy: 0.53 - ETA: 0s - loss: 0.6932 - accuracy: 0.53 - ETA: 0s - loss: 0.6931 - accuracy: 0.53 - ETA: 0s - loss: 0.6929 - accuracy: 0.53 - ETA: 0s - loss: 0.6928 - accuracy: 0.53 - ETA: 0s - loss: 0.6927 - accuracy: 0.53 - ETA: 0s - loss: 0.6927 - accuracy: 0.54 - 1s 75us/step - loss: 0.6927 - accuracy: 0.5401 - val_loss: 0.6899 - val_accuracy: 0.5710\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:11 - loss: 0.6951 - accuracy: 0.50 - ETA: 1s - loss: 0.6943 - accuracy: 0.5200 - ETA: 1s - loss: 0.6945 - accuracy: 0.50 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - 1s 75us/step - loss: 0.6939 - accuracy: 0.5148 - val_loss: 0.6928 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 1:14 - loss: 0.6962 - accuracy: 0.50 - ETA: 2s - loss: 0.6955 - accuracy: 0.4880 - ETA: 1s - loss: 0.6955 - accuracy: 0.48 - ETA: 1s - loss: 0.6952 - accuracy: 0.48 - ETA: 0s - loss: 0.6950 - accuracy: 0.49 - ETA: 0s - loss: 0.6950 - accuracy: 0.49 - ETA: 0s - loss: 0.6950 - accuracy: 0.49 - ETA: 0s - loss: 0.6949 - accuracy: 0.49 - ETA: 0s - loss: 0.6949 - accuracy: 0.49 - ETA: 0s - loss: 0.6948 - accuracy: 0.50 - ETA: 0s - loss: 0.6947 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - 1s 74us/step - loss: 0.6937 - accuracy: 0.5180 - val_loss: 0.6924 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:35 - loss: 0.6934 - accuracy: 0.56 - ETA: 2s - loss: 0.6935 - accuracy: 0.4950 - ETA: 1s - loss: 0.6932 - accuracy: 0.51 - ETA: 1s - loss: 0.6923 - accuracy: 0.54 - ETA: 0s - loss: 0.6919 - accuracy: 0.54 - ETA: 0s - loss: 0.6912 - accuracy: 0.55 - ETA: 0s - loss: 0.6900 - accuracy: 0.55 - ETA: 0s - loss: 0.6889 - accuracy: 0.56 - ETA: 0s - loss: 0.6875 - accuracy: 0.57 - ETA: 0s - loss: 0.6850 - accuracy: 0.57 - ETA: 0s - loss: 0.6829 - accuracy: 0.58 - ETA: 0s - loss: 0.6804 - accuracy: 0.58 - ETA: 0s - loss: 0.6783 - accuracy: 0.59 - ETA: 0s - loss: 0.6766 - accuracy: 0.59 - ETA: 0s - loss: 0.6742 - accuracy: 0.59 - ETA: 0s - loss: 0.6718 - accuracy: 0.60 - ETA: 0s - loss: 0.6703 - accuracy: 0.60 - 1s 80us/step - loss: 0.6700 - accuracy: 0.6051 - val_loss: 0.6261 - val_accuracy: 0.7124\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:41 - loss: 0.6956 - accuracy: 0.50 - ETA: 2s - loss: 0.6944 - accuracy: 0.4896 - ETA: 1s - loss: 0.6940 - accuracy: 0.49 - ETA: 1s - loss: 0.6939 - accuracy: 0.48 - ETA: 1s - loss: 0.6937 - accuracy: 0.48 - ETA: 0s - loss: 0.6935 - accuracy: 0.49 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6919 - accuracy: 0.53 - ETA: 0s - loss: 0.6915 - accuracy: 0.54 - ETA: 0s - loss: 0.6908 - accuracy: 0.54 - ETA: 0s - loss: 0.6902 - accuracy: 0.54 - ETA: 0s - loss: 0.6897 - accuracy: 0.55 - ETA: 0s - loss: 0.6890 - accuracy: 0.55 - 1s 79us/step - loss: 0.6884 - accuracy: 0.5530 - val_loss: 0.6734 - val_accuracy: 0.7045\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:36 - loss: 0.6935 - accuracy: 0.62 - ETA: 2s - loss: 0.6928 - accuracy: 0.5275 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6918 - accuracy: 0.51 - ETA: 0s - loss: 0.6905 - accuracy: 0.53 - ETA: 0s - loss: 0.6895 - accuracy: 0.54 - ETA: 0s - loss: 0.6889 - accuracy: 0.54 - ETA: 0s - loss: 0.6880 - accuracy: 0.54 - ETA: 0s - loss: 0.6871 - accuracy: 0.54 - ETA: 0s - loss: 0.6858 - accuracy: 0.54 - ETA: 0s - loss: 0.6841 - accuracy: 0.55 - ETA: 0s - loss: 0.6821 - accuracy: 0.55 - ETA: 0s - loss: 0.6812 - accuracy: 0.55 - ETA: 0s - loss: 0.6799 - accuracy: 0.56 - ETA: 0s - loss: 0.6790 - accuracy: 0.56 - ETA: 0s - loss: 0.6775 - accuracy: 0.56 - ETA: 0s - loss: 0.6758 - accuracy: 0.56 - 1s 82us/step - loss: 0.6747 - accuracy: 0.5687 - val_loss: 0.6390 - val_accuracy: 0.7074\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:35 - loss: 0.6922 - accuracy: 0.81 - ETA: 2s - loss: 0.6931 - accuracy: 0.5380 - ETA: 1s - loss: 0.6929 - accuracy: 0.52 - ETA: 1s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6920 - accuracy: 0.53 - ETA: 0s - loss: 0.6917 - accuracy: 0.54 - ETA: 0s - loss: 0.6910 - accuracy: 0.55 - ETA: 0s - loss: 0.6902 - accuracy: 0.56 - ETA: 0s - loss: 0.6892 - accuracy: 0.57 - ETA: 0s - loss: 0.6879 - accuracy: 0.57 - ETA: 0s - loss: 0.6867 - accuracy: 0.58 - ETA: 0s - loss: 0.6846 - accuracy: 0.59 - ETA: 0s - loss: 0.6830 - accuracy: 0.59 - ETA: 0s - loss: 0.6813 - accuracy: 0.60 - ETA: 0s - loss: 0.6799 - accuracy: 0.60 - ETA: 0s - loss: 0.6779 - accuracy: 0.60 - ETA: 0s - loss: 0.6768 - accuracy: 0.61 - 1s 82us/step - loss: 0.6760 - accuracy: 0.6154 - val_loss: 0.6461 - val_accuracy: 0.7138\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 30us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:38 - loss: 0.6921 - accuracy: 0.75 - ETA: 2s - loss: 0.6933 - accuracy: 0.5108 - ETA: 1s - loss: 0.6917 - accuracy: 0.53 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 1s - loss: 0.6907 - accuracy: 0.53 - ETA: 0s - loss: 0.6898 - accuracy: 0.53 - ETA: 0s - loss: 0.6885 - accuracy: 0.54 - ETA: 0s - loss: 0.6870 - accuracy: 0.55 - ETA: 0s - loss: 0.6857 - accuracy: 0.55 - ETA: 0s - loss: 0.6849 - accuracy: 0.55 - ETA: 0s - loss: 0.6832 - accuracy: 0.55 - ETA: 0s - loss: 0.6820 - accuracy: 0.55 - ETA: 0s - loss: 0.6800 - accuracy: 0.56 - ETA: 0s - loss: 0.6773 - accuracy: 0.56 - ETA: 0s - loss: 0.6758 - accuracy: 0.56 - ETA: 0s - loss: 0.6740 - accuracy: 0.57 - ETA: 0s - loss: 0.6726 - accuracy: 0.57 - 1s 81us/step - loss: 0.6713 - accuracy: 0.5773 - val_loss: 0.6289 - val_accuracy: 0.7038\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:34 - loss: 0.6895 - accuracy: 0.56 - ETA: 2s - loss: 0.6922 - accuracy: 0.5247 - ETA: 1s - loss: 0.6919 - accuracy: 0.52 - ETA: 1s - loss: 0.6909 - accuracy: 0.53 - ETA: 0s - loss: 0.6905 - accuracy: 0.52 - ETA: 0s - loss: 0.6898 - accuracy: 0.52 - ETA: 0s - loss: 0.6892 - accuracy: 0.52 - ETA: 0s - loss: 0.6874 - accuracy: 0.53 - ETA: 0s - loss: 0.6862 - accuracy: 0.53 - ETA: 0s - loss: 0.6848 - accuracy: 0.53 - ETA: 0s - loss: 0.6840 - accuracy: 0.54 - ETA: 0s - loss: 0.6824 - accuracy: 0.55 - ETA: 0s - loss: 0.6816 - accuracy: 0.55 - ETA: 0s - loss: 0.6797 - accuracy: 0.56 - ETA: 0s - loss: 0.6782 - accuracy: 0.56 - ETA: 0s - loss: 0.6772 - accuracy: 0.57 - 1s 78us/step - loss: 0.6761 - accuracy: 0.5744 - val_loss: 0.6486 - val_accuracy: 0.6946\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:35 - loss: 0.6967 - accuracy: 0.43 - ETA: 2s - loss: 0.6928 - accuracy: 0.5375 - ETA: 1s - loss: 0.6912 - accuracy: 0.54 - ETA: 1s - loss: 0.6899 - accuracy: 0.54 - ETA: 0s - loss: 0.6879 - accuracy: 0.56 - ETA: 0s - loss: 0.6858 - accuracy: 0.56 - ETA: 0s - loss: 0.6829 - accuracy: 0.57 - ETA: 0s - loss: 0.6800 - accuracy: 0.58 - ETA: 0s - loss: 0.6772 - accuracy: 0.59 - ETA: 0s - loss: 0.6751 - accuracy: 0.59 - ETA: 0s - loss: 0.6725 - accuracy: 0.60 - ETA: 0s - loss: 0.6704 - accuracy: 0.60 - ETA: 0s - loss: 0.6677 - accuracy: 0.60 - ETA: 0s - loss: 0.6657 - accuracy: 0.61 - ETA: 0s - loss: 0.6634 - accuracy: 0.61 - ETA: 0s - loss: 0.6616 - accuracy: 0.61 - ETA: 0s - loss: 0.6588 - accuracy: 0.62 - 1s 82us/step - loss: 0.6563 - accuracy: 0.6251 - val_loss: 0.6019 - val_accuracy: 0.7173\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:37 - loss: 0.6912 - accuracy: 0.50 - ETA: 2s - loss: 0.6922 - accuracy: 0.5387 - ETA: 1s - loss: 0.6914 - accuracy: 0.54 - ETA: 1s - loss: 0.6910 - accuracy: 0.55 - ETA: 0s - loss: 0.6898 - accuracy: 0.55 - ETA: 0s - loss: 0.6877 - accuracy: 0.57 - ETA: 0s - loss: 0.6847 - accuracy: 0.58 - ETA: 0s - loss: 0.6818 - accuracy: 0.58 - ETA: 0s - loss: 0.6803 - accuracy: 0.59 - ETA: 0s - loss: 0.6787 - accuracy: 0.59 - ETA: 0s - loss: 0.6748 - accuracy: 0.60 - ETA: 0s - loss: 0.6724 - accuracy: 0.60 - ETA: 0s - loss: 0.6689 - accuracy: 0.61 - ETA: 0s - loss: 0.6656 - accuracy: 0.62 - ETA: 0s - loss: 0.6645 - accuracy: 0.62 - ETA: 0s - loss: 0.6630 - accuracy: 0.62 - ETA: 0s - loss: 0.6608 - accuracy: 0.62 - 1s 81us/step - loss: 0.6603 - accuracy: 0.6276 - val_loss: 0.6099 - val_accuracy: 0.7237\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:34 - loss: 0.6913 - accuracy: 0.62 - ETA: 2s - loss: 0.6940 - accuracy: 0.5166 - ETA: 1s - loss: 0.6935 - accuracy: 0.52 - ETA: 1s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6895 - accuracy: 0.53 - ETA: 0s - loss: 0.6880 - accuracy: 0.53 - ETA: 0s - loss: 0.6867 - accuracy: 0.54 - ETA: 0s - loss: 0.6846 - accuracy: 0.55 - ETA: 0s - loss: 0.6826 - accuracy: 0.56 - ETA: 0s - loss: 0.6813 - accuracy: 0.56 - ETA: 0s - loss: 0.6801 - accuracy: 0.57 - ETA: 0s - loss: 0.6781 - accuracy: 0.57 - ETA: 0s - loss: 0.6758 - accuracy: 0.58 - ETA: 0s - loss: 0.6739 - accuracy: 0.58 - ETA: 0s - loss: 0.6719 - accuracy: 0.58 - ETA: 0s - loss: 0.6707 - accuracy: 0.59 - ETA: 0s - loss: 0.6688 - accuracy: 0.59 - 1s 84us/step - loss: 0.6685 - accuracy: 0.5961 - val_loss: 0.6238 - val_accuracy: 0.7081\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:30 - loss: 0.6952 - accuracy: 0.43 - ETA: 2s - loss: 0.6937 - accuracy: 0.5063 - ETA: 1s - loss: 0.6932 - accuracy: 0.52 - ETA: 1s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6917 - accuracy: 0.52 - ETA: 0s - loss: 0.6896 - accuracy: 0.54 - ETA: 0s - loss: 0.6881 - accuracy: 0.55 - ETA: 0s - loss: 0.6855 - accuracy: 0.56 - ETA: 0s - loss: 0.6820 - accuracy: 0.57 - ETA: 0s - loss: 0.6793 - accuracy: 0.58 - ETA: 0s - loss: 0.6768 - accuracy: 0.59 - ETA: 0s - loss: 0.6738 - accuracy: 0.59 - ETA: 0s - loss: 0.6712 - accuracy: 0.60 - ETA: 0s - loss: 0.6690 - accuracy: 0.60 - ETA: 0s - loss: 0.6669 - accuracy: 0.60 - ETA: 0s - loss: 0.6655 - accuracy: 0.60 - ETA: 0s - loss: 0.6637 - accuracy: 0.61 - 1s 80us/step - loss: 0.6632 - accuracy: 0.6133 - val_loss: 0.6148 - val_accuracy: 0.7138\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:31 - loss: 0.6943 - accuracy: 0.43 - ETA: 2s - loss: 0.6932 - accuracy: 0.5306 - ETA: 1s - loss: 0.6920 - accuracy: 0.53 - ETA: 1s - loss: 0.6907 - accuracy: 0.54 - ETA: 0s - loss: 0.6894 - accuracy: 0.55 - ETA: 0s - loss: 0.6882 - accuracy: 0.55 - ETA: 0s - loss: 0.6858 - accuracy: 0.56 - ETA: 0s - loss: 0.6838 - accuracy: 0.57 - ETA: 0s - loss: 0.6809 - accuracy: 0.57 - ETA: 0s - loss: 0.6783 - accuracy: 0.58 - ETA: 0s - loss: 0.6760 - accuracy: 0.59 - ETA: 0s - loss: 0.6729 - accuracy: 0.59 - ETA: 0s - loss: 0.6704 - accuracy: 0.60 - ETA: 0s - loss: 0.6687 - accuracy: 0.60 - ETA: 0s - loss: 0.6664 - accuracy: 0.60 - ETA: 0s - loss: 0.6641 - accuracy: 0.61 - ETA: 0s - loss: 0.6621 - accuracy: 0.61 - 1s 81us/step - loss: 0.6615 - accuracy: 0.6149 - val_loss: 0.6126 - val_accuracy: 0.7173\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:30 - loss: 0.6994 - accuracy: 0.37 - ETA: 2s - loss: 0.6932 - accuracy: 0.5472 - ETA: 1s - loss: 0.6927 - accuracy: 0.55 - ETA: 1s - loss: 0.6912 - accuracy: 0.56 - ETA: 0s - loss: 0.6901 - accuracy: 0.57 - ETA: 0s - loss: 0.6893 - accuracy: 0.57 - ETA: 0s - loss: 0.6873 - accuracy: 0.58 - ETA: 0s - loss: 0.6847 - accuracy: 0.59 - ETA: 0s - loss: 0.6830 - accuracy: 0.59 - ETA: 0s - loss: 0.6814 - accuracy: 0.60 - ETA: 0s - loss: 0.6781 - accuracy: 0.60 - ETA: 0s - loss: 0.6751 - accuracy: 0.61 - ETA: 0s - loss: 0.6721 - accuracy: 0.61 - ETA: 0s - loss: 0.6695 - accuracy: 0.62 - ETA: 0s - loss: 0.6675 - accuracy: 0.62 - ETA: 0s - loss: 0.6654 - accuracy: 0.62 - ETA: 0s - loss: 0.6630 - accuracy: 0.62 - 1s 79us/step - loss: 0.6628 - accuracy: 0.6295 - val_loss: 0.6181 - val_accuracy: 0.6875\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:31 - loss: 0.6882 - accuracy: 0.62 - ETA: 2s - loss: 0.6935 - accuracy: 0.5459 - ETA: 1s - loss: 0.6927 - accuracy: 0.53 - ETA: 1s - loss: 0.6920 - accuracy: 0.53 - ETA: 1s - loss: 0.6897 - accuracy: 0.55 - ETA: 0s - loss: 0.6872 - accuracy: 0.57 - ETA: 0s - loss: 0.6841 - accuracy: 0.57 - ETA: 0s - loss: 0.6810 - accuracy: 0.58 - ETA: 0s - loss: 0.6770 - accuracy: 0.59 - ETA: 0s - loss: 0.6736 - accuracy: 0.60 - ETA: 0s - loss: 0.6699 - accuracy: 0.60 - ETA: 0s - loss: 0.6671 - accuracy: 0.61 - ETA: 0s - loss: 0.6639 - accuracy: 0.61 - ETA: 0s - loss: 0.6619 - accuracy: 0.62 - ETA: 0s - loss: 0.6600 - accuracy: 0.62 - ETA: 0s - loss: 0.6567 - accuracy: 0.62 - ETA: 0s - loss: 0.6530 - accuracy: 0.63 - 1s 81us/step - loss: 0.6525 - accuracy: 0.6324 - val_loss: 0.5983 - val_accuracy: 0.7244\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:30 - loss: 0.6929 - accuracy: 0.56 - ETA: 2s - loss: 0.6927 - accuracy: 0.5362 - ETA: 1s - loss: 0.6915 - accuracy: 0.54 - ETA: 1s - loss: 0.6898 - accuracy: 0.56 - ETA: 0s - loss: 0.6874 - accuracy: 0.57 - ETA: 0s - loss: 0.6845 - accuracy: 0.58 - ETA: 0s - loss: 0.6822 - accuracy: 0.58 - ETA: 0s - loss: 0.6793 - accuracy: 0.59 - ETA: 0s - loss: 0.6763 - accuracy: 0.60 - ETA: 0s - loss: 0.6742 - accuracy: 0.60 - ETA: 0s - loss: 0.6692 - accuracy: 0.61 - ETA: 0s - loss: 0.6642 - accuracy: 0.62 - ETA: 0s - loss: 0.6605 - accuracy: 0.62 - ETA: 0s - loss: 0.6564 - accuracy: 0.63 - ETA: 0s - loss: 0.6540 - accuracy: 0.63 - ETA: 0s - loss: 0.6516 - accuracy: 0.63 - ETA: 0s - loss: 0.6484 - accuracy: 0.64 - 1s 79us/step - loss: 0.6475 - accuracy: 0.6451 - val_loss: 0.5898 - val_accuracy: 0.7145\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:32 - loss: 0.6904 - accuracy: 0.81 - ETA: 2s - loss: 0.6931 - accuracy: 0.5378 - ETA: 1s - loss: 0.6910 - accuracy: 0.55 - ETA: 1s - loss: 0.6889 - accuracy: 0.56 - ETA: 0s - loss: 0.6868 - accuracy: 0.56 - ETA: 0s - loss: 0.6841 - accuracy: 0.57 - ETA: 0s - loss: 0.6799 - accuracy: 0.58 - ETA: 0s - loss: 0.6757 - accuracy: 0.59 - ETA: 0s - loss: 0.6739 - accuracy: 0.60 - ETA: 0s - loss: 0.6713 - accuracy: 0.60 - ETA: 0s - loss: 0.6687 - accuracy: 0.61 - ETA: 0s - loss: 0.6646 - accuracy: 0.61 - ETA: 0s - loss: 0.6610 - accuracy: 0.62 - ETA: 0s - loss: 0.6585 - accuracy: 0.62 - ETA: 0s - loss: 0.6552 - accuracy: 0.63 - ETA: 0s - loss: 0.6521 - accuracy: 0.63 - ETA: 0s - loss: 0.6495 - accuracy: 0.63 - 1s 81us/step - loss: 0.6475 - accuracy: 0.6408 - val_loss: 0.5903 - val_accuracy: 0.7124\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 1:33 - loss: 0.6977 - accuracy: 0.31 - ETA: 2s - loss: 0.6943 - accuracy: 0.5052 - ETA: 1s - loss: 0.6917 - accuracy: 0.53 - ETA: 1s - loss: 0.6907 - accuracy: 0.52 - ETA: 1s - loss: 0.6892 - accuracy: 0.54 - ETA: 0s - loss: 0.6870 - accuracy: 0.56 - ETA: 0s - loss: 0.6846 - accuracy: 0.57 - ETA: 0s - loss: 0.6826 - accuracy: 0.57 - ETA: 0s - loss: 0.6805 - accuracy: 0.58 - ETA: 0s - loss: 0.6771 - accuracy: 0.59 - ETA: 0s - loss: 0.6739 - accuracy: 0.59 - ETA: 0s - loss: 0.6704 - accuracy: 0.60 - ETA: 0s - loss: 0.6676 - accuracy: 0.60 - ETA: 0s - loss: 0.6649 - accuracy: 0.61 - ETA: 0s - loss: 0.6614 - accuracy: 0.62 - ETA: 0s - loss: 0.6580 - accuracy: 0.62 - ETA: 0s - loss: 0.6554 - accuracy: 0.62 - 1s 81us/step - loss: 0.6542 - accuracy: 0.6309 - val_loss: 0.5963 - val_accuracy: 0.7081\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:33 - loss: 0.6975 - accuracy: 0.31 - ETA: 2s - loss: 0.6944 - accuracy: 0.5078 - ETA: 1s - loss: 0.6929 - accuracy: 0.54 - ETA: 1s - loss: 0.6909 - accuracy: 0.55 - ETA: 1s - loss: 0.6888 - accuracy: 0.57 - ETA: 0s - loss: 0.6856 - accuracy: 0.58 - ETA: 0s - loss: 0.6831 - accuracy: 0.59 - ETA: 0s - loss: 0.6805 - accuracy: 0.60 - ETA: 0s - loss: 0.6768 - accuracy: 0.60 - ETA: 0s - loss: 0.6735 - accuracy: 0.61 - ETA: 0s - loss: 0.6704 - accuracy: 0.61 - ETA: 0s - loss: 0.6673 - accuracy: 0.62 - ETA: 0s - loss: 0.6638 - accuracy: 0.62 - ETA: 0s - loss: 0.6610 - accuracy: 0.63 - ETA: 0s - loss: 0.6580 - accuracy: 0.63 - ETA: 0s - loss: 0.6549 - accuracy: 0.63 - ETA: 0s - loss: 0.6524 - accuracy: 0.64 - ETA: 0s - loss: 0.6499 - accuracy: 0.64 - 1s 84us/step - loss: 0.6491 - accuracy: 0.6466 - val_loss: 0.5927 - val_accuracy: 0.7152\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:33 - loss: 0.6958 - accuracy: 0.25 - ETA: 2s - loss: 0.6918 - accuracy: 0.5268 - ETA: 1s - loss: 0.6915 - accuracy: 0.51 - ETA: 1s - loss: 0.6892 - accuracy: 0.53 - ETA: 0s - loss: 0.6868 - accuracy: 0.54 - ETA: 0s - loss: 0.6835 - accuracy: 0.55 - ETA: 0s - loss: 0.6803 - accuracy: 0.56 - ETA: 0s - loss: 0.6758 - accuracy: 0.58 - ETA: 0s - loss: 0.6731 - accuracy: 0.59 - ETA: 0s - loss: 0.6699 - accuracy: 0.59 - ETA: 0s - loss: 0.6661 - accuracy: 0.60 - ETA: 0s - loss: 0.6630 - accuracy: 0.61 - ETA: 0s - loss: 0.6604 - accuracy: 0.61 - ETA: 0s - loss: 0.6570 - accuracy: 0.62 - ETA: 0s - loss: 0.6545 - accuracy: 0.62 - ETA: 0s - loss: 0.6519 - accuracy: 0.62 - ETA: 0s - loss: 0.6498 - accuracy: 0.63 - 1s 81us/step - loss: 0.6486 - accuracy: 0.6348 - val_loss: 0.6000 - val_accuracy: 0.7124\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:17 - loss: 0.6948 - accuracy: 0.37 - ETA: 2s - loss: 0.6939 - accuracy: 0.5000 - ETA: 1s - loss: 0.6925 - accuracy: 0.51 - ETA: 1s - loss: 0.6910 - accuracy: 0.51 - ETA: 0s - loss: 0.6897 - accuracy: 0.51 - ETA: 0s - loss: 0.6895 - accuracy: 0.51 - ETA: 0s - loss: 0.6887 - accuracy: 0.52 - ETA: 0s - loss: 0.6875 - accuracy: 0.52 - ETA: 0s - loss: 0.6865 - accuracy: 0.53 - ETA: 0s - loss: 0.6858 - accuracy: 0.53 - ETA: 0s - loss: 0.6851 - accuracy: 0.53 - ETA: 0s - loss: 0.6845 - accuracy: 0.53 - ETA: 0s - loss: 0.6837 - accuracy: 0.54 - ETA: 0s - loss: 0.6832 - accuracy: 0.54 - ETA: 0s - loss: 0.6828 - accuracy: 0.54 - ETA: 0s - loss: 0.6825 - accuracy: 0.54 - 1s 74us/step - loss: 0.6825 - accuracy: 0.5433 - val_loss: 0.6693 - val_accuracy: 0.6974\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:17 - loss: 0.6895 - accuracy: 0.56 - ETA: 2s - loss: 0.6939 - accuracy: 0.5084 - ETA: 1s - loss: 0.6935 - accuracy: 0.49 - ETA: 1s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6916 - accuracy: 0.51 - ETA: 0s - loss: 0.6903 - accuracy: 0.51 - ETA: 0s - loss: 0.6895 - accuracy: 0.51 - ETA: 0s - loss: 0.6884 - accuracy: 0.52 - ETA: 0s - loss: 0.6877 - accuracy: 0.52 - ETA: 0s - loss: 0.6868 - accuracy: 0.52 - ETA: 0s - loss: 0.6858 - accuracy: 0.53 - ETA: 0s - loss: 0.6847 - accuracy: 0.53 - ETA: 0s - loss: 0.6836 - accuracy: 0.53 - ETA: 0s - loss: 0.6826 - accuracy: 0.54 - ETA: 0s - loss: 0.6815 - accuracy: 0.54 - ETA: 0s - loss: 0.6809 - accuracy: 0.54 - 1s 76us/step - loss: 0.6799 - accuracy: 0.5500 - val_loss: 0.6618 - val_accuracy: 0.5909\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:19 - loss: 0.6925 - accuracy: 0.62 - ETA: 2s - loss: 0.6894 - accuracy: 0.5536 - ETA: 1s - loss: 0.6893 - accuracy: 0.54 - ETA: 1s - loss: 0.6894 - accuracy: 0.54 - ETA: 0s - loss: 0.6890 - accuracy: 0.55 - ETA: 0s - loss: 0.6886 - accuracy: 0.55 - ETA: 0s - loss: 0.6878 - accuracy: 0.55 - ETA: 0s - loss: 0.6868 - accuracy: 0.55 - ETA: 0s - loss: 0.6865 - accuracy: 0.55 - ETA: 0s - loss: 0.6857 - accuracy: 0.55 - ETA: 0s - loss: 0.6857 - accuracy: 0.55 - ETA: 0s - loss: 0.6849 - accuracy: 0.56 - ETA: 0s - loss: 0.6845 - accuracy: 0.56 - ETA: 0s - loss: 0.6840 - accuracy: 0.56 - ETA: 0s - loss: 0.6839 - accuracy: 0.56 - ETA: 0s - loss: 0.6836 - accuracy: 0.56 - ETA: 0s - loss: 0.6830 - accuracy: 0.56 - 1s 79us/step - loss: 0.6828 - accuracy: 0.5651 - val_loss: 0.6711 - val_accuracy: 0.6967\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:22 - loss: 0.6931 - accuracy: 0.68 - ETA: 2s - loss: 0.6916 - accuracy: 0.5177 - ETA: 1s - loss: 0.6904 - accuracy: 0.52 - ETA: 1s - loss: 0.6891 - accuracy: 0.53 - ETA: 0s - loss: 0.6887 - accuracy: 0.53 - ETA: 0s - loss: 0.6877 - accuracy: 0.54 - ETA: 0s - loss: 0.6873 - accuracy: 0.54 - ETA: 0s - loss: 0.6870 - accuracy: 0.54 - ETA: 0s - loss: 0.6861 - accuracy: 0.54 - ETA: 0s - loss: 0.6855 - accuracy: 0.54 - ETA: 0s - loss: 0.6848 - accuracy: 0.54 - ETA: 0s - loss: 0.6838 - accuracy: 0.55 - ETA: 0s - loss: 0.6825 - accuracy: 0.55 - ETA: 0s - loss: 0.6818 - accuracy: 0.55 - ETA: 0s - loss: 0.6810 - accuracy: 0.56 - ETA: 0s - loss: 0.6801 - accuracy: 0.56 - 1s 75us/step - loss: 0.6798 - accuracy: 0.5624 - val_loss: 0.6608 - val_accuracy: 0.7067\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:16 - loss: 0.6924 - accuracy: 0.62 - ETA: 2s - loss: 0.6890 - accuracy: 0.5505 - ETA: 1s - loss: 0.6902 - accuracy: 0.52 - ETA: 1s - loss: 0.6905 - accuracy: 0.51 - ETA: 0s - loss: 0.6900 - accuracy: 0.51 - ETA: 0s - loss: 0.6903 - accuracy: 0.50 - ETA: 0s - loss: 0.6899 - accuracy: 0.51 - ETA: 0s - loss: 0.6892 - accuracy: 0.52 - ETA: 0s - loss: 0.6884 - accuracy: 0.52 - ETA: 0s - loss: 0.6876 - accuracy: 0.52 - ETA: 0s - loss: 0.6865 - accuracy: 0.53 - ETA: 0s - loss: 0.6857 - accuracy: 0.53 - ETA: 0s - loss: 0.6849 - accuracy: 0.53 - ETA: 0s - loss: 0.6843 - accuracy: 0.53 - ETA: 0s - loss: 0.6837 - accuracy: 0.53 - ETA: 0s - loss: 0.6830 - accuracy: 0.54 - 1s 75us/step - loss: 0.6828 - accuracy: 0.5450 - val_loss: 0.6676 - val_accuracy: 0.6151\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:16 - loss: 0.6939 - accuracy: 0.56 - ETA: 2s - loss: 0.6936 - accuracy: 0.4873 - ETA: 1s - loss: 0.6932 - accuracy: 0.49 - ETA: 0s - loss: 0.6923 - accuracy: 0.50 - ETA: 0s - loss: 0.6915 - accuracy: 0.51 - ETA: 0s - loss: 0.6907 - accuracy: 0.51 - ETA: 0s - loss: 0.6895 - accuracy: 0.52 - ETA: 0s - loss: 0.6882 - accuracy: 0.53 - ETA: 0s - loss: 0.6871 - accuracy: 0.53 - ETA: 0s - loss: 0.6865 - accuracy: 0.54 - ETA: 0s - loss: 0.6856 - accuracy: 0.54 - ETA: 0s - loss: 0.6839 - accuracy: 0.55 - ETA: 0s - loss: 0.6825 - accuracy: 0.55 - ETA: 0s - loss: 0.6814 - accuracy: 0.55 - ETA: 0s - loss: 0.6803 - accuracy: 0.56 - ETA: 0s - loss: 0.6794 - accuracy: 0.56 - 1s 75us/step - loss: 0.6788 - accuracy: 0.5629 - val_loss: 0.6610 - val_accuracy: 0.7024\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 30us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:22 - loss: 0.6918 - accuracy: 0.56 - ETA: 2s - loss: 0.6929 - accuracy: 0.5083 - ETA: 1s - loss: 0.6914 - accuracy: 0.52 - ETA: 1s - loss: 0.6899 - accuracy: 0.55 - ETA: 0s - loss: 0.6879 - accuracy: 0.56 - ETA: 0s - loss: 0.6859 - accuracy: 0.57 - ETA: 0s - loss: 0.6838 - accuracy: 0.58 - ETA: 0s - loss: 0.6823 - accuracy: 0.58 - ETA: 0s - loss: 0.6797 - accuracy: 0.59 - ETA: 0s - loss: 0.6777 - accuracy: 0.60 - ETA: 0s - loss: 0.6758 - accuracy: 0.60 - ETA: 0s - loss: 0.6741 - accuracy: 0.61 - ETA: 0s - loss: 0.6724 - accuracy: 0.61 - ETA: 0s - loss: 0.6709 - accuracy: 0.61 - ETA: 0s - loss: 0.6693 - accuracy: 0.61 - ETA: 0s - loss: 0.6683 - accuracy: 0.61 - 1s 75us/step - loss: 0.6677 - accuracy: 0.6204 - val_loss: 0.6390 - val_accuracy: 0.7045\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:21 - loss: 0.6952 - accuracy: 0.37 - ETA: 2s - loss: 0.6930 - accuracy: 0.5174 - ETA: 1s - loss: 0.6908 - accuracy: 0.54 - ETA: 1s - loss: 0.6890 - accuracy: 0.56 - ETA: 0s - loss: 0.6873 - accuracy: 0.57 - ETA: 0s - loss: 0.6857 - accuracy: 0.58 - ETA: 0s - loss: 0.6838 - accuracy: 0.59 - ETA: 0s - loss: 0.6817 - accuracy: 0.59 - ETA: 0s - loss: 0.6796 - accuracy: 0.60 - ETA: 0s - loss: 0.6782 - accuracy: 0.60 - ETA: 0s - loss: 0.6772 - accuracy: 0.60 - ETA: 0s - loss: 0.6749 - accuracy: 0.61 - ETA: 0s - loss: 0.6732 - accuracy: 0.61 - ETA: 0s - loss: 0.6715 - accuracy: 0.61 - ETA: 0s - loss: 0.6692 - accuracy: 0.62 - ETA: 0s - loss: 0.6683 - accuracy: 0.62 - 1s 77us/step - loss: 0.6673 - accuracy: 0.6228 - val_loss: 0.6376 - val_accuracy: 0.7102\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:19 - loss: 0.6950 - accuracy: 0.50 - ETA: 2s - loss: 0.6927 - accuracy: 0.5224 - ETA: 1s - loss: 0.6907 - accuracy: 0.53 - ETA: 1s - loss: 0.6894 - accuracy: 0.54 - ETA: 0s - loss: 0.6873 - accuracy: 0.56 - ETA: 0s - loss: 0.6852 - accuracy: 0.56 - ETA: 0s - loss: 0.6837 - accuracy: 0.57 - ETA: 0s - loss: 0.6821 - accuracy: 0.57 - ETA: 0s - loss: 0.6804 - accuracy: 0.57 - ETA: 0s - loss: 0.6786 - accuracy: 0.58 - ETA: 0s - loss: 0.6774 - accuracy: 0.58 - ETA: 0s - loss: 0.6757 - accuracy: 0.59 - ETA: 0s - loss: 0.6741 - accuracy: 0.59 - ETA: 0s - loss: 0.6729 - accuracy: 0.60 - ETA: 0s - loss: 0.6714 - accuracy: 0.60 - ETA: 0s - loss: 0.6697 - accuracy: 0.60 - 1s 74us/step - loss: 0.6693 - accuracy: 0.6098 - val_loss: 0.6401 - val_accuracy: 0.7152\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:22 - loss: 0.6934 - accuracy: 0.75 - ETA: 2s - loss: 0.6935 - accuracy: 0.5153 - ETA: 1s - loss: 0.6912 - accuracy: 0.54 - ETA: 1s - loss: 0.6898 - accuracy: 0.55 - ETA: 0s - loss: 0.6871 - accuracy: 0.57 - ETA: 0s - loss: 0.6848 - accuracy: 0.58 - ETA: 0s - loss: 0.6827 - accuracy: 0.58 - ETA: 0s - loss: 0.6809 - accuracy: 0.59 - ETA: 0s - loss: 0.6786 - accuracy: 0.59 - ETA: 0s - loss: 0.6760 - accuracy: 0.60 - ETA: 0s - loss: 0.6732 - accuracy: 0.61 - ETA: 0s - loss: 0.6710 - accuracy: 0.61 - ETA: 0s - loss: 0.6693 - accuracy: 0.61 - ETA: 0s - loss: 0.6680 - accuracy: 0.62 - ETA: 0s - loss: 0.6662 - accuracy: 0.62 - ETA: 0s - loss: 0.6646 - accuracy: 0.62 - 1s 76us/step - loss: 0.6633 - accuracy: 0.6261 - val_loss: 0.6315 - val_accuracy: 0.7145\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:20 - loss: 0.6899 - accuracy: 0.50 - ETA: 2s - loss: 0.6906 - accuracy: 0.5481 - ETA: 1s - loss: 0.6897 - accuracy: 0.55 - ETA: 1s - loss: 0.6881 - accuracy: 0.55 - ETA: 0s - loss: 0.6872 - accuracy: 0.56 - ETA: 0s - loss: 0.6865 - accuracy: 0.57 - ETA: 0s - loss: 0.6850 - accuracy: 0.57 - ETA: 0s - loss: 0.6838 - accuracy: 0.58 - ETA: 0s - loss: 0.6830 - accuracy: 0.58 - ETA: 0s - loss: 0.6818 - accuracy: 0.59 - ETA: 0s - loss: 0.6804 - accuracy: 0.59 - ETA: 0s - loss: 0.6791 - accuracy: 0.60 - ETA: 0s - loss: 0.6775 - accuracy: 0.60 - ETA: 0s - loss: 0.6765 - accuracy: 0.60 - ETA: 0s - loss: 0.6763 - accuracy: 0.60 - ETA: 0s - loss: 0.6752 - accuracy: 0.60 - 1s 76us/step - loss: 0.6742 - accuracy: 0.6097 - val_loss: 0.6512 - val_accuracy: 0.7124\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:16 - loss: 0.6913 - accuracy: 0.75 - ETA: 2s - loss: 0.6917 - accuracy: 0.5295 - ETA: 1s - loss: 0.6907 - accuracy: 0.52 - ETA: 1s - loss: 0.6889 - accuracy: 0.53 - ETA: 0s - loss: 0.6875 - accuracy: 0.55 - ETA: 0s - loss: 0.6848 - accuracy: 0.56 - ETA: 0s - loss: 0.6836 - accuracy: 0.57 - ETA: 0s - loss: 0.6819 - accuracy: 0.57 - ETA: 0s - loss: 0.6798 - accuracy: 0.58 - ETA: 0s - loss: 0.6780 - accuracy: 0.58 - ETA: 0s - loss: 0.6759 - accuracy: 0.58 - ETA: 0s - loss: 0.6746 - accuracy: 0.59 - ETA: 0s - loss: 0.6736 - accuracy: 0.59 - ETA: 0s - loss: 0.6719 - accuracy: 0.59 - ETA: 0s - loss: 0.6706 - accuracy: 0.59 - ETA: 0s - loss: 0.6695 - accuracy: 0.60 - 1s 75us/step - loss: 0.6691 - accuracy: 0.6018 - val_loss: 0.6452 - val_accuracy: 0.7081\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:21 - loss: 0.6925 - accuracy: 0.62 - ETA: 2s - loss: 0.6937 - accuracy: 0.5104 - ETA: 1s - loss: 0.6901 - accuracy: 0.52 - ETA: 1s - loss: 0.6892 - accuracy: 0.53 - ETA: 0s - loss: 0.6868 - accuracy: 0.55 - ETA: 0s - loss: 0.6837 - accuracy: 0.56 - ETA: 0s - loss: 0.6816 - accuracy: 0.57 - ETA: 0s - loss: 0.6792 - accuracy: 0.58 - ETA: 0s - loss: 0.6776 - accuracy: 0.59 - ETA: 0s - loss: 0.6757 - accuracy: 0.59 - ETA: 0s - loss: 0.6734 - accuracy: 0.60 - ETA: 0s - loss: 0.6712 - accuracy: 0.61 - ETA: 0s - loss: 0.6681 - accuracy: 0.61 - ETA: 0s - loss: 0.6672 - accuracy: 0.62 - ETA: 0s - loss: 0.6655 - accuracy: 0.62 - ETA: 0s - loss: 0.6629 - accuracy: 0.62 - 1s 74us/step - loss: 0.6626 - accuracy: 0.6262 - val_loss: 0.6256 - val_accuracy: 0.7024\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 30us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 1:19 - loss: 0.6926 - accuracy: 0.62 - ETA: 2s - loss: 0.6919 - accuracy: 0.5409 - ETA: 1s - loss: 0.6877 - accuracy: 0.57 - ETA: 1s - loss: 0.6845 - accuracy: 0.58 - ETA: 0s - loss: 0.6812 - accuracy: 0.59 - ETA: 0s - loss: 0.6781 - accuracy: 0.60 - ETA: 0s - loss: 0.6754 - accuracy: 0.61 - ETA: 0s - loss: 0.6725 - accuracy: 0.61 - ETA: 0s - loss: 0.6700 - accuracy: 0.61 - ETA: 0s - loss: 0.6687 - accuracy: 0.62 - ETA: 0s - loss: 0.6671 - accuracy: 0.62 - ETA: 0s - loss: 0.6642 - accuracy: 0.62 - ETA: 0s - loss: 0.6616 - accuracy: 0.63 - ETA: 0s - loss: 0.6602 - accuracy: 0.63 - ETA: 0s - loss: 0.6588 - accuracy: 0.63 - ETA: 0s - loss: 0.6573 - accuracy: 0.63 - 1s 78us/step - loss: 0.6565 - accuracy: 0.6408 - val_loss: 0.6210 - val_accuracy: 0.7152\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:17 - loss: 0.6928 - accuracy: 0.62 - ETA: 2s - loss: 0.6896 - accuracy: 0.5270 - ETA: 1s - loss: 0.6838 - accuracy: 0.56 - ETA: 1s - loss: 0.6813 - accuracy: 0.57 - ETA: 0s - loss: 0.6797 - accuracy: 0.57 - ETA: 0s - loss: 0.6779 - accuracy: 0.58 - ETA: 0s - loss: 0.6756 - accuracy: 0.59 - ETA: 0s - loss: 0.6736 - accuracy: 0.60 - ETA: 0s - loss: 0.6717 - accuracy: 0.60 - ETA: 0s - loss: 0.6692 - accuracy: 0.61 - ETA: 0s - loss: 0.6670 - accuracy: 0.61 - ETA: 0s - loss: 0.6648 - accuracy: 0.62 - ETA: 0s - loss: 0.6628 - accuracy: 0.62 - ETA: 0s - loss: 0.6610 - accuracy: 0.63 - ETA: 0s - loss: 0.6591 - accuracy: 0.63 - ETA: 0s - loss: 0.6581 - accuracy: 0.63 - 1s 75us/step - loss: 0.6576 - accuracy: 0.6341 - val_loss: 0.6220 - val_accuracy: 0.7088\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 30us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:19 - loss: 0.6979 - accuracy: 0.50 - ETA: 2s - loss: 0.6921 - accuracy: 0.5236 - ETA: 1s - loss: 0.6894 - accuracy: 0.52 - ETA: 1s - loss: 0.6883 - accuracy: 0.53 - ETA: 0s - loss: 0.6865 - accuracy: 0.54 - ETA: 0s - loss: 0.6844 - accuracy: 0.56 - ETA: 0s - loss: 0.6820 - accuracy: 0.57 - ETA: 0s - loss: 0.6805 - accuracy: 0.58 - ETA: 0s - loss: 0.6786 - accuracy: 0.59 - ETA: 0s - loss: 0.6760 - accuracy: 0.59 - ETA: 0s - loss: 0.6746 - accuracy: 0.59 - ETA: 0s - loss: 0.6728 - accuracy: 0.60 - ETA: 0s - loss: 0.6713 - accuracy: 0.60 - ETA: 0s - loss: 0.6690 - accuracy: 0.61 - ETA: 0s - loss: 0.6675 - accuracy: 0.61 - ETA: 0s - loss: 0.6665 - accuracy: 0.61 - 1s 77us/step - loss: 0.6645 - accuracy: 0.6217 - val_loss: 0.6318 - val_accuracy: 0.6989\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:17 - loss: 0.6957 - accuracy: 0.50 - ETA: 2s - loss: 0.6934 - accuracy: 0.5026 - ETA: 1s - loss: 0.6899 - accuracy: 0.54 - ETA: 1s - loss: 0.6869 - accuracy: 0.57 - ETA: 0s - loss: 0.6815 - accuracy: 0.59 - ETA: 0s - loss: 0.6801 - accuracy: 0.59 - ETA: 0s - loss: 0.6763 - accuracy: 0.60 - ETA: 0s - loss: 0.6724 - accuracy: 0.61 - ETA: 0s - loss: 0.6696 - accuracy: 0.62 - ETA: 0s - loss: 0.6670 - accuracy: 0.62 - ETA: 0s - loss: 0.6650 - accuracy: 0.63 - ETA: 0s - loss: 0.6628 - accuracy: 0.63 - ETA: 0s - loss: 0.6608 - accuracy: 0.63 - ETA: 0s - loss: 0.6589 - accuracy: 0.64 - ETA: 0s - loss: 0.6561 - accuracy: 0.64 - ETA: 0s - loss: 0.6537 - accuracy: 0.64 - 1s 77us/step - loss: 0.6512 - accuracy: 0.6532 - val_loss: 0.6095 - val_accuracy: 0.7145\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:17 - loss: 0.7060 - accuracy: 0.43 - ETA: 2s - loss: 0.6902 - accuracy: 0.5453 - ETA: 1s - loss: 0.6876 - accuracy: 0.56 - ETA: 1s - loss: 0.6843 - accuracy: 0.58 - ETA: 0s - loss: 0.6818 - accuracy: 0.59 - ETA: 0s - loss: 0.6787 - accuracy: 0.59 - ETA: 0s - loss: 0.6760 - accuracy: 0.60 - ETA: 0s - loss: 0.6733 - accuracy: 0.61 - ETA: 0s - loss: 0.6709 - accuracy: 0.61 - ETA: 0s - loss: 0.6677 - accuracy: 0.62 - ETA: 0s - loss: 0.6647 - accuracy: 0.63 - ETA: 0s - loss: 0.6628 - accuracy: 0.63 - ETA: 0s - loss: 0.6610 - accuracy: 0.63 - ETA: 0s - loss: 0.6603 - accuracy: 0.63 - ETA: 0s - loss: 0.6588 - accuracy: 0.63 - ETA: 0s - loss: 0.6570 - accuracy: 0.64 - 1s 75us/step - loss: 0.6565 - accuracy: 0.6436 - val_loss: 0.6238 - val_accuracy: 0.7102\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:34 - loss: 0.6932 - accuracy: 0.50 - ETA: 2s - loss: 0.6934 - accuracy: 0.5300 - ETA: 1s - loss: 0.6932 - accuracy: 0.53 - ETA: 1s - loss: 0.6931 - accuracy: 0.53 - ETA: 0s - loss: 0.6925 - accuracy: 0.54 - ETA: 0s - loss: 0.6918 - accuracy: 0.55 - ETA: 0s - loss: 0.6909 - accuracy: 0.56 - ETA: 0s - loss: 0.6894 - accuracy: 0.56 - ETA: 0s - loss: 0.6882 - accuracy: 0.57 - ETA: 0s - loss: 0.6870 - accuracy: 0.57 - ETA: 0s - loss: 0.6855 - accuracy: 0.58 - ETA: 0s - loss: 0.6834 - accuracy: 0.59 - ETA: 0s - loss: 0.6817 - accuracy: 0.59 - ETA: 0s - loss: 0.6793 - accuracy: 0.59 - ETA: 0s - loss: 0.6777 - accuracy: 0.60 - ETA: 0s - loss: 0.6764 - accuracy: 0.60 - ETA: 0s - loss: 0.6749 - accuracy: 0.61 - 1s 81us/step - loss: 0.6743 - accuracy: 0.6124 - val_loss: 0.6355 - val_accuracy: 0.7166\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:38 - loss: 0.6932 - accuracy: 0.43 - ETA: 2s - loss: 0.6914 - accuracy: 0.5395 - ETA: 1s - loss: 0.6880 - accuracy: 0.54 - ETA: 1s - loss: 0.6823 - accuracy: 0.55 - ETA: 1s - loss: 0.6820 - accuracy: 0.56 - ETA: 0s - loss: 0.6785 - accuracy: 0.57 - ETA: 0s - loss: 0.6733 - accuracy: 0.58 - ETA: 0s - loss: 0.6695 - accuracy: 0.58 - ETA: 0s - loss: 0.6647 - accuracy: 0.59 - ETA: 0s - loss: 0.6631 - accuracy: 0.60 - ETA: 0s - loss: 0.6609 - accuracy: 0.60 - ETA: 0s - loss: 0.6596 - accuracy: 0.61 - ETA: 0s - loss: 0.6572 - accuracy: 0.61 - ETA: 0s - loss: 0.6555 - accuracy: 0.62 - ETA: 0s - loss: 0.6537 - accuracy: 0.62 - ETA: 0s - loss: 0.6526 - accuracy: 0.62 - ETA: 0s - loss: 0.6516 - accuracy: 0.62 - 1s 83us/step - loss: 0.6502 - accuracy: 0.6314 - val_loss: 0.6001 - val_accuracy: 0.7223\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:35 - loss: 0.6935 - accuracy: 0.56 - ETA: 2s - loss: 0.6937 - accuracy: 0.5025 - ETA: 1s - loss: 0.6931 - accuracy: 0.51 - ETA: 1s - loss: 0.6906 - accuracy: 0.52 - ETA: 0s - loss: 0.6900 - accuracy: 0.51 - ETA: 0s - loss: 0.6887 - accuracy: 0.51 - ETA: 0s - loss: 0.6877 - accuracy: 0.52 - ETA: 0s - loss: 0.6861 - accuracy: 0.53 - ETA: 0s - loss: 0.6829 - accuracy: 0.54 - ETA: 0s - loss: 0.6816 - accuracy: 0.55 - ETA: 0s - loss: 0.6809 - accuracy: 0.55 - ETA: 0s - loss: 0.6800 - accuracy: 0.56 - ETA: 0s - loss: 0.6784 - accuracy: 0.57 - ETA: 0s - loss: 0.6758 - accuracy: 0.57 - ETA: 0s - loss: 0.6732 - accuracy: 0.58 - ETA: 0s - loss: 0.6727 - accuracy: 0.58 - ETA: 0s - loss: 0.6716 - accuracy: 0.58 - 1s 80us/step - loss: 0.6709 - accuracy: 0.5896 - val_loss: 0.6319 - val_accuracy: 0.7145\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:34 - loss: 0.6932 - accuracy: 0.68 - ETA: 2s - loss: 0.6937 - accuracy: 0.5066 - ETA: 1s - loss: 0.6934 - accuracy: 0.52 - ETA: 1s - loss: 0.6922 - accuracy: 0.52 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 0s - loss: 0.6885 - accuracy: 0.54 - ETA: 0s - loss: 0.6850 - accuracy: 0.55 - ETA: 0s - loss: 0.6820 - accuracy: 0.56 - ETA: 0s - loss: 0.6796 - accuracy: 0.57 - ETA: 0s - loss: 0.6765 - accuracy: 0.57 - ETA: 0s - loss: 0.6736 - accuracy: 0.58 - ETA: 0s - loss: 0.6717 - accuracy: 0.58 - ETA: 0s - loss: 0.6688 - accuracy: 0.59 - ETA: 0s - loss: 0.6666 - accuracy: 0.59 - ETA: 0s - loss: 0.6640 - accuracy: 0.59 - ETA: 0s - loss: 0.6612 - accuracy: 0.60 - ETA: 0s - loss: 0.6595 - accuracy: 0.60 - 1s 81us/step - loss: 0.6574 - accuracy: 0.6084 - val_loss: 0.6044 - val_accuracy: 0.7031\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:38 - loss: 0.6926 - accuracy: 0.75 - ETA: 2s - loss: 0.6939 - accuracy: 0.4902 - ETA: 1s - loss: 0.6926 - accuracy: 0.51 - ETA: 1s - loss: 0.6913 - accuracy: 0.52 - ETA: 0s - loss: 0.6903 - accuracy: 0.52 - ETA: 0s - loss: 0.6886 - accuracy: 0.52 - ETA: 0s - loss: 0.6875 - accuracy: 0.53 - ETA: 0s - loss: 0.6850 - accuracy: 0.53 - ETA: 0s - loss: 0.6839 - accuracy: 0.54 - ETA: 0s - loss: 0.6810 - accuracy: 0.54 - ETA: 0s - loss: 0.6780 - accuracy: 0.55 - ETA: 0s - loss: 0.6751 - accuracy: 0.55 - ETA: 0s - loss: 0.6730 - accuracy: 0.55 - ETA: 0s - loss: 0.6713 - accuracy: 0.56 - ETA: 0s - loss: 0.6691 - accuracy: 0.56 - ETA: 0s - loss: 0.6664 - accuracy: 0.57 - ETA: 0s - loss: 0.6647 - accuracy: 0.57 - 1s 81us/step - loss: 0.6645 - accuracy: 0.5756 - val_loss: 0.6139 - val_accuracy: 0.7202\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:36 - loss: 0.6881 - accuracy: 0.56 - ETA: 2s - loss: 0.6923 - accuracy: 0.5166 - ETA: 1s - loss: 0.6921 - accuracy: 0.51 - ETA: 1s - loss: 0.6913 - accuracy: 0.51 - ETA: 1s - loss: 0.6908 - accuracy: 0.51 - ETA: 0s - loss: 0.6898 - accuracy: 0.52 - ETA: 0s - loss: 0.6886 - accuracy: 0.53 - ETA: 0s - loss: 0.6864 - accuracy: 0.54 - ETA: 0s - loss: 0.6854 - accuracy: 0.55 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6823 - accuracy: 0.56 - ETA: 0s - loss: 0.6804 - accuracy: 0.57 - ETA: 0s - loss: 0.6786 - accuracy: 0.58 - ETA: 0s - loss: 0.6771 - accuracy: 0.58 - ETA: 0s - loss: 0.6757 - accuracy: 0.59 - ETA: 0s - loss: 0.6744 - accuracy: 0.59 - ETA: 0s - loss: 0.6730 - accuracy: 0.60 - 1s 82us/step - loss: 0.6715 - accuracy: 0.6046 - val_loss: 0.6384 - val_accuracy: 0.6861\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 30us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:33 - loss: 0.6944 - accuracy: 0.50 - ETA: 2s - loss: 0.6944 - accuracy: 0.4923 - ETA: 1s - loss: 0.6940 - accuracy: 0.49 - ETA: 1s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6918 - accuracy: 0.52 - ETA: 0s - loss: 0.6900 - accuracy: 0.53 - ETA: 0s - loss: 0.6868 - accuracy: 0.55 - ETA: 0s - loss: 0.6847 - accuracy: 0.55 - ETA: 0s - loss: 0.6816 - accuracy: 0.56 - ETA: 0s - loss: 0.6775 - accuracy: 0.57 - ETA: 0s - loss: 0.6755 - accuracy: 0.57 - ETA: 0s - loss: 0.6728 - accuracy: 0.58 - ETA: 0s - loss: 0.6696 - accuracy: 0.58 - ETA: 0s - loss: 0.6668 - accuracy: 0.59 - ETA: 0s - loss: 0.6645 - accuracy: 0.59 - ETA: 0s - loss: 0.6629 - accuracy: 0.60 - ETA: 0s - loss: 0.6607 - accuracy: 0.60 - 1s 81us/step - loss: 0.6595 - accuracy: 0.6051 - val_loss: 0.5973 - val_accuracy: 0.7095\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 31us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:34 - loss: 0.6943 - accuracy: 0.62 - ETA: 2s - loss: 0.6940 - accuracy: 0.5337 - ETA: 1s - loss: 0.6937 - accuracy: 0.52 - ETA: 1s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6905 - accuracy: 0.53 - ETA: 0s - loss: 0.6871 - accuracy: 0.55 - ETA: 0s - loss: 0.6838 - accuracy: 0.56 - ETA: 0s - loss: 0.6790 - accuracy: 0.57 - ETA: 0s - loss: 0.6722 - accuracy: 0.58 - ETA: 0s - loss: 0.6670 - accuracy: 0.59 - ETA: 0s - loss: 0.6622 - accuracy: 0.60 - ETA: 0s - loss: 0.6606 - accuracy: 0.60 - ETA: 0s - loss: 0.6585 - accuracy: 0.61 - ETA: 0s - loss: 0.6553 - accuracy: 0.61 - ETA: 0s - loss: 0.6547 - accuracy: 0.61 - ETA: 0s - loss: 0.6517 - accuracy: 0.62 - ETA: 0s - loss: 0.6503 - accuracy: 0.62 - 1s 82us/step - loss: 0.6491 - accuracy: 0.6289 - val_loss: 0.5885 - val_accuracy: 0.7188\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:33 - loss: 0.6913 - accuracy: 0.62 - ETA: 2s - loss: 0.6934 - accuracy: 0.5510 - ETA: 1s - loss: 0.6921 - accuracy: 0.56 - ETA: 1s - loss: 0.6912 - accuracy: 0.55 - ETA: 0s - loss: 0.6895 - accuracy: 0.55 - ETA: 0s - loss: 0.6888 - accuracy: 0.55 - ETA: 0s - loss: 0.6868 - accuracy: 0.55 - ETA: 0s - loss: 0.6840 - accuracy: 0.56 - ETA: 0s - loss: 0.6812 - accuracy: 0.56 - ETA: 0s - loss: 0.6794 - accuracy: 0.56 - ETA: 0s - loss: 0.6761 - accuracy: 0.57 - ETA: 0s - loss: 0.6728 - accuracy: 0.57 - ETA: 0s - loss: 0.6709 - accuracy: 0.58 - ETA: 0s - loss: 0.6675 - accuracy: 0.58 - ETA: 0s - loss: 0.6645 - accuracy: 0.59 - ETA: 0s - loss: 0.6620 - accuracy: 0.59 - ETA: 0s - loss: 0.6607 - accuracy: 0.59 - 1s 81us/step - loss: 0.6588 - accuracy: 0.6003 - val_loss: 0.6016 - val_accuracy: 0.7060\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:32 - loss: 0.6942 - accuracy: 0.43 - ETA: 2s - loss: 0.6928 - accuracy: 0.5025 - ETA: 1s - loss: 0.6906 - accuracy: 0.51 - ETA: 1s - loss: 0.6896 - accuracy: 0.53 - ETA: 1s - loss: 0.6855 - accuracy: 0.55 - ETA: 0s - loss: 0.6828 - accuracy: 0.56 - ETA: 0s - loss: 0.6807 - accuracy: 0.56 - ETA: 0s - loss: 0.6759 - accuracy: 0.57 - ETA: 0s - loss: 0.6724 - accuracy: 0.58 - ETA: 0s - loss: 0.6690 - accuracy: 0.59 - ETA: 0s - loss: 0.6653 - accuracy: 0.60 - ETA: 0s - loss: 0.6632 - accuracy: 0.60 - ETA: 0s - loss: 0.6594 - accuracy: 0.61 - ETA: 0s - loss: 0.6559 - accuracy: 0.61 - ETA: 0s - loss: 0.6520 - accuracy: 0.61 - ETA: 0s - loss: 0.6500 - accuracy: 0.62 - ETA: 0s - loss: 0.6502 - accuracy: 0.62 - 1s 81us/step - loss: 0.6489 - accuracy: 0.6272 - val_loss: 0.6044 - val_accuracy: 0.7216\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 29us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:33 - loss: 0.6963 - accuracy: 0.31 - ETA: 2s - loss: 0.6934 - accuracy: 0.5026 - ETA: 1s - loss: 0.6924 - accuracy: 0.51 - ETA: 1s - loss: 0.6912 - accuracy: 0.52 - ETA: 0s - loss: 0.6884 - accuracy: 0.55 - ETA: 0s - loss: 0.6849 - accuracy: 0.55 - ETA: 0s - loss: 0.6802 - accuracy: 0.57 - ETA: 0s - loss: 0.6773 - accuracy: 0.57 - ETA: 0s - loss: 0.6727 - accuracy: 0.58 - ETA: 0s - loss: 0.6695 - accuracy: 0.58 - ETA: 0s - loss: 0.6649 - accuracy: 0.59 - ETA: 0s - loss: 0.6613 - accuracy: 0.59 - ETA: 0s - loss: 0.6572 - accuracy: 0.60 - ETA: 0s - loss: 0.6552 - accuracy: 0.61 - ETA: 0s - loss: 0.6527 - accuracy: 0.61 - ETA: 0s - loss: 0.6501 - accuracy: 0.61 - ETA: 0s - loss: 0.6468 - accuracy: 0.62 - 1s 80us/step - loss: 0.6461 - accuracy: 0.6222 - val_loss: 0.5823 - val_accuracy: 0.7244\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 1:33 - loss: 0.6996 - accuracy: 0.50 - ETA: 2s - loss: 0.6947 - accuracy: 0.4832 - ETA: 1s - loss: 0.6940 - accuracy: 0.49 - ETA: 1s - loss: 0.6937 - accuracy: 0.49 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6912 - accuracy: 0.52 - ETA: 0s - loss: 0.6890 - accuracy: 0.53 - ETA: 0s - loss: 0.6860 - accuracy: 0.54 - ETA: 0s - loss: 0.6832 - accuracy: 0.55 - ETA: 0s - loss: 0.6797 - accuracy: 0.56 - ETA: 0s - loss: 0.6771 - accuracy: 0.56 - ETA: 0s - loss: 0.6727 - accuracy: 0.57 - ETA: 0s - loss: 0.6701 - accuracy: 0.58 - ETA: 0s - loss: 0.6682 - accuracy: 0.58 - ETA: 0s - loss: 0.6654 - accuracy: 0.59 - ETA: 0s - loss: 0.6627 - accuracy: 0.59 - ETA: 0s - loss: 0.6600 - accuracy: 0.60 - 1s 80us/step - loss: 0.6596 - accuracy: 0.6037 - val_loss: 0.6081 - val_accuracy: 0.6896\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:34 - loss: 0.6772 - accuracy: 0.81 - ETA: 2s - loss: 0.6894 - accuracy: 0.5510 - ETA: 1s - loss: 0.6901 - accuracy: 0.54 - ETA: 1s - loss: 0.6892 - accuracy: 0.54 - ETA: 1s - loss: 0.6869 - accuracy: 0.55 - ETA: 0s - loss: 0.6838 - accuracy: 0.56 - ETA: 0s - loss: 0.6811 - accuracy: 0.57 - ETA: 0s - loss: 0.6769 - accuracy: 0.58 - ETA: 0s - loss: 0.6722 - accuracy: 0.59 - ETA: 0s - loss: 0.6672 - accuracy: 0.60 - ETA: 0s - loss: 0.6624 - accuracy: 0.60 - ETA: 0s - loss: 0.6594 - accuracy: 0.61 - ETA: 0s - loss: 0.6561 - accuracy: 0.61 - ETA: 0s - loss: 0.6528 - accuracy: 0.62 - ETA: 0s - loss: 0.6490 - accuracy: 0.62 - ETA: 0s - loss: 0.6464 - accuracy: 0.63 - ETA: 0s - loss: 0.6438 - accuracy: 0.63 - 1s 82us/step - loss: 0.6416 - accuracy: 0.6395 - val_loss: 0.5859 - val_accuracy: 0.7244\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 30us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:30 - loss: 0.6948 - accuracy: 0.50 - ETA: 2s - loss: 0.6927 - accuracy: 0.5466 - ETA: 1s - loss: 0.6925 - accuracy: 0.53 - ETA: 1s - loss: 0.6919 - accuracy: 0.53 - ETA: 0s - loss: 0.6894 - accuracy: 0.54 - ETA: 0s - loss: 0.6857 - accuracy: 0.55 - ETA: 0s - loss: 0.6809 - accuracy: 0.56 - ETA: 0s - loss: 0.6757 - accuracy: 0.57 - ETA: 0s - loss: 0.6733 - accuracy: 0.57 - ETA: 0s - loss: 0.6705 - accuracy: 0.58 - ETA: 0s - loss: 0.6662 - accuracy: 0.59 - ETA: 0s - loss: 0.6617 - accuracy: 0.60 - ETA: 0s - loss: 0.6600 - accuracy: 0.60 - ETA: 0s - loss: 0.6557 - accuracy: 0.61 - ETA: 0s - loss: 0.6531 - accuracy: 0.61 - ETA: 0s - loss: 0.6496 - accuracy: 0.62 - ETA: 0s - loss: 0.6481 - accuracy: 0.62 - 1s 79us/step - loss: 0.6483 - accuracy: 0.6249 - val_loss: 0.5970 - val_accuracy: 0.7152\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 1:32 - loss: 0.6940 - accuracy: 0.50 - ETA: 2s - loss: 0.6926 - accuracy: 0.5332 - ETA: 1s - loss: 0.6908 - accuracy: 0.53 - ETA: 1s - loss: 0.6880 - accuracy: 0.54 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6786 - accuracy: 0.57 - ETA: 0s - loss: 0.6728 - accuracy: 0.58 - ETA: 0s - loss: 0.6680 - accuracy: 0.59 - ETA: 0s - loss: 0.6639 - accuracy: 0.59 - ETA: 0s - loss: 0.6600 - accuracy: 0.60 - ETA: 0s - loss: 0.6558 - accuracy: 0.61 - ETA: 0s - loss: 0.6512 - accuracy: 0.62 - ETA: 0s - loss: 0.6479 - accuracy: 0.62 - ETA: 0s - loss: 0.6448 - accuracy: 0.63 - ETA: 0s - loss: 0.6434 - accuracy: 0.63 - ETA: 0s - loss: 0.6418 - accuracy: 0.63 - ETA: 0s - loss: 0.6395 - accuracy: 0.64 - 1s 82us/step - loss: 0.6377 - accuracy: 0.6458 - val_loss: 0.5815 - val_accuracy: 0.7251\n",
      "2815/2815 [==============================] - ETA:  - ETA:  - 0s 27us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:30 - loss: 0.6921 - accuracy: 0.68 - ETA: 2s - loss: 0.6912 - accuracy: 0.5387 - ETA: 1s - loss: 0.6889 - accuracy: 0.53 - ETA: 1s - loss: 0.6887 - accuracy: 0.52 - ETA: 0s - loss: 0.6861 - accuracy: 0.53 - ETA: 0s - loss: 0.6830 - accuracy: 0.55 - ETA: 0s - loss: 0.6806 - accuracy: 0.55 - ETA: 0s - loss: 0.6770 - accuracy: 0.56 - ETA: 0s - loss: 0.6732 - accuracy: 0.57 - ETA: 0s - loss: 0.6689 - accuracy: 0.58 - ETA: 0s - loss: 0.6629 - accuracy: 0.59 - ETA: 0s - loss: 0.6604 - accuracy: 0.60 - ETA: 0s - loss: 0.6576 - accuracy: 0.60 - ETA: 0s - loss: 0.6567 - accuracy: 0.61 - ETA: 0s - loss: 0.6514 - accuracy: 0.61 - ETA: 0s - loss: 0.6488 - accuracy: 0.62 - ETA: 0s - loss: 0.6460 - accuracy: 0.62 - 1s 81us/step - loss: 0.6443 - accuracy: 0.6307 - val_loss: 0.5861 - val_accuracy: 0.7180\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:33 - loss: 0.6948 - accuracy: 0.43 - ETA: 2s - loss: 0.6937 - accuracy: 0.5510 - ETA: 1s - loss: 0.6915 - accuracy: 0.55 - ETA: 1s - loss: 0.6886 - accuracy: 0.54 - ETA: 1s - loss: 0.6854 - accuracy: 0.56 - ETA: 0s - loss: 0.6808 - accuracy: 0.57 - ETA: 0s - loss: 0.6753 - accuracy: 0.58 - ETA: 0s - loss: 0.6704 - accuracy: 0.59 - ETA: 0s - loss: 0.6659 - accuracy: 0.60 - ETA: 0s - loss: 0.6620 - accuracy: 0.60 - ETA: 0s - loss: 0.6573 - accuracy: 0.61 - ETA: 0s - loss: 0.6537 - accuracy: 0.62 - ETA: 0s - loss: 0.6512 - accuracy: 0.62 - ETA: 0s - loss: 0.6489 - accuracy: 0.63 - ETA: 0s - loss: 0.6465 - accuracy: 0.64 - ETA: 0s - loss: 0.6446 - accuracy: 0.64 - ETA: 0s - loss: 0.6411 - accuracy: 0.64 - 1s 81us/step - loss: 0.6403 - accuracy: 0.6487 - val_loss: 0.5788 - val_accuracy: 0.7266\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:34 - loss: 0.6915 - accuracy: 0.56 - ETA: 2s - loss: 0.6913 - accuracy: 0.5250 - ETA: 1s - loss: 0.6893 - accuracy: 0.53 - ETA: 1s - loss: 0.6864 - accuracy: 0.54 - ETA: 0s - loss: 0.6815 - accuracy: 0.55 - ETA: 0s - loss: 0.6781 - accuracy: 0.57 - ETA: 0s - loss: 0.6722 - accuracy: 0.58 - ETA: 0s - loss: 0.6660 - accuracy: 0.60 - ETA: 0s - loss: 0.6625 - accuracy: 0.60 - ETA: 0s - loss: 0.6586 - accuracy: 0.61 - ETA: 0s - loss: 0.6570 - accuracy: 0.61 - ETA: 0s - loss: 0.6523 - accuracy: 0.62 - ETA: 0s - loss: 0.6500 - accuracy: 0.63 - ETA: 0s - loss: 0.6470 - accuracy: 0.63 - ETA: 0s - loss: 0.6438 - accuracy: 0.64 - ETA: 0s - loss: 0.6422 - accuracy: 0.64 - ETA: 0s - loss: 0.6400 - accuracy: 0.64 - 1s 80us/step - loss: 0.6396 - accuracy: 0.6467 - val_loss: 0.5978 - val_accuracy: 0.6946\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 28us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 41s - loss: 0.6936 - accuracy: 0.406 - ETA: 1s - loss: 0.6924 - accuracy: 0.518 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6911 - accuracy: 0.51 - ETA: 0s - loss: 0.6901 - accuracy: 0.52 - ETA: 0s - loss: 0.6891 - accuracy: 0.53 - ETA: 0s - loss: 0.6872 - accuracy: 0.54 - ETA: 0s - loss: 0.6857 - accuracy: 0.55 - ETA: 0s - loss: 0.6834 - accuracy: 0.55 - 1s 45us/step - loss: 0.6832 - accuracy: 0.5596 - val_loss: 0.6609 - val_accuracy: 0.6399\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 41s - loss: 0.6941 - accuracy: 0.593 - ETA: 1s - loss: 0.6932 - accuracy: 0.528 - ETA: 0s - loss: 0.6916 - accuracy: 0.53 - ETA: 0s - loss: 0.6907 - accuracy: 0.52 - ETA: 0s - loss: 0.6902 - accuracy: 0.52 - ETA: 0s - loss: 0.6891 - accuracy: 0.53 - ETA: 0s - loss: 0.6879 - accuracy: 0.54 - ETA: 0s - loss: 0.6865 - accuracy: 0.54 - ETA: 0s - loss: 0.6848 - accuracy: 0.55 - 1s 46us/step - loss: 0.6844 - accuracy: 0.5566 - val_loss: 0.6639 - val_accuracy: 0.6889\n",
      "2815/2815 [==============================] - ETA:  - 0s 16us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 42s - loss: 0.6912 - accuracy: 0.718 - ETA: 1s - loss: 0.6929 - accuracy: 0.541 - ETA: 0s - loss: 0.6923 - accuracy: 0.54 - ETA: 0s - loss: 0.6917 - accuracy: 0.55 - ETA: 0s - loss: 0.6910 - accuracy: 0.55 - ETA: 0s - loss: 0.6900 - accuracy: 0.55 - ETA: 0s - loss: 0.6887 - accuracy: 0.56 - ETA: 0s - loss: 0.6884 - accuracy: 0.56 - ETA: 0s - loss: 0.6873 - accuracy: 0.56 - 1s 48us/step - loss: 0.6864 - accuracy: 0.5644 - val_loss: 0.6729 - val_accuracy: 0.6911\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 41s - loss: 0.6920 - accuracy: 0.593 - ETA: 1s - loss: 0.6918 - accuracy: 0.543 - ETA: 0s - loss: 0.6906 - accuracy: 0.55 - ETA: 0s - loss: 0.6895 - accuracy: 0.56 - ETA: 0s - loss: 0.6881 - accuracy: 0.56 - ETA: 0s - loss: 0.6863 - accuracy: 0.57 - ETA: 0s - loss: 0.6845 - accuracy: 0.58 - ETA: 0s - loss: 0.6820 - accuracy: 0.58 - ETA: 0s - loss: 0.6807 - accuracy: 0.58 - 1s 45us/step - loss: 0.6802 - accuracy: 0.5888 - val_loss: 0.6564 - val_accuracy: 0.7067\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 42s - loss: 0.6937 - accuracy: 0.531 - ETA: 1s - loss: 0.6923 - accuracy: 0.533 - ETA: 0s - loss: 0.6918 - accuracy: 0.53 - ETA: 0s - loss: 0.6910 - accuracy: 0.54 - ETA: 0s - loss: 0.6896 - accuracy: 0.55 - ETA: 0s - loss: 0.6882 - accuracy: 0.55 - ETA: 0s - loss: 0.6865 - accuracy: 0.56 - ETA: 0s - loss: 0.6849 - accuracy: 0.56 - ETA: 0s - loss: 0.6832 - accuracy: 0.57 - 1s 45us/step - loss: 0.6828 - accuracy: 0.5740 - val_loss: 0.6612 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 40s - loss: 0.6943 - accuracy: 0.531 - ETA: 0s - loss: 0.6934 - accuracy: 0.520 - ETA: 0s - loss: 0.6928 - accuracy: 0.54 - ETA: 0s - loss: 0.6922 - accuracy: 0.55 - ETA: 0s - loss: 0.6912 - accuracy: 0.56 - ETA: 0s - loss: 0.6900 - accuracy: 0.57 - ETA: 0s - loss: 0.6892 - accuracy: 0.57 - ETA: 0s - loss: 0.6873 - accuracy: 0.58 - 1s 44us/step - loss: 0.6863 - accuracy: 0.5883 - val_loss: 0.6732 - val_accuracy: 0.6612\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 40s - loss: 0.6938 - accuracy: 0.531 - ETA: 1s - loss: 0.6920 - accuracy: 0.542 - ETA: 0s - loss: 0.6896 - accuracy: 0.55 - ETA: 0s - loss: 0.6878 - accuracy: 0.55 - ETA: 0s - loss: 0.6860 - accuracy: 0.56 - ETA: 0s - loss: 0.6838 - accuracy: 0.56 - ETA: 0s - loss: 0.6812 - accuracy: 0.57 - ETA: 0s - loss: 0.6782 - accuracy: 0.58 - ETA: 0s - loss: 0.6748 - accuracy: 0.59 - 1s 45us/step - loss: 0.6741 - accuracy: 0.5978 - val_loss: 0.6390 - val_accuracy: 0.7003\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 40s - loss: 0.6942 - accuracy: 0.406 - ETA: 1s - loss: 0.6923 - accuracy: 0.530 - ETA: 0s - loss: 0.6919 - accuracy: 0.52 - ETA: 0s - loss: 0.6909 - accuracy: 0.53 - ETA: 0s - loss: 0.6891 - accuracy: 0.55 - ETA: 0s - loss: 0.6868 - accuracy: 0.56 - ETA: 0s - loss: 0.6845 - accuracy: 0.57 - ETA: 0s - loss: 0.6816 - accuracy: 0.58 - ETA: 0s - loss: 0.6791 - accuracy: 0.58 - 1s 44us/step - loss: 0.6790 - accuracy: 0.5889 - val_loss: 0.6520 - val_accuracy: 0.7131\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 40s - loss: 0.6934 - accuracy: 0.562 - ETA: 1s - loss: 0.6929 - accuracy: 0.528 - ETA: 0s - loss: 0.6911 - accuracy: 0.54 - ETA: 0s - loss: 0.6892 - accuracy: 0.55 - ETA: 0s - loss: 0.6869 - accuracy: 0.57 - ETA: 0s - loss: 0.6844 - accuracy: 0.57 - ETA: 0s - loss: 0.6813 - accuracy: 0.58 - ETA: 0s - loss: 0.6779 - accuracy: 0.59 - ETA: 0s - loss: 0.6752 - accuracy: 0.59 - 1s 44us/step - loss: 0.6750 - accuracy: 0.6003 - val_loss: 0.6437 - val_accuracy: 0.6989\n",
      "2815/2815 [==============================] - ETA:  - 0s 16us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 40s - loss: 0.6950 - accuracy: 0.437 - ETA: 1s - loss: 0.6931 - accuracy: 0.508 - ETA: 0s - loss: 0.6920 - accuracy: 0.53 - ETA: 0s - loss: 0.6906 - accuracy: 0.54 - ETA: 0s - loss: 0.6886 - accuracy: 0.56 - ETA: 0s - loss: 0.6872 - accuracy: 0.56 - ETA: 0s - loss: 0.6855 - accuracy: 0.57 - ETA: 0s - loss: 0.6830 - accuracy: 0.58 - ETA: 0s - loss: 0.6802 - accuracy: 0.59 - 1s 45us/step - loss: 0.6797 - accuracy: 0.5916 - val_loss: 0.6527 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 39s - loss: 0.6944 - accuracy: 0.406 - ETA: 1s - loss: 0.6932 - accuracy: 0.536 - ETA: 0s - loss: 0.6915 - accuracy: 0.56 - ETA: 0s - loss: 0.6896 - accuracy: 0.58 - ETA: 0s - loss: 0.6874 - accuracy: 0.58 - ETA: 0s - loss: 0.6855 - accuracy: 0.59 - ETA: 0s - loss: 0.6841 - accuracy: 0.59 - ETA: 0s - loss: 0.6819 - accuracy: 0.59 - 1s 44us/step - loss: 0.6790 - accuracy: 0.6007 - val_loss: 0.6514 - val_accuracy: 0.7109\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 40s - loss: 0.6943 - accuracy: 0.468 - ETA: 1s - loss: 0.6930 - accuracy: 0.526 - ETA: 0s - loss: 0.6915 - accuracy: 0.54 - ETA: 0s - loss: 0.6896 - accuracy: 0.56 - ETA: 0s - loss: 0.6874 - accuracy: 0.57 - ETA: 0s - loss: 0.6850 - accuracy: 0.58 - ETA: 0s - loss: 0.6826 - accuracy: 0.59 - ETA: 0s - loss: 0.6795 - accuracy: 0.59 - ETA: 0s - loss: 0.6765 - accuracy: 0.60 - 1s 45us/step - loss: 0.6764 - accuracy: 0.6028 - val_loss: 0.6489 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 41s - loss: 0.6967 - accuracy: 0.468 - ETA: 1s - loss: 0.6913 - accuracy: 0.524 - ETA: 0s - loss: 0.6898 - accuracy: 0.52 - ETA: 0s - loss: 0.6879 - accuracy: 0.54 - ETA: 0s - loss: 0.6855 - accuracy: 0.56 - ETA: 0s - loss: 0.6814 - accuracy: 0.58 - ETA: 0s - loss: 0.6789 - accuracy: 0.58 - ETA: 0s - loss: 0.6752 - accuracy: 0.59 - ETA: 0s - loss: 0.6721 - accuracy: 0.60 - 1s 45us/step - loss: 0.6718 - accuracy: 0.6079 - val_loss: 0.6329 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 41s - loss: 0.6953 - accuracy: 0.500 - ETA: 1s - loss: 0.6917 - accuracy: 0.551 - ETA: 0s - loss: 0.6888 - accuracy: 0.57 - ETA: 0s - loss: 0.6854 - accuracy: 0.59 - ETA: 0s - loss: 0.6825 - accuracy: 0.60 - ETA: 0s - loss: 0.6788 - accuracy: 0.61 - ETA: 0s - loss: 0.6740 - accuracy: 0.62 - ETA: 0s - loss: 0.6700 - accuracy: 0.62 - ETA: 0s - loss: 0.6659 - accuracy: 0.63 - 1s 45us/step - loss: 0.6649 - accuracy: 0.6353 - val_loss: 0.6184 - val_accuracy: 0.7145\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 42s - loss: 0.6950 - accuracy: 0.562 - ETA: 1s - loss: 0.6936 - accuracy: 0.529 - ETA: 0s - loss: 0.6921 - accuracy: 0.54 - ETA: 0s - loss: 0.6904 - accuracy: 0.55 - ETA: 0s - loss: 0.6884 - accuracy: 0.56 - ETA: 0s - loss: 0.6864 - accuracy: 0.57 - ETA: 0s - loss: 0.6835 - accuracy: 0.58 - ETA: 0s - loss: 0.6800 - accuracy: 0.59 - ETA: 0s - loss: 0.6769 - accuracy: 0.60 - 1s 46us/step - loss: 0.6759 - accuracy: 0.6027 - val_loss: 0.6394 - val_accuracy: 0.6989\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 40s - loss: 0.6935 - accuracy: 0.531 - ETA: 1s - loss: 0.6925 - accuracy: 0.559 - ETA: 0s - loss: 0.6896 - accuracy: 0.56 - ETA: 0s - loss: 0.6867 - accuracy: 0.57 - ETA: 0s - loss: 0.6830 - accuracy: 0.59 - ETA: 0s - loss: 0.6792 - accuracy: 0.60 - ETA: 0s - loss: 0.6756 - accuracy: 0.61 - ETA: 0s - loss: 0.6717 - accuracy: 0.61 - ETA: 0s - loss: 0.6680 - accuracy: 0.62 - 1s 46us/step - loss: 0.6667 - accuracy: 0.6264 - val_loss: 0.6251 - val_accuracy: 0.7053\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 41s - loss: 0.6918 - accuracy: 0.562 - ETA: 1s - loss: 0.6933 - accuracy: 0.533 - ETA: 0s - loss: 0.6900 - accuracy: 0.57 - ETA: 0s - loss: 0.6876 - accuracy: 0.58 - ETA: 0s - loss: 0.6844 - accuracy: 0.59 - ETA: 0s - loss: 0.6806 - accuracy: 0.60 - ETA: 0s - loss: 0.6763 - accuracy: 0.61 - ETA: 0s - loss: 0.6717 - accuracy: 0.62 - ETA: 0s - loss: 0.6670 - accuracy: 0.62 - 1s 45us/step - loss: 0.6667 - accuracy: 0.6280 - val_loss: 0.6225 - val_accuracy: 0.7195\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 42s - loss: 0.6928 - accuracy: 0.562 - ETA: 1s - loss: 0.6915 - accuracy: 0.526 - ETA: 0s - loss: 0.6895 - accuracy: 0.54 - ETA: 0s - loss: 0.6870 - accuracy: 0.55 - ETA: 0s - loss: 0.6827 - accuracy: 0.57 - ETA: 0s - loss: 0.6796 - accuracy: 0.58 - ETA: 0s - loss: 0.6750 - accuracy: 0.59 - ETA: 0s - loss: 0.6720 - accuracy: 0.60 - ETA: 0s - loss: 0.6670 - accuracy: 0.61 - 1s 46us/step - loss: 0.6653 - accuracy: 0.6171 - val_loss: 0.6292 - val_accuracy: 0.6960\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 36s - loss: 0.6942 - accuracy: 0.468 - ETA: 0s - loss: 0.6935 - accuracy: 0.520 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - 1s 42us/step - loss: 0.6934 - accuracy: 0.5137 - val_loss: 0.6926 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 35s - loss: 0.6930 - accuracy: 0.531 - ETA: 0s - loss: 0.6935 - accuracy: 0.518 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - 1s 42us/step - loss: 0.6933 - accuracy: 0.5196 - val_loss: 0.6924 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 35s - loss: 0.6936 - accuracy: 0.375 - ETA: 0s - loss: 0.6938 - accuracy: 0.494 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - 1s 42us/step - loss: 0.6935 - accuracy: 0.5126 - val_loss: 0.6928 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 36s - loss: 0.6940 - accuracy: 0.500 - ETA: 0s - loss: 0.6939 - accuracy: 0.491 - ETA: 0s - loss: 0.6940 - accuracy: 0.48 - ETA: 0s - loss: 0.6940 - accuracy: 0.48 - ETA: 0s - loss: 0.6938 - accuracy: 0.49 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - 1s 42us/step - loss: 0.6936 - accuracy: 0.5082 - val_loss: 0.6927 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 36s - loss: 0.6934 - accuracy: 0.562 - ETA: 1s - loss: 0.6923 - accuracy: 0.535 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - 1s 43us/step - loss: 0.6925 - accuracy: 0.5146 - val_loss: 0.6910 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 35s - loss: 0.6933 - accuracy: 0.375 - ETA: 0s - loss: 0.6921 - accuracy: 0.531 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6932 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.50 - 1s 42us/step - loss: 0.6929 - accuracy: 0.5048 - val_loss: 0.6920 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 24us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 35s - loss: 0.6920 - accuracy: 0.593 - ETA: 0s - loss: 0.6928 - accuracy: 0.509 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6922 - accuracy: 0.52 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - 1s 43us/step - loss: 0.6922 - accuracy: 0.5248 - val_loss: 0.6904 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 35s - loss: 0.6951 - accuracy: 0.500 - ETA: 0s - loss: 0.6935 - accuracy: 0.528 - ETA: 0s - loss: 0.6931 - accuracy: 0.53 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - 1s 42us/step - loss: 0.6929 - accuracy: 0.5171 - val_loss: 0.6918 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 37s - loss: 0.6936 - accuracy: 0.625 - ETA: 0s - loss: 0.6939 - accuracy: 0.532 - ETA: 0s - loss: 0.6940 - accuracy: 0.52 - ETA: 0s - loss: 0.6940 - accuracy: 0.52 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - 1s 42us/step - loss: 0.6940 - accuracy: 0.5143 - val_loss: 0.6933 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 37s - loss: 0.6931 - accuracy: 0.593 - ETA: 0s - loss: 0.6940 - accuracy: 0.522 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - 1s 42us/step - loss: 0.6941 - accuracy: 0.5131 - val_loss: 0.6935 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 35s - loss: 0.6931 - accuracy: 0.468 - ETA: 0s - loss: 0.6941 - accuracy: 0.518 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - 1s 42us/step - loss: 0.6937 - accuracy: 0.5126 - val_loss: 0.6931 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 35s - loss: 0.6942 - accuracy: 0.500 - ETA: 0s - loss: 0.6950 - accuracy: 0.490 - ETA: 0s - loss: 0.6947 - accuracy: 0.49 - ETA: 0s - loss: 0.6947 - accuracy: 0.48 - ETA: 0s - loss: 0.6946 - accuracy: 0.48 - ETA: 0s - loss: 0.6947 - accuracy: 0.48 - ETA: 0s - loss: 0.6946 - accuracy: 0.48 - ETA: 0s - loss: 0.6945 - accuracy: 0.48 - 1s 41us/step - loss: 0.6944 - accuracy: 0.4917 - val_loss: 0.6938 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 36s - loss: 0.6953 - accuracy: 0.531 - ETA: 0s - loss: 0.6950 - accuracy: 0.492 - ETA: 0s - loss: 0.6950 - accuracy: 0.48 - ETA: 0s - loss: 0.6950 - accuracy: 0.49 - ETA: 0s - loss: 0.6948 - accuracy: 0.50 - ETA: 0s - loss: 0.6948 - accuracy: 0.50 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - 1s 42us/step - loss: 0.6944 - accuracy: 0.5143 - val_loss: 0.6931 - val_accuracy: 0.5263\n",
      "2815/2815 [==============================] - ETA:  - 0s 16us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 35s - loss: 0.6913 - accuracy: 0.593 - ETA: 0s - loss: 0.6946 - accuracy: 0.502 - ETA: 0s - loss: 0.6948 - accuracy: 0.49 - ETA: 0s - loss: 0.6947 - accuracy: 0.49 - ETA: 0s - loss: 0.6944 - accuracy: 0.50 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - 1s 43us/step - loss: 0.6942 - accuracy: 0.5084 - val_loss: 0.6930 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 35s - loss: 0.6959 - accuracy: 0.562 - ETA: 0s - loss: 0.6952 - accuracy: 0.504 - ETA: 0s - loss: 0.6947 - accuracy: 0.52 - ETA: 0s - loss: 0.6946 - accuracy: 0.52 - ETA: 0s - loss: 0.6943 - accuracy: 0.52 - ETA: 0s - loss: 0.6940 - accuracy: 0.53 - ETA: 0s - loss: 0.6939 - accuracy: 0.53 - ETA: 0s - loss: 0.6938 - accuracy: 0.53 - 1s 42us/step - loss: 0.6938 - accuracy: 0.5296 - val_loss: 0.6917 - val_accuracy: 0.5405\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 35s - loss: 0.6948 - accuracy: 0.531 - ETA: 0s - loss: 0.6950 - accuracy: 0.515 - ETA: 0s - loss: 0.6947 - accuracy: 0.52 - ETA: 0s - loss: 0.6948 - accuracy: 0.51 - ETA: 0s - loss: 0.6950 - accuracy: 0.50 - ETA: 0s - loss: 0.6948 - accuracy: 0.51 - ETA: 0s - loss: 0.6948 - accuracy: 0.51 - ETA: 0s - loss: 0.6947 - accuracy: 0.51 - 1s 42us/step - loss: 0.6946 - accuracy: 0.5126 - val_loss: 0.6934 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 37s - loss: 0.6959 - accuracy: 0.406 - ETA: 0s - loss: 0.6951 - accuracy: 0.481 - ETA: 0s - loss: 0.6950 - accuracy: 0.49 - ETA: 0s - loss: 0.6949 - accuracy: 0.50 - ETA: 0s - loss: 0.6948 - accuracy: 0.50 - ETA: 0s - loss: 0.6947 - accuracy: 0.51 - ETA: 0s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - 1s 44us/step - loss: 0.6945 - accuracy: 0.5112 - val_loss: 0.6937 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 37s - loss: 0.6958 - accuracy: 0.562 - ETA: 0s - loss: 0.6950 - accuracy: 0.495 - ETA: 0s - loss: 0.6948 - accuracy: 0.50 - ETA: 0s - loss: 0.6947 - accuracy: 0.50 - ETA: 0s - loss: 0.6948 - accuracy: 0.50 - ETA: 0s - loss: 0.6947 - accuracy: 0.50 - ETA: 0s - loss: 0.6947 - accuracy: 0.50 - ETA: 0s - loss: 0.6946 - accuracy: 0.50 - 1s 43us/step - loss: 0.6945 - accuracy: 0.5081 - val_loss: 0.6939 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6934 - accuracy: 0.531 - ETA: 1s - loss: 0.6923 - accuracy: 0.533 - ETA: 0s - loss: 0.6919 - accuracy: 0.53 - ETA: 0s - loss: 0.6915 - accuracy: 0.52 - ETA: 0s - loss: 0.6909 - accuracy: 0.52 - ETA: 0s - loss: 0.6896 - accuracy: 0.53 - ETA: 0s - loss: 0.6879 - accuracy: 0.54 - ETA: 0s - loss: 0.6862 - accuracy: 0.54 - ETA: 0s - loss: 0.6839 - accuracy: 0.55 - 1s 46us/step - loss: 0.6837 - accuracy: 0.5521 - val_loss: 0.6589 - val_accuracy: 0.6747\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6940 - accuracy: 0.437 - ETA: 1s - loss: 0.6930 - accuracy: 0.531 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6919 - accuracy: 0.54 - ETA: 0s - loss: 0.6905 - accuracy: 0.54 - ETA: 0s - loss: 0.6892 - accuracy: 0.55 - ETA: 0s - loss: 0.6874 - accuracy: 0.56 - ETA: 0s - loss: 0.6857 - accuracy: 0.56 - ETA: 0s - loss: 0.6830 - accuracy: 0.57 - 1s 47us/step - loss: 0.6814 - accuracy: 0.5771 - val_loss: 0.6548 - val_accuracy: 0.6925\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6980 - accuracy: 0.375 - ETA: 1s - loss: 0.6937 - accuracy: 0.505 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6922 - accuracy: 0.54 - ETA: 0s - loss: 0.6914 - accuracy: 0.54 - ETA: 0s - loss: 0.6900 - accuracy: 0.55 - ETA: 0s - loss: 0.6883 - accuracy: 0.56 - ETA: 0s - loss: 0.6854 - accuracy: 0.57 - 1s 47us/step - loss: 0.6839 - accuracy: 0.5781 - val_loss: 0.6580 - val_accuracy: 0.6634\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6934 - accuracy: 0.531 - ETA: 1s - loss: 0.6931 - accuracy: 0.494 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.55 - ETA: 0s - loss: 0.6913 - accuracy: 0.56 - ETA: 0s - loss: 0.6901 - accuracy: 0.56 - ETA: 0s - loss: 0.6885 - accuracy: 0.57 - ETA: 0s - loss: 0.6868 - accuracy: 0.58 - ETA: 0s - loss: 0.6849 - accuracy: 0.59 - 1s 47us/step - loss: 0.6840 - accuracy: 0.5932 - val_loss: 0.6628 - val_accuracy: 0.7060\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6943 - accuracy: 0.375 - ETA: 1s - loss: 0.6934 - accuracy: 0.512 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.53 - ETA: 0s - loss: 0.6917 - accuracy: 0.54 - ETA: 0s - loss: 0.6908 - accuracy: 0.54 - ETA: 0s - loss: 0.6893 - accuracy: 0.55 - ETA: 0s - loss: 0.6874 - accuracy: 0.55 - ETA: 0s - loss: 0.6856 - accuracy: 0.56 - 1s 47us/step - loss: 0.6846 - accuracy: 0.5705 - val_loss: 0.6605 - val_accuracy: 0.7081\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 49s - loss: 0.6937 - accuracy: 0.468 - ETA: 1s - loss: 0.6934 - accuracy: 0.500 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.53 - ETA: 0s - loss: 0.6908 - accuracy: 0.54 - ETA: 0s - loss: 0.6891 - accuracy: 0.55 - ETA: 0s - loss: 0.6870 - accuracy: 0.56 - ETA: 0s - loss: 0.6844 - accuracy: 0.57 - ETA: 0s - loss: 0.6818 - accuracy: 0.57 - ETA: 0s - loss: 0.6797 - accuracy: 0.58 - 1s 51us/step - loss: 0.6797 - accuracy: 0.5816 - val_loss: 0.6485 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6936 - accuracy: 0.468 - ETA: 1s - loss: 0.6935 - accuracy: 0.537 - ETA: 0s - loss: 0.6926 - accuracy: 0.55 - ETA: 0s - loss: 0.6911 - accuracy: 0.57 - ETA: 0s - loss: 0.6892 - accuracy: 0.58 - ETA: 0s - loss: 0.6861 - accuracy: 0.59 - ETA: 0s - loss: 0.6827 - accuracy: 0.60 - ETA: 0s - loss: 0.6791 - accuracy: 0.60 - ETA: 0s - loss: 0.6748 - accuracy: 0.61 - 1s 47us/step - loss: 0.6738 - accuracy: 0.6146 - val_loss: 0.6314 - val_accuracy: 0.7038\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 48s - loss: 0.6953 - accuracy: 0.437 - ETA: 1s - loss: 0.6932 - accuracy: 0.514 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6900 - accuracy: 0.53 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6842 - accuracy: 0.56 - ETA: 0s - loss: 0.6803 - accuracy: 0.57 - ETA: 0s - loss: 0.6754 - accuracy: 0.58 - ETA: 0s - loss: 0.6723 - accuracy: 0.59 - 1s 48us/step - loss: 0.6700 - accuracy: 0.5964 - val_loss: 0.6294 - val_accuracy: 0.7102\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6930 - accuracy: 0.562 - ETA: 1s - loss: 0.6930 - accuracy: 0.557 - ETA: 0s - loss: 0.6914 - accuracy: 0.58 - ETA: 0s - loss: 0.6898 - accuracy: 0.58 - ETA: 0s - loss: 0.6870 - accuracy: 0.59 - ETA: 0s - loss: 0.6837 - accuracy: 0.60 - ETA: 0s - loss: 0.6785 - accuracy: 0.61 - ETA: 0s - loss: 0.6752 - accuracy: 0.62 - ETA: 0s - loss: 0.6718 - accuracy: 0.62 - 1s 47us/step - loss: 0.6697 - accuracy: 0.6280 - val_loss: 0.6280 - val_accuracy: 0.7131\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 1:03 - loss: 0.6941 - accuracy: 0.59 - ETA: 1s - loss: 0.6937 - accuracy: 0.5345 - ETA: 0s - loss: 0.6922 - accuracy: 0.55 - ETA: 0s - loss: 0.6900 - accuracy: 0.56 - ETA: 0s - loss: 0.6872 - accuracy: 0.57 - ETA: 0s - loss: 0.6833 - accuracy: 0.58 - ETA: 0s - loss: 0.6788 - accuracy: 0.59 - ETA: 0s - loss: 0.6729 - accuracy: 0.60 - ETA: 0s - loss: 0.6688 - accuracy: 0.61 - 1s 51us/step - loss: 0.6669 - accuracy: 0.6181 - val_loss: 0.6199 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6931 - accuracy: 0.343 - ETA: 1s - loss: 0.6927 - accuracy: 0.500 - ETA: 0s - loss: 0.6907 - accuracy: 0.50 - ETA: 0s - loss: 0.6888 - accuracy: 0.51 - ETA: 0s - loss: 0.6874 - accuracy: 0.53 - ETA: 0s - loss: 0.6845 - accuracy: 0.54 - ETA: 0s - loss: 0.6817 - accuracy: 0.56 - ETA: 0s - loss: 0.6771 - accuracy: 0.57 - ETA: 0s - loss: 0.6722 - accuracy: 0.58 - 1s 48us/step - loss: 0.6708 - accuracy: 0.5872 - val_loss: 0.6320 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 46s - loss: 0.6974 - accuracy: 0.343 - ETA: 1s - loss: 0.6935 - accuracy: 0.519 - ETA: 0s - loss: 0.6922 - accuracy: 0.54 - ETA: 0s - loss: 0.6904 - accuracy: 0.55 - ETA: 0s - loss: 0.6882 - accuracy: 0.56 - ETA: 0s - loss: 0.6854 - accuracy: 0.57 - ETA: 0s - loss: 0.6817 - accuracy: 0.58 - ETA: 0s - loss: 0.6772 - accuracy: 0.59 - ETA: 0s - loss: 0.6734 - accuracy: 0.60 - 1s 48us/step - loss: 0.6708 - accuracy: 0.6059 - val_loss: 0.6339 - val_accuracy: 0.6903\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6943 - accuracy: 0.468 - ETA: 1s - loss: 0.6930 - accuracy: 0.510 - ETA: 0s - loss: 0.6905 - accuracy: 0.52 - ETA: 0s - loss: 0.6902 - accuracy: 0.51 - ETA: 0s - loss: 0.6883 - accuracy: 0.52 - ETA: 0s - loss: 0.6845 - accuracy: 0.54 - ETA: 0s - loss: 0.6806 - accuracy: 0.56 - ETA: 0s - loss: 0.6761 - accuracy: 0.57 - ETA: 0s - loss: 0.6721 - accuracy: 0.58 - 1s 48us/step - loss: 0.6705 - accuracy: 0.5913 - val_loss: 0.6251 - val_accuracy: 0.7173\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 46s - loss: 0.6956 - accuracy: 0.500 - ETA: 1s - loss: 0.6938 - accuracy: 0.526 - ETA: 0s - loss: 0.6923 - accuracy: 0.54 - ETA: 0s - loss: 0.6889 - accuracy: 0.56 - ETA: 0s - loss: 0.6857 - accuracy: 0.57 - ETA: 0s - loss: 0.6810 - accuracy: 0.59 - ETA: 0s - loss: 0.6758 - accuracy: 0.60 - ETA: 0s - loss: 0.6702 - accuracy: 0.61 - ETA: 0s - loss: 0.6645 - accuracy: 0.62 - 1s 47us/step - loss: 0.6626 - accuracy: 0.6257 - val_loss: 0.6078 - val_accuracy: 0.7202\n",
      "2815/2815 [==============================] - ETA:  - 0s 16us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6944 - accuracy: 0.562 - ETA: 1s - loss: 0.6943 - accuracy: 0.505 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6903 - accuracy: 0.53 - ETA: 0s - loss: 0.6880 - accuracy: 0.55 - ETA: 0s - loss: 0.6839 - accuracy: 0.57 - ETA: 0s - loss: 0.6805 - accuracy: 0.58 - ETA: 0s - loss: 0.6760 - accuracy: 0.59 - ETA: 0s - loss: 0.6708 - accuracy: 0.60 - 1s 48us/step - loss: 0.6677 - accuracy: 0.6092 - val_loss: 0.6129 - val_accuracy: 0.7067\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6971 - accuracy: 0.406 - ETA: 1s - loss: 0.6912 - accuracy: 0.515 - ETA: 0s - loss: 0.6874 - accuracy: 0.53 - ETA: 0s - loss: 0.6853 - accuracy: 0.53 - ETA: 0s - loss: 0.6831 - accuracy: 0.53 - ETA: 0s - loss: 0.6807 - accuracy: 0.55 - ETA: 0s - loss: 0.6766 - accuracy: 0.56 - ETA: 0s - loss: 0.6731 - accuracy: 0.58 - ETA: 0s - loss: 0.6694 - accuracy: 0.59 - 1s 47us/step - loss: 0.6673 - accuracy: 0.5970 - val_loss: 0.6262 - val_accuracy: 0.7045\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 46s - loss: 0.6966 - accuracy: 0.468 - ETA: 1s - loss: 0.6917 - accuracy: 0.518 - ETA: 0s - loss: 0.6905 - accuracy: 0.52 - ETA: 0s - loss: 0.6878 - accuracy: 0.54 - ETA: 0s - loss: 0.6854 - accuracy: 0.55 - ETA: 0s - loss: 0.6820 - accuracy: 0.57 - ETA: 0s - loss: 0.6785 - accuracy: 0.58 - ETA: 0s - loss: 0.6734 - accuracy: 0.59 - ETA: 0s - loss: 0.6682 - accuracy: 0.60 - 1s 48us/step - loss: 0.6645 - accuracy: 0.6137 - val_loss: 0.6132 - val_accuracy: 0.7124\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 45s - loss: 0.6976 - accuracy: 0.500 - ETA: 1s - loss: 0.6924 - accuracy: 0.533 - ETA: 0s - loss: 0.6911 - accuracy: 0.54 - ETA: 0s - loss: 0.6883 - accuracy: 0.56 - ETA: 0s - loss: 0.6840 - accuracy: 0.58 - ETA: 0s - loss: 0.6786 - accuracy: 0.59 - ETA: 0s - loss: 0.6736 - accuracy: 0.61 - ETA: 0s - loss: 0.6687 - accuracy: 0.62 - ETA: 0s - loss: 0.6634 - accuracy: 0.62 - 1s 48us/step - loss: 0.6601 - accuracy: 0.6343 - val_loss: 0.6131 - val_accuracy: 0.7081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 41s - loss: 0.6958 - accuracy: 0.375 - ETA: 1s - loss: 0.6925 - accuracy: 0.521 - ETA: 0s - loss: 0.6909 - accuracy: 0.52 - ETA: 0s - loss: 0.6897 - accuracy: 0.52 - ETA: 0s - loss: 0.6886 - accuracy: 0.52 - ETA: 0s - loss: 0.6870 - accuracy: 0.53 - ETA: 0s - loss: 0.6858 - accuracy: 0.53 - ETA: 0s - loss: 0.6841 - accuracy: 0.54 - 1s 43us/step - loss: 0.6832 - accuracy: 0.5461 - val_loss: 0.6660 - val_accuracy: 0.6974\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 38s - loss: 0.6936 - accuracy: 0.562 - ETA: 1s - loss: 0.6933 - accuracy: 0.491 - ETA: 0s - loss: 0.6926 - accuracy: 0.49 - ETA: 0s - loss: 0.6920 - accuracy: 0.50 - ETA: 0s - loss: 0.6914 - accuracy: 0.51 - ETA: 0s - loss: 0.6910 - accuracy: 0.52 - ETA: 0s - loss: 0.6904 - accuracy: 0.53 - ETA: 0s - loss: 0.6899 - accuracy: 0.53 - 1s 43us/step - loss: 0.6893 - accuracy: 0.5441 - val_loss: 0.6828 - val_accuracy: 0.7081\n",
      "2815/2815 [==============================] - ETA:  - 0s 16us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 39s - loss: 0.6935 - accuracy: 0.468 - ETA: 1s - loss: 0.6932 - accuracy: 0.496 - ETA: 0s - loss: 0.6913 - accuracy: 0.51 - ETA: 0s - loss: 0.6893 - accuracy: 0.52 - ETA: 0s - loss: 0.6879 - accuracy: 0.53 - ETA: 0s - loss: 0.6866 - accuracy: 0.53 - ETA: 0s - loss: 0.6848 - accuracy: 0.54 - ETA: 0s - loss: 0.6841 - accuracy: 0.54 - ETA: 0s - loss: 0.6827 - accuracy: 0.54 - 1s 44us/step - loss: 0.6827 - accuracy: 0.5496 - val_loss: 0.6693 - val_accuracy: 0.6293\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 38s - loss: 0.6935 - accuracy: 0.562 - ETA: 0s - loss: 0.6900 - accuracy: 0.531 - ETA: 0s - loss: 0.6885 - accuracy: 0.52 - ETA: 0s - loss: 0.6871 - accuracy: 0.53 - ETA: 0s - loss: 0.6850 - accuracy: 0.54 - ETA: 0s - loss: 0.6830 - accuracy: 0.54 - ETA: 0s - loss: 0.6816 - accuracy: 0.55 - ETA: 0s - loss: 0.6791 - accuracy: 0.56 - 1s 43us/step - loss: 0.6772 - accuracy: 0.5676 - val_loss: 0.6557 - val_accuracy: 0.6903\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 37s - loss: 0.6976 - accuracy: 0.437 - ETA: 0s - loss: 0.6931 - accuracy: 0.504 - ETA: 0s - loss: 0.6923 - accuracy: 0.54 - ETA: 0s - loss: 0.6914 - accuracy: 0.56 - ETA: 0s - loss: 0.6904 - accuracy: 0.56 - ETA: 0s - loss: 0.6896 - accuracy: 0.57 - ETA: 0s - loss: 0.6883 - accuracy: 0.57 - ETA: 0s - loss: 0.6870 - accuracy: 0.58 - 1s 43us/step - loss: 0.6856 - accuracy: 0.5967 - val_loss: 0.6742 - val_accuracy: 0.6271\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 38s - loss: 0.6958 - accuracy: 0.437 - ETA: 0s - loss: 0.6914 - accuracy: 0.537 - ETA: 0s - loss: 0.6891 - accuracy: 0.56 - ETA: 0s - loss: 0.6870 - accuracy: 0.57 - ETA: 0s - loss: 0.6861 - accuracy: 0.57 - ETA: 0s - loss: 0.6850 - accuracy: 0.57 - ETA: 0s - loss: 0.6833 - accuracy: 0.58 - ETA: 0s - loss: 0.6818 - accuracy: 0.58 - 1s 43us/step - loss: 0.6802 - accuracy: 0.5912 - val_loss: 0.6650 - val_accuracy: 0.7053\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 38s - loss: 0.6937 - accuracy: 0.531 - ETA: 0s - loss: 0.6861 - accuracy: 0.560 - ETA: 0s - loss: 0.6831 - accuracy: 0.56 - ETA: 0s - loss: 0.6787 - accuracy: 0.58 - ETA: 0s - loss: 0.6739 - accuracy: 0.60 - ETA: 0s - loss: 0.6710 - accuracy: 0.61 - ETA: 0s - loss: 0.6692 - accuracy: 0.61 - ETA: 0s - loss: 0.6659 - accuracy: 0.62 - 1s 43us/step - loss: 0.6638 - accuracy: 0.6273 - val_loss: 0.6332 - val_accuracy: 0.7017\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 37s - loss: 0.6934 - accuracy: 0.406 - ETA: 0s - loss: 0.6907 - accuracy: 0.559 - ETA: 0s - loss: 0.6855 - accuracy: 0.57 - ETA: 0s - loss: 0.6823 - accuracy: 0.57 - ETA: 0s - loss: 0.6784 - accuracy: 0.58 - ETA: 0s - loss: 0.6756 - accuracy: 0.59 - ETA: 0s - loss: 0.6723 - accuracy: 0.60 - ETA: 0s - loss: 0.6699 - accuracy: 0.60 - 1s 43us/step - loss: 0.6678 - accuracy: 0.6101 - val_loss: 0.6380 - val_accuracy: 0.7038\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 38s - loss: 0.6951 - accuracy: 0.406 - ETA: 0s - loss: 0.6914 - accuracy: 0.525 - ETA: 0s - loss: 0.6871 - accuracy: 0.55 - ETA: 0s - loss: 0.6837 - accuracy: 0.56 - ETA: 0s - loss: 0.6806 - accuracy: 0.57 - ETA: 0s - loss: 0.6778 - accuracy: 0.57 - ETA: 0s - loss: 0.6754 - accuracy: 0.58 - ETA: 0s - loss: 0.6739 - accuracy: 0.58 - ETA: 0s - loss: 0.6716 - accuracy: 0.59 - 1s 44us/step - loss: 0.6714 - accuracy: 0.5937 - val_loss: 0.6467 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 38s - loss: 0.6950 - accuracy: 0.468 - ETA: 0s - loss: 0.6914 - accuracy: 0.541 - ETA: 0s - loss: 0.6888 - accuracy: 0.56 - ETA: 0s - loss: 0.6869 - accuracy: 0.56 - ETA: 0s - loss: 0.6848 - accuracy: 0.57 - ETA: 0s - loss: 0.6821 - accuracy: 0.58 - ETA: 0s - loss: 0.6797 - accuracy: 0.59 - ETA: 0s - loss: 0.6779 - accuracy: 0.59 - 1s 43us/step - loss: 0.6768 - accuracy: 0.6020 - val_loss: 0.6557 - val_accuracy: 0.7102\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 39s - loss: 0.6948 - accuracy: 0.437 - ETA: 1s - loss: 0.6923 - accuracy: 0.524 - ETA: 0s - loss: 0.6904 - accuracy: 0.55 - ETA: 0s - loss: 0.6888 - accuracy: 0.56 - ETA: 0s - loss: 0.6866 - accuracy: 0.58 - ETA: 0s - loss: 0.6838 - accuracy: 0.59 - ETA: 0s - loss: 0.6822 - accuracy: 0.59 - ETA: 0s - loss: 0.6802 - accuracy: 0.60 - 1s 43us/step - loss: 0.6776 - accuracy: 0.6073 - val_loss: 0.6564 - val_accuracy: 0.6868\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 41s - loss: 0.6911 - accuracy: 0.656 - ETA: 1s - loss: 0.6901 - accuracy: 0.525 - ETA: 0s - loss: 0.6874 - accuracy: 0.55 - ETA: 0s - loss: 0.6845 - accuracy: 0.56 - ETA: 0s - loss: 0.6820 - accuracy: 0.57 - ETA: 0s - loss: 0.6791 - accuracy: 0.58 - ETA: 0s - loss: 0.6756 - accuracy: 0.59 - ETA: 0s - loss: 0.6733 - accuracy: 0.60 - ETA: 0s - loss: 0.6706 - accuracy: 0.60 - 1s 45us/step - loss: 0.6705 - accuracy: 0.6084 - val_loss: 0.6464 - val_accuracy: 0.6989\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 41s - loss: 0.6927 - accuracy: 0.625 - ETA: 1s - loss: 0.6896 - accuracy: 0.553 - ETA: 0s - loss: 0.6845 - accuracy: 0.58 - ETA: 0s - loss: 0.6786 - accuracy: 0.60 - ETA: 0s - loss: 0.6742 - accuracy: 0.62 - ETA: 0s - loss: 0.6703 - accuracy: 0.62 - ETA: 0s - loss: 0.6653 - accuracy: 0.63 - ETA: 0s - loss: 0.6601 - accuracy: 0.64 - ETA: 0s - loss: 0.6582 - accuracy: 0.64 - 1s 45us/step - loss: 0.6573 - accuracy: 0.6460 - val_loss: 0.6183 - val_accuracy: 0.7166\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 39s - loss: 0.6938 - accuracy: 0.625 - ETA: 1s - loss: 0.6838 - accuracy: 0.557 - ETA: 0s - loss: 0.6820 - accuracy: 0.55 - ETA: 0s - loss: 0.6782 - accuracy: 0.56 - ETA: 0s - loss: 0.6759 - accuracy: 0.57 - ETA: 0s - loss: 0.6726 - accuracy: 0.58 - ETA: 0s - loss: 0.6696 - accuracy: 0.59 - ETA: 0s - loss: 0.6664 - accuracy: 0.60 - 1s 43us/step - loss: 0.6630 - accuracy: 0.6119 - val_loss: 0.6330 - val_accuracy: 0.6889\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 39s - loss: 0.6997 - accuracy: 0.468 - ETA: 1s - loss: 0.6888 - accuracy: 0.566 - ETA: 0s - loss: 0.6849 - accuracy: 0.58 - ETA: 0s - loss: 0.6797 - accuracy: 0.60 - ETA: 0s - loss: 0.6758 - accuracy: 0.61 - ETA: 0s - loss: 0.6725 - accuracy: 0.61 - ETA: 0s - loss: 0.6685 - accuracy: 0.62 - ETA: 0s - loss: 0.6656 - accuracy: 0.62 - ETA: 0s - loss: 0.6626 - accuracy: 0.63 - 1s 45us/step - loss: 0.6620 - accuracy: 0.6349 - val_loss: 0.6294 - val_accuracy: 0.7088\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 39s - loss: 0.7005 - accuracy: 0.281 - ETA: 1s - loss: 0.6919 - accuracy: 0.548 - ETA: 0s - loss: 0.6858 - accuracy: 0.58 - ETA: 0s - loss: 0.6807 - accuracy: 0.59 - ETA: 0s - loss: 0.6768 - accuracy: 0.60 - ETA: 0s - loss: 0.6730 - accuracy: 0.61 - ETA: 0s - loss: 0.6687 - accuracy: 0.62 - ETA: 0s - loss: 0.6661 - accuracy: 0.62 - ETA: 0s - loss: 0.6626 - accuracy: 0.62 - 1s 44us/step - loss: 0.6626 - accuracy: 0.6270 - val_loss: 0.6273 - val_accuracy: 0.7109\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 39s - loss: 0.6984 - accuracy: 0.312 - ETA: 1s - loss: 0.6858 - accuracy: 0.540 - ETA: 0s - loss: 0.6840 - accuracy: 0.55 - ETA: 0s - loss: 0.6814 - accuracy: 0.56 - ETA: 0s - loss: 0.6770 - accuracy: 0.58 - ETA: 0s - loss: 0.6734 - accuracy: 0.60 - ETA: 0s - loss: 0.6692 - accuracy: 0.61 - ETA: 0s - loss: 0.6665 - accuracy: 0.61 - ETA: 0s - loss: 0.6633 - accuracy: 0.62 - 1s 44us/step - loss: 0.6634 - accuracy: 0.6237 - val_loss: 0.6287 - val_accuracy: 0.7038\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 38s - loss: 0.7008 - accuracy: 0.406 - ETA: 1s - loss: 0.6908 - accuracy: 0.528 - ETA: 0s - loss: 0.6872 - accuracy: 0.55 - ETA: 0s - loss: 0.6830 - accuracy: 0.58 - ETA: 0s - loss: 0.6786 - accuracy: 0.59 - ETA: 0s - loss: 0.6741 - accuracy: 0.60 - ETA: 0s - loss: 0.6704 - accuracy: 0.61 - ETA: 0s - loss: 0.6671 - accuracy: 0.62 - ETA: 0s - loss: 0.6646 - accuracy: 0.62 - 1s 45us/step - loss: 0.6642 - accuracy: 0.6246 - val_loss: 0.6341 - val_accuracy: 0.6911\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6928 - accuracy: 0.562 - ETA: 1s - loss: 0.6935 - accuracy: 0.496 - ETA: 0s - loss: 0.6929 - accuracy: 0.50 - ETA: 0s - loss: 0.6908 - accuracy: 0.51 - ETA: 0s - loss: 0.6889 - accuracy: 0.51 - ETA: 0s - loss: 0.6859 - accuracy: 0.52 - ETA: 0s - loss: 0.6830 - accuracy: 0.53 - ETA: 0s - loss: 0.6802 - accuracy: 0.54 - ETA: 0s - loss: 0.6775 - accuracy: 0.55 - 1s 47us/step - loss: 0.6766 - accuracy: 0.5577 - val_loss: 0.6395 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 46s - loss: 0.6937 - accuracy: 0.468 - ETA: 1s - loss: 0.6926 - accuracy: 0.526 - ETA: 0s - loss: 0.6897 - accuracy: 0.54 - ETA: 0s - loss: 0.6850 - accuracy: 0.55 - ETA: 0s - loss: 0.6800 - accuracy: 0.56 - ETA: 0s - loss: 0.6754 - accuracy: 0.57 - ETA: 0s - loss: 0.6721 - accuracy: 0.58 - ETA: 0s - loss: 0.6677 - accuracy: 0.58 - ETA: 0s - loss: 0.6636 - accuracy: 0.59 - 1s 46us/step - loss: 0.6627 - accuracy: 0.5963 - val_loss: 0.6053 - val_accuracy: 0.7230\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6936 - accuracy: 0.531 - ETA: 1s - loss: 0.6921 - accuracy: 0.524 - ETA: 0s - loss: 0.6922 - accuracy: 0.50 - ETA: 0s - loss: 0.6907 - accuracy: 0.51 - ETA: 0s - loss: 0.6889 - accuracy: 0.51 - ETA: 0s - loss: 0.6873 - accuracy: 0.52 - ETA: 0s - loss: 0.6857 - accuracy: 0.53 - ETA: 0s - loss: 0.6838 - accuracy: 0.53 - ETA: 0s - loss: 0.6826 - accuracy: 0.54 - 1s 47us/step - loss: 0.6819 - accuracy: 0.5476 - val_loss: 0.6525 - val_accuracy: 0.6911\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 46s - loss: 0.6931 - accuracy: 0.562 - ETA: 1s - loss: 0.6931 - accuracy: 0.514 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6910 - accuracy: 0.52 - ETA: 0s - loss: 0.6880 - accuracy: 0.52 - ETA: 0s - loss: 0.6856 - accuracy: 0.53 - ETA: 0s - loss: 0.6829 - accuracy: 0.54 - ETA: 0s - loss: 0.6799 - accuracy: 0.55 - ETA: 0s - loss: 0.6763 - accuracy: 0.56 - 1s 49us/step - loss: 0.6726 - accuracy: 0.5769 - val_loss: 0.6230 - val_accuracy: 0.6996\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6937 - accuracy: 0.656 - ETA: 1s - loss: 0.6927 - accuracy: 0.530 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6910 - accuracy: 0.51 - ETA: 0s - loss: 0.6874 - accuracy: 0.51 - ETA: 0s - loss: 0.6844 - accuracy: 0.53 - ETA: 0s - loss: 0.6797 - accuracy: 0.54 - ETA: 0s - loss: 0.6768 - accuracy: 0.55 - ETA: 0s - loss: 0.6735 - accuracy: 0.56 - 1s 46us/step - loss: 0.6739 - accuracy: 0.5694 - val_loss: 0.6390 - val_accuracy: 0.7131\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 49s - loss: 0.6948 - accuracy: 0.343 - ETA: 1s - loss: 0.6936 - accuracy: 0.499 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6918 - accuracy: 0.53 - ETA: 0s - loss: 0.6907 - accuracy: 0.53 - 1s 47us/step - loss: 0.6903 - accuracy: 0.5398 - val_loss: 0.6734 - val_accuracy: 0.6925\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6938 - accuracy: 0.562 - ETA: 1s - loss: 0.6931 - accuracy: 0.522 - ETA: 0s - loss: 0.6898 - accuracy: 0.53 - ETA: 0s - loss: 0.6850 - accuracy: 0.55 - ETA: 0s - loss: 0.6825 - accuracy: 0.56 - ETA: 0s - loss: 0.6769 - accuracy: 0.57 - ETA: 0s - loss: 0.6720 - accuracy: 0.58 - ETA: 0s - loss: 0.6677 - accuracy: 0.59 - ETA: 0s - loss: 0.6640 - accuracy: 0.60 - 1s 48us/step - loss: 0.6606 - accuracy: 0.6103 - val_loss: 0.6018 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6969 - accuracy: 0.406 - ETA: 1s - loss: 0.6942 - accuracy: 0.481 - ETA: 0s - loss: 0.6934 - accuracy: 0.49 - ETA: 0s - loss: 0.6916 - accuracy: 0.51 - ETA: 0s - loss: 0.6892 - accuracy: 0.53 - ETA: 0s - loss: 0.6860 - accuracy: 0.54 - ETA: 0s - loss: 0.6831 - accuracy: 0.55 - ETA: 0s - loss: 0.6787 - accuracy: 0.56 - ETA: 0s - loss: 0.6733 - accuracy: 0.57 - 1s 47us/step - loss: 0.6712 - accuracy: 0.5779 - val_loss: 0.6116 - val_accuracy: 0.7109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6948 - accuracy: 0.531 - ETA: 1s - loss: 0.6915 - accuracy: 0.518 - ETA: 0s - loss: 0.6887 - accuracy: 0.53 - ETA: 0s - loss: 0.6844 - accuracy: 0.55 - ETA: 0s - loss: 0.6798 - accuracy: 0.56 - ETA: 0s - loss: 0.6758 - accuracy: 0.58 - ETA: 0s - loss: 0.6693 - accuracy: 0.59 - ETA: 0s - loss: 0.6649 - accuracy: 0.60 - ETA: 0s - loss: 0.6595 - accuracy: 0.61 - 1s 48us/step - loss: 0.6561 - accuracy: 0.6187 - val_loss: 0.5927 - val_accuracy: 0.7124\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6920 - accuracy: 0.562 - ETA: 1s - loss: 0.6939 - accuracy: 0.513 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6904 - accuracy: 0.52 - ETA: 0s - loss: 0.6874 - accuracy: 0.53 - ETA: 0s - loss: 0.6839 - accuracy: 0.54 - ETA: 0s - loss: 0.6781 - accuracy: 0.55 - ETA: 0s - loss: 0.6743 - accuracy: 0.56 - ETA: 0s - loss: 0.6711 - accuracy: 0.57 - 1s 48us/step - loss: 0.6687 - accuracy: 0.5812 - val_loss: 0.6181 - val_accuracy: 0.7173\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 48s - loss: 0.6933 - accuracy: 0.562 - ETA: 1s - loss: 0.6925 - accuracy: 0.550 - ETA: 0s - loss: 0.6908 - accuracy: 0.55 - ETA: 0s - loss: 0.6852 - accuracy: 0.57 - ETA: 0s - loss: 0.6775 - accuracy: 0.58 - ETA: 0s - loss: 0.6699 - accuracy: 0.60 - ETA: 0s - loss: 0.6645 - accuracy: 0.61 - ETA: 0s - loss: 0.6573 - accuracy: 0.62 - ETA: 0s - loss: 0.6527 - accuracy: 0.62 - 1s 48us/step - loss: 0.6495 - accuracy: 0.6313 - val_loss: 0.5956 - val_accuracy: 0.7202\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 45s - loss: 0.6937 - accuracy: 0.625 - ETA: 1s - loss: 0.6939 - accuracy: 0.515 - ETA: 0s - loss: 0.6925 - accuracy: 0.53 - ETA: 0s - loss: 0.6886 - accuracy: 0.54 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6771 - accuracy: 0.58 - ETA: 0s - loss: 0.6709 - accuracy: 0.59 - ETA: 0s - loss: 0.6650 - accuracy: 0.60 - ETA: 0s - loss: 0.6598 - accuracy: 0.60 - 1s 47us/step - loss: 0.6579 - accuracy: 0.6109 - val_loss: 0.5981 - val_accuracy: 0.7138\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 46s - loss: 0.6961 - accuracy: 0.406 - ETA: 1s - loss: 0.6941 - accuracy: 0.511 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - ETA: 0s - loss: 0.6893 - accuracy: 0.53 - ETA: 0s - loss: 0.6850 - accuracy: 0.55 - ETA: 0s - loss: 0.6777 - accuracy: 0.57 - ETA: 0s - loss: 0.6727 - accuracy: 0.58 - ETA: 0s - loss: 0.6657 - accuracy: 0.59 - ETA: 0s - loss: 0.6604 - accuracy: 0.60 - 1s 48us/step - loss: 0.6578 - accuracy: 0.6094 - val_loss: 0.5973 - val_accuracy: 0.7209\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6989 - accuracy: 0.375 - ETA: 1s - loss: 0.6914 - accuracy: 0.551 - ETA: 0s - loss: 0.6874 - accuracy: 0.56 - ETA: 0s - loss: 0.6818 - accuracy: 0.58 - ETA: 0s - loss: 0.6745 - accuracy: 0.59 - ETA: 0s - loss: 0.6667 - accuracy: 0.60 - ETA: 0s - loss: 0.6614 - accuracy: 0.61 - ETA: 0s - loss: 0.6568 - accuracy: 0.62 - ETA: 0s - loss: 0.6520 - accuracy: 0.63 - 1s 48us/step - loss: 0.6493 - accuracy: 0.6334 - val_loss: 0.5892 - val_accuracy: 0.7244\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 46s - loss: 0.6992 - accuracy: 0.375 - ETA: 1s - loss: 0.6942 - accuracy: 0.502 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6881 - accuracy: 0.52 - ETA: 0s - loss: 0.6822 - accuracy: 0.55 - ETA: 0s - loss: 0.6751 - accuracy: 0.56 - ETA: 0s - loss: 0.6699 - accuracy: 0.58 - ETA: 0s - loss: 0.6624 - accuracy: 0.59 - ETA: 0s - loss: 0.6568 - accuracy: 0.60 - 1s 48us/step - loss: 0.6524 - accuracy: 0.6162 - val_loss: 0.5968 - val_accuracy: 0.7109\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 46s - loss: 0.6981 - accuracy: 0.500 - ETA: 1s - loss: 0.6942 - accuracy: 0.533 - ETA: 0s - loss: 0.6923 - accuracy: 0.55 - ETA: 0s - loss: 0.6875 - accuracy: 0.57 - ETA: 0s - loss: 0.6821 - accuracy: 0.58 - ETA: 0s - loss: 0.6742 - accuracy: 0.59 - ETA: 0s - loss: 0.6683 - accuracy: 0.61 - ETA: 0s - loss: 0.6630 - accuracy: 0.61 - ETA: 0s - loss: 0.6577 - accuracy: 0.62 - 1s 48us/step - loss: 0.6535 - accuracy: 0.6308 - val_loss: 0.5929 - val_accuracy: 0.7216\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6944 - accuracy: 0.625 - ETA: 1s - loss: 0.6928 - accuracy: 0.541 - ETA: 0s - loss: 0.6919 - accuracy: 0.53 - ETA: 0s - loss: 0.6865 - accuracy: 0.54 - ETA: 0s - loss: 0.6813 - accuracy: 0.56 - ETA: 0s - loss: 0.6775 - accuracy: 0.57 - ETA: 0s - loss: 0.6723 - accuracy: 0.58 - ETA: 0s - loss: 0.6675 - accuracy: 0.60 - ETA: 0s - loss: 0.6630 - accuracy: 0.60 - 1s 47us/step - loss: 0.6605 - accuracy: 0.6145 - val_loss: 0.6086 - val_accuracy: 0.7237\n",
      "2814/2814 [==============================] - ETA:  - ETA:  - 0s 24us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 46s - loss: 0.6952 - accuracy: 0.500 - ETA: 1s - loss: 0.6945 - accuracy: 0.519 - ETA: 0s - loss: 0.6909 - accuracy: 0.54 - ETA: 0s - loss: 0.6847 - accuracy: 0.55 - ETA: 0s - loss: 0.6796 - accuracy: 0.57 - ETA: 0s - loss: 0.6744 - accuracy: 0.58 - ETA: 0s - loss: 0.6684 - accuracy: 0.59 - ETA: 0s - loss: 0.6630 - accuracy: 0.60 - ETA: 0s - loss: 0.6588 - accuracy: 0.61 - 1s 48us/step - loss: 0.6557 - accuracy: 0.6223 - val_loss: 0.6057 - val_accuracy: 0.7173\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 41s - loss: 0.6945 - accuracy: 0.406 - ETA: 1s - loss: 0.6928 - accuracy: 0.528 - ETA: 0s - loss: 0.6922 - accuracy: 0.52 - ETA: 0s - loss: 0.6917 - accuracy: 0.53 - ETA: 0s - loss: 0.6908 - accuracy: 0.54 - ETA: 0s - loss: 0.6898 - accuracy: 0.55 - ETA: 0s - loss: 0.6886 - accuracy: 0.56 - ETA: 0s - loss: 0.6872 - accuracy: 0.56 - ETA: 0s - loss: 0.6861 - accuracy: 0.57 - 1s 45us/step - loss: 0.6859 - accuracy: 0.5719 - val_loss: 0.6689 - val_accuracy: 0.7053\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 44s - loss: 0.6896 - accuracy: 0.656 - ETA: 1s - loss: 0.6924 - accuracy: 0.529 - ETA: 0s - loss: 0.6918 - accuracy: 0.53 - ETA: 0s - loss: 0.6909 - accuracy: 0.54 - ETA: 0s - loss: 0.6894 - accuracy: 0.55 - ETA: 0s - loss: 0.6877 - accuracy: 0.55 - ETA: 0s - loss: 0.6860 - accuracy: 0.56 - ETA: 0s - loss: 0.6844 - accuracy: 0.57 - ETA: 0s - loss: 0.6822 - accuracy: 0.57 - 1s 45us/step - loss: 0.6820 - accuracy: 0.5805 - val_loss: 0.6607 - val_accuracy: 0.7124\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 41s - loss: 0.6939 - accuracy: 0.562 - ETA: 1s - loss: 0.6931 - accuracy: 0.528 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6909 - accuracy: 0.53 - ETA: 0s - loss: 0.6894 - accuracy: 0.54 - ETA: 0s - loss: 0.6878 - accuracy: 0.55 - ETA: 0s - loss: 0.6868 - accuracy: 0.56 - ETA: 0s - loss: 0.6849 - accuracy: 0.56 - ETA: 0s - loss: 0.6834 - accuracy: 0.57 - 1s 45us/step - loss: 0.6832 - accuracy: 0.5741 - val_loss: 0.6623 - val_accuracy: 0.7124\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 40s - loss: 0.6935 - accuracy: 0.312 - ETA: 1s - loss: 0.6927 - accuracy: 0.520 - ETA: 0s - loss: 0.6917 - accuracy: 0.53 - ETA: 0s - loss: 0.6907 - accuracy: 0.54 - ETA: 0s - loss: 0.6891 - accuracy: 0.55 - ETA: 0s - loss: 0.6879 - accuracy: 0.56 - ETA: 0s - loss: 0.6867 - accuracy: 0.56 - ETA: 0s - loss: 0.6853 - accuracy: 0.56 - ETA: 0s - loss: 0.6833 - accuracy: 0.57 - 1s 44us/step - loss: 0.6832 - accuracy: 0.5750 - val_loss: 0.6654 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 40s - loss: 0.6942 - accuracy: 0.468 - ETA: 1s - loss: 0.6934 - accuracy: 0.501 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6918 - accuracy: 0.52 - ETA: 0s - loss: 0.6907 - accuracy: 0.52 - ETA: 0s - loss: 0.6895 - accuracy: 0.53 - ETA: 0s - loss: 0.6883 - accuracy: 0.53 - ETA: 0s - loss: 0.6871 - accuracy: 0.54 - 1s 45us/step - loss: 0.6869 - accuracy: 0.5422 - val_loss: 0.6716 - val_accuracy: 0.6776\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 42s - loss: 0.6913 - accuracy: 0.562 - ETA: 1s - loss: 0.6919 - accuracy: 0.523 - ETA: 0s - loss: 0.6902 - accuracy: 0.52 - ETA: 0s - loss: 0.6892 - accuracy: 0.52 - ETA: 0s - loss: 0.6891 - accuracy: 0.52 - ETA: 0s - loss: 0.6875 - accuracy: 0.53 - ETA: 0s - loss: 0.6861 - accuracy: 0.54 - ETA: 0s - loss: 0.6848 - accuracy: 0.54 - ETA: 0s - loss: 0.6830 - accuracy: 0.55 - 1s 45us/step - loss: 0.6827 - accuracy: 0.5588 - val_loss: 0.6657 - val_accuracy: 0.6790\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 41s - loss: 0.6936 - accuracy: 0.500 - ETA: 1s - loss: 0.6933 - accuracy: 0.534 - ETA: 0s - loss: 0.6920 - accuracy: 0.56 - ETA: 0s - loss: 0.6907 - accuracy: 0.56 - ETA: 0s - loss: 0.6886 - accuracy: 0.58 - ETA: 0s - loss: 0.6865 - accuracy: 0.59 - ETA: 0s - loss: 0.6841 - accuracy: 0.59 - ETA: 0s - loss: 0.6814 - accuracy: 0.60 - ETA: 0s - loss: 0.6781 - accuracy: 0.61 - 1s 46us/step - loss: 0.6765 - accuracy: 0.6136 - val_loss: 0.6434 - val_accuracy: 0.7202\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 40s - loss: 0.6953 - accuracy: 0.468 - ETA: 1s - loss: 0.6920 - accuracy: 0.533 - ETA: 0s - loss: 0.6909 - accuracy: 0.53 - ETA: 0s - loss: 0.6887 - accuracy: 0.53 - ETA: 0s - loss: 0.6871 - accuracy: 0.54 - ETA: 0s - loss: 0.6848 - accuracy: 0.54 - ETA: 0s - loss: 0.6833 - accuracy: 0.55 - ETA: 0s - loss: 0.6811 - accuracy: 0.56 - ETA: 0s - loss: 0.6786 - accuracy: 0.57 - 1s 45us/step - loss: 0.6779 - accuracy: 0.5764 - val_loss: 0.6508 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 41s - loss: 0.6961 - accuracy: 0.375 - ETA: 1s - loss: 0.6927 - accuracy: 0.523 - ETA: 0s - loss: 0.6917 - accuracy: 0.54 - ETA: 0s - loss: 0.6904 - accuracy: 0.55 - ETA: 0s - loss: 0.6886 - accuracy: 0.56 - ETA: 0s - loss: 0.6866 - accuracy: 0.57 - ETA: 0s - loss: 0.6852 - accuracy: 0.58 - ETA: 0s - loss: 0.6829 - accuracy: 0.58 - ETA: 0s - loss: 0.6797 - accuracy: 0.59 - 1s 46us/step - loss: 0.6784 - accuracy: 0.6022 - val_loss: 0.6476 - val_accuracy: 0.7081\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 40s - loss: 0.6943 - accuracy: 0.468 - ETA: 1s - loss: 0.6927 - accuracy: 0.550 - ETA: 0s - loss: 0.6913 - accuracy: 0.57 - ETA: 0s - loss: 0.6896 - accuracy: 0.58 - ETA: 0s - loss: 0.6881 - accuracy: 0.58 - ETA: 0s - loss: 0.6851 - accuracy: 0.59 - ETA: 0s - loss: 0.6828 - accuracy: 0.60 - ETA: 0s - loss: 0.6802 - accuracy: 0.60 - ETA: 0s - loss: 0.6771 - accuracy: 0.61 - 1s 45us/step - loss: 0.6758 - accuracy: 0.6160 - val_loss: 0.6426 - val_accuracy: 0.7138\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 41s - loss: 0.6931 - accuracy: 0.500 - ETA: 1s - loss: 0.6938 - accuracy: 0.510 - ETA: 0s - loss: 0.6921 - accuracy: 0.54 - ETA: 0s - loss: 0.6913 - accuracy: 0.55 - ETA: 0s - loss: 0.6894 - accuracy: 0.56 - ETA: 0s - loss: 0.6867 - accuracy: 0.58 - ETA: 0s - loss: 0.6837 - accuracy: 0.58 - ETA: 0s - loss: 0.6807 - accuracy: 0.59 - ETA: 0s - loss: 0.6782 - accuracy: 0.59 - 1s 44us/step - loss: 0.6780 - accuracy: 0.5996 - val_loss: 0.6481 - val_accuracy: 0.7060\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 41s - loss: 0.6918 - accuracy: 0.625 - ETA: 1s - loss: 0.6926 - accuracy: 0.549 - ETA: 0s - loss: 0.6917 - accuracy: 0.55 - ETA: 0s - loss: 0.6903 - accuracy: 0.56 - ETA: 0s - loss: 0.6885 - accuracy: 0.57 - ETA: 0s - loss: 0.6864 - accuracy: 0.59 - ETA: 0s - loss: 0.6836 - accuracy: 0.60 - ETA: 0s - loss: 0.6808 - accuracy: 0.61 - ETA: 0s - loss: 0.6772 - accuracy: 0.61 - 1s 45us/step - loss: 0.6765 - accuracy: 0.6163 - val_loss: 0.6489 - val_accuracy: 0.6662\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 41s - loss: 0.6975 - accuracy: 0.375 - ETA: 1s - loss: 0.6934 - accuracy: 0.511 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6910 - accuracy: 0.54 - ETA: 0s - loss: 0.6893 - accuracy: 0.56 - ETA: 0s - loss: 0.6867 - accuracy: 0.58 - ETA: 0s - loss: 0.6848 - accuracy: 0.58 - ETA: 0s - loss: 0.6820 - accuracy: 0.59 - ETA: 0s - loss: 0.6796 - accuracy: 0.60 - 1s 47us/step - loss: 0.6778 - accuracy: 0.6067 - val_loss: 0.6427 - val_accuracy: 0.7081\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 42s - loss: 0.6955 - accuracy: 0.437 - ETA: 1s - loss: 0.6931 - accuracy: 0.518 - ETA: 0s - loss: 0.6907 - accuracy: 0.53 - ETA: 0s - loss: 0.6889 - accuracy: 0.54 - ETA: 0s - loss: 0.6859 - accuracy: 0.56 - ETA: 0s - loss: 0.6829 - accuracy: 0.57 - ETA: 0s - loss: 0.6793 - accuracy: 0.58 - ETA: 0s - loss: 0.6754 - accuracy: 0.59 - ETA: 0s - loss: 0.6718 - accuracy: 0.60 - 1s 45us/step - loss: 0.6717 - accuracy: 0.6023 - val_loss: 0.6350 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 41s - loss: 0.6937 - accuracy: 0.531 - ETA: 1s - loss: 0.6924 - accuracy: 0.543 - ETA: 0s - loss: 0.6904 - accuracy: 0.55 - ETA: 0s - loss: 0.6879 - accuracy: 0.56 - ETA: 0s - loss: 0.6854 - accuracy: 0.58 - ETA: 0s - loss: 0.6824 - accuracy: 0.59 - ETA: 0s - loss: 0.6791 - accuracy: 0.60 - ETA: 0s - loss: 0.6753 - accuracy: 0.61 - ETA: 0s - loss: 0.6717 - accuracy: 0.61 - 1s 46us/step - loss: 0.6702 - accuracy: 0.6221 - val_loss: 0.6292 - val_accuracy: 0.7053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 41s - loss: 0.6941 - accuracy: 0.625 - ETA: 1s - loss: 0.6937 - accuracy: 0.525 - ETA: 0s - loss: 0.6916 - accuracy: 0.53 - ETA: 0s - loss: 0.6893 - accuracy: 0.55 - ETA: 0s - loss: 0.6867 - accuracy: 0.56 - ETA: 0s - loss: 0.6831 - accuracy: 0.58 - ETA: 0s - loss: 0.6795 - accuracy: 0.59 - ETA: 0s - loss: 0.6755 - accuracy: 0.60 - ETA: 0s - loss: 0.6718 - accuracy: 0.61 - 1s 45us/step - loss: 0.6709 - accuracy: 0.6119 - val_loss: 0.6293 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 44s - loss: 0.6943 - accuracy: 0.500 - ETA: 1s - loss: 0.6919 - accuracy: 0.538 - ETA: 0s - loss: 0.6877 - accuracy: 0.56 - ETA: 0s - loss: 0.6849 - accuracy: 0.57 - ETA: 0s - loss: 0.6819 - accuracy: 0.58 - ETA: 0s - loss: 0.6782 - accuracy: 0.59 - ETA: 0s - loss: 0.6755 - accuracy: 0.60 - ETA: 0s - loss: 0.6708 - accuracy: 0.61 - ETA: 0s - loss: 0.6667 - accuracy: 0.62 - 1s 46us/step - loss: 0.6657 - accuracy: 0.6223 - val_loss: 0.6212 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 41s - loss: 0.6974 - accuracy: 0.437 - ETA: 1s - loss: 0.6878 - accuracy: 0.533 - ETA: 0s - loss: 0.6884 - accuracy: 0.53 - ETA: 0s - loss: 0.6880 - accuracy: 0.54 - ETA: 0s - loss: 0.6855 - accuracy: 0.55 - ETA: 0s - loss: 0.6832 - accuracy: 0.57 - ETA: 0s - loss: 0.6798 - accuracy: 0.58 - ETA: 0s - loss: 0.6768 - accuracy: 0.59 - ETA: 0s - loss: 0.6740 - accuracy: 0.60 - 1s 48us/step - loss: 0.6705 - accuracy: 0.6097 - val_loss: 0.6369 - val_accuracy: 0.7017\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 36s - loss: 0.6937 - accuracy: 0.468 - ETA: 0s - loss: 0.6941 - accuracy: 0.456 - ETA: 0s - loss: 0.6939 - accuracy: 0.47 - ETA: 0s - loss: 0.6938 - accuracy: 0.48 - ETA: 0s - loss: 0.6938 - accuracy: 0.48 - ETA: 0s - loss: 0.6938 - accuracy: 0.48 - ETA: 0s - loss: 0.6936 - accuracy: 0.49 - ETA: 0s - loss: 0.6935 - accuracy: 0.49 - 1s 42us/step - loss: 0.6934 - accuracy: 0.5007 - val_loss: 0.6922 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 36s - loss: 0.6942 - accuracy: 0.562 - ETA: 0s - loss: 0.6948 - accuracy: 0.474 - ETA: 0s - loss: 0.6947 - accuracy: 0.46 - ETA: 0s - loss: 0.6946 - accuracy: 0.46 - ETA: 0s - loss: 0.6945 - accuracy: 0.46 - ETA: 0s - loss: 0.6943 - accuracy: 0.47 - ETA: 0s - loss: 0.6942 - accuracy: 0.47 - ETA: 0s - loss: 0.6941 - accuracy: 0.48 - 1s 43us/step - loss: 0.6941 - accuracy: 0.4874 - val_loss: 0.6929 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 36s - loss: 0.6978 - accuracy: 0.406 - ETA: 0s - loss: 0.6936 - accuracy: 0.492 - ETA: 0s - loss: 0.6937 - accuracy: 0.49 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - 1s 42us/step - loss: 0.6933 - accuracy: 0.5096 - val_loss: 0.6924 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 37s - loss: 0.6932 - accuracy: 0.593 - ETA: 0s - loss: 0.6937 - accuracy: 0.506 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - 1s 44us/step - loss: 0.6934 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 35s - loss: 0.6943 - accuracy: 0.468 - ETA: 0s - loss: 0.6937 - accuracy: 0.494 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - 1s 42us/step - loss: 0.6934 - accuracy: 0.5135 - val_loss: 0.6927 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 37s - loss: 0.6940 - accuracy: 0.437 - ETA: 0s - loss: 0.6937 - accuracy: 0.498 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - 1s 44us/step - loss: 0.6936 - accuracy: 0.5084 - val_loss: 0.6935 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 35s - loss: 0.6941 - accuracy: 0.468 - ETA: 0s - loss: 0.6941 - accuracy: 0.512 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - 1s 42us/step - loss: 0.6934 - accuracy: 0.5169 - val_loss: 0.6922 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 16us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 37s - loss: 0.6961 - accuracy: 0.343 - ETA: 0s - loss: 0.6938 - accuracy: 0.523 - ETA: 0s - loss: 0.6939 - accuracy: 0.53 - ETA: 0s - loss: 0.6937 - accuracy: 0.54 - ETA: 0s - loss: 0.6936 - accuracy: 0.54 - ETA: 0s - loss: 0.6937 - accuracy: 0.53 - ETA: 0s - loss: 0.6936 - accuracy: 0.53 - ETA: 0s - loss: 0.6935 - accuracy: 0.53 - 1s 43us/step - loss: 0.6936 - accuracy: 0.5339 - val_loss: 0.6928 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 16us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 37s - loss: 0.6922 - accuracy: 0.562 - ETA: 0s - loss: 0.6936 - accuracy: 0.506 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - 1s 46us/step - loss: 0.6934 - accuracy: 0.5118 - val_loss: 0.6916 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 37s - loss: 0.6996 - accuracy: 0.312 - ETA: 0s - loss: 0.6943 - accuracy: 0.506 - ETA: 0s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - 1s 43us/step - loss: 0.6939 - accuracy: 0.5128 - val_loss: 0.6930 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 36s - loss: 0.6909 - accuracy: 0.625 - ETA: 0s - loss: 0.6939 - accuracy: 0.522 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.52 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6933 - accuracy: 0.52 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - 1s 43us/step - loss: 0.6935 - accuracy: 0.5136 - val_loss: 0.6916 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 17us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 36s - loss: 0.6933 - accuracy: 0.593 - ETA: 0s - loss: 0.6941 - accuracy: 0.521 - ETA: 0s - loss: 0.6940 - accuracy: 0.52 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - 1s 43us/step - loss: 0.6940 - accuracy: 0.5103 - val_loss: 0.6936 - val_accuracy: 0.5128\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 36s - loss: 0.6952 - accuracy: 0.500 - ETA: 0s - loss: 0.6939 - accuracy: 0.527 - ETA: 0s - loss: 0.6932 - accuracy: 0.53 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - 1s 43us/step - loss: 0.6945 - accuracy: 0.5134 - val_loss: 0.6932 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 16us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 36s - loss: 0.6964 - accuracy: 0.593 - ETA: 1s - loss: 0.6958 - accuracy: 0.478 - ETA: 0s - loss: 0.6953 - accuracy: 0.49 - ETA: 0s - loss: 0.6951 - accuracy: 0.49 - ETA: 0s - loss: 0.6950 - accuracy: 0.50 - ETA: 0s - loss: 0.6949 - accuracy: 0.50 - ETA: 0s - loss: 0.6948 - accuracy: 0.49 - ETA: 0s - loss: 0.6947 - accuracy: 0.50 - ETA: 0s - loss: 0.6946 - accuracy: 0.50 - 1s 44us/step - loss: 0.6946 - accuracy: 0.5050 - val_loss: 0.6930 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 36s - loss: 0.6920 - accuracy: 0.531 - ETA: 0s - loss: 0.6955 - accuracy: 0.463 - ETA: 0s - loss: 0.6948 - accuracy: 0.49 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - 1s 43us/step - loss: 0.6942 - accuracy: 0.5192 - val_loss: 0.6929 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 35s - loss: 0.6942 - accuracy: 0.562 - ETA: 1s - loss: 0.6945 - accuracy: 0.512 - ETA: 0s - loss: 0.6948 - accuracy: 0.50 - ETA: 0s - loss: 0.6947 - accuracy: 0.50 - ETA: 0s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6947 - accuracy: 0.50 - ETA: 0s - loss: 0.6946 - accuracy: 0.50 - ETA: 0s - loss: 0.6946 - accuracy: 0.50 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - 1s 44us/step - loss: 0.6944 - accuracy: 0.5122 - val_loss: 0.6935 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 35s - loss: 0.6909 - accuracy: 0.562 - ETA: 0s - loss: 0.6927 - accuracy: 0.522 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6922 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6922 - accuracy: 0.52 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - 1s 44us/step - loss: 0.6920 - accuracy: 0.5204 - val_loss: 0.6900 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 36s - loss: 0.6968 - accuracy: 0.468 - ETA: 1s - loss: 0.6959 - accuracy: 0.455 - ETA: 0s - loss: 0.6954 - accuracy: 0.47 - ETA: 0s - loss: 0.6952 - accuracy: 0.48 - ETA: 0s - loss: 0.6952 - accuracy: 0.48 - ETA: 0s - loss: 0.6950 - accuracy: 0.48 - ETA: 0s - loss: 0.6949 - accuracy: 0.49 - ETA: 0s - loss: 0.6947 - accuracy: 0.49 - 1s 43us/step - loss: 0.6946 - accuracy: 0.4963 - val_loss: 0.6936 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6917 - accuracy: 0.531 - ETA: 1s - loss: 0.6923 - accuracy: 0.535 - ETA: 0s - loss: 0.6918 - accuracy: 0.52 - ETA: 0s - loss: 0.6914 - accuracy: 0.51 - ETA: 0s - loss: 0.6909 - accuracy: 0.52 - ETA: 0s - loss: 0.6894 - accuracy: 0.52 - ETA: 0s - loss: 0.6882 - accuracy: 0.53 - ETA: 0s - loss: 0.6852 - accuracy: 0.54 - ETA: 0s - loss: 0.6835 - accuracy: 0.54 - ETA: 0s - loss: 0.6824 - accuracy: 0.55 - 1s 49us/step - loss: 0.6824 - accuracy: 0.5511 - val_loss: 0.6582 - val_accuracy: 0.7109\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6961 - accuracy: 0.437 - ETA: 1s - loss: 0.6935 - accuracy: 0.515 - ETA: 0s - loss: 0.6928 - accuracy: 0.54 - ETA: 0s - loss: 0.6920 - accuracy: 0.55 - ETA: 0s - loss: 0.6907 - accuracy: 0.57 - ETA: 0s - loss: 0.6890 - accuracy: 0.57 - ETA: 0s - loss: 0.6869 - accuracy: 0.58 - ETA: 0s - loss: 0.6847 - accuracy: 0.59 - ETA: 0s - loss: 0.6827 - accuracy: 0.60 - 1s 47us/step - loss: 0.6814 - accuracy: 0.6039 - val_loss: 0.6598 - val_accuracy: 0.6790\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6939 - accuracy: 0.343 - ETA: 1s - loss: 0.6930 - accuracy: 0.520 - ETA: 0s - loss: 0.6914 - accuracy: 0.52 - ETA: 0s - loss: 0.6902 - accuracy: 0.51 - ETA: 0s - loss: 0.6880 - accuracy: 0.52 - ETA: 0s - loss: 0.6856 - accuracy: 0.52 - ETA: 0s - loss: 0.6834 - accuracy: 0.52 - ETA: 0s - loss: 0.6813 - accuracy: 0.53 - ETA: 0s - loss: 0.6800 - accuracy: 0.55 - 1s 47us/step - loss: 0.6794 - accuracy: 0.5547 - val_loss: 0.6553 - val_accuracy: 0.6847\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6938 - accuracy: 0.468 - ETA: 1s - loss: 0.6936 - accuracy: 0.489 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6919 - accuracy: 0.53 - ETA: 0s - loss: 0.6902 - accuracy: 0.54 - ETA: 0s - loss: 0.6875 - accuracy: 0.56 - ETA: 0s - loss: 0.6845 - accuracy: 0.57 - ETA: 0s - loss: 0.6803 - accuracy: 0.58 - ETA: 0s - loss: 0.6770 - accuracy: 0.58 - 1s 46us/step - loss: 0.6768 - accuracy: 0.5879 - val_loss: 0.6443 - val_accuracy: 0.7081\n",
      "2814/2814 [==============================] - ETA:  - 0s 16us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6941 - accuracy: 0.500 - ETA: 1s - loss: 0.6929 - accuracy: 0.507 - ETA: 0s - loss: 0.6918 - accuracy: 0.51 - ETA: 0s - loss: 0.6911 - accuracy: 0.52 - ETA: 0s - loss: 0.6895 - accuracy: 0.53 - ETA: 0s - loss: 0.6876 - accuracy: 0.54 - ETA: 0s - loss: 0.6853 - accuracy: 0.55 - ETA: 0s - loss: 0.6829 - accuracy: 0.56 - ETA: 0s - loss: 0.6798 - accuracy: 0.57 - 1s 47us/step - loss: 0.6783 - accuracy: 0.5748 - val_loss: 0.6444 - val_accuracy: 0.7102\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6948 - accuracy: 0.406 - ETA: 1s - loss: 0.6929 - accuracy: 0.525 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6913 - accuracy: 0.53 - ETA: 0s - loss: 0.6895 - accuracy: 0.55 - ETA: 0s - loss: 0.6876 - accuracy: 0.55 - ETA: 0s - loss: 0.6857 - accuracy: 0.56 - ETA: 0s - loss: 0.6830 - accuracy: 0.57 - ETA: 0s - loss: 0.6805 - accuracy: 0.58 - 1s 48us/step - loss: 0.6794 - accuracy: 0.5826 - val_loss: 0.6514 - val_accuracy: 0.6911\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 46s - loss: 0.6958 - accuracy: 0.500 - ETA: 1s - loss: 0.6935 - accuracy: 0.511 - ETA: 0s - loss: 0.6918 - accuracy: 0.52 - ETA: 0s - loss: 0.6894 - accuracy: 0.54 - ETA: 0s - loss: 0.6879 - accuracy: 0.54 - ETA: 0s - loss: 0.6865 - accuracy: 0.55 - ETA: 0s - loss: 0.6838 - accuracy: 0.56 - ETA: 0s - loss: 0.6809 - accuracy: 0.57 - ETA: 0s - loss: 0.6774 - accuracy: 0.58 - 1s 48us/step - loss: 0.6746 - accuracy: 0.5880 - val_loss: 0.6365 - val_accuracy: 0.7017\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6945 - accuracy: 0.406 - ETA: 1s - loss: 0.6926 - accuracy: 0.548 - ETA: 0s - loss: 0.6919 - accuracy: 0.53 - ETA: 0s - loss: 0.6912 - accuracy: 0.53 - ETA: 0s - loss: 0.6900 - accuracy: 0.54 - ETA: 0s - loss: 0.6881 - accuracy: 0.55 - ETA: 0s - loss: 0.6865 - accuracy: 0.55 - ETA: 0s - loss: 0.6838 - accuracy: 0.56 - ETA: 0s - loss: 0.6806 - accuracy: 0.57 - 1s 47us/step - loss: 0.6791 - accuracy: 0.5786 - val_loss: 0.6451 - val_accuracy: 0.7088\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6971 - accuracy: 0.343 - ETA: 1s - loss: 0.6936 - accuracy: 0.513 - ETA: 0s - loss: 0.6920 - accuracy: 0.53 - ETA: 0s - loss: 0.6907 - accuracy: 0.53 - ETA: 0s - loss: 0.6881 - accuracy: 0.55 - ETA: 0s - loss: 0.6846 - accuracy: 0.56 - ETA: 0s - loss: 0.6816 - accuracy: 0.57 - ETA: 0s - loss: 0.6784 - accuracy: 0.58 - ETA: 0s - loss: 0.6736 - accuracy: 0.59 - 1s 48us/step - loss: 0.6706 - accuracy: 0.6042 - val_loss: 0.6213 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 45s - loss: 0.6933 - accuracy: 0.562 - ETA: 1s - loss: 0.6904 - accuracy: 0.585 - ETA: 0s - loss: 0.6878 - accuracy: 0.56 - ETA: 0s - loss: 0.6843 - accuracy: 0.56 - ETA: 0s - loss: 0.6803 - accuracy: 0.58 - ETA: 0s - loss: 0.6780 - accuracy: 0.58 - ETA: 0s - loss: 0.6749 - accuracy: 0.59 - ETA: 0s - loss: 0.6713 - accuracy: 0.60 - ETA: 0s - loss: 0.6675 - accuracy: 0.61 - 1s 47us/step - loss: 0.6649 - accuracy: 0.6137 - val_loss: 0.6234 - val_accuracy: 0.6932\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6970 - accuracy: 0.406 - ETA: 1s - loss: 0.6937 - accuracy: 0.510 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6913 - accuracy: 0.54 - ETA: 0s - loss: 0.6899 - accuracy: 0.55 - ETA: 0s - loss: 0.6877 - accuracy: 0.56 - ETA: 0s - loss: 0.6851 - accuracy: 0.57 - ETA: 0s - loss: 0.6828 - accuracy: 0.57 - ETA: 0s - loss: 0.6803 - accuracy: 0.58 - 1s 47us/step - loss: 0.6791 - accuracy: 0.5855 - val_loss: 0.6456 - val_accuracy: 0.7131\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 46s - loss: 0.6930 - accuracy: 0.656 - ETA: 1s - loss: 0.6939 - accuracy: 0.505 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6917 - accuracy: 0.52 - ETA: 0s - loss: 0.6905 - accuracy: 0.52 - ETA: 0s - loss: 0.6883 - accuracy: 0.53 - ETA: 0s - loss: 0.6859 - accuracy: 0.54 - ETA: 0s - loss: 0.6834 - accuracy: 0.55 - ETA: 0s - loss: 0.6814 - accuracy: 0.56 - 1s 47us/step - loss: 0.6800 - accuracy: 0.5660 - val_loss: 0.6496 - val_accuracy: 0.6996\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 46s - loss: 0.7020 - accuracy: 0.375 - ETA: 1s - loss: 0.6929 - accuracy: 0.525 - ETA: 0s - loss: 0.6908 - accuracy: 0.53 - ETA: 0s - loss: 0.6873 - accuracy: 0.56 - ETA: 0s - loss: 0.6844 - accuracy: 0.57 - ETA: 0s - loss: 0.6805 - accuracy: 0.58 - ETA: 0s - loss: 0.6764 - accuracy: 0.59 - ETA: 0s - loss: 0.6712 - accuracy: 0.60 - ETA: 0s - loss: 0.6674 - accuracy: 0.61 - 1s 47us/step - loss: 0.6653 - accuracy: 0.6196 - val_loss: 0.6154 - val_accuracy: 0.7202\n",
      "2815/2815 [==============================] - ETA:  - 0s 16us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6951 - accuracy: 0.500 - ETA: 1s - loss: 0.6938 - accuracy: 0.536 - ETA: 0s - loss: 0.6921 - accuracy: 0.55 - ETA: 0s - loss: 0.6900 - accuracy: 0.56 - ETA: 0s - loss: 0.6872 - accuracy: 0.57 - ETA: 0s - loss: 0.6845 - accuracy: 0.58 - ETA: 0s - loss: 0.6810 - accuracy: 0.59 - ETA: 0s - loss: 0.6773 - accuracy: 0.60 - ETA: 0s - loss: 0.6732 - accuracy: 0.60 - 1s 49us/step - loss: 0.6704 - accuracy: 0.6132 - val_loss: 0.6268 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - 0s 16us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6961 - accuracy: 0.500 - ETA: 1s - loss: 0.6923 - accuracy: 0.508 - ETA: 0s - loss: 0.6914 - accuracy: 0.51 - ETA: 0s - loss: 0.6885 - accuracy: 0.52 - ETA: 0s - loss: 0.6859 - accuracy: 0.53 - ETA: 0s - loss: 0.6836 - accuracy: 0.55 - ETA: 0s - loss: 0.6800 - accuracy: 0.56 - ETA: 0s - loss: 0.6751 - accuracy: 0.58 - ETA: 0s - loss: 0.6718 - accuracy: 0.58 - 1s 49us/step - loss: 0.6674 - accuracy: 0.5974 - val_loss: 0.6207 - val_accuracy: 0.7045\n",
      "2815/2815 [==============================] - ETA:  - 0s 16us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6928 - accuracy: 0.593 - ETA: 1s - loss: 0.6934 - accuracy: 0.520 - ETA: 0s - loss: 0.6922 - accuracy: 0.54 - ETA: 0s - loss: 0.6906 - accuracy: 0.54 - ETA: 0s - loss: 0.6869 - accuracy: 0.56 - ETA: 0s - loss: 0.6829 - accuracy: 0.57 - ETA: 0s - loss: 0.6777 - accuracy: 0.59 - ETA: 0s - loss: 0.6713 - accuracy: 0.60 - ETA: 0s - loss: 0.6658 - accuracy: 0.61 - 1s 47us/step - loss: 0.6640 - accuracy: 0.6170 - val_loss: 0.6113 - val_accuracy: 0.7109\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 48s - loss: 0.6969 - accuracy: 0.437 - ETA: 1s - loss: 0.6937 - accuracy: 0.519 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6919 - accuracy: 0.51 - ETA: 0s - loss: 0.6901 - accuracy: 0.52 - ETA: 0s - loss: 0.6873 - accuracy: 0.54 - ETA: 0s - loss: 0.6838 - accuracy: 0.55 - ETA: 0s - loss: 0.6802 - accuracy: 0.56 - ETA: 0s - loss: 0.6761 - accuracy: 0.57 - 1s 49us/step - loss: 0.6727 - accuracy: 0.5837 - val_loss: 0.6257 - val_accuracy: 0.7109\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 49s - loss: 0.6975 - accuracy: 0.375 - ETA: 1s - loss: 0.6943 - accuracy: 0.498 - ETA: 0s - loss: 0.6928 - accuracy: 0.53 - ETA: 0s - loss: 0.6908 - accuracy: 0.55 - ETA: 0s - loss: 0.6874 - accuracy: 0.56 - ETA: 0s - loss: 0.6842 - accuracy: 0.58 - ETA: 0s - loss: 0.6798 - accuracy: 0.59 - ETA: 0s - loss: 0.6750 - accuracy: 0.60 - ETA: 0s - loss: 0.6697 - accuracy: 0.61 - 1s 49us/step - loss: 0.6653 - accuracy: 0.6181 - val_loss: 0.6182 - val_accuracy: 0.7124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 39s - loss: 0.6941 - accuracy: 0.500 - ETA: 0s - loss: 0.6915 - accuracy: 0.519 - ETA: 0s - loss: 0.6889 - accuracy: 0.52 - ETA: 0s - loss: 0.6868 - accuracy: 0.53 - ETA: 0s - loss: 0.6847 - accuracy: 0.55 - ETA: 0s - loss: 0.6825 - accuracy: 0.55 - ETA: 0s - loss: 0.6815 - accuracy: 0.56 - ETA: 0s - loss: 0.6798 - accuracy: 0.56 - ETA: 0s - loss: 0.6786 - accuracy: 0.56 - 1s 44us/step - loss: 0.6784 - accuracy: 0.5661 - val_loss: 0.6596 - val_accuracy: 0.7045\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 41s - loss: 0.6903 - accuracy: 0.531 - ETA: 1s - loss: 0.6905 - accuracy: 0.538 - ETA: 0s - loss: 0.6868 - accuracy: 0.55 - ETA: 0s - loss: 0.6848 - accuracy: 0.55 - ETA: 0s - loss: 0.6828 - accuracy: 0.56 - ETA: 0s - loss: 0.6807 - accuracy: 0.56 - ETA: 0s - loss: 0.6797 - accuracy: 0.56 - ETA: 0s - loss: 0.6787 - accuracy: 0.56 - 1s 43us/step - loss: 0.6773 - accuracy: 0.5714 - val_loss: 0.6604 - val_accuracy: 0.7053\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 39s - loss: 0.6931 - accuracy: 0.593 - ETA: 0s - loss: 0.6928 - accuracy: 0.503 - ETA: 0s - loss: 0.6908 - accuracy: 0.52 - ETA: 0s - loss: 0.6891 - accuracy: 0.54 - ETA: 0s - loss: 0.6868 - accuracy: 0.56 - ETA: 0s - loss: 0.6851 - accuracy: 0.57 - ETA: 0s - loss: 0.6836 - accuracy: 0.58 - ETA: 0s - loss: 0.6812 - accuracy: 0.59 - 1s 43us/step - loss: 0.6790 - accuracy: 0.6044 - val_loss: 0.6620 - val_accuracy: 0.6669\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 40s - loss: 0.6939 - accuracy: 0.406 - ETA: 1s - loss: 0.6891 - accuracy: 0.541 - ETA: 0s - loss: 0.6859 - accuracy: 0.55 - ETA: 0s - loss: 0.6852 - accuracy: 0.55 - ETA: 0s - loss: 0.6836 - accuracy: 0.55 - ETA: 0s - loss: 0.6822 - accuracy: 0.56 - ETA: 0s - loss: 0.6809 - accuracy: 0.56 - ETA: 0s - loss: 0.6796 - accuracy: 0.56 - 1s 43us/step - loss: 0.6786 - accuracy: 0.5664 - val_loss: 0.6615 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 37s - loss: 0.6941 - accuracy: 0.468 - ETA: 1s - loss: 0.6917 - accuracy: 0.510 - ETA: 0s - loss: 0.6876 - accuracy: 0.53 - ETA: 0s - loss: 0.6868 - accuracy: 0.53 - ETA: 0s - loss: 0.6829 - accuracy: 0.55 - ETA: 0s - loss: 0.6807 - accuracy: 0.56 - ETA: 0s - loss: 0.6784 - accuracy: 0.57 - ETA: 0s - loss: 0.6764 - accuracy: 0.58 - ETA: 0s - loss: 0.6746 - accuracy: 0.58 - 1s 44us/step - loss: 0.6746 - accuracy: 0.5867 - val_loss: 0.6538 - val_accuracy: 0.7045\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 38s - loss: 0.6907 - accuracy: 0.562 - ETA: 1s - loss: 0.6870 - accuracy: 0.535 - ETA: 0s - loss: 0.6887 - accuracy: 0.52 - ETA: 0s - loss: 0.6876 - accuracy: 0.52 - ETA: 0s - loss: 0.6859 - accuracy: 0.54 - ETA: 0s - loss: 0.6850 - accuracy: 0.54 - ETA: 0s - loss: 0.6835 - accuracy: 0.55 - ETA: 0s - loss: 0.6819 - accuracy: 0.55 - 1s 43us/step - loss: 0.6805 - accuracy: 0.5593 - val_loss: 0.6676 - val_accuracy: 0.6726\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 38s - loss: 0.6922 - accuracy: 0.562 - ETA: 0s - loss: 0.6913 - accuracy: 0.539 - ETA: 0s - loss: 0.6879 - accuracy: 0.55 - ETA: 0s - loss: 0.6850 - accuracy: 0.56 - ETA: 0s - loss: 0.6824 - accuracy: 0.56 - ETA: 0s - loss: 0.6801 - accuracy: 0.57 - ETA: 0s - loss: 0.6772 - accuracy: 0.58 - ETA: 0s - loss: 0.6757 - accuracy: 0.58 - ETA: 0s - loss: 0.6737 - accuracy: 0.59 - 1s 44us/step - loss: 0.6734 - accuracy: 0.5967 - val_loss: 0.6493 - val_accuracy: 0.6946\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 38s - loss: 0.6944 - accuracy: 0.593 - ETA: 0s - loss: 0.6896 - accuracy: 0.558 - ETA: 0s - loss: 0.6861 - accuracy: 0.56 - ETA: 0s - loss: 0.6835 - accuracy: 0.57 - ETA: 0s - loss: 0.6806 - accuracy: 0.58 - ETA: 0s - loss: 0.6774 - accuracy: 0.59 - ETA: 0s - loss: 0.6747 - accuracy: 0.60 - ETA: 0s - loss: 0.6722 - accuracy: 0.61 - ETA: 0s - loss: 0.6701 - accuracy: 0.61 - 1s 44us/step - loss: 0.6699 - accuracy: 0.6143 - val_loss: 0.6435 - val_accuracy: 0.7102\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 39s - loss: 0.6922 - accuracy: 0.500 - ETA: 1s - loss: 0.6892 - accuracy: 0.540 - ETA: 0s - loss: 0.6822 - accuracy: 0.57 - ETA: 0s - loss: 0.6802 - accuracy: 0.58 - ETA: 0s - loss: 0.6762 - accuracy: 0.59 - ETA: 0s - loss: 0.6734 - accuracy: 0.60 - ETA: 0s - loss: 0.6707 - accuracy: 0.60 - ETA: 0s - loss: 0.6678 - accuracy: 0.61 - ETA: 0s - loss: 0.6645 - accuracy: 0.62 - 1s 44us/step - loss: 0.6643 - accuracy: 0.6212 - val_loss: 0.6316 - val_accuracy: 0.7074\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 38s - loss: 0.6948 - accuracy: 0.406 - ETA: 0s - loss: 0.6890 - accuracy: 0.534 - ETA: 0s - loss: 0.6862 - accuracy: 0.55 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6808 - accuracy: 0.57 - ETA: 0s - loss: 0.6774 - accuracy: 0.58 - ETA: 0s - loss: 0.6740 - accuracy: 0.59 - ETA: 0s - loss: 0.6716 - accuracy: 0.60 - ETA: 0s - loss: 0.6694 - accuracy: 0.60 - 1s 46us/step - loss: 0.6687 - accuracy: 0.6062 - val_loss: 0.6423 - val_accuracy: 0.7067\n",
      "2814/2814 [==============================] - ETA:  - 0s 16us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 39s - loss: 0.6941 - accuracy: 0.593 - ETA: 1s - loss: 0.6897 - accuracy: 0.525 - ETA: 0s - loss: 0.6894 - accuracy: 0.51 - ETA: 0s - loss: 0.6887 - accuracy: 0.53 - ETA: 0s - loss: 0.6869 - accuracy: 0.54 - ETA: 0s - loss: 0.6847 - accuracy: 0.55 - ETA: 0s - loss: 0.6813 - accuracy: 0.57 - ETA: 0s - loss: 0.6786 - accuracy: 0.58 - ETA: 0s - loss: 0.6758 - accuracy: 0.59 - 1s 45us/step - loss: 0.6753 - accuracy: 0.5936 - val_loss: 0.6456 - val_accuracy: 0.7102\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 40s - loss: 0.6926 - accuracy: 0.625 - ETA: 1s - loss: 0.6924 - accuracy: 0.518 - ETA: 0s - loss: 0.6903 - accuracy: 0.53 - ETA: 0s - loss: 0.6873 - accuracy: 0.55 - ETA: 0s - loss: 0.6834 - accuracy: 0.57 - ETA: 0s - loss: 0.6810 - accuracy: 0.57 - ETA: 0s - loss: 0.6778 - accuracy: 0.58 - ETA: 0s - loss: 0.6756 - accuracy: 0.59 - ETA: 0s - loss: 0.6732 - accuracy: 0.59 - 1s 44us/step - loss: 0.6730 - accuracy: 0.5983 - val_loss: 0.6502 - val_accuracy: 0.6982\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 40s - loss: 0.6934 - accuracy: 0.531 - ETA: 1s - loss: 0.6864 - accuracy: 0.568 - ETA: 0s - loss: 0.6818 - accuracy: 0.58 - ETA: 0s - loss: 0.6768 - accuracy: 0.59 - ETA: 0s - loss: 0.6733 - accuracy: 0.60 - ETA: 0s - loss: 0.6695 - accuracy: 0.61 - ETA: 0s - loss: 0.6667 - accuracy: 0.61 - ETA: 0s - loss: 0.6615 - accuracy: 0.63 - ETA: 0s - loss: 0.6585 - accuracy: 0.63 - 1s 46us/step - loss: 0.6579 - accuracy: 0.6367 - val_loss: 0.6247 - val_accuracy: 0.7202\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 38s - loss: 0.6937 - accuracy: 0.687 - ETA: 1s - loss: 0.6892 - accuracy: 0.527 - ETA: 0s - loss: 0.6853 - accuracy: 0.54 - ETA: 0s - loss: 0.6801 - accuracy: 0.57 - ETA: 0s - loss: 0.6746 - accuracy: 0.59 - ETA: 0s - loss: 0.6709 - accuracy: 0.60 - ETA: 0s - loss: 0.6670 - accuracy: 0.61 - ETA: 0s - loss: 0.6647 - accuracy: 0.61 - ETA: 0s - loss: 0.6611 - accuracy: 0.62 - 1s 44us/step - loss: 0.6609 - accuracy: 0.6272 - val_loss: 0.6270 - val_accuracy: 0.7060\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 40s - loss: 0.6915 - accuracy: 0.562 - ETA: 1s - loss: 0.6891 - accuracy: 0.540 - ETA: 0s - loss: 0.6826 - accuracy: 0.58 - ETA: 0s - loss: 0.6755 - accuracy: 0.60 - ETA: 0s - loss: 0.6726 - accuracy: 0.60 - ETA: 0s - loss: 0.6676 - accuracy: 0.61 - ETA: 0s - loss: 0.6650 - accuracy: 0.61 - ETA: 0s - loss: 0.6629 - accuracy: 0.62 - ETA: 0s - loss: 0.6589 - accuracy: 0.62 - 1s 44us/step - loss: 0.6590 - accuracy: 0.6282 - val_loss: 0.6242 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 39s - loss: 0.6948 - accuracy: 0.562 - ETA: 0s - loss: 0.6895 - accuracy: 0.567 - ETA: 0s - loss: 0.6842 - accuracy: 0.59 - ETA: 0s - loss: 0.6775 - accuracy: 0.61 - ETA: 0s - loss: 0.6738 - accuracy: 0.61 - ETA: 0s - loss: 0.6688 - accuracy: 0.62 - ETA: 0s - loss: 0.6650 - accuracy: 0.63 - ETA: 0s - loss: 0.6605 - accuracy: 0.64 - 1s 43us/step - loss: 0.6589 - accuracy: 0.6427 - val_loss: 0.6223 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 38s - loss: 0.6966 - accuracy: 0.437 - ETA: 1s - loss: 0.6901 - accuracy: 0.524 - ETA: 0s - loss: 0.6870 - accuracy: 0.54 - ETA: 0s - loss: 0.6848 - accuracy: 0.56 - ETA: 0s - loss: 0.6819 - accuracy: 0.57 - ETA: 0s - loss: 0.6780 - accuracy: 0.58 - ETA: 0s - loss: 0.6735 - accuracy: 0.60 - ETA: 0s - loss: 0.6698 - accuracy: 0.60 - ETA: 0s - loss: 0.6656 - accuracy: 0.61 - 1s 45us/step - loss: 0.6641 - accuracy: 0.6202 - val_loss: 0.6290 - val_accuracy: 0.7102\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 40s - loss: 0.6939 - accuracy: 0.500 - ETA: 1s - loss: 0.6872 - accuracy: 0.568 - ETA: 0s - loss: 0.6822 - accuracy: 0.58 - ETA: 0s - loss: 0.6769 - accuracy: 0.59 - ETA: 0s - loss: 0.6729 - accuracy: 0.60 - ETA: 0s - loss: 0.6681 - accuracy: 0.61 - ETA: 0s - loss: 0.6640 - accuracy: 0.62 - ETA: 0s - loss: 0.6613 - accuracy: 0.63 - ETA: 0s - loss: 0.6577 - accuracy: 0.64 - 1s 45us/step - loss: 0.6572 - accuracy: 0.6405 - val_loss: 0.6262 - val_accuracy: 0.7081\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 46s - loss: 0.6935 - accuracy: 0.437 - ETA: 1s - loss: 0.6935 - accuracy: 0.490 - ETA: 0s - loss: 0.6929 - accuracy: 0.50 - ETA: 0s - loss: 0.6904 - accuracy: 0.51 - ETA: 0s - loss: 0.6868 - accuracy: 0.52 - ETA: 0s - loss: 0.6836 - accuracy: 0.53 - ETA: 0s - loss: 0.6796 - accuracy: 0.54 - ETA: 0s - loss: 0.6765 - accuracy: 0.56 - ETA: 0s - loss: 0.6734 - accuracy: 0.56 - 1s 47us/step - loss: 0.6718 - accuracy: 0.5713 - val_loss: 0.6240 - val_accuracy: 0.6932\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 46s - loss: 0.6949 - accuracy: 0.281 - ETA: 1s - loss: 0.6935 - accuracy: 0.512 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.53 - ETA: 0s - loss: 0.6922 - accuracy: 0.54 - ETA: 0s - loss: 0.6912 - accuracy: 0.55 - ETA: 0s - loss: 0.6884 - accuracy: 0.56 - ETA: 0s - loss: 0.6860 - accuracy: 0.57 - ETA: 0s - loss: 0.6831 - accuracy: 0.58 - 1s 47us/step - loss: 0.6814 - accuracy: 0.5881 - val_loss: 0.6489 - val_accuracy: 0.6349\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 46s - loss: 0.6931 - accuracy: 0.687 - ETA: 1s - loss: 0.6916 - accuracy: 0.527 - ETA: 0s - loss: 0.6900 - accuracy: 0.53 - ETA: 0s - loss: 0.6870 - accuracy: 0.56 - ETA: 0s - loss: 0.6842 - accuracy: 0.57 - ETA: 0s - loss: 0.6799 - accuracy: 0.58 - ETA: 0s - loss: 0.6761 - accuracy: 0.59 - ETA: 0s - loss: 0.6734 - accuracy: 0.59 - ETA: 0s - loss: 0.6710 - accuracy: 0.60 - 1s 48us/step - loss: 0.6688 - accuracy: 0.6063 - val_loss: 0.6182 - val_accuracy: 0.7230\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6932 - accuracy: 0.500 - ETA: 1s - loss: 0.6933 - accuracy: 0.516 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6919 - accuracy: 0.52 - ETA: 0s - loss: 0.6906 - accuracy: 0.52 - ETA: 0s - loss: 0.6886 - accuracy: 0.53 - ETA: 0s - loss: 0.6860 - accuracy: 0.53 - ETA: 0s - loss: 0.6838 - accuracy: 0.54 - ETA: 0s - loss: 0.6816 - accuracy: 0.54 - 1s 48us/step - loss: 0.6795 - accuracy: 0.5477 - val_loss: 0.6393 - val_accuracy: 0.6847\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 46s - loss: 0.6948 - accuracy: 0.375 - ETA: 1s - loss: 0.6922 - accuracy: 0.511 - ETA: 0s - loss: 0.6897 - accuracy: 0.51 - ETA: 0s - loss: 0.6859 - accuracy: 0.53 - ETA: 0s - loss: 0.6836 - accuracy: 0.54 - ETA: 0s - loss: 0.6810 - accuracy: 0.55 - ETA: 0s - loss: 0.6787 - accuracy: 0.56 - ETA: 0s - loss: 0.6748 - accuracy: 0.57 - ETA: 0s - loss: 0.6720 - accuracy: 0.58 - 1s 47us/step - loss: 0.6703 - accuracy: 0.5872 - val_loss: 0.6271 - val_accuracy: 0.7244\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 46s - loss: 0.6918 - accuracy: 0.656 - ETA: 1s - loss: 0.6920 - accuracy: 0.522 - ETA: 0s - loss: 0.6915 - accuracy: 0.52 - ETA: 0s - loss: 0.6893 - accuracy: 0.53 - ETA: 0s - loss: 0.6860 - accuracy: 0.54 - ETA: 0s - loss: 0.6806 - accuracy: 0.55 - ETA: 0s - loss: 0.6759 - accuracy: 0.56 - ETA: 0s - loss: 0.6711 - accuracy: 0.57 - ETA: 0s - loss: 0.6676 - accuracy: 0.58 - 1s 46us/step - loss: 0.6670 - accuracy: 0.5867 - val_loss: 0.6148 - val_accuracy: 0.7067\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6910 - accuracy: 0.625 - ETA: 1s - loss: 0.6914 - accuracy: 0.550 - ETA: 0s - loss: 0.6906 - accuracy: 0.53 - ETA: 0s - loss: 0.6863 - accuracy: 0.55 - ETA: 0s - loss: 0.6807 - accuracy: 0.56 - ETA: 0s - loss: 0.6766 - accuracy: 0.58 - ETA: 0s - loss: 0.6704 - accuracy: 0.59 - ETA: 0s - loss: 0.6652 - accuracy: 0.60 - ETA: 0s - loss: 0.6614 - accuracy: 0.60 - 1s 47us/step - loss: 0.6591 - accuracy: 0.6117 - val_loss: 0.5947 - val_accuracy: 0.7202\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6948 - accuracy: 0.562 - ETA: 1s - loss: 0.6937 - accuracy: 0.554 - ETA: 0s - loss: 0.6934 - accuracy: 0.54 - ETA: 0s - loss: 0.6926 - accuracy: 0.55 - ETA: 0s - loss: 0.6908 - accuracy: 0.55 - ETA: 0s - loss: 0.6879 - accuracy: 0.55 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - ETA: 0s - loss: 0.6783 - accuracy: 0.57 - ETA: 0s - loss: 0.6744 - accuracy: 0.58 - 1s 48us/step - loss: 0.6715 - accuracy: 0.5855 - val_loss: 0.6156 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - 0s 16us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 45s - loss: 0.6947 - accuracy: 0.468 - ETA: 1s - loss: 0.6930 - accuracy: 0.539 - ETA: 0s - loss: 0.6917 - accuracy: 0.54 - ETA: 0s - loss: 0.6899 - accuracy: 0.54 - ETA: 0s - loss: 0.6855 - accuracy: 0.55 - ETA: 0s - loss: 0.6798 - accuracy: 0.57 - ETA: 0s - loss: 0.6750 - accuracy: 0.57 - ETA: 0s - loss: 0.6710 - accuracy: 0.58 - ETA: 0s - loss: 0.6668 - accuracy: 0.59 - 1s 47us/step - loss: 0.6638 - accuracy: 0.6005 - val_loss: 0.6053 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - 0s 16us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 49s - loss: 0.6944 - accuracy: 0.531 - ETA: 1s - loss: 0.6916 - accuracy: 0.541 - ETA: 0s - loss: 0.6908 - accuracy: 0.53 - ETA: 0s - loss: 0.6863 - accuracy: 0.55 - ETA: 0s - loss: 0.6803 - accuracy: 0.57 - ETA: 0s - loss: 0.6747 - accuracy: 0.58 - ETA: 0s - loss: 0.6691 - accuracy: 0.59 - ETA: 0s - loss: 0.6641 - accuracy: 0.60 - ETA: 0s - loss: 0.6600 - accuracy: 0.60 - 1s 48us/step - loss: 0.6577 - accuracy: 0.6138 - val_loss: 0.5943 - val_accuracy: 0.7202\n",
      "2814/2814 [==============================] - ETA:  - 0s 17us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6950 - accuracy: 0.375 - ETA: 1s - loss: 0.6939 - accuracy: 0.513 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6901 - accuracy: 0.53 - ETA: 0s - loss: 0.6876 - accuracy: 0.55 - ETA: 0s - loss: 0.6822 - accuracy: 0.56 - ETA: 0s - loss: 0.6769 - accuracy: 0.57 - ETA: 0s - loss: 0.6704 - accuracy: 0.58 - ETA: 0s - loss: 0.6655 - accuracy: 0.59 - 1s 47us/step - loss: 0.6635 - accuracy: 0.6009 - val_loss: 0.6004 - val_accuracy: 0.7053\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 46s - loss: 0.6933 - accuracy: 0.375 - ETA: 1s - loss: 0.6925 - accuracy: 0.524 - ETA: 0s - loss: 0.6918 - accuracy: 0.54 - ETA: 0s - loss: 0.6895 - accuracy: 0.55 - ETA: 0s - loss: 0.6863 - accuracy: 0.56 - ETA: 0s - loss: 0.6813 - accuracy: 0.58 - ETA: 0s - loss: 0.6768 - accuracy: 0.59 - ETA: 0s - loss: 0.6722 - accuracy: 0.59 - ETA: 0s - loss: 0.6666 - accuracy: 0.60 - 1s 47us/step - loss: 0.6642 - accuracy: 0.6137 - val_loss: 0.6101 - val_accuracy: 0.7081\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6919 - accuracy: 0.625 - ETA: 1s - loss: 0.6916 - accuracy: 0.541 - ETA: 0s - loss: 0.6903 - accuracy: 0.53 - ETA: 0s - loss: 0.6865 - accuracy: 0.55 - ETA: 0s - loss: 0.6810 - accuracy: 0.56 - ETA: 0s - loss: 0.6759 - accuracy: 0.57 - ETA: 0s - loss: 0.6712 - accuracy: 0.58 - ETA: 0s - loss: 0.6663 - accuracy: 0.59 - ETA: 0s - loss: 0.6619 - accuracy: 0.60 - 1s 48us/step - loss: 0.6582 - accuracy: 0.6109 - val_loss: 0.5982 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 48s - loss: 0.6909 - accuracy: 0.656 - ETA: 1s - loss: 0.6915 - accuracy: 0.525 - ETA: 0s - loss: 0.6871 - accuracy: 0.54 - ETA: 0s - loss: 0.6816 - accuracy: 0.57 - ETA: 0s - loss: 0.6755 - accuracy: 0.58 - ETA: 0s - loss: 0.6701 - accuracy: 0.59 - ETA: 0s - loss: 0.6652 - accuracy: 0.60 - ETA: 0s - loss: 0.6601 - accuracy: 0.61 - ETA: 0s - loss: 0.6555 - accuracy: 0.62 - 1s 48us/step - loss: 0.6526 - accuracy: 0.6262 - val_loss: 0.5952 - val_accuracy: 0.7230\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6969 - accuracy: 0.468 - ETA: 1s - loss: 0.6931 - accuracy: 0.541 - ETA: 0s - loss: 0.6903 - accuracy: 0.54 - ETA: 0s - loss: 0.6860 - accuracy: 0.56 - ETA: 0s - loss: 0.6825 - accuracy: 0.57 - ETA: 0s - loss: 0.6762 - accuracy: 0.58 - ETA: 0s - loss: 0.6698 - accuracy: 0.59 - ETA: 0s - loss: 0.6628 - accuracy: 0.61 - ETA: 0s - loss: 0.6565 - accuracy: 0.61 - 1s 49us/step - loss: 0.6525 - accuracy: 0.6247 - val_loss: 0.5885 - val_accuracy: 0.7251\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 48s - loss: 0.6997 - accuracy: 0.343 - ETA: 1s - loss: 0.6940 - accuracy: 0.528 - ETA: 0s - loss: 0.6893 - accuracy: 0.53 - ETA: 0s - loss: 0.6850 - accuracy: 0.55 - ETA: 0s - loss: 0.6796 - accuracy: 0.57 - ETA: 0s - loss: 0.6740 - accuracy: 0.58 - ETA: 0s - loss: 0.6687 - accuracy: 0.59 - ETA: 0s - loss: 0.6635 - accuracy: 0.60 - ETA: 0s - loss: 0.6596 - accuracy: 0.61 - 1s 48us/step - loss: 0.6580 - accuracy: 0.6165 - val_loss: 0.6086 - val_accuracy: 0.7024\n",
      "2814/2814 [==============================] - ETA:  - 0s 16us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 48s - loss: 0.6909 - accuracy: 0.562 - ETA: 1s - loss: 0.6926 - accuracy: 0.515 - ETA: 0s - loss: 0.6904 - accuracy: 0.52 - ETA: 0s - loss: 0.6864 - accuracy: 0.54 - ETA: 0s - loss: 0.6811 - accuracy: 0.56 - ETA: 0s - loss: 0.6735 - accuracy: 0.58 - ETA: 0s - loss: 0.6681 - accuracy: 0.59 - ETA: 0s - loss: 0.6628 - accuracy: 0.60 - ETA: 0s - loss: 0.6570 - accuracy: 0.61 - 1s 49us/step - loss: 0.6526 - accuracy: 0.6213 - val_loss: 0.5928 - val_accuracy: 0.7188\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6935 - accuracy: 0.562 - ETA: 1s - loss: 0.6934 - accuracy: 0.544 - ETA: 0s - loss: 0.6888 - accuracy: 0.56 - ETA: 0s - loss: 0.6843 - accuracy: 0.57 - ETA: 0s - loss: 0.6776 - accuracy: 0.59 - ETA: 0s - loss: 0.6701 - accuracy: 0.60 - ETA: 0s - loss: 0.6623 - accuracy: 0.61 - ETA: 0s - loss: 0.6563 - accuracy: 0.62 - ETA: 0s - loss: 0.6513 - accuracy: 0.63 - 1s 48us/step - loss: 0.6490 - accuracy: 0.6340 - val_loss: 0.5981 - val_accuracy: 0.7124\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 43s - loss: 0.6934 - accuracy: 0.500 - ETA: 1s - loss: 0.6931 - accuracy: 0.518 - ETA: 0s - loss: 0.6920 - accuracy: 0.53 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - ETA: 0s - loss: 0.6909 - accuracy: 0.52 - ETA: 0s - loss: 0.6902 - accuracy: 0.52 - ETA: 0s - loss: 0.6890 - accuracy: 0.53 - ETA: 0s - loss: 0.6872 - accuracy: 0.54 - ETA: 0s - loss: 0.6861 - accuracy: 0.54 - 1s 45us/step - loss: 0.6860 - accuracy: 0.5464 - val_loss: 0.6682 - val_accuracy: 0.6982\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 42s - loss: 0.6934 - accuracy: 0.625 - ETA: 1s - loss: 0.6928 - accuracy: 0.522 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6916 - accuracy: 0.52 - ETA: 0s - loss: 0.6908 - accuracy: 0.52 - ETA: 0s - loss: 0.6898 - accuracy: 0.53 - ETA: 0s - loss: 0.6892 - accuracy: 0.53 - ETA: 0s - loss: 0.6880 - accuracy: 0.53 - 1s 48us/step - loss: 0.6870 - accuracy: 0.5425 - val_loss: 0.6712 - val_accuracy: 0.7031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 40s - loss: 0.6942 - accuracy: 0.437 - ETA: 1s - loss: 0.6937 - accuracy: 0.497 - ETA: 0s - loss: 0.6929 - accuracy: 0.50 - ETA: 0s - loss: 0.6923 - accuracy: 0.50 - ETA: 0s - loss: 0.6908 - accuracy: 0.50 - ETA: 0s - loss: 0.6903 - accuracy: 0.50 - ETA: 0s - loss: 0.6890 - accuracy: 0.51 - ETA: 0s - loss: 0.6881 - accuracy: 0.51 - ETA: 0s - loss: 0.6872 - accuracy: 0.51 - 1s 45us/step - loss: 0.6867 - accuracy: 0.5220 - val_loss: 0.6712 - val_accuracy: 0.5923\n",
      "2815/2815 [==============================] - ETA:  - 0s 16us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 41s - loss: 0.6936 - accuracy: 0.500 - ETA: 1s - loss: 0.6927 - accuracy: 0.551 - ETA: 0s - loss: 0.6912 - accuracy: 0.56 - ETA: 0s - loss: 0.6900 - accuracy: 0.57 - ETA: 0s - loss: 0.6885 - accuracy: 0.57 - ETA: 0s - loss: 0.6861 - accuracy: 0.58 - ETA: 0s - loss: 0.6843 - accuracy: 0.58 - ETA: 0s - loss: 0.6818 - accuracy: 0.59 - ETA: 0s - loss: 0.6799 - accuracy: 0.59 - 1s 44us/step - loss: 0.6799 - accuracy: 0.5934 - val_loss: 0.6571 - val_accuracy: 0.7152\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 41s - loss: 0.6933 - accuracy: 0.468 - ETA: 1s - loss: 0.6924 - accuracy: 0.520 - ETA: 0s - loss: 0.6909 - accuracy: 0.52 - ETA: 0s - loss: 0.6904 - accuracy: 0.52 - ETA: 0s - loss: 0.6886 - accuracy: 0.52 - ETA: 0s - loss: 0.6869 - accuracy: 0.53 - ETA: 0s - loss: 0.6856 - accuracy: 0.53 - ETA: 0s - loss: 0.6847 - accuracy: 0.54 - ETA: 0s - loss: 0.6831 - accuracy: 0.55 - 1s 46us/step - loss: 0.6824 - accuracy: 0.5559 - val_loss: 0.6613 - val_accuracy: 0.6754\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 40s - loss: 0.6932 - accuracy: 0.562 - ETA: 1s - loss: 0.6916 - accuracy: 0.548 - ETA: 0s - loss: 0.6899 - accuracy: 0.54 - ETA: 0s - loss: 0.6898 - accuracy: 0.54 - ETA: 0s - loss: 0.6880 - accuracy: 0.55 - ETA: 0s - loss: 0.6865 - accuracy: 0.56 - ETA: 0s - loss: 0.6849 - accuracy: 0.56 - ETA: 0s - loss: 0.6831 - accuracy: 0.57 - ETA: 0s - loss: 0.6800 - accuracy: 0.57 - 1s 45us/step - loss: 0.6796 - accuracy: 0.5793 - val_loss: 0.6603 - val_accuracy: 0.6790\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 41s - loss: 0.6921 - accuracy: 0.468 - ETA: 1s - loss: 0.6932 - accuracy: 0.519 - ETA: 0s - loss: 0.6920 - accuracy: 0.53 - ETA: 0s - loss: 0.6900 - accuracy: 0.53 - ETA: 0s - loss: 0.6880 - accuracy: 0.54 - ETA: 0s - loss: 0.6858 - accuracy: 0.55 - ETA: 0s - loss: 0.6837 - accuracy: 0.56 - ETA: 0s - loss: 0.6804 - accuracy: 0.57 - ETA: 0s - loss: 0.6778 - accuracy: 0.58 - 1s 46us/step - loss: 0.6764 - accuracy: 0.5892 - val_loss: 0.6441 - val_accuracy: 0.6889\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 41s - loss: 0.6924 - accuracy: 0.562 - ETA: 1s - loss: 0.6930 - accuracy: 0.519 - ETA: 0s - loss: 0.6917 - accuracy: 0.52 - ETA: 0s - loss: 0.6897 - accuracy: 0.53 - ETA: 0s - loss: 0.6874 - accuracy: 0.55 - ETA: 0s - loss: 0.6853 - accuracy: 0.56 - ETA: 0s - loss: 0.6826 - accuracy: 0.57 - ETA: 0s - loss: 0.6804 - accuracy: 0.57 - ETA: 0s - loss: 0.6781 - accuracy: 0.58 - 1s 45us/step - loss: 0.6778 - accuracy: 0.5891 - val_loss: 0.6460 - val_accuracy: 0.7024\n",
      "2815/2815 [==============================] - ETA:  - 0s 16us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 41s - loss: 0.6955 - accuracy: 0.531 - ETA: 1s - loss: 0.6927 - accuracy: 0.527 - ETA: 0s - loss: 0.6915 - accuracy: 0.53 - ETA: 0s - loss: 0.6896 - accuracy: 0.54 - ETA: 0s - loss: 0.6871 - accuracy: 0.56 - ETA: 0s - loss: 0.6852 - accuracy: 0.56 - ETA: 0s - loss: 0.6830 - accuracy: 0.57 - ETA: 0s - loss: 0.6797 - accuracy: 0.58 - ETA: 0s - loss: 0.6767 - accuracy: 0.59 - 1s 45us/step - loss: 0.6766 - accuracy: 0.5954 - val_loss: 0.6458 - val_accuracy: 0.7131\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 41s - loss: 0.6935 - accuracy: 0.500 - ETA: 1s - loss: 0.6913 - accuracy: 0.542 - ETA: 0s - loss: 0.6915 - accuracy: 0.52 - ETA: 0s - loss: 0.6901 - accuracy: 0.53 - ETA: 0s - loss: 0.6880 - accuracy: 0.54 - ETA: 0s - loss: 0.6858 - accuracy: 0.55 - ETA: 0s - loss: 0.6838 - accuracy: 0.56 - ETA: 0s - loss: 0.6817 - accuracy: 0.56 - ETA: 0s - loss: 0.6787 - accuracy: 0.57 - 1s 47us/step - loss: 0.6778 - accuracy: 0.5819 - val_loss: 0.6465 - val_accuracy: 0.7024\n",
      "2814/2814 [==============================] - ETA:  - 0s 16us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 41s - loss: 0.6928 - accuracy: 0.531 - ETA: 1s - loss: 0.6934 - accuracy: 0.530 - ETA: 0s - loss: 0.6926 - accuracy: 0.54 - ETA: 0s - loss: 0.6917 - accuracy: 0.55 - ETA: 0s - loss: 0.6900 - accuracy: 0.56 - ETA: 0s - loss: 0.6880 - accuracy: 0.57 - ETA: 0s - loss: 0.6853 - accuracy: 0.58 - ETA: 0s - loss: 0.6827 - accuracy: 0.58 - ETA: 0s - loss: 0.6805 - accuracy: 0.59 - 1s 46us/step - loss: 0.6797 - accuracy: 0.5961 - val_loss: 0.6514 - val_accuracy: 0.7138\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 42s - loss: 0.6944 - accuracy: 0.500 - ETA: 1s - loss: 0.6927 - accuracy: 0.516 - ETA: 0s - loss: 0.6916 - accuracy: 0.52 - ETA: 0s - loss: 0.6908 - accuracy: 0.52 - ETA: 0s - loss: 0.6893 - accuracy: 0.54 - ETA: 0s - loss: 0.6876 - accuracy: 0.54 - ETA: 0s - loss: 0.6857 - accuracy: 0.55 - ETA: 0s - loss: 0.6833 - accuracy: 0.56 - ETA: 0s - loss: 0.6807 - accuracy: 0.57 - 1s 45us/step - loss: 0.6807 - accuracy: 0.5758 - val_loss: 0.6587 - val_accuracy: 0.6982\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 42s - loss: 0.6979 - accuracy: 0.437 - ETA: 1s - loss: 0.6917 - accuracy: 0.518 - ETA: 0s - loss: 0.6899 - accuracy: 0.52 - ETA: 0s - loss: 0.6882 - accuracy: 0.53 - ETA: 0s - loss: 0.6861 - accuracy: 0.55 - ETA: 0s - loss: 0.6828 - accuracy: 0.56 - ETA: 0s - loss: 0.6792 - accuracy: 0.58 - ETA: 0s - loss: 0.6754 - accuracy: 0.59 - ETA: 0s - loss: 0.6718 - accuracy: 0.60 - 1s 46us/step - loss: 0.6703 - accuracy: 0.6079 - val_loss: 0.6305 - val_accuracy: 0.7010\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 43s - loss: 0.6951 - accuracy: 0.500 - ETA: 1s - loss: 0.6935 - accuracy: 0.537 - ETA: 0s - loss: 0.6916 - accuracy: 0.54 - ETA: 0s - loss: 0.6894 - accuracy: 0.56 - ETA: 0s - loss: 0.6876 - accuracy: 0.57 - ETA: 0s - loss: 0.6845 - accuracy: 0.58 - ETA: 0s - loss: 0.6815 - accuracy: 0.59 - ETA: 0s - loss: 0.6771 - accuracy: 0.60 - ETA: 0s - loss: 0.6732 - accuracy: 0.61 - 1s 47us/step - loss: 0.6714 - accuracy: 0.6200 - val_loss: 0.6295 - val_accuracy: 0.7074\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 41s - loss: 0.6946 - accuracy: 0.468 - ETA: 1s - loss: 0.6924 - accuracy: 0.531 - ETA: 0s - loss: 0.6901 - accuracy: 0.55 - ETA: 0s - loss: 0.6872 - accuracy: 0.56 - ETA: 0s - loss: 0.6851 - accuracy: 0.56 - ETA: 0s - loss: 0.6824 - accuracy: 0.57 - ETA: 0s - loss: 0.6799 - accuracy: 0.58 - ETA: 0s - loss: 0.6776 - accuracy: 0.58 - ETA: 0s - loss: 0.6747 - accuracy: 0.59 - 1s 46us/step - loss: 0.6737 - accuracy: 0.5981 - val_loss: 0.6395 - val_accuracy: 0.7109\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 41s - loss: 0.6972 - accuracy: 0.437 - ETA: 1s - loss: 0.6930 - accuracy: 0.519 - ETA: 0s - loss: 0.6911 - accuracy: 0.54 - ETA: 0s - loss: 0.6888 - accuracy: 0.55 - ETA: 0s - loss: 0.6866 - accuracy: 0.56 - ETA: 0s - loss: 0.6831 - accuracy: 0.58 - ETA: 0s - loss: 0.6790 - accuracy: 0.59 - ETA: 0s - loss: 0.6747 - accuracy: 0.60 - ETA: 0s - loss: 0.6704 - accuracy: 0.61 - 1s 46us/step - loss: 0.6690 - accuracy: 0.6170 - val_loss: 0.6270 - val_accuracy: 0.7031\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 41s - loss: 0.6865 - accuracy: 0.625 - ETA: 1s - loss: 0.6919 - accuracy: 0.551 - ETA: 0s - loss: 0.6886 - accuracy: 0.58 - ETA: 0s - loss: 0.6862 - accuracy: 0.59 - ETA: 0s - loss: 0.6834 - accuracy: 0.60 - ETA: 0s - loss: 0.6795 - accuracy: 0.60 - ETA: 0s - loss: 0.6749 - accuracy: 0.61 - ETA: 0s - loss: 0.6704 - accuracy: 0.62 - ETA: 0s - loss: 0.6667 - accuracy: 0.62 - 1s 45us/step - loss: 0.6660 - accuracy: 0.6297 - val_loss: 0.6222 - val_accuracy: 0.7109\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 40s - loss: 0.6943 - accuracy: 0.531 - ETA: 1s - loss: 0.6926 - accuracy: 0.531 - ETA: 0s - loss: 0.6905 - accuracy: 0.53 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6839 - accuracy: 0.56 - ETA: 0s - loss: 0.6810 - accuracy: 0.57 - ETA: 0s - loss: 0.6784 - accuracy: 0.58 - ETA: 0s - loss: 0.6743 - accuracy: 0.59 - ETA: 0s - loss: 0.6704 - accuracy: 0.60 - 1s 46us/step - loss: 0.6683 - accuracy: 0.6133 - val_loss: 0.6342 - val_accuracy: 0.7095\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 34s - loss: 0.6940 - accuracy: 0.531 - ETA: 0s - loss: 0.6938 - accuracy: 0.493 - ETA: 0s - loss: 0.6938 - accuracy: 0.49 - ETA: 0s - loss: 0.6938 - accuracy: 0.49 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - 1s 42us/step - loss: 0.6935 - accuracy: 0.5086 - val_loss: 0.6927 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 36s - loss: 0.6923 - accuracy: 0.625 - ETA: 0s - loss: 0.6934 - accuracy: 0.529 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - 1s 43us/step - loss: 0.6929 - accuracy: 0.5214 - val_loss: 0.6919 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 35s - loss: 0.6943 - accuracy: 0.468 - ETA: 0s - loss: 0.6936 - accuracy: 0.503 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - 1s 42us/step - loss: 0.6934 - accuracy: 0.5126 - val_loss: 0.6927 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 36s - loss: 0.6946 - accuracy: 0.406 - ETA: 0s - loss: 0.6935 - accuracy: 0.509 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - 1s 42us/step - loss: 0.6930 - accuracy: 0.5179 - val_loss: 0.6922 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 35s - loss: 0.6951 - accuracy: 0.468 - ETA: 0s - loss: 0.6943 - accuracy: 0.476 - ETA: 0s - loss: 0.6940 - accuracy: 0.48 - ETA: 0s - loss: 0.6940 - accuracy: 0.48 - ETA: 0s - loss: 0.6941 - accuracy: 0.48 - ETA: 0s - loss: 0.6940 - accuracy: 0.48 - ETA: 0s - loss: 0.6939 - accuracy: 0.49 - ETA: 0s - loss: 0.6937 - accuracy: 0.49 - 1s 42us/step - loss: 0.6937 - accuracy: 0.5002 - val_loss: 0.6928 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 36s - loss: 0.6943 - accuracy: 0.281 - ETA: 0s - loss: 0.6939 - accuracy: 0.473 - ETA: 0s - loss: 0.6939 - accuracy: 0.47 - ETA: 0s - loss: 0.6938 - accuracy: 0.47 - ETA: 0s - loss: 0.6938 - accuracy: 0.48 - ETA: 0s - loss: 0.6938 - accuracy: 0.49 - ETA: 0s - loss: 0.6938 - accuracy: 0.49 - ETA: 0s - loss: 0.6937 - accuracy: 0.49 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - 1s 45us/step - loss: 0.6937 - accuracy: 0.5006 - val_loss: 0.6936 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 36s - loss: 0.6955 - accuracy: 0.375 - ETA: 0s - loss: 0.6941 - accuracy: 0.491 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - 1s 42us/step - loss: 0.6936 - accuracy: 0.5148 - val_loss: 0.6926 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 35s - loss: 0.6945 - accuracy: 0.531 - ETA: 0s - loss: 0.6942 - accuracy: 0.516 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - 1s 43us/step - loss: 0.6941 - accuracy: 0.5136 - val_loss: 0.6934 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 36s - loss: 0.6960 - accuracy: 0.468 - ETA: 0s - loss: 0.6946 - accuracy: 0.503 - ETA: 0s - loss: 0.6945 - accuracy: 0.49 - ETA: 0s - loss: 0.6944 - accuracy: 0.49 - ETA: 0s - loss: 0.6945 - accuracy: 0.49 - ETA: 0s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - 1s 44us/step - loss: 0.6942 - accuracy: 0.5058 - val_loss: 0.6934 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 37s - loss: 0.6945 - accuracy: 0.406 - ETA: 0s - loss: 0.6938 - accuracy: 0.530 - ETA: 0s - loss: 0.6939 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - 1s 42us/step - loss: 0.6938 - accuracy: 0.5151 - val_loss: 0.6932 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 37s - loss: 0.6955 - accuracy: 0.437 - ETA: 0s - loss: 0.6941 - accuracy: 0.515 - ETA: 0s - loss: 0.6941 - accuracy: 0.52 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.52 - ETA: 0s - loss: 0.6940 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - 1s 43us/step - loss: 0.6938 - accuracy: 0.5189 - val_loss: 0.6928 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 37s - loss: 0.6937 - accuracy: 0.687 - ETA: 0s - loss: 0.6940 - accuracy: 0.522 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - 1s 42us/step - loss: 0.6941 - accuracy: 0.5104 - val_loss: 0.6936 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 36s - loss: 0.6928 - accuracy: 0.500 - ETA: 0s - loss: 0.6948 - accuracy: 0.515 - ETA: 0s - loss: 0.6945 - accuracy: 0.52 - ETA: 0s - loss: 0.6946 - accuracy: 0.52 - ETA: 0s - loss: 0.6944 - accuracy: 0.52 - ETA: 0s - loss: 0.6944 - accuracy: 0.52 - ETA: 0s - loss: 0.6943 - accuracy: 0.52 - ETA: 0s - loss: 0.6942 - accuracy: 0.52 - 1s 43us/step - loss: 0.6942 - accuracy: 0.5212 - val_loss: 0.6931 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 36s - loss: 0.6956 - accuracy: 0.437 - ETA: 0s - loss: 0.6952 - accuracy: 0.504 - ETA: 0s - loss: 0.6947 - accuracy: 0.51 - ETA: 0s - loss: 0.6947 - accuracy: 0.51 - ETA: 0s - loss: 0.6949 - accuracy: 0.51 - ETA: 0s - loss: 0.6947 - accuracy: 0.51 - ETA: 0s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - 1s 43us/step - loss: 0.6943 - accuracy: 0.5152 - val_loss: 0.6931 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 36s - loss: 0.6951 - accuracy: 0.531 - ETA: 0s - loss: 0.6948 - accuracy: 0.513 - ETA: 0s - loss: 0.6948 - accuracy: 0.51 - ETA: 0s - loss: 0.6947 - accuracy: 0.50 - ETA: 0s - loss: 0.6946 - accuracy: 0.50 - ETA: 0s - loss: 0.6945 - accuracy: 0.50 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - 1s 43us/step - loss: 0.6939 - accuracy: 0.5150 - val_loss: 0.6921 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 35s - loss: 0.6884 - accuracy: 0.687 - ETA: 0s - loss: 0.6947 - accuracy: 0.508 - ETA: 0s - loss: 0.6949 - accuracy: 0.50 - ETA: 0s - loss: 0.6950 - accuracy: 0.50 - ETA: 0s - loss: 0.6949 - accuracy: 0.50 - ETA: 0s - loss: 0.6947 - accuracy: 0.50 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - 1s 42us/step - loss: 0.6941 - accuracy: 0.5137 - val_loss: 0.6926 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 39s - loss: 0.6923 - accuracy: 0.500 - ETA: 0s - loss: 0.6942 - accuracy: 0.540 - ETA: 0s - loss: 0.6939 - accuracy: 0.53 - ETA: 0s - loss: 0.6938 - accuracy: 0.54 - ETA: 0s - loss: 0.6939 - accuracy: 0.53 - ETA: 0s - loss: 0.6939 - accuracy: 0.53 - ETA: 0s - loss: 0.6938 - accuracy: 0.53 - ETA: 0s - loss: 0.6936 - accuracy: 0.53 - ETA: 0s - loss: 0.6936 - accuracy: 0.53 - 1s 44us/step - loss: 0.6936 - accuracy: 0.5353 - val_loss: 0.6922 - val_accuracy: 0.5497\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 36s - loss: 0.6949 - accuracy: 0.406 - ETA: 0s - loss: 0.6947 - accuracy: 0.511 - ETA: 0s - loss: 0.6948 - accuracy: 0.50 - ETA: 0s - loss: 0.6947 - accuracy: 0.51 - ETA: 0s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6946 - accuracy: 0.51 - ETA: 0s - loss: 0.6946 - accuracy: 0.51 - 1s 44us/step - loss: 0.6946 - accuracy: 0.5109 - val_loss: 0.6944 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 48s - loss: 0.6927 - accuracy: 0.562 - ETA: 1s - loss: 0.6924 - accuracy: 0.548 - ETA: 0s - loss: 0.6910 - accuracy: 0.55 - ETA: 0s - loss: 0.6896 - accuracy: 0.56 - ETA: 0s - loss: 0.6874 - accuracy: 0.56 - ETA: 0s - loss: 0.6851 - accuracy: 0.57 - ETA: 0s - loss: 0.6832 - accuracy: 0.57 - ETA: 0s - loss: 0.6801 - accuracy: 0.58 - ETA: 0s - loss: 0.6782 - accuracy: 0.58 - 1s 49us/step - loss: 0.6768 - accuracy: 0.5927 - val_loss: 0.6461 - val_accuracy: 0.7031\n",
      "2815/2815 [==============================] - ETA:  - 0s 16us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 48s - loss: 0.6944 - accuracy: 0.437 - ETA: 1s - loss: 0.6935 - accuracy: 0.501 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6921 - accuracy: 0.51 - ETA: 0s - loss: 0.6914 - accuracy: 0.51 - ETA: 0s - loss: 0.6901 - accuracy: 0.51 - ETA: 0s - loss: 0.6891 - accuracy: 0.51 - ETA: 0s - loss: 0.6886 - accuracy: 0.51 - ETA: 0s - loss: 0.6871 - accuracy: 0.52 - 1s 47us/step - loss: 0.6864 - accuracy: 0.5253 - val_loss: 0.6695 - val_accuracy: 0.5724\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6930 - accuracy: 0.593 - ETA: 1s - loss: 0.6930 - accuracy: 0.502 - ETA: 0s - loss: 0.6922 - accuracy: 0.52 - ETA: 0s - loss: 0.6912 - accuracy: 0.53 - ETA: 0s - loss: 0.6900 - accuracy: 0.54 - ETA: 0s - loss: 0.6888 - accuracy: 0.55 - ETA: 0s - loss: 0.6867 - accuracy: 0.56 - ETA: 0s - loss: 0.6846 - accuracy: 0.56 - ETA: 0s - loss: 0.6823 - accuracy: 0.57 - 1s 48us/step - loss: 0.6810 - accuracy: 0.5737 - val_loss: 0.6526 - val_accuracy: 0.7188\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 46s - loss: 0.6940 - accuracy: 0.468 - ETA: 1s - loss: 0.6936 - accuracy: 0.511 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.52 - ETA: 0s - loss: 0.6921 - accuracy: 0.52 - ETA: 0s - loss: 0.6908 - accuracy: 0.53 - ETA: 0s - loss: 0.6886 - accuracy: 0.54 - ETA: 0s - loss: 0.6866 - accuracy: 0.55 - ETA: 0s - loss: 0.6842 - accuracy: 0.56 - 1s 47us/step - loss: 0.6825 - accuracy: 0.5701 - val_loss: 0.6519 - val_accuracy: 0.7131\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 48s - loss: 0.6939 - accuracy: 0.406 - ETA: 1s - loss: 0.6935 - accuracy: 0.502 - ETA: 0s - loss: 0.6918 - accuracy: 0.52 - ETA: 0s - loss: 0.6911 - accuracy: 0.52 - ETA: 0s - loss: 0.6902 - accuracy: 0.53 - ETA: 0s - loss: 0.6889 - accuracy: 0.53 - ETA: 0s - loss: 0.6865 - accuracy: 0.55 - ETA: 0s - loss: 0.6837 - accuracy: 0.56 - ETA: 0s - loss: 0.6814 - accuracy: 0.56 - 1s 48us/step - loss: 0.6792 - accuracy: 0.5710 - val_loss: 0.6460 - val_accuracy: 0.7138\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 49s - loss: 0.6939 - accuracy: 0.406 - ETA: 1s - loss: 0.6935 - accuracy: 0.518 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - ETA: 0s - loss: 0.6910 - accuracy: 0.52 - ETA: 0s - loss: 0.6896 - accuracy: 0.53 - ETA: 0s - loss: 0.6890 - accuracy: 0.53 - ETA: 0s - loss: 0.6876 - accuracy: 0.53 - ETA: 0s - loss: 0.6866 - accuracy: 0.54 - 1s 49us/step - loss: 0.6857 - accuracy: 0.5414 - val_loss: 0.6685 - val_accuracy: 0.6989\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6962 - accuracy: 0.468 - ETA: 1s - loss: 0.6942 - accuracy: 0.498 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6911 - accuracy: 0.52 - ETA: 0s - loss: 0.6899 - accuracy: 0.53 - ETA: 0s - loss: 0.6872 - accuracy: 0.55 - ETA: 0s - loss: 0.6840 - accuracy: 0.56 - ETA: 0s - loss: 0.6805 - accuracy: 0.57 - ETA: 0s - loss: 0.6774 - accuracy: 0.58 - 1s 48us/step - loss: 0.6747 - accuracy: 0.5895 - val_loss: 0.6310 - val_accuracy: 0.7031\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 46s - loss: 0.6917 - accuracy: 0.625 - ETA: 1s - loss: 0.6940 - accuracy: 0.509 - ETA: 0s - loss: 0.6932 - accuracy: 0.52 - ETA: 0s - loss: 0.6920 - accuracy: 0.52 - ETA: 0s - loss: 0.6902 - accuracy: 0.54 - ETA: 0s - loss: 0.6878 - accuracy: 0.55 - ETA: 0s - loss: 0.6855 - accuracy: 0.55 - ETA: 0s - loss: 0.6828 - accuracy: 0.56 - ETA: 0s - loss: 0.6794 - accuracy: 0.57 - 1s 48us/step - loss: 0.6773 - accuracy: 0.5756 - val_loss: 0.6391 - val_accuracy: 0.7102\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 48s - loss: 0.6950 - accuracy: 0.375 - ETA: 1s - loss: 0.6936 - accuracy: 0.514 - ETA: 0s - loss: 0.6920 - accuracy: 0.53 - ETA: 0s - loss: 0.6905 - accuracy: 0.54 - ETA: 0s - loss: 0.6872 - accuracy: 0.55 - ETA: 0s - loss: 0.6837 - accuracy: 0.57 - ETA: 0s - loss: 0.6789 - accuracy: 0.58 - ETA: 0s - loss: 0.6753 - accuracy: 0.59 - ETA: 0s - loss: 0.6715 - accuracy: 0.60 - 1s 48us/step - loss: 0.6695 - accuracy: 0.6031 - val_loss: 0.6242 - val_accuracy: 0.7081\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6908 - accuracy: 0.625 - ETA: 1s - loss: 0.6920 - accuracy: 0.558 - ETA: 0s - loss: 0.6909 - accuracy: 0.55 - ETA: 0s - loss: 0.6892 - accuracy: 0.56 - ETA: 0s - loss: 0.6865 - accuracy: 0.57 - ETA: 0s - loss: 0.6830 - accuracy: 0.58 - ETA: 0s - loss: 0.6792 - accuracy: 0.59 - ETA: 0s - loss: 0.6743 - accuracy: 0.60 - ETA: 0s - loss: 0.6708 - accuracy: 0.60 - 1s 48us/step - loss: 0.6678 - accuracy: 0.6133 - val_loss: 0.6201 - val_accuracy: 0.7102\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 46s - loss: 0.6954 - accuracy: 0.406 - ETA: 1s - loss: 0.6943 - accuracy: 0.461 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6919 - accuracy: 0.53 - ETA: 0s - loss: 0.6901 - accuracy: 0.55 - ETA: 0s - loss: 0.6875 - accuracy: 0.57 - ETA: 0s - loss: 0.6845 - accuracy: 0.57 - ETA: 0s - loss: 0.6816 - accuracy: 0.58 - 1s 48us/step - loss: 0.6789 - accuracy: 0.5919 - val_loss: 0.6390 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6960 - accuracy: 0.312 - ETA: 1s - loss: 0.6933 - accuracy: 0.506 - ETA: 0s - loss: 0.6930 - accuracy: 0.50 - ETA: 0s - loss: 0.6918 - accuracy: 0.51 - ETA: 0s - loss: 0.6902 - accuracy: 0.53 - ETA: 0s - loss: 0.6878 - accuracy: 0.54 - ETA: 0s - loss: 0.6851 - accuracy: 0.55 - ETA: 0s - loss: 0.6817 - accuracy: 0.56 - ETA: 0s - loss: 0.6788 - accuracy: 0.57 - 1s 48us/step - loss: 0.6761 - accuracy: 0.5825 - val_loss: 0.6392 - val_accuracy: 0.7102\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 48s - loss: 0.6946 - accuracy: 0.468 - ETA: 1s - loss: 0.6926 - accuracy: 0.530 - ETA: 0s - loss: 0.6912 - accuracy: 0.52 - ETA: 0s - loss: 0.6884 - accuracy: 0.53 - ETA: 0s - loss: 0.6863 - accuracy: 0.54 - ETA: 0s - loss: 0.6833 - accuracy: 0.55 - ETA: 0s - loss: 0.6794 - accuracy: 0.57 - ETA: 0s - loss: 0.6754 - accuracy: 0.58 - ETA: 0s - loss: 0.6712 - accuracy: 0.59 - 1s 48us/step - loss: 0.6684 - accuracy: 0.6010 - val_loss: 0.6200 - val_accuracy: 0.7124\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6951 - accuracy: 0.468 - ETA: 1s - loss: 0.6934 - accuracy: 0.544 - ETA: 0s - loss: 0.6908 - accuracy: 0.56 - ETA: 0s - loss: 0.6889 - accuracy: 0.57 - ETA: 0s - loss: 0.6862 - accuracy: 0.57 - ETA: 0s - loss: 0.6823 - accuracy: 0.58 - ETA: 0s - loss: 0.6781 - accuracy: 0.60 - ETA: 0s - loss: 0.6725 - accuracy: 0.61 - ETA: 0s - loss: 0.6674 - accuracy: 0.61 - 1s 47us/step - loss: 0.6657 - accuracy: 0.6214 - val_loss: 0.6170 - val_accuracy: 0.7081\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 46s - loss: 0.6929 - accuracy: 0.625 - ETA: 1s - loss: 0.6939 - accuracy: 0.482 - ETA: 0s - loss: 0.6931 - accuracy: 0.49 - ETA: 0s - loss: 0.6905 - accuracy: 0.50 - ETA: 0s - loss: 0.6876 - accuracy: 0.51 - ETA: 0s - loss: 0.6844 - accuracy: 0.53 - ETA: 0s - loss: 0.6809 - accuracy: 0.54 - ETA: 0s - loss: 0.6779 - accuracy: 0.56 - ETA: 0s - loss: 0.6745 - accuracy: 0.57 - 1s 48us/step - loss: 0.6717 - accuracy: 0.5793 - val_loss: 0.6315 - val_accuracy: 0.6903\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 50s - loss: 0.6983 - accuracy: 0.437 - ETA: 1s - loss: 0.6941 - accuracy: 0.498 - ETA: 0s - loss: 0.6923 - accuracy: 0.51 - ETA: 0s - loss: 0.6905 - accuracy: 0.51 - ETA: 0s - loss: 0.6875 - accuracy: 0.51 - ETA: 0s - loss: 0.6852 - accuracy: 0.52 - ETA: 0s - loss: 0.6809 - accuracy: 0.53 - ETA: 0s - loss: 0.6789 - accuracy: 0.55 - ETA: 0s - loss: 0.6748 - accuracy: 0.56 - 1s 49us/step - loss: 0.6718 - accuracy: 0.5796 - val_loss: 0.6363 - val_accuracy: 0.6932\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6943 - accuracy: 0.593 - ETA: 1s - loss: 0.6916 - accuracy: 0.546 - ETA: 0s - loss: 0.6911 - accuracy: 0.54 - ETA: 0s - loss: 0.6886 - accuracy: 0.56 - ETA: 0s - loss: 0.6858 - accuracy: 0.57 - ETA: 0s - loss: 0.6816 - accuracy: 0.58 - ETA: 0s - loss: 0.6774 - accuracy: 0.59 - ETA: 0s - loss: 0.6724 - accuracy: 0.61 - ETA: 0s - loss: 0.6665 - accuracy: 0.61 - 1s 48us/step - loss: 0.6636 - accuracy: 0.6231 - val_loss: 0.6075 - val_accuracy: 0.7017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6939 - accuracy: 0.656 - ETA: 1s - loss: 0.6941 - accuracy: 0.506 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6919 - accuracy: 0.54 - ETA: 0s - loss: 0.6899 - accuracy: 0.56 - ETA: 0s - loss: 0.6872 - accuracy: 0.57 - ETA: 0s - loss: 0.6839 - accuracy: 0.58 - ETA: 0s - loss: 0.6805 - accuracy: 0.58 - ETA: 0s - loss: 0.6768 - accuracy: 0.59 - 1s 48us/step - loss: 0.6736 - accuracy: 0.6015 - val_loss: 0.6371 - val_accuracy: 0.6989\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 38s - loss: 0.6936 - accuracy: 0.531 - ETA: 1s - loss: 0.6929 - accuracy: 0.500 - ETA: 0s - loss: 0.6918 - accuracy: 0.50 - ETA: 0s - loss: 0.6902 - accuracy: 0.51 - ETA: 0s - loss: 0.6887 - accuracy: 0.52 - ETA: 0s - loss: 0.6881 - accuracy: 0.52 - ETA: 0s - loss: 0.6870 - accuracy: 0.53 - ETA: 0s - loss: 0.6857 - accuracy: 0.54 - ETA: 0s - loss: 0.6844 - accuracy: 0.54 - 1s 44us/step - loss: 0.6843 - accuracy: 0.5456 - val_loss: 0.6700 - val_accuracy: 0.6868\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 39s - loss: 0.6940 - accuracy: 0.406 - ETA: 1s - loss: 0.6924 - accuracy: 0.510 - ETA: 0s - loss: 0.6913 - accuracy: 0.50 - ETA: 0s - loss: 0.6903 - accuracy: 0.50 - ETA: 0s - loss: 0.6895 - accuracy: 0.50 - ETA: 0s - loss: 0.6885 - accuracy: 0.51 - ETA: 0s - loss: 0.6871 - accuracy: 0.51 - ETA: 0s - loss: 0.6862 - accuracy: 0.52 - ETA: 0s - loss: 0.6853 - accuracy: 0.52 - 1s 44us/step - loss: 0.6853 - accuracy: 0.5240 - val_loss: 0.6737 - val_accuracy: 0.5966\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 37s - loss: 0.6935 - accuracy: 0.500 - ETA: 0s - loss: 0.6919 - accuracy: 0.506 - ETA: 0s - loss: 0.6902 - accuracy: 0.51 - ETA: 0s - loss: 0.6859 - accuracy: 0.54 - ETA: 0s - loss: 0.6831 - accuracy: 0.54 - ETA: 0s - loss: 0.6811 - accuracy: 0.55 - ETA: 0s - loss: 0.6787 - accuracy: 0.56 - ETA: 0s - loss: 0.6769 - accuracy: 0.56 - ETA: 0s - loss: 0.6749 - accuracy: 0.57 - 1s 44us/step - loss: 0.6748 - accuracy: 0.5760 - val_loss: 0.6535 - val_accuracy: 0.7067\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 38s - loss: 0.6924 - accuracy: 0.531 - ETA: 1s - loss: 0.6908 - accuracy: 0.518 - ETA: 0s - loss: 0.6881 - accuracy: 0.53 - ETA: 0s - loss: 0.6858 - accuracy: 0.55 - ETA: 0s - loss: 0.6834 - accuracy: 0.55 - ETA: 0s - loss: 0.6821 - accuracy: 0.56 - ETA: 0s - loss: 0.6808 - accuracy: 0.57 - ETA: 0s - loss: 0.6788 - accuracy: 0.57 - 1s 43us/step - loss: 0.6767 - accuracy: 0.5803 - val_loss: 0.6553 - val_accuracy: 0.7010\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 37s - loss: 0.6934 - accuracy: 0.406 - ETA: 1s - loss: 0.6904 - accuracy: 0.533 - ETA: 0s - loss: 0.6889 - accuracy: 0.52 - ETA: 0s - loss: 0.6882 - accuracy: 0.52 - ETA: 0s - loss: 0.6879 - accuracy: 0.52 - ETA: 0s - loss: 0.6872 - accuracy: 0.52 - ETA: 0s - loss: 0.6862 - accuracy: 0.53 - ETA: 0s - loss: 0.6856 - accuracy: 0.53 - ETA: 0s - loss: 0.6850 - accuracy: 0.53 - 1s 44us/step - loss: 0.6849 - accuracy: 0.5348 - val_loss: 0.6748 - val_accuracy: 0.6406\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 39s - loss: 0.6926 - accuracy: 0.468 - ETA: 0s - loss: 0.6874 - accuracy: 0.551 - ETA: 0s - loss: 0.6869 - accuracy: 0.55 - ETA: 0s - loss: 0.6855 - accuracy: 0.55 - ETA: 0s - loss: 0.6843 - accuracy: 0.55 - ETA: 0s - loss: 0.6835 - accuracy: 0.56 - ETA: 0s - loss: 0.6812 - accuracy: 0.57 - ETA: 0s - loss: 0.6798 - accuracy: 0.57 - 1s 43us/step - loss: 0.6793 - accuracy: 0.5807 - val_loss: 0.6650 - val_accuracy: 0.6996\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 41s - loss: 0.6927 - accuracy: 0.562 - ETA: 1s - loss: 0.6876 - accuracy: 0.527 - ETA: 0s - loss: 0.6859 - accuracy: 0.54 - ETA: 0s - loss: 0.6846 - accuracy: 0.54 - ETA: 0s - loss: 0.6821 - accuracy: 0.55 - ETA: 0s - loss: 0.6813 - accuracy: 0.56 - ETA: 0s - loss: 0.6792 - accuracy: 0.57 - ETA: 0s - loss: 0.6775 - accuracy: 0.57 - ETA: 0s - loss: 0.6750 - accuracy: 0.58 - 1s 44us/step - loss: 0.6750 - accuracy: 0.5846 - val_loss: 0.6526 - val_accuracy: 0.6697\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 37s - loss: 0.6937 - accuracy: 0.468 - ETA: 0s - loss: 0.6913 - accuracy: 0.533 - ETA: 0s - loss: 0.6878 - accuracy: 0.56 - ETA: 0s - loss: 0.6846 - accuracy: 0.57 - ETA: 0s - loss: 0.6813 - accuracy: 0.58 - ETA: 0s - loss: 0.6782 - accuracy: 0.59 - ETA: 0s - loss: 0.6751 - accuracy: 0.60 - ETA: 0s - loss: 0.6729 - accuracy: 0.60 - ETA: 0s - loss: 0.6705 - accuracy: 0.61 - 1s 44us/step - loss: 0.6702 - accuracy: 0.6168 - val_loss: 0.6421 - val_accuracy: 0.7088\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 40s - loss: 0.6945 - accuracy: 0.437 - ETA: 1s - loss: 0.6923 - accuracy: 0.533 - ETA: 0s - loss: 0.6896 - accuracy: 0.55 - ETA: 0s - loss: 0.6859 - accuracy: 0.57 - ETA: 0s - loss: 0.6817 - accuracy: 0.59 - ETA: 0s - loss: 0.6779 - accuracy: 0.60 - ETA: 0s - loss: 0.6749 - accuracy: 0.60 - ETA: 0s - loss: 0.6718 - accuracy: 0.61 - ETA: 0s - loss: 0.6686 - accuracy: 0.61 - 1s 45us/step - loss: 0.6683 - accuracy: 0.6192 - val_loss: 0.6358 - val_accuracy: 0.7081\n",
      "2815/2815 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 40s - loss: 0.6940 - accuracy: 0.593 - ETA: 1s - loss: 0.6912 - accuracy: 0.544 - ETA: 0s - loss: 0.6863 - accuracy: 0.56 - ETA: 0s - loss: 0.6842 - accuracy: 0.57 - ETA: 0s - loss: 0.6808 - accuracy: 0.59 - ETA: 0s - loss: 0.6781 - accuracy: 0.59 - ETA: 0s - loss: 0.6749 - accuracy: 0.60 - ETA: 0s - loss: 0.6721 - accuracy: 0.61 - ETA: 0s - loss: 0.6697 - accuracy: 0.61 - 1s 47us/step - loss: 0.6681 - accuracy: 0.6219 - val_loss: 0.6382 - val_accuracy: 0.7138\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 43s - loss: 0.6934 - accuracy: 0.656 - ETA: 1s - loss: 0.6905 - accuracy: 0.533 - ETA: 0s - loss: 0.6883 - accuracy: 0.55 - ETA: 0s - loss: 0.6856 - accuracy: 0.56 - ETA: 0s - loss: 0.6816 - accuracy: 0.57 - ETA: 0s - loss: 0.6798 - accuracy: 0.58 - ETA: 0s - loss: 0.6776 - accuracy: 0.59 - ETA: 0s - loss: 0.6746 - accuracy: 0.59 - 1s 44us/step - loss: 0.6717 - accuracy: 0.6067 - val_loss: 0.6442 - val_accuracy: 0.7003\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 40s - loss: 0.6962 - accuracy: 0.312 - ETA: 1s - loss: 0.6907 - accuracy: 0.527 - ETA: 0s - loss: 0.6895 - accuracy: 0.53 - ETA: 0s - loss: 0.6874 - accuracy: 0.55 - ETA: 0s - loss: 0.6854 - accuracy: 0.56 - ETA: 0s - loss: 0.6829 - accuracy: 0.57 - ETA: 0s - loss: 0.6799 - accuracy: 0.58 - ETA: 0s - loss: 0.6770 - accuracy: 0.58 - 1s 43us/step - loss: 0.6749 - accuracy: 0.5904 - val_loss: 0.6528 - val_accuracy: 0.7045\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 40s - loss: 0.6960 - accuracy: 0.437 - ETA: 1s - loss: 0.6906 - accuracy: 0.540 - ETA: 0s - loss: 0.6864 - accuracy: 0.57 - ETA: 0s - loss: 0.6820 - accuracy: 0.59 - ETA: 0s - loss: 0.6777 - accuracy: 0.60 - ETA: 0s - loss: 0.6725 - accuracy: 0.61 - ETA: 0s - loss: 0.6690 - accuracy: 0.62 - ETA: 0s - loss: 0.6663 - accuracy: 0.62 - 1s 44us/step - loss: 0.6637 - accuracy: 0.6272 - val_loss: 0.6312 - val_accuracy: 0.7116\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 39s - loss: 0.6953 - accuracy: 0.500 - ETA: 1s - loss: 0.6867 - accuracy: 0.584 - ETA: 0s - loss: 0.6809 - accuracy: 0.60 - ETA: 0s - loss: 0.6752 - accuracy: 0.61 - ETA: 0s - loss: 0.6703 - accuracy: 0.62 - ETA: 0s - loss: 0.6668 - accuracy: 0.62 - ETA: 0s - loss: 0.6633 - accuracy: 0.63 - ETA: 0s - loss: 0.6601 - accuracy: 0.63 - ETA: 0s - loss: 0.6558 - accuracy: 0.64 - 1s 45us/step - loss: 0.6549 - accuracy: 0.6458 - val_loss: 0.6171 - val_accuracy: 0.7124\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 39s - loss: 0.6921 - accuracy: 0.593 - ETA: 1s - loss: 0.6920 - accuracy: 0.526 - ETA: 0s - loss: 0.6877 - accuracy: 0.55 - ETA: 0s - loss: 0.6839 - accuracy: 0.58 - ETA: 0s - loss: 0.6802 - accuracy: 0.59 - ETA: 0s - loss: 0.6754 - accuracy: 0.60 - ETA: 0s - loss: 0.6718 - accuracy: 0.61 - ETA: 0s - loss: 0.6671 - accuracy: 0.62 - ETA: 0s - loss: 0.6642 - accuracy: 0.62 - 1s 45us/step - loss: 0.6632 - accuracy: 0.6304 - val_loss: 0.6258 - val_accuracy: 0.7045\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 39s - loss: 0.6903 - accuracy: 0.656 - ETA: 1s - loss: 0.6882 - accuracy: 0.549 - ETA: 0s - loss: 0.6849 - accuracy: 0.56 - ETA: 0s - loss: 0.6807 - accuracy: 0.58 - ETA: 0s - loss: 0.6780 - accuracy: 0.59 - ETA: 0s - loss: 0.6732 - accuracy: 0.60 - ETA: 0s - loss: 0.6695 - accuracy: 0.61 - ETA: 0s - loss: 0.6669 - accuracy: 0.61 - ETA: 0s - loss: 0.6636 - accuracy: 0.62 - 1s 44us/step - loss: 0.6630 - accuracy: 0.6231 - val_loss: 0.6286 - val_accuracy: 0.7053\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 39s - loss: 0.6956 - accuracy: 0.562 - ETA: 1s - loss: 0.6918 - accuracy: 0.528 - ETA: 0s - loss: 0.6905 - accuracy: 0.54 - ETA: 0s - loss: 0.6874 - accuracy: 0.56 - ETA: 0s - loss: 0.6838 - accuracy: 0.58 - ETA: 0s - loss: 0.6790 - accuracy: 0.60 - ETA: 0s - loss: 0.6743 - accuracy: 0.61 - ETA: 0s - loss: 0.6704 - accuracy: 0.62 - ETA: 0s - loss: 0.6668 - accuracy: 0.62 - 1s 44us/step - loss: 0.6665 - accuracy: 0.6285 - val_loss: 0.6299 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 38s - loss: 0.6947 - accuracy: 0.437 - ETA: 1s - loss: 0.6906 - accuracy: 0.536 - ETA: 0s - loss: 0.6864 - accuracy: 0.57 - ETA: 0s - loss: 0.6815 - accuracy: 0.58 - ETA: 0s - loss: 0.6777 - accuracy: 0.60 - ETA: 0s - loss: 0.6731 - accuracy: 0.61 - ETA: 0s - loss: 0.6694 - accuracy: 0.62 - ETA: 0s - loss: 0.6655 - accuracy: 0.62 - ETA: 0s - loss: 0.6626 - accuracy: 0.63 - 1s 45us/step - loss: 0.6621 - accuracy: 0.6348 - val_loss: 0.6325 - val_accuracy: 0.7095\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 46s - loss: 0.6941 - accuracy: 0.500 - ETA: 1s - loss: 0.6925 - accuracy: 0.531 - ETA: 0s - loss: 0.6900 - accuracy: 0.54 - ETA: 0s - loss: 0.6884 - accuracy: 0.54 - ETA: 0s - loss: 0.6855 - accuracy: 0.55 - ETA: 0s - loss: 0.6822 - accuracy: 0.55 - ETA: 0s - loss: 0.6782 - accuracy: 0.56 - ETA: 0s - loss: 0.6752 - accuracy: 0.57 - ETA: 0s - loss: 0.6718 - accuracy: 0.58 - 1s 48us/step - loss: 0.6698 - accuracy: 0.5847 - val_loss: 0.6248 - val_accuracy: 0.7216\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 46s - loss: 0.6946 - accuracy: 0.406 - ETA: 1s - loss: 0.6935 - accuracy: 0.502 - ETA: 0s - loss: 0.6925 - accuracy: 0.50 - ETA: 0s - loss: 0.6903 - accuracy: 0.51 - ETA: 0s - loss: 0.6870 - accuracy: 0.52 - ETA: 0s - loss: 0.6830 - accuracy: 0.54 - ETA: 0s - loss: 0.6799 - accuracy: 0.55 - ETA: 0s - loss: 0.6763 - accuracy: 0.56 - ETA: 0s - loss: 0.6730 - accuracy: 0.57 - 1s 47us/step - loss: 0.6712 - accuracy: 0.5745 - val_loss: 0.6238 - val_accuracy: 0.6932\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6946 - accuracy: 0.437 - ETA: 1s - loss: 0.6928 - accuracy: 0.520 - ETA: 0s - loss: 0.6910 - accuracy: 0.52 - ETA: 0s - loss: 0.6878 - accuracy: 0.52 - ETA: 0s - loss: 0.6836 - accuracy: 0.54 - ETA: 0s - loss: 0.6782 - accuracy: 0.55 - ETA: 0s - loss: 0.6741 - accuracy: 0.56 - ETA: 0s - loss: 0.6688 - accuracy: 0.58 - ETA: 0s - loss: 0.6645 - accuracy: 0.58 - 1s 47us/step - loss: 0.6629 - accuracy: 0.5928 - val_loss: 0.6099 - val_accuracy: 0.7109\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 48s - loss: 0.6944 - accuracy: 0.500 - ETA: 1s - loss: 0.6931 - accuracy: 0.519 - ETA: 0s - loss: 0.6924 - accuracy: 0.52 - ETA: 0s - loss: 0.6914 - accuracy: 0.52 - ETA: 0s - loss: 0.6899 - accuracy: 0.53 - ETA: 0s - loss: 0.6882 - accuracy: 0.54 - ETA: 0s - loss: 0.6860 - accuracy: 0.54 - ETA: 0s - loss: 0.6837 - accuracy: 0.54 - ETA: 0s - loss: 0.6822 - accuracy: 0.54 - 1s 48us/step - loss: 0.6816 - accuracy: 0.5488 - val_loss: 0.6553 - val_accuracy: 0.6996\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 48s - loss: 0.6939 - accuracy: 0.343 - ETA: 1s - loss: 0.6934 - accuracy: 0.515 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6917 - accuracy: 0.52 - ETA: 0s - loss: 0.6875 - accuracy: 0.54 - ETA: 0s - loss: 0.6829 - accuracy: 0.55 - ETA: 0s - loss: 0.6796 - accuracy: 0.56 - ETA: 0s - loss: 0.6756 - accuracy: 0.57 - ETA: 0s - loss: 0.6716 - accuracy: 0.57 - 1s 48us/step - loss: 0.6698 - accuracy: 0.5808 - val_loss: 0.6261 - val_accuracy: 0.7173\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6931 - accuracy: 0.531 - ETA: 1s - loss: 0.6922 - accuracy: 0.536 - ETA: 0s - loss: 0.6912 - accuracy: 0.53 - ETA: 0s - loss: 0.6885 - accuracy: 0.54 - ETA: 0s - loss: 0.6836 - accuracy: 0.56 - ETA: 0s - loss: 0.6803 - accuracy: 0.56 - ETA: 0s - loss: 0.6768 - accuracy: 0.57 - ETA: 0s - loss: 0.6719 - accuracy: 0.58 - ETA: 0s - loss: 0.6678 - accuracy: 0.59 - 1s 48us/step - loss: 0.6658 - accuracy: 0.5947 - val_loss: 0.6162 - val_accuracy: 0.7045\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 48s - loss: 0.6935 - accuracy: 0.500 - ETA: 1s - loss: 0.6932 - accuracy: 0.539 - ETA: 0s - loss: 0.6896 - accuracy: 0.55 - ETA: 0s - loss: 0.6866 - accuracy: 0.56 - ETA: 0s - loss: 0.6818 - accuracy: 0.58 - ETA: 0s - loss: 0.6760 - accuracy: 0.58 - ETA: 0s - loss: 0.6716 - accuracy: 0.59 - ETA: 0s - loss: 0.6666 - accuracy: 0.60 - ETA: 0s - loss: 0.6613 - accuracy: 0.61 - 1s 47us/step - loss: 0.6602 - accuracy: 0.6124 - val_loss: 0.5985 - val_accuracy: 0.7237\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.7007 - accuracy: 0.406 - ETA: 1s - loss: 0.6944 - accuracy: 0.493 - ETA: 0s - loss: 0.6930 - accuracy: 0.50 - ETA: 0s - loss: 0.6905 - accuracy: 0.51 - ETA: 0s - loss: 0.6877 - accuracy: 0.53 - ETA: 0s - loss: 0.6832 - accuracy: 0.54 - ETA: 0s - loss: 0.6761 - accuracy: 0.56 - ETA: 0s - loss: 0.6713 - accuracy: 0.57 - ETA: 0s - loss: 0.6672 - accuracy: 0.58 - 1s 47us/step - loss: 0.6652 - accuracy: 0.5884 - val_loss: 0.6065 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6947 - accuracy: 0.468 - ETA: 1s - loss: 0.6944 - accuracy: 0.496 - ETA: 0s - loss: 0.6937 - accuracy: 0.53 - ETA: 0s - loss: 0.6931 - accuracy: 0.54 - ETA: 0s - loss: 0.6913 - accuracy: 0.55 - ETA: 0s - loss: 0.6879 - accuracy: 0.56 - ETA: 0s - loss: 0.6834 - accuracy: 0.57 - ETA: 0s - loss: 0.6776 - accuracy: 0.58 - ETA: 0s - loss: 0.6734 - accuracy: 0.59 - 1s 48us/step - loss: 0.6699 - accuracy: 0.6017 - val_loss: 0.6099 - val_accuracy: 0.7237\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 48s - loss: 0.6939 - accuracy: 0.531 - ETA: 1s - loss: 0.6944 - accuracy: 0.507 - ETA: 0s - loss: 0.6933 - accuracy: 0.53 - ETA: 0s - loss: 0.6915 - accuracy: 0.54 - ETA: 0s - loss: 0.6888 - accuracy: 0.55 - ETA: 0s - loss: 0.6858 - accuracy: 0.56 - ETA: 0s - loss: 0.6808 - accuracy: 0.57 - ETA: 0s - loss: 0.6735 - accuracy: 0.58 - ETA: 0s - loss: 0.6696 - accuracy: 0.58 - ETA: 0s - loss: 0.6653 - accuracy: 0.59 - 1s 53us/step - loss: 0.6610 - accuracy: 0.6026 - val_loss: 0.6073 - val_accuracy: 0.7124\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 46s - loss: 0.6959 - accuracy: 0.500 - ETA: 1s - loss: 0.6936 - accuracy: 0.539 - ETA: 0s - loss: 0.6925 - accuracy: 0.54 - ETA: 0s - loss: 0.6899 - accuracy: 0.56 - ETA: 0s - loss: 0.6860 - accuracy: 0.57 - ETA: 0s - loss: 0.6825 - accuracy: 0.59 - ETA: 0s - loss: 0.6787 - accuracy: 0.60 - ETA: 0s - loss: 0.6755 - accuracy: 0.61 - ETA: 0s - loss: 0.6709 - accuracy: 0.62 - 1s 47us/step - loss: 0.6693 - accuracy: 0.6224 - val_loss: 0.6343 - val_accuracy: 0.7102\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 46s - loss: 0.6990 - accuracy: 0.437 - ETA: 1s - loss: 0.6936 - accuracy: 0.514 - ETA: 0s - loss: 0.6912 - accuracy: 0.53 - ETA: 0s - loss: 0.6876 - accuracy: 0.55 - ETA: 0s - loss: 0.6822 - accuracy: 0.56 - ETA: 0s - loss: 0.6757 - accuracy: 0.58 - ETA: 0s - loss: 0.6691 - accuracy: 0.59 - ETA: 0s - loss: 0.6635 - accuracy: 0.60 - ETA: 0s - loss: 0.6582 - accuracy: 0.60 - 1s 47us/step - loss: 0.6560 - accuracy: 0.6133 - val_loss: 0.6038 - val_accuracy: 0.7095\n",
      "2814/2814 [==============================] - ETA:  - 0s 14us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 46s - loss: 0.6951 - accuracy: 0.531 - ETA: 1s - loss: 0.6917 - accuracy: 0.563 - ETA: 0s - loss: 0.6888 - accuracy: 0.57 - ETA: 0s - loss: 0.6839 - accuracy: 0.58 - ETA: 0s - loss: 0.6784 - accuracy: 0.59 - ETA: 0s - loss: 0.6742 - accuracy: 0.59 - ETA: 0s - loss: 0.6686 - accuracy: 0.60 - ETA: 0s - loss: 0.6636 - accuracy: 0.61 - ETA: 0s - loss: 0.6582 - accuracy: 0.62 - 1s 48us/step - loss: 0.6545 - accuracy: 0.6273 - val_loss: 0.5946 - val_accuracy: 0.7131\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 47s - loss: 0.6957 - accuracy: 0.468 - ETA: 1s - loss: 0.6948 - accuracy: 0.492 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6889 - accuracy: 0.54 - ETA: 0s - loss: 0.6835 - accuracy: 0.56 - ETA: 0s - loss: 0.6768 - accuracy: 0.57 - ETA: 0s - loss: 0.6715 - accuracy: 0.58 - ETA: 0s - loss: 0.6638 - accuracy: 0.60 - ETA: 0s - loss: 0.6580 - accuracy: 0.61 - 1s 47us/step - loss: 0.6564 - accuracy: 0.6143 - val_loss: 0.6020 - val_accuracy: 0.6996\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 48s - loss: 0.6946 - accuracy: 0.531 - ETA: 1s - loss: 0.6938 - accuracy: 0.522 - ETA: 0s - loss: 0.6919 - accuracy: 0.55 - ETA: 0s - loss: 0.6863 - accuracy: 0.56 - ETA: 0s - loss: 0.6796 - accuracy: 0.58 - ETA: 0s - loss: 0.6725 - accuracy: 0.59 - ETA: 0s - loss: 0.6646 - accuracy: 0.60 - ETA: 0s - loss: 0.6588 - accuracy: 0.61 - ETA: 0s - loss: 0.6537 - accuracy: 0.62 - 1s 47us/step - loss: 0.6517 - accuracy: 0.6251 - val_loss: 0.5957 - val_accuracy: 0.7145\n",
      "2815/2815 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 49s - loss: 0.6927 - accuracy: 0.656 - ETA: 1s - loss: 0.6940 - accuracy: 0.516 - ETA: 0s - loss: 0.6911 - accuracy: 0.52 - ETA: 0s - loss: 0.6850 - accuracy: 0.55 - ETA: 0s - loss: 0.6785 - accuracy: 0.57 - ETA: 0s - loss: 0.6718 - accuracy: 0.58 - ETA: 0s - loss: 0.6639 - accuracy: 0.60 - ETA: 0s - loss: 0.6588 - accuracy: 0.61 - ETA: 0s - loss: 0.6545 - accuracy: 0.62 - 1s 49us/step - loss: 0.6512 - accuracy: 0.6285 - val_loss: 0.5935 - val_accuracy: 0.7138\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6961 - accuracy: 0.531 - ETA: 1s - loss: 0.6939 - accuracy: 0.520 - ETA: 0s - loss: 0.6918 - accuracy: 0.52 - ETA: 0s - loss: 0.6883 - accuracy: 0.54 - ETA: 0s - loss: 0.6836 - accuracy: 0.56 - ETA: 0s - loss: 0.6750 - accuracy: 0.58 - ETA: 0s - loss: 0.6678 - accuracy: 0.59 - ETA: 0s - loss: 0.6608 - accuracy: 0.60 - ETA: 0s - loss: 0.6557 - accuracy: 0.61 - 1s 48us/step - loss: 0.6519 - accuracy: 0.6221 - val_loss: 0.5890 - val_accuracy: 0.7173\n",
      "2814/2814 [==============================] - ETA:  - 0s 16us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 47s - loss: 0.6948 - accuracy: 0.562 - ETA: 1s - loss: 0.6925 - accuracy: 0.541 - ETA: 0s - loss: 0.6880 - accuracy: 0.56 - ETA: 0s - loss: 0.6823 - accuracy: 0.57 - ETA: 0s - loss: 0.6760 - accuracy: 0.59 - ETA: 0s - loss: 0.6688 - accuracy: 0.60 - ETA: 0s - loss: 0.6625 - accuracy: 0.60 - ETA: 0s - loss: 0.6560 - accuracy: 0.62 - ETA: 0s - loss: 0.6507 - accuracy: 0.62 - 1s 47us/step - loss: 0.6483 - accuracy: 0.6329 - val_loss: 0.5934 - val_accuracy: 0.7053\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6954 - accuracy: 0.484 - ETA: 0s - loss: 0.6931 - accuracy: 0.529 - ETA: 0s - loss: 0.6924 - accuracy: 0.54 - ETA: 0s - loss: 0.6917 - accuracy: 0.55 - ETA: 0s - loss: 0.6906 - accuracy: 0.56 - 0s 29us/step - loss: 0.6900 - accuracy: 0.5706 - val_loss: 0.6820 - val_accuracy: 0.6982\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6933 - accuracy: 0.546 - ETA: 0s - loss: 0.6922 - accuracy: 0.525 - ETA: 0s - loss: 0.6919 - accuracy: 0.51 - ETA: 0s - loss: 0.6907 - accuracy: 0.52 - ETA: 0s - loss: 0.6892 - accuracy: 0.53 - 0s 29us/step - loss: 0.6883 - accuracy: 0.5429 - val_loss: 0.6775 - val_accuracy: 0.6101\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6943 - accuracy: 0.312 - ETA: 0s - loss: 0.6930 - accuracy: 0.512 - ETA: 0s - loss: 0.6918 - accuracy: 0.54 - ETA: 0s - loss: 0.6904 - accuracy: 0.55 - ETA: 0s - loss: 0.6889 - accuracy: 0.56 - 0s 29us/step - loss: 0.6882 - accuracy: 0.5723 - val_loss: 0.6779 - val_accuracy: 0.6825\n",
      "2815/2815 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6941 - accuracy: 0.437 - ETA: 0s - loss: 0.6927 - accuracy: 0.537 - ETA: 0s - loss: 0.6921 - accuracy: 0.53 - ETA: 0s - loss: 0.6912 - accuracy: 0.54 - ETA: 0s - loss: 0.6899 - accuracy: 0.55 - 0s 29us/step - loss: 0.6894 - accuracy: 0.5559 - val_loss: 0.6796 - val_accuracy: 0.6307\n",
      "2814/2814 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6936 - accuracy: 0.484 - ETA: 0s - loss: 0.6929 - accuracy: 0.505 - ETA: 0s - loss: 0.6914 - accuracy: 0.53 - ETA: 0s - loss: 0.6895 - accuracy: 0.54 - ETA: 0s - loss: 0.6876 - accuracy: 0.56 - 0s 29us/step - loss: 0.6869 - accuracy: 0.5631 - val_loss: 0.6744 - val_accuracy: 0.6584\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6936 - accuracy: 0.531 - ETA: 0s - loss: 0.6925 - accuracy: 0.518 - ETA: 0s - loss: 0.6919 - accuracy: 0.50 - ETA: 0s - loss: 0.6906 - accuracy: 0.50 - ETA: 0s - loss: 0.6891 - accuracy: 0.51 - 0s 29us/step - loss: 0.6886 - accuracy: 0.5136 - val_loss: 0.6802 - val_accuracy: 0.5334\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6933 - accuracy: 0.546 - ETA: 0s - loss: 0.6917 - accuracy: 0.538 - ETA: 0s - loss: 0.6891 - accuracy: 0.55 - ETA: 0s - loss: 0.6864 - accuracy: 0.55 - ETA: 0s - loss: 0.6831 - accuracy: 0.57 - 0s 29us/step - loss: 0.6816 - accuracy: 0.5785 - val_loss: 0.6622 - val_accuracy: 0.6676\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6948 - accuracy: 0.437 - ETA: 0s - loss: 0.6926 - accuracy: 0.519 - ETA: 0s - loss: 0.6901 - accuracy: 0.52 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6850 - accuracy: 0.55 - 0s 29us/step - loss: 0.6844 - accuracy: 0.5589 - val_loss: 0.6694 - val_accuracy: 0.7102\n",
      "2815/2815 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6946 - accuracy: 0.421 - ETA: 0s - loss: 0.6931 - accuracy: 0.529 - ETA: 0s - loss: 0.6914 - accuracy: 0.53 - ETA: 0s - loss: 0.6894 - accuracy: 0.55 - ETA: 0s - loss: 0.6867 - accuracy: 0.56 - 0s 29us/step - loss: 0.6860 - accuracy: 0.5685 - val_loss: 0.6695 - val_accuracy: 0.6932\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6946 - accuracy: 0.437 - ETA: 0s - loss: 0.6928 - accuracy: 0.532 - ETA: 0s - loss: 0.6913 - accuracy: 0.53 - ETA: 0s - loss: 0.6893 - accuracy: 0.55 - ETA: 0s - loss: 0.6870 - accuracy: 0.56 - 0s 29us/step - loss: 0.6859 - accuracy: 0.5676 - val_loss: 0.6706 - val_accuracy: 0.6108\n",
      "2814/2814 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6934 - accuracy: 0.515 - ETA: 0s - loss: 0.6929 - accuracy: 0.524 - ETA: 0s - loss: 0.6914 - accuracy: 0.54 - ETA: 0s - loss: 0.6897 - accuracy: 0.56 - ETA: 0s - loss: 0.6877 - accuracy: 0.57 - 0s 29us/step - loss: 0.6869 - accuracy: 0.5772 - val_loss: 0.6711 - val_accuracy: 0.7024\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 21s - loss: 0.6947 - accuracy: 0.421 - ETA: 0s - loss: 0.6934 - accuracy: 0.515 - ETA: 0s - loss: 0.6925 - accuracy: 0.53 - ETA: 0s - loss: 0.6916 - accuracy: 0.55 - ETA: 0s - loss: 0.6897 - accuracy: 0.56 - 0s 29us/step - loss: 0.6892 - accuracy: 0.5679 - val_loss: 0.6805 - val_accuracy: 0.6790\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6947 - accuracy: 0.546 - ETA: 0s - loss: 0.6909 - accuracy: 0.562 - ETA: 0s - loss: 0.6878 - accuracy: 0.56 - ETA: 0s - loss: 0.6858 - accuracy: 0.57 - ETA: 0s - loss: 0.6827 - accuracy: 0.58 - 0s 30us/step - loss: 0.6809 - accuracy: 0.5887 - val_loss: 0.6586 - val_accuracy: 0.7053\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 21s - loss: 0.6927 - accuracy: 0.531 - ETA: 0s - loss: 0.6919 - accuracy: 0.536 - ETA: 0s - loss: 0.6887 - accuracy: 0.55 - ETA: 0s - loss: 0.6849 - accuracy: 0.57 - ETA: 0s - loss: 0.6804 - accuracy: 0.59 - 0s 29us/step - loss: 0.6778 - accuracy: 0.6015 - val_loss: 0.6489 - val_accuracy: 0.6932\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 21s - loss: 0.6917 - accuracy: 0.625 - ETA: 0s - loss: 0.6884 - accuracy: 0.554 - ETA: 0s - loss: 0.6850 - accuracy: 0.57 - ETA: 0s - loss: 0.6814 - accuracy: 0.58 - ETA: 0s - loss: 0.6769 - accuracy: 0.60 - 0s 30us/step - loss: 0.6749 - accuracy: 0.6088 - val_loss: 0.6465 - val_accuracy: 0.7045\n",
      "2815/2815 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 22s - loss: 0.6950 - accuracy: 0.546 - ETA: 0s - loss: 0.6922 - accuracy: 0.532 - ETA: 0s - loss: 0.6895 - accuracy: 0.53 - ETA: 0s - loss: 0.6862 - accuracy: 0.55 - ETA: 0s - loss: 0.6832 - accuracy: 0.57 - 0s 30us/step - loss: 0.6808 - accuracy: 0.5811 - val_loss: 0.6585 - val_accuracy: 0.6783\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 21s - loss: 0.6966 - accuracy: 0.484 - ETA: 0s - loss: 0.6913 - accuracy: 0.544 - ETA: 0s - loss: 0.6886 - accuracy: 0.56 - ETA: 0s - loss: 0.6852 - accuracy: 0.58 - ETA: 0s - loss: 0.6813 - accuracy: 0.59 - 0s 30us/step - loss: 0.6791 - accuracy: 0.6024 - val_loss: 0.6544 - val_accuracy: 0.7038\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6954 - accuracy: 0.546 - ETA: 0s - loss: 0.6932 - accuracy: 0.530 - ETA: 0s - loss: 0.6912 - accuracy: 0.55 - ETA: 0s - loss: 0.6885 - accuracy: 0.58 - ETA: 0s - loss: 0.6854 - accuracy: 0.59 - 0s 30us/step - loss: 0.6831 - accuracy: 0.6002 - val_loss: 0.6618 - val_accuracy: 0.6811\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 18s - loss: 0.6940 - accuracy: 0.531 - ETA: 0s - loss: 0.6937 - accuracy: 0.508 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.50 - 0s 27us/step - loss: 0.6935 - accuracy: 0.5092 - val_loss: 0.6930 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 18s - loss: 0.6944 - accuracy: 0.453 - ETA: 0s - loss: 0.6939 - accuracy: 0.491 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - 0s 27us/step - loss: 0.6936 - accuracy: 0.5082 - val_loss: 0.6931 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 17s - loss: 0.6943 - accuracy: 0.468 - ETA: 0s - loss: 0.6931 - accuracy: 0.510 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - 0s 27us/step - loss: 0.6931 - accuracy: 0.5162 - val_loss: 0.6925 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 18s - loss: 0.6935 - accuracy: 0.453 - ETA: 0s - loss: 0.6941 - accuracy: 0.489 - ETA: 0s - loss: 0.6939 - accuracy: 0.49 - ETA: 0s - loss: 0.6939 - accuracy: 0.49 - ETA: 0s - loss: 0.6938 - accuracy: 0.49 - 0s 28us/step - loss: 0.6938 - accuracy: 0.4965 - val_loss: 0.6931 - val_accuracy: 0.5263\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 18s - loss: 0.6936 - accuracy: 0.531 - ETA: 0s - loss: 0.6936 - accuracy: 0.514 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6936 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - 0s 27us/step - loss: 0.6935 - accuracy: 0.5143 - val_loss: 0.6931 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 18s - loss: 0.6936 - accuracy: 0.453 - ETA: 0s - loss: 0.6937 - accuracy: 0.505 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - 0s 27us/step - loss: 0.6936 - accuracy: 0.5069 - val_loss: 0.6935 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 18s - loss: 0.6874 - accuracy: 0.640 - ETA: 0s - loss: 0.6935 - accuracy: 0.514 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6935 - accuracy: 0.51 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - 0s 27us/step - loss: 0.6933 - accuracy: 0.5117 - val_loss: 0.6922 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 18s - loss: 0.6946 - accuracy: 0.531 - ETA: 0s - loss: 0.6944 - accuracy: 0.498 - ETA: 0s - loss: 0.6943 - accuracy: 0.50 - ETA: 0s - loss: 0.6943 - accuracy: 0.49 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - 0s 28us/step - loss: 0.6942 - accuracy: 0.5039 - val_loss: 0.6937 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 19s - loss: 0.6947 - accuracy: 0.484 - ETA: 0s - loss: 0.6938 - accuracy: 0.530 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - 0s 28us/step - loss: 0.6937 - accuracy: 0.5240 - val_loss: 0.6930 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 17s - loss: 0.6957 - accuracy: 0.468 - ETA: 0s - loss: 0.6939 - accuracy: 0.510 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.52 - ETA: 0s - loss: 0.6935 - accuracy: 0.52 - 0s 27us/step - loss: 0.6935 - accuracy: 0.5297 - val_loss: 0.6928 - val_accuracy: 0.5497\n",
      "2814/2814 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 18s - loss: 0.6977 - accuracy: 0.406 - ETA: 0s - loss: 0.6952 - accuracy: 0.483 - ETA: 0s - loss: 0.6948 - accuracy: 0.49 - ETA: 0s - loss: 0.6946 - accuracy: 0.49 - ETA: 0s - loss: 0.6944 - accuracy: 0.49 - 0s 28us/step - loss: 0.6945 - accuracy: 0.4966 - val_loss: 0.6936 - val_accuracy: 0.5156\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 18s - loss: 0.6943 - accuracy: 0.468 - ETA: 0s - loss: 0.6940 - accuracy: 0.523 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - 0s 27us/step - loss: 0.6939 - accuracy: 0.5146 - val_loss: 0.6938 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 18s - loss: 0.6965 - accuracy: 0.484 - ETA: 0s - loss: 0.6942 - accuracy: 0.516 - ETA: 0s - loss: 0.6940 - accuracy: 0.52 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - 0s 28us/step - loss: 0.6938 - accuracy: 0.5223 - val_loss: 0.6924 - val_accuracy: 0.5334\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 18s - loss: 0.6903 - accuracy: 0.625 - ETA: 0s - loss: 0.6951 - accuracy: 0.513 - ETA: 0s - loss: 0.6949 - accuracy: 0.51 - ETA: 0s - loss: 0.6948 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - 0s 28us/step - loss: 0.6944 - accuracy: 0.5168 - val_loss: 0.6935 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 19s - loss: 0.6947 - accuracy: 0.484 - ETA: 0s - loss: 0.6946 - accuracy: 0.512 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - 0s 28us/step - loss: 0.6945 - accuracy: 0.5148 - val_loss: 0.6941 - val_accuracy: 0.5327\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 18s - loss: 0.6954 - accuracy: 0.531 - ETA: 0s - loss: 0.6939 - accuracy: 0.520 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - 0s 27us/step - loss: 0.6938 - accuracy: 0.5154 - val_loss: 0.6928 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 18s - loss: 0.6927 - accuracy: 0.531 - ETA: 0s - loss: 0.6946 - accuracy: 0.502 - ETA: 0s - loss: 0.6944 - accuracy: 0.50 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - 0s 28us/step - loss: 0.6941 - accuracy: 0.5064 - val_loss: 0.6933 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 18s - loss: 0.6906 - accuracy: 0.593 - ETA: 0s - loss: 0.6953 - accuracy: 0.503 - ETA: 0s - loss: 0.6954 - accuracy: 0.50 - ETA: 0s - loss: 0.6954 - accuracy: 0.49 - ETA: 0s - loss: 0.6954 - accuracy: 0.49 - 0s 28us/step - loss: 0.6954 - accuracy: 0.4900 - val_loss: 0.6946 - val_accuracy: 0.5178\n",
      "2814/2814 [==============================] - ETA:  - 0s 11us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 23s - loss: 0.6924 - accuracy: 0.562 - ETA: 0s - loss: 0.6930 - accuracy: 0.526 - ETA: 0s - loss: 0.6923 - accuracy: 0.53 - ETA: 0s - loss: 0.6913 - accuracy: 0.54 - ETA: 0s - loss: 0.6898 - accuracy: 0.55 - 0s 31us/step - loss: 0.6884 - accuracy: 0.5682 - val_loss: 0.6758 - val_accuracy: 0.7102\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6938 - accuracy: 0.406 - ETA: 0s - loss: 0.6932 - accuracy: 0.539 - ETA: 0s - loss: 0.6925 - accuracy: 0.54 - ETA: 0s - loss: 0.6914 - accuracy: 0.54 - ETA: 0s - loss: 0.6901 - accuracy: 0.55 - 0s 32us/step - loss: 0.6886 - accuracy: 0.5594 - val_loss: 0.6763 - val_accuracy: 0.6825\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 25s - loss: 0.6922 - accuracy: 0.593 - ETA: 0s - loss: 0.6929 - accuracy: 0.546 - ETA: 0s - loss: 0.6922 - accuracy: 0.54 - ETA: 0s - loss: 0.6908 - accuracy: 0.56 - ETA: 0s - loss: 0.6896 - accuracy: 0.56 - ETA: 0s - loss: 0.6881 - accuracy: 0.57 - 0s 36us/step - loss: 0.6868 - accuracy: 0.5791 - val_loss: 0.6705 - val_accuracy: 0.6896\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 30s - loss: 0.6954 - accuracy: 0.343 - ETA: 0s - loss: 0.6933 - accuracy: 0.494 - ETA: 0s - loss: 0.6925 - accuracy: 0.50 - ETA: 0s - loss: 0.6911 - accuracy: 0.51 - ETA: 0s - loss: 0.6899 - accuracy: 0.51 - 0s 33us/step - loss: 0.6892 - accuracy: 0.5127 - val_loss: 0.6792 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6934 - accuracy: 0.500 - ETA: 0s - loss: 0.6928 - accuracy: 0.511 - ETA: 0s - loss: 0.6917 - accuracy: 0.52 - ETA: 0s - loss: 0.6903 - accuracy: 0.52 - ETA: 0s - loss: 0.6888 - accuracy: 0.53 - 0s 31us/step - loss: 0.6874 - accuracy: 0.5413 - val_loss: 0.6721 - val_accuracy: 0.6989\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6935 - accuracy: 0.531 - ETA: 0s - loss: 0.6934 - accuracy: 0.520 - ETA: 0s - loss: 0.6930 - accuracy: 0.54 - ETA: 0s - loss: 0.6922 - accuracy: 0.56 - ETA: 0s - loss: 0.6904 - accuracy: 0.57 - 0s 32us/step - loss: 0.6888 - accuracy: 0.5881 - val_loss: 0.6753 - val_accuracy: 0.6911\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6943 - accuracy: 0.515 - ETA: 0s - loss: 0.6929 - accuracy: 0.517 - ETA: 0s - loss: 0.6910 - accuracy: 0.54 - ETA: 0s - loss: 0.6885 - accuracy: 0.55 - ETA: 0s - loss: 0.6853 - accuracy: 0.57 - 0s 32us/step - loss: 0.6826 - accuracy: 0.5812 - val_loss: 0.6569 - val_accuracy: 0.7081\n",
      "2815/2815 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6931 - accuracy: 0.531 - ETA: 0s - loss: 0.6928 - accuracy: 0.517 - ETA: 0s - loss: 0.6917 - accuracy: 0.51 - ETA: 0s - loss: 0.6906 - accuracy: 0.51 - ETA: 0s - loss: 0.6879 - accuracy: 0.52 - 0s 31us/step - loss: 0.6861 - accuracy: 0.5317 - val_loss: 0.6678 - val_accuracy: 0.5753\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6975 - accuracy: 0.437 - ETA: 0s - loss: 0.6939 - accuracy: 0.506 - ETA: 0s - loss: 0.6929 - accuracy: 0.53 - ETA: 0s - loss: 0.6907 - accuracy: 0.55 - ETA: 0s - loss: 0.6877 - accuracy: 0.56 - 0s 31us/step - loss: 0.6849 - accuracy: 0.5748 - val_loss: 0.6609 - val_accuracy: 0.6982\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6929 - accuracy: 0.500 - ETA: 0s - loss: 0.6933 - accuracy: 0.520 - ETA: 0s - loss: 0.6914 - accuracy: 0.53 - ETA: 0s - loss: 0.6878 - accuracy: 0.55 - ETA: 0s - loss: 0.6855 - accuracy: 0.56 - 0s 32us/step - loss: 0.6831 - accuracy: 0.5745 - val_loss: 0.6577 - val_accuracy: 0.6847\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 25s - loss: 0.6922 - accuracy: 0.625 - ETA: 0s - loss: 0.6930 - accuracy: 0.522 - ETA: 0s - loss: 0.6911 - accuracy: 0.55 - ETA: 0s - loss: 0.6886 - accuracy: 0.56 - ETA: 0s - loss: 0.6854 - accuracy: 0.57 - 0s 32us/step - loss: 0.6830 - accuracy: 0.5861 - val_loss: 0.6573 - val_accuracy: 0.7017\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6945 - accuracy: 0.468 - ETA: 0s - loss: 0.6937 - accuracy: 0.505 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6913 - accuracy: 0.52 - ETA: 0s - loss: 0.6898 - accuracy: 0.53 - 0s 32us/step - loss: 0.6879 - accuracy: 0.5434 - val_loss: 0.6741 - val_accuracy: 0.6903\n",
      "2814/2814 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 25s - loss: 0.6988 - accuracy: 0.453 - ETA: 0s - loss: 0.6945 - accuracy: 0.507 - ETA: 0s - loss: 0.6930 - accuracy: 0.54 - ETA: 0s - loss: 0.6906 - accuracy: 0.56 - ETA: 0s - loss: 0.6878 - accuracy: 0.57 - 0s 33us/step - loss: 0.6844 - accuracy: 0.5831 - val_loss: 0.6616 - val_accuracy: 0.7038\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6976 - accuracy: 0.406 - ETA: 0s - loss: 0.6928 - accuracy: 0.524 - ETA: 0s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6867 - accuracy: 0.56 - ETA: 0s - loss: 0.6812 - accuracy: 0.58 - 0s 31us/step - loss: 0.6772 - accuracy: 0.5928 - val_loss: 0.6399 - val_accuracy: 0.7038\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6917 - accuracy: 0.562 - ETA: 0s - loss: 0.6927 - accuracy: 0.526 - ETA: 0s - loss: 0.6902 - accuracy: 0.53 - ETA: 0s - loss: 0.6862 - accuracy: 0.56 - ETA: 0s - loss: 0.6825 - accuracy: 0.57 - ETA: 0s - loss: 0.6773 - accuracy: 0.59 - 0s 33us/step - loss: 0.6772 - accuracy: 0.5954 - val_loss: 0.6405 - val_accuracy: 0.7095\n",
      "2815/2815 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 25s - loss: 0.6957 - accuracy: 0.468 - ETA: 0s - loss: 0.6926 - accuracy: 0.549 - ETA: 0s - loss: 0.6897 - accuracy: 0.57 - ETA: 0s - loss: 0.6856 - accuracy: 0.58 - ETA: 0s - loss: 0.6795 - accuracy: 0.60 - 0s 32us/step - loss: 0.6742 - accuracy: 0.6157 - val_loss: 0.6351 - val_accuracy: 0.7024\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6969 - accuracy: 0.421 - ETA: 0s - loss: 0.6927 - accuracy: 0.538 - ETA: 0s - loss: 0.6905 - accuracy: 0.54 - ETA: 0s - loss: 0.6874 - accuracy: 0.56 - ETA: 0s - loss: 0.6832 - accuracy: 0.57 - 0s 33us/step - loss: 0.6795 - accuracy: 0.5862 - val_loss: 0.6481 - val_accuracy: 0.7017\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6953 - accuracy: 0.453 - ETA: 0s - loss: 0.6930 - accuracy: 0.524 - ETA: 0s - loss: 0.6909 - accuracy: 0.54 - ETA: 0s - loss: 0.6875 - accuracy: 0.57 - ETA: 0s - loss: 0.6823 - accuracy: 0.58 - 0s 32us/step - loss: 0.6785 - accuracy: 0.5968 - val_loss: 0.6459 - val_accuracy: 0.7060\n",
      "2814/2814 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6938 - accuracy: 0.437 - ETA: 0s - loss: 0.6918 - accuracy: 0.529 - ETA: 0s - loss: 0.6897 - accuracy: 0.54 - ETA: 0s - loss: 0.6877 - accuracy: 0.56 - ETA: 0s - loss: 0.6854 - accuracy: 0.57 - 0s 29us/step - loss: 0.6842 - accuracy: 0.5775 - val_loss: 0.6702 - val_accuracy: 0.6982\n",
      "2815/2815 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 19s - loss: 0.6954 - accuracy: 0.468 - ETA: 0s - loss: 0.6920 - accuracy: 0.519 - ETA: 0s - loss: 0.6909 - accuracy: 0.53 - ETA: 0s - loss: 0.6889 - accuracy: 0.54 - ETA: 0s - loss: 0.6861 - accuracy: 0.55 - 0s 29us/step - loss: 0.6840 - accuracy: 0.5590 - val_loss: 0.6647 - val_accuracy: 0.6463\n",
      "2815/2815 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 19s - loss: 0.6943 - accuracy: 0.406 - ETA: 0s - loss: 0.6906 - accuracy: 0.528 - ETA: 0s - loss: 0.6862 - accuracy: 0.55 - ETA: 0s - loss: 0.6831 - accuracy: 0.56 - ETA: 0s - loss: 0.6788 - accuracy: 0.57 - 0s 29us/step - loss: 0.6775 - accuracy: 0.5782 - val_loss: 0.6561 - val_accuracy: 0.7109\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6946 - accuracy: 0.359 - ETA: 0s - loss: 0.6892 - accuracy: 0.532 - ETA: 0s - loss: 0.6863 - accuracy: 0.55 - ETA: 0s - loss: 0.6835 - accuracy: 0.56 - ETA: 0s - loss: 0.6809 - accuracy: 0.56 - 0s 29us/step - loss: 0.6793 - accuracy: 0.5684 - val_loss: 0.6594 - val_accuracy: 0.6967\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6937 - accuracy: 0.546 - ETA: 0s - loss: 0.6904 - accuracy: 0.538 - ETA: 0s - loss: 0.6865 - accuracy: 0.55 - ETA: 0s - loss: 0.6827 - accuracy: 0.57 - ETA: 0s - loss: 0.6795 - accuracy: 0.57 - 0s 29us/step - loss: 0.6782 - accuracy: 0.5829 - val_loss: 0.6580 - val_accuracy: 0.7024\n",
      "2814/2814 [==============================] - ETA:  - 0s 11us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6935 - accuracy: 0.562 - ETA: 0s - loss: 0.6899 - accuracy: 0.532 - ETA: 0s - loss: 0.6856 - accuracy: 0.56 - ETA: 0s - loss: 0.6820 - accuracy: 0.57 - ETA: 0s - loss: 0.6781 - accuracy: 0.58 - 0s 30us/step - loss: 0.6762 - accuracy: 0.5940 - val_loss: 0.6596 - val_accuracy: 0.6974\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6950 - accuracy: 0.500 - ETA: 0s - loss: 0.6897 - accuracy: 0.554 - ETA: 0s - loss: 0.6841 - accuracy: 0.58 - ETA: 0s - loss: 0.6795 - accuracy: 0.59 - ETA: 0s - loss: 0.6742 - accuracy: 0.60 - 0s 30us/step - loss: 0.6724 - accuracy: 0.6135 - val_loss: 0.6447 - val_accuracy: 0.7060\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6940 - accuracy: 0.531 - ETA: 0s - loss: 0.6902 - accuracy: 0.540 - ETA: 0s - loss: 0.6883 - accuracy: 0.54 - ETA: 0s - loss: 0.6853 - accuracy: 0.55 - ETA: 0s - loss: 0.6819 - accuracy: 0.56 - 0s 31us/step - loss: 0.6804 - accuracy: 0.5741 - val_loss: 0.6607 - val_accuracy: 0.7003\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6945 - accuracy: 0.515 - ETA: 0s - loss: 0.6920 - accuracy: 0.545 - ETA: 0s - loss: 0.6884 - accuracy: 0.57 - ETA: 0s - loss: 0.6840 - accuracy: 0.59 - ETA: 0s - loss: 0.6803 - accuracy: 0.59 - 0s 31us/step - loss: 0.6786 - accuracy: 0.6042 - val_loss: 0.6604 - val_accuracy: 0.7074\n",
      "2815/2815 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 19s - loss: 0.6944 - accuracy: 0.390 - ETA: 0s - loss: 0.6868 - accuracy: 0.552 - ETA: 0s - loss: 0.6812 - accuracy: 0.57 - ETA: 0s - loss: 0.6781 - accuracy: 0.58 - ETA: 0s - loss: 0.6727 - accuracy: 0.60 - 0s 29us/step - loss: 0.6713 - accuracy: 0.6076 - val_loss: 0.6469 - val_accuracy: 0.7024\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6944 - accuracy: 0.515 - ETA: 0s - loss: 0.6881 - accuracy: 0.543 - ETA: 0s - loss: 0.6822 - accuracy: 0.57 - ETA: 0s - loss: 0.6768 - accuracy: 0.58 - ETA: 0s - loss: 0.6712 - accuracy: 0.60 - 0s 29us/step - loss: 0.6702 - accuracy: 0.6067 - val_loss: 0.6428 - val_accuracy: 0.7003\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 22s - loss: 0.6944 - accuracy: 0.406 - ETA: 0s - loss: 0.6887 - accuracy: 0.535 - ETA: 0s - loss: 0.6835 - accuracy: 0.56 - ETA: 0s - loss: 0.6776 - accuracy: 0.59 - ETA: 0s - loss: 0.6727 - accuracy: 0.60 - 0s 30us/step - loss: 0.6697 - accuracy: 0.6111 - val_loss: 0.6449 - val_accuracy: 0.7053\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 26s - loss: 0.6973 - accuracy: 0.500 - ETA: 0s - loss: 0.6889 - accuracy: 0.555 - ETA: 0s - loss: 0.6839 - accuracy: 0.58 - ETA: 0s - loss: 0.6785 - accuracy: 0.60 - ETA: 0s - loss: 0.6732 - accuracy: 0.61 - 0s 31us/step - loss: 0.6712 - accuracy: 0.6195 - val_loss: 0.6427 - val_accuracy: 0.7095\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6947 - accuracy: 0.578 - ETA: 0s - loss: 0.6856 - accuracy: 0.564 - ETA: 0s - loss: 0.6787 - accuracy: 0.60 - ETA: 0s - loss: 0.6708 - accuracy: 0.61 - ETA: 0s - loss: 0.6635 - accuracy: 0.63 - 0s 29us/step - loss: 0.6618 - accuracy: 0.6346 - val_loss: 0.6262 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6939 - accuracy: 0.531 - ETA: 0s - loss: 0.6827 - accuracy: 0.563 - ETA: 0s - loss: 0.6761 - accuracy: 0.59 - ETA: 0s - loss: 0.6678 - accuracy: 0.61 - ETA: 0s - loss: 0.6615 - accuracy: 0.63 - 0s 29us/step - loss: 0.6589 - accuracy: 0.6350 - val_loss: 0.6252 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 21s - loss: 0.6952 - accuracy: 0.421 - ETA: 0s - loss: 0.6897 - accuracy: 0.556 - ETA: 0s - loss: 0.6829 - accuracy: 0.59 - ETA: 0s - loss: 0.6785 - accuracy: 0.59 - ETA: 0s - loss: 0.6740 - accuracy: 0.60 - 0s 31us/step - loss: 0.6688 - accuracy: 0.6178 - val_loss: 0.6400 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6938 - accuracy: 0.609 - ETA: 0s - loss: 0.6875 - accuracy: 0.565 - ETA: 0s - loss: 0.6803 - accuracy: 0.59 - ETA: 0s - loss: 0.6734 - accuracy: 0.61 - ETA: 0s - loss: 0.6670 - accuracy: 0.62 - 0s 31us/step - loss: 0.6634 - accuracy: 0.6335 - val_loss: 0.6289 - val_accuracy: 0.7017\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 19s - loss: 0.6944 - accuracy: 0.546 - ETA: 0s - loss: 0.6877 - accuracy: 0.531 - ETA: 0s - loss: 0.6815 - accuracy: 0.56 - ETA: 0s - loss: 0.6741 - accuracy: 0.59 - ETA: 0s - loss: 0.6675 - accuracy: 0.61 - 0s 28us/step - loss: 0.6657 - accuracy: 0.6155 - val_loss: 0.6391 - val_accuracy: 0.7017\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6940 - accuracy: 0.484 - ETA: 0s - loss: 0.6931 - accuracy: 0.513 - ETA: 0s - loss: 0.6919 - accuracy: 0.52 - ETA: 0s - loss: 0.6908 - accuracy: 0.53 - ETA: 0s - loss: 0.6882 - accuracy: 0.53 - 0s 30us/step - loss: 0.6863 - accuracy: 0.5440 - val_loss: 0.6647 - val_accuracy: 0.6868\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6931 - accuracy: 0.546 - ETA: 0s - loss: 0.6926 - accuracy: 0.522 - ETA: 0s - loss: 0.6914 - accuracy: 0.52 - ETA: 0s - loss: 0.6877 - accuracy: 0.54 - ETA: 0s - loss: 0.6834 - accuracy: 0.55 - 0s 31us/step - loss: 0.6809 - accuracy: 0.5562 - val_loss: 0.6465 - val_accuracy: 0.7038\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 23s - loss: 0.6930 - accuracy: 0.515 - ETA: 0s - loss: 0.6925 - accuracy: 0.529 - ETA: 0s - loss: 0.6911 - accuracy: 0.53 - ETA: 0s - loss: 0.6886 - accuracy: 0.55 - ETA: 0s - loss: 0.6859 - accuracy: 0.56 - 0s 30us/step - loss: 0.6848 - accuracy: 0.5655 - val_loss: 0.6634 - val_accuracy: 0.6726\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 23s - loss: 0.6937 - accuracy: 0.625 - ETA: 0s - loss: 0.6935 - accuracy: 0.521 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6917 - accuracy: 0.51 - ETA: 0s - loss: 0.6883 - accuracy: 0.52 - 0s 30us/step - loss: 0.6861 - accuracy: 0.5300 - val_loss: 0.6666 - val_accuracy: 0.7166\n",
      "2814/2814 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 23s - loss: 0.6931 - accuracy: 0.531 - ETA: 0s - loss: 0.6931 - accuracy: 0.514 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6914 - accuracy: 0.51 - ETA: 0s - loss: 0.6894 - accuracy: 0.51 - 0s 30us/step - loss: 0.6879 - accuracy: 0.5189 - val_loss: 0.6715 - val_accuracy: 0.6236\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6933 - accuracy: 0.484 - ETA: 0s - loss: 0.6930 - accuracy: 0.528 - ETA: 0s - loss: 0.6918 - accuracy: 0.53 - ETA: 0s - loss: 0.6885 - accuracy: 0.54 - ETA: 0s - loss: 0.6820 - accuracy: 0.55 - 0s 30us/step - loss: 0.6801 - accuracy: 0.5624 - val_loss: 0.6483 - val_accuracy: 0.6847\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 23s - loss: 0.6962 - accuracy: 0.500 - ETA: 0s - loss: 0.6941 - accuracy: 0.501 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6904 - accuracy: 0.52 - ETA: 0s - loss: 0.6863 - accuracy: 0.53 - 0s 31us/step - loss: 0.6831 - accuracy: 0.5475 - val_loss: 0.6508 - val_accuracy: 0.7166\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6941 - accuracy: 0.609 - ETA: 0s - loss: 0.6924 - accuracy: 0.522 - ETA: 0s - loss: 0.6880 - accuracy: 0.53 - ETA: 0s - loss: 0.6822 - accuracy: 0.54 - ETA: 0s - loss: 0.6747 - accuracy: 0.56 - 0s 31us/step - loss: 0.6727 - accuracy: 0.5762 - val_loss: 0.6251 - val_accuracy: 0.7081\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6936 - accuracy: 0.500 - ETA: 0s - loss: 0.6930 - accuracy: 0.522 - ETA: 0s - loss: 0.6909 - accuracy: 0.52 - ETA: 0s - loss: 0.6879 - accuracy: 0.53 - ETA: 0s - loss: 0.6845 - accuracy: 0.54 - 0s 31us/step - loss: 0.6816 - accuracy: 0.5595 - val_loss: 0.6470 - val_accuracy: 0.6932\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6956 - accuracy: 0.406 - ETA: 0s - loss: 0.6933 - accuracy: 0.536 - ETA: 0s - loss: 0.6910 - accuracy: 0.55 - ETA: 0s - loss: 0.6873 - accuracy: 0.56 - ETA: 0s - loss: 0.6810 - accuracy: 0.57 - 0s 31us/step - loss: 0.6760 - accuracy: 0.5862 - val_loss: 0.6270 - val_accuracy: 0.7060\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 23s - loss: 0.6963 - accuracy: 0.375 - ETA: 0s - loss: 0.6926 - accuracy: 0.547 - ETA: 0s - loss: 0.6900 - accuracy: 0.55 - ETA: 0s - loss: 0.6844 - accuracy: 0.57 - ETA: 0s - loss: 0.6779 - accuracy: 0.58 - 0s 31us/step - loss: 0.6729 - accuracy: 0.5927 - val_loss: 0.6197 - val_accuracy: 0.7031\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6942 - accuracy: 0.468 - ETA: 0s - loss: 0.6928 - accuracy: 0.524 - ETA: 0s - loss: 0.6911 - accuracy: 0.52 - ETA: 0s - loss: 0.6874 - accuracy: 0.53 - ETA: 0s - loss: 0.6797 - accuracy: 0.55 - 0s 32us/step - loss: 0.6745 - accuracy: 0.5718 - val_loss: 0.6274 - val_accuracy: 0.7045\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6941 - accuracy: 0.515 - ETA: 0s - loss: 0.6921 - accuracy: 0.537 - ETA: 0s - loss: 0.6892 - accuracy: 0.53 - ETA: 0s - loss: 0.6867 - accuracy: 0.55 - ETA: 0s - loss: 0.6814 - accuracy: 0.57 - 0s 33us/step - loss: 0.6767 - accuracy: 0.5825 - val_loss: 0.6361 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6955 - accuracy: 0.375 - ETA: 0s - loss: 0.6927 - accuracy: 0.521 - ETA: 0s - loss: 0.6895 - accuracy: 0.53 - ETA: 0s - loss: 0.6839 - accuracy: 0.56 - ETA: 0s - loss: 0.6770 - accuracy: 0.58 - 0s 32us/step - loss: 0.6697 - accuracy: 0.5950 - val_loss: 0.6125 - val_accuracy: 0.6996\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6957 - accuracy: 0.500 - ETA: 0s - loss: 0.6936 - accuracy: 0.520 - ETA: 0s - loss: 0.6915 - accuracy: 0.53 - ETA: 0s - loss: 0.6886 - accuracy: 0.55 - ETA: 0s - loss: 0.6840 - accuracy: 0.57 - ETA: 0s - loss: 0.6780 - accuracy: 0.58 - 0s 34us/step - loss: 0.6778 - accuracy: 0.5872 - val_loss: 0.6341 - val_accuracy: 0.6989\n",
      "2815/2815 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6945 - accuracy: 0.500 - ETA: 0s - loss: 0.6933 - accuracy: 0.527 - ETA: 0s - loss: 0.6908 - accuracy: 0.54 - ETA: 0s - loss: 0.6874 - accuracy: 0.55 - ETA: 0s - loss: 0.6817 - accuracy: 0.57 - ETA: 0s - loss: 0.6738 - accuracy: 0.58 - 0s 34us/step - loss: 0.6725 - accuracy: 0.5879 - val_loss: 0.6169 - val_accuracy: 0.6982\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 23s - loss: 0.6944 - accuracy: 0.531 - ETA: 0s - loss: 0.6921 - accuracy: 0.535 - ETA: 0s - loss: 0.6889 - accuracy: 0.54 - ETA: 0s - loss: 0.6795 - accuracy: 0.57 - ETA: 0s - loss: 0.6694 - accuracy: 0.59 - 0s 31us/step - loss: 0.6644 - accuracy: 0.6032 - val_loss: 0.6049 - val_accuracy: 0.7188\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 26s - loss: 0.6938 - accuracy: 0.609 - ETA: 0s - loss: 0.6906 - accuracy: 0.535 - ETA: 0s - loss: 0.6881 - accuracy: 0.53 - ETA: 0s - loss: 0.6855 - accuracy: 0.54 - ETA: 0s - loss: 0.6814 - accuracy: 0.56 - ETA: 0s - loss: 0.6768 - accuracy: 0.58 - 0s 35us/step - loss: 0.6758 - accuracy: 0.5837 - val_loss: 0.6401 - val_accuracy: 0.7003\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 21s - loss: 0.6946 - accuracy: 0.437 - ETA: 0s - loss: 0.6925 - accuracy: 0.534 - ETA: 0s - loss: 0.6913 - accuracy: 0.54 - ETA: 0s - loss: 0.6903 - accuracy: 0.54 - ETA: 0s - loss: 0.6887 - accuracy: 0.55 - 0s 31us/step - loss: 0.6872 - accuracy: 0.5581 - val_loss: 0.6755 - val_accuracy: 0.6939\n",
      "2815/2815 [==============================] - ETA:  - 0s 12us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 21s - loss: 0.6935 - accuracy: 0.484 - ETA: 0s - loss: 0.6927 - accuracy: 0.534 - ETA: 0s - loss: 0.6917 - accuracy: 0.55 - ETA: 0s - loss: 0.6906 - accuracy: 0.55 - ETA: 0s - loss: 0.6894 - accuracy: 0.56 - 0s 30us/step - loss: 0.6886 - accuracy: 0.5666 - val_loss: 0.6792 - val_accuracy: 0.7031\n",
      "2815/2815 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 21s - loss: 0.6935 - accuracy: 0.421 - ETA: 0s - loss: 0.6921 - accuracy: 0.521 - ETA: 0s - loss: 0.6910 - accuracy: 0.51 - ETA: 0s - loss: 0.6896 - accuracy: 0.52 - ETA: 0s - loss: 0.6884 - accuracy: 0.52 - 0s 31us/step - loss: 0.6873 - accuracy: 0.5371 - val_loss: 0.6750 - val_accuracy: 0.6193\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 21s - loss: 0.6940 - accuracy: 0.500 - ETA: 0s - loss: 0.6931 - accuracy: 0.529 - ETA: 0s - loss: 0.6919 - accuracy: 0.54 - ETA: 0s - loss: 0.6909 - accuracy: 0.55 - ETA: 0s - loss: 0.6897 - accuracy: 0.55 - 0s 29us/step - loss: 0.6890 - accuracy: 0.5593 - val_loss: 0.6814 - val_accuracy: 0.5767\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6935 - accuracy: 0.578 - ETA: 0s - loss: 0.6928 - accuracy: 0.552 - ETA: 0s - loss: 0.6918 - accuracy: 0.56 - ETA: 0s - loss: 0.6906 - accuracy: 0.56 - ETA: 0s - loss: 0.6890 - accuracy: 0.56 - 0s 29us/step - loss: 0.6885 - accuracy: 0.5682 - val_loss: 0.6784 - val_accuracy: 0.6456\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 21s - loss: 0.6971 - accuracy: 0.515 - ETA: 0s - loss: 0.6936 - accuracy: 0.503 - ETA: 0s - loss: 0.6925 - accuracy: 0.51 - ETA: 0s - loss: 0.6910 - accuracy: 0.52 - ETA: 0s - loss: 0.6899 - accuracy: 0.52 - 0s 29us/step - loss: 0.6895 - accuracy: 0.5289 - val_loss: 0.6813 - val_accuracy: 0.6271\n",
      "2814/2814 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6940 - accuracy: 0.484 - ETA: 0s - loss: 0.6928 - accuracy: 0.541 - ETA: 0s - loss: 0.6913 - accuracy: 0.55 - ETA: 0s - loss: 0.6890 - accuracy: 0.57 - ETA: 0s - loss: 0.6866 - accuracy: 0.59 - 0s 29us/step - loss: 0.6848 - accuracy: 0.5982 - val_loss: 0.6665 - val_accuracy: 0.7081\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 21s - loss: 0.6922 - accuracy: 0.546 - ETA: 0s - loss: 0.6944 - accuracy: 0.479 - ETA: 0s - loss: 0.6930 - accuracy: 0.52 - ETA: 0s - loss: 0.6916 - accuracy: 0.55 - ETA: 0s - loss: 0.6895 - accuracy: 0.56 - 0s 29us/step - loss: 0.6887 - accuracy: 0.5723 - val_loss: 0.6762 - val_accuracy: 0.7067\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 21s - loss: 0.6941 - accuracy: 0.546 - ETA: 0s - loss: 0.6917 - accuracy: 0.534 - ETA: 0s - loss: 0.6916 - accuracy: 0.51 - ETA: 0s - loss: 0.6902 - accuracy: 0.52 - ETA: 0s - loss: 0.6885 - accuracy: 0.53 - 0s 29us/step - loss: 0.6873 - accuracy: 0.5433 - val_loss: 0.6724 - val_accuracy: 0.6705\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 21s - loss: 0.6946 - accuracy: 0.468 - ETA: 0s - loss: 0.6936 - accuracy: 0.515 - ETA: 0s - loss: 0.6920 - accuracy: 0.53 - ETA: 0s - loss: 0.6894 - accuracy: 0.54 - ETA: 0s - loss: 0.6859 - accuracy: 0.55 - 0s 29us/step - loss: 0.6849 - accuracy: 0.5601 - val_loss: 0.6673 - val_accuracy: 0.6080\n",
      "2814/2814 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6939 - accuracy: 0.546 - ETA: 0s - loss: 0.6908 - accuracy: 0.534 - ETA: 0s - loss: 0.6888 - accuracy: 0.53 - ETA: 0s - loss: 0.6873 - accuracy: 0.54 - ETA: 0s - loss: 0.6853 - accuracy: 0.56 - 0s 30us/step - loss: 0.6836 - accuracy: 0.5677 - val_loss: 0.6654 - val_accuracy: 0.6925\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 21s - loss: 0.6931 - accuracy: 0.578 - ETA: 0s - loss: 0.6913 - accuracy: 0.566 - ETA: 0s - loss: 0.6893 - accuracy: 0.57 - ETA: 0s - loss: 0.6865 - accuracy: 0.59 - ETA: 0s - loss: 0.6834 - accuracy: 0.60 - 0s 29us/step - loss: 0.6821 - accuracy: 0.6092 - val_loss: 0.6640 - val_accuracy: 0.7003\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6953 - accuracy: 0.500 - ETA: 0s - loss: 0.6924 - accuracy: 0.537 - ETA: 0s - loss: 0.6901 - accuracy: 0.54 - ETA: 0s - loss: 0.6869 - accuracy: 0.55 - ETA: 0s - loss: 0.6834 - accuracy: 0.57 - 0s 29us/step - loss: 0.6816 - accuracy: 0.5814 - val_loss: 0.6571 - val_accuracy: 0.6761\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6927 - accuracy: 0.578 - ETA: 0s - loss: 0.6913 - accuracy: 0.552 - ETA: 0s - loss: 0.6878 - accuracy: 0.56 - ETA: 0s - loss: 0.6844 - accuracy: 0.58 - ETA: 0s - loss: 0.6793 - accuracy: 0.59 - 0s 29us/step - loss: 0.6769 - accuracy: 0.6045 - val_loss: 0.6490 - val_accuracy: 0.7074\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 21s - loss: 0.6939 - accuracy: 0.515 - ETA: 0s - loss: 0.6919 - accuracy: 0.548 - ETA: 0s - loss: 0.6897 - accuracy: 0.56 - ETA: 0s - loss: 0.6859 - accuracy: 0.58 - ETA: 0s - loss: 0.6816 - accuracy: 0.59 - 0s 29us/step - loss: 0.6790 - accuracy: 0.6033 - val_loss: 0.6515 - val_accuracy: 0.7024\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6965 - accuracy: 0.453 - ETA: 0s - loss: 0.6930 - accuracy: 0.546 - ETA: 0s - loss: 0.6904 - accuracy: 0.56 - ETA: 0s - loss: 0.6866 - accuracy: 0.58 - ETA: 0s - loss: 0.6821 - accuracy: 0.59 - 0s 29us/step - loss: 0.6804 - accuracy: 0.5991 - val_loss: 0.6536 - val_accuracy: 0.6953\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6949 - accuracy: 0.437 - ETA: 0s - loss: 0.6934 - accuracy: 0.540 - ETA: 0s - loss: 0.6918 - accuracy: 0.56 - ETA: 0s - loss: 0.6893 - accuracy: 0.58 - ETA: 0s - loss: 0.6863 - accuracy: 0.59 - 0s 29us/step - loss: 0.6849 - accuracy: 0.6053 - val_loss: 0.6660 - val_accuracy: 0.7045\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 23s - loss: 0.6972 - accuracy: 0.546 - ETA: 0s - loss: 0.6938 - accuracy: 0.513 - ETA: 0s - loss: 0.6909 - accuracy: 0.54 - ETA: 0s - loss: 0.6872 - accuracy: 0.56 - ETA: 0s - loss: 0.6833 - accuracy: 0.58 - 0s 31us/step - loss: 0.6796 - accuracy: 0.5963 - val_loss: 0.6562 - val_accuracy: 0.7038\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 18s - loss: 0.6938 - accuracy: 0.421 - ETA: 0s - loss: 0.6938 - accuracy: 0.486 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - 0s 27us/step - loss: 0.6936 - accuracy: 0.5077 - val_loss: 0.6932 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 18s - loss: 0.6928 - accuracy: 0.578 - ETA: 0s - loss: 0.6932 - accuracy: 0.512 - ETA: 0s - loss: 0.6932 - accuracy: 0.50 - ETA: 0s - loss: 0.6932 - accuracy: 0.50 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - 0s 27us/step - loss: 0.6930 - accuracy: 0.5122 - val_loss: 0.6926 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 17s - loss: 0.6956 - accuracy: 0.437 - ETA: 0s - loss: 0.6941 - accuracy: 0.509 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - 0s 27us/step - loss: 0.6937 - accuracy: 0.5163 - val_loss: 0.6931 - val_accuracy: 0.5490\n",
      "2815/2815 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 17s - loss: 0.6945 - accuracy: 0.453 - ETA: 0s - loss: 0.6927 - accuracy: 0.527 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6927 - accuracy: 0.51 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - 0s 27us/step - loss: 0.6926 - accuracy: 0.5163 - val_loss: 0.6919 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 17s - loss: 0.6923 - accuracy: 0.546 - ETA: 0s - loss: 0.6936 - accuracy: 0.500 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - ETA: 0s - loss: 0.6933 - accuracy: 0.50 - ETA: 0s - loss: 0.6934 - accuracy: 0.50 - 0s 27us/step - loss: 0.6935 - accuracy: 0.5074 - val_loss: 0.6925 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 18s - loss: 0.6941 - accuracy: 0.453 - ETA: 0s - loss: 0.6936 - accuracy: 0.514 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - 0s 28us/step - loss: 0.6937 - accuracy: 0.5090 - val_loss: 0.6936 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 18s - loss: 0.6946 - accuracy: 0.531 - ETA: 0s - loss: 0.6936 - accuracy: 0.526 - ETA: 0s - loss: 0.6934 - accuracy: 0.52 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - 0s 27us/step - loss: 0.6933 - accuracy: 0.5166 - val_loss: 0.6923 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 18s - loss: 0.6932 - accuracy: 0.546 - ETA: 0s - loss: 0.6942 - accuracy: 0.484 - ETA: 0s - loss: 0.6942 - accuracy: 0.48 - ETA: 0s - loss: 0.6941 - accuracy: 0.49 - ETA: 0s - loss: 0.6939 - accuracy: 0.49 - 0s 27us/step - loss: 0.6939 - accuracy: 0.5016 - val_loss: 0.6928 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 18s - loss: 0.6935 - accuracy: 0.468 - ETA: 0s - loss: 0.6939 - accuracy: 0.503 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6938 - accuracy: 0.51 - 0s 27us/step - loss: 0.6938 - accuracy: 0.5146 - val_loss: 0.6931 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 18s - loss: 0.6941 - accuracy: 0.515 - ETA: 0s - loss: 0.6942 - accuracy: 0.493 - ETA: 0s - loss: 0.6942 - accuracy: 0.49 - ETA: 0s - loss: 0.6941 - accuracy: 0.49 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - 0s 27us/step - loss: 0.6940 - accuracy: 0.5053 - val_loss: 0.6934 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 19s - loss: 0.6943 - accuracy: 0.500 - ETA: 0s - loss: 0.6941 - accuracy: 0.511 - ETA: 0s - loss: 0.6942 - accuracy: 0.50 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - ETA: 0s - loss: 0.6941 - accuracy: 0.51 - 0s 28us/step - loss: 0.6941 - accuracy: 0.5143 - val_loss: 0.6935 - val_accuracy: 0.5263\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 18s - loss: 0.6966 - accuracy: 0.453 - ETA: 0s - loss: 0.6946 - accuracy: 0.491 - ETA: 0s - loss: 0.6947 - accuracy: 0.49 - ETA: 0s - loss: 0.6946 - accuracy: 0.49 - ETA: 0s - loss: 0.6946 - accuracy: 0.49 - 0s 27us/step - loss: 0.6945 - accuracy: 0.4964 - val_loss: 0.6942 - val_accuracy: 0.4986\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 19s - loss: 0.6955 - accuracy: 0.375 - ETA: 0s - loss: 0.6949 - accuracy: 0.495 - ETA: 0s - loss: 0.6948 - accuracy: 0.50 - ETA: 0s - loss: 0.6947 - accuracy: 0.51 - ETA: 0s - loss: 0.6947 - accuracy: 0.50 - 0s 28us/step - loss: 0.6948 - accuracy: 0.5088 - val_loss: 0.6943 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 19s - loss: 0.6948 - accuracy: 0.453 - ETA: 0s - loss: 0.6948 - accuracy: 0.508 - ETA: 0s - loss: 0.6950 - accuracy: 0.49 - ETA: 0s - loss: 0.6949 - accuracy: 0.49 - ETA: 0s - loss: 0.6947 - accuracy: 0.49 - 0s 28us/step - loss: 0.6948 - accuracy: 0.4980 - val_loss: 0.6941 - val_accuracy: 0.5263\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 19s - loss: 0.6935 - accuracy: 0.515 - ETA: 0s - loss: 0.6939 - accuracy: 0.513 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - ETA: 0s - loss: 0.6935 - accuracy: 0.52 - 0s 28us/step - loss: 0.6935 - accuracy: 0.5216 - val_loss: 0.6925 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 18s - loss: 0.6936 - accuracy: 0.562 - ETA: 0s - loss: 0.6942 - accuracy: 0.524 - ETA: 0s - loss: 0.6940 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - 0s 28us/step - loss: 0.6937 - accuracy: 0.5207 - val_loss: 0.6929 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 17s - loss: 0.6957 - accuracy: 0.453 - ETA: 0s - loss: 0.6947 - accuracy: 0.507 - ETA: 0s - loss: 0.6945 - accuracy: 0.50 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - 0s 28us/step - loss: 0.6943 - accuracy: 0.5102 - val_loss: 0.6934 - val_accuracy: 0.5277\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 17s - loss: 0.6958 - accuracy: 0.453 - ETA: 0s - loss: 0.6941 - accuracy: 0.524 - ETA: 0s - loss: 0.6942 - accuracy: 0.52 - ETA: 0s - loss: 0.6942 - accuracy: 0.52 - ETA: 0s - loss: 0.6941 - accuracy: 0.52 - 0s 28us/step - loss: 0.6940 - accuracy: 0.5309 - val_loss: 0.6937 - val_accuracy: 0.5163\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 23s - loss: 0.6952 - accuracy: 0.359 - ETA: 0s - loss: 0.6940 - accuracy: 0.477 - ETA: 0s - loss: 0.6934 - accuracy: 0.49 - ETA: 0s - loss: 0.6930 - accuracy: 0.50 - ETA: 0s - loss: 0.6924 - accuracy: 0.50 - 0s 31us/step - loss: 0.6919 - accuracy: 0.5066 - val_loss: 0.6870 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 23s - loss: 0.6945 - accuracy: 0.453 - ETA: 0s - loss: 0.6919 - accuracy: 0.517 - ETA: 0s - loss: 0.6911 - accuracy: 0.51 - ETA: 0s - loss: 0.6896 - accuracy: 0.52 - ETA: 0s - loss: 0.6879 - accuracy: 0.53 - 0s 30us/step - loss: 0.6871 - accuracy: 0.5412 - val_loss: 0.6719 - val_accuracy: 0.6591\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6936 - accuracy: 0.578 - ETA: 0s - loss: 0.6932 - accuracy: 0.519 - ETA: 0s - loss: 0.6922 - accuracy: 0.52 - ETA: 0s - loss: 0.6913 - accuracy: 0.53 - ETA: 0s - loss: 0.6894 - accuracy: 0.54 - 0s 31us/step - loss: 0.6881 - accuracy: 0.5479 - val_loss: 0.6724 - val_accuracy: 0.6676\n",
      "2815/2815 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 23s - loss: 0.6932 - accuracy: 0.593 - ETA: 0s - loss: 0.6934 - accuracy: 0.521 - ETA: 0s - loss: 0.6923 - accuracy: 0.52 - ETA: 0s - loss: 0.6914 - accuracy: 0.52 - ETA: 0s - loss: 0.6901 - accuracy: 0.53 - 0s 31us/step - loss: 0.6887 - accuracy: 0.5409 - val_loss: 0.6748 - val_accuracy: 0.6683\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6930 - accuracy: 0.546 - ETA: 0s - loss: 0.6929 - accuracy: 0.509 - ETA: 0s - loss: 0.6917 - accuracy: 0.53 - ETA: 0s - loss: 0.6900 - accuracy: 0.54 - ETA: 0s - loss: 0.6874 - accuracy: 0.55 - 0s 31us/step - loss: 0.6858 - accuracy: 0.5619 - val_loss: 0.6684 - val_accuracy: 0.6996\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6940 - accuracy: 0.421 - ETA: 0s - loss: 0.6933 - accuracy: 0.513 - ETA: 0s - loss: 0.6928 - accuracy: 0.51 - ETA: 0s - loss: 0.6917 - accuracy: 0.52 - ETA: 0s - loss: 0.6900 - accuracy: 0.53 - 0s 31us/step - loss: 0.6890 - accuracy: 0.5395 - val_loss: 0.6758 - val_accuracy: 0.6626\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6951 - accuracy: 0.421 - ETA: 0s - loss: 0.6935 - accuracy: 0.525 - ETA: 0s - loss: 0.6922 - accuracy: 0.53 - ETA: 0s - loss: 0.6895 - accuracy: 0.56 - ETA: 0s - loss: 0.6853 - accuracy: 0.58 - 0s 31us/step - loss: 0.6829 - accuracy: 0.5871 - val_loss: 0.6569 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6946 - accuracy: 0.484 - ETA: 0s - loss: 0.6929 - accuracy: 0.518 - ETA: 0s - loss: 0.6904 - accuracy: 0.53 - ETA: 0s - loss: 0.6867 - accuracy: 0.55 - ETA: 0s - loss: 0.6834 - accuracy: 0.56 - 0s 30us/step - loss: 0.6812 - accuracy: 0.5770 - val_loss: 0.6540 - val_accuracy: 0.7088\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6917 - accuracy: 0.671 - ETA: 0s - loss: 0.6934 - accuracy: 0.525 - ETA: 0s - loss: 0.6915 - accuracy: 0.55 - ETA: 0s - loss: 0.6884 - accuracy: 0.57 - ETA: 0s - loss: 0.6850 - accuracy: 0.58 - 0s 31us/step - loss: 0.6825 - accuracy: 0.5861 - val_loss: 0.6586 - val_accuracy: 0.7067\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 25s - loss: 0.6918 - accuracy: 0.531 - ETA: 0s - loss: 0.6924 - accuracy: 0.536 - ETA: 0s - loss: 0.6906 - accuracy: 0.54 - ETA: 0s - loss: 0.6885 - accuracy: 0.56 - ETA: 0s - loss: 0.6850 - accuracy: 0.57 - 0s 31us/step - loss: 0.6828 - accuracy: 0.5848 - val_loss: 0.6579 - val_accuracy: 0.7031\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 23s - loss: 0.6943 - accuracy: 0.453 - ETA: 0s - loss: 0.6936 - accuracy: 0.518 - ETA: 0s - loss: 0.6919 - accuracy: 0.52 - ETA: 0s - loss: 0.6892 - accuracy: 0.53 - ETA: 0s - loss: 0.6863 - accuracy: 0.54 - 0s 31us/step - loss: 0.6847 - accuracy: 0.5546 - val_loss: 0.6646 - val_accuracy: 0.6868\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 23s - loss: 0.6924 - accuracy: 0.578 - ETA: 0s - loss: 0.6933 - accuracy: 0.512 - ETA: 0s - loss: 0.6919 - accuracy: 0.53 - ETA: 0s - loss: 0.6895 - accuracy: 0.55 - ETA: 0s - loss: 0.6861 - accuracy: 0.56 - 0s 30us/step - loss: 0.6843 - accuracy: 0.5754 - val_loss: 0.6646 - val_accuracy: 0.6967\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 22s - loss: 0.6959 - accuracy: 0.484 - ETA: 0s - loss: 0.6926 - accuracy: 0.538 - ETA: 0s - loss: 0.6896 - accuracy: 0.56 - ETA: 0s - loss: 0.6858 - accuracy: 0.58 - ETA: 0s - loss: 0.6807 - accuracy: 0.60 - 0s 31us/step - loss: 0.6767 - accuracy: 0.6091 - val_loss: 0.6410 - val_accuracy: 0.6982\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 23s - loss: 0.6963 - accuracy: 0.437 - ETA: 0s - loss: 0.6925 - accuracy: 0.556 - ETA: 0s - loss: 0.6896 - accuracy: 0.56 - ETA: 0s - loss: 0.6861 - accuracy: 0.57 - ETA: 0s - loss: 0.6817 - accuracy: 0.58 - 0s 31us/step - loss: 0.6782 - accuracy: 0.5946 - val_loss: 0.6463 - val_accuracy: 0.7088\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 23s - loss: 0.6955 - accuracy: 0.562 - ETA: 0s - loss: 0.6944 - accuracy: 0.504 - ETA: 0s - loss: 0.6925 - accuracy: 0.53 - ETA: 0s - loss: 0.6898 - accuracy: 0.54 - ETA: 0s - loss: 0.6868 - accuracy: 0.56 - 0s 31us/step - loss: 0.6834 - accuracy: 0.5740 - val_loss: 0.6531 - val_accuracy: 0.7017\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 23s - loss: 0.6944 - accuracy: 0.531 - ETA: 0s - loss: 0.6924 - accuracy: 0.514 - ETA: 0s - loss: 0.6892 - accuracy: 0.52 - ETA: 0s - loss: 0.6860 - accuracy: 0.54 - ETA: 0s - loss: 0.6812 - accuracy: 0.56 - 0s 31us/step - loss: 0.6772 - accuracy: 0.5787 - val_loss: 0.6427 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 23s - loss: 0.6929 - accuracy: 0.500 - ETA: 0s - loss: 0.6904 - accuracy: 0.544 - ETA: 0s - loss: 0.6874 - accuracy: 0.56 - ETA: 0s - loss: 0.6836 - accuracy: 0.58 - ETA: 0s - loss: 0.6789 - accuracy: 0.59 - 0s 31us/step - loss: 0.6755 - accuracy: 0.5979 - val_loss: 0.6394 - val_accuracy: 0.6918\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 23s - loss: 0.6955 - accuracy: 0.406 - ETA: 0s - loss: 0.6939 - accuracy: 0.511 - ETA: 0s - loss: 0.6919 - accuracy: 0.55 - ETA: 0s - loss: 0.6886 - accuracy: 0.57 - ETA: 0s - loss: 0.6845 - accuracy: 0.58 - 0s 31us/step - loss: 0.6816 - accuracy: 0.5934 - val_loss: 0.6532 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 19s - loss: 0.6940 - accuracy: 0.515 - ETA: 0s - loss: 0.6901 - accuracy: 0.535 - ETA: 0s - loss: 0.6867 - accuracy: 0.55 - ETA: 0s - loss: 0.6841 - accuracy: 0.56 - ETA: 0s - loss: 0.6811 - accuracy: 0.57 - 0s 28us/step - loss: 0.6806 - accuracy: 0.5788 - val_loss: 0.6635 - val_accuracy: 0.6974\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 19s - loss: 0.6944 - accuracy: 0.515 - ETA: 0s - loss: 0.6925 - accuracy: 0.536 - ETA: 0s - loss: 0.6912 - accuracy: 0.55 - ETA: 0s - loss: 0.6902 - accuracy: 0.56 - ETA: 0s - loss: 0.6891 - accuracy: 0.56 - 0s 28us/step - loss: 0.6888 - accuracy: 0.5653 - val_loss: 0.6829 - val_accuracy: 0.6996\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6951 - accuracy: 0.468 - ETA: 0s - loss: 0.6931 - accuracy: 0.513 - ETA: 0s - loss: 0.6919 - accuracy: 0.53 - ETA: 0s - loss: 0.6903 - accuracy: 0.54 - ETA: 0s - loss: 0.6888 - accuracy: 0.55 - 0s 29us/step - loss: 0.6879 - accuracy: 0.5687 - val_loss: 0.6780 - val_accuracy: 0.6676\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 19s - loss: 0.6940 - accuracy: 0.406 - ETA: 0s - loss: 0.6894 - accuracy: 0.535 - ETA: 0s - loss: 0.6859 - accuracy: 0.55 - ETA: 0s - loss: 0.6828 - accuracy: 0.56 - ETA: 0s - loss: 0.6798 - accuracy: 0.56 - 0s 28us/step - loss: 0.6793 - accuracy: 0.5662 - val_loss: 0.6620 - val_accuracy: 0.6996\n",
      "2814/2814 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6936 - accuracy: 0.562 - ETA: 0s - loss: 0.6912 - accuracy: 0.537 - ETA: 0s - loss: 0.6883 - accuracy: 0.55 - ETA: 0s - loss: 0.6868 - accuracy: 0.56 - ETA: 0s - loss: 0.6850 - accuracy: 0.56 - 0s 29us/step - loss: 0.6843 - accuracy: 0.5653 - val_loss: 0.6713 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6938 - accuracy: 0.515 - ETA: 0s - loss: 0.6907 - accuracy: 0.528 - ETA: 0s - loss: 0.6886 - accuracy: 0.54 - ETA: 0s - loss: 0.6867 - accuracy: 0.54 - ETA: 0s - loss: 0.6850 - accuracy: 0.55 - 0s 28us/step - loss: 0.6848 - accuracy: 0.5541 - val_loss: 0.6754 - val_accuracy: 0.6996\n",
      "2814/2814 [==============================] - ETA:  - 0s 15us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6916 - accuracy: 0.578 - ETA: 0s - loss: 0.6869 - accuracy: 0.558 - ETA: 0s - loss: 0.6846 - accuracy: 0.57 - ETA: 0s - loss: 0.6795 - accuracy: 0.59 - ETA: 0s - loss: 0.6748 - accuracy: 0.60 - 0s 29us/step - loss: 0.6720 - accuracy: 0.6094 - val_loss: 0.6449 - val_accuracy: 0.7010\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 21s - loss: 0.6908 - accuracy: 0.562 - ETA: 0s - loss: 0.6919 - accuracy: 0.535 - ETA: 0s - loss: 0.6899 - accuracy: 0.56 - ETA: 0s - loss: 0.6877 - accuracy: 0.58 - ETA: 0s - loss: 0.6847 - accuracy: 0.60 - 0s 29us/step - loss: 0.6837 - accuracy: 0.6064 - val_loss: 0.6693 - val_accuracy: 0.6712\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6903 - accuracy: 0.546 - ETA: 0s - loss: 0.6868 - accuracy: 0.560 - ETA: 0s - loss: 0.6811 - accuracy: 0.58 - ETA: 0s - loss: 0.6766 - accuracy: 0.59 - ETA: 0s - loss: 0.6714 - accuracy: 0.60 - 0s 29us/step - loss: 0.6699 - accuracy: 0.6113 - val_loss: 0.6448 - val_accuracy: 0.7131\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6952 - accuracy: 0.453 - ETA: 0s - loss: 0.6888 - accuracy: 0.556 - ETA: 0s - loss: 0.6825 - accuracy: 0.58 - ETA: 0s - loss: 0.6759 - accuracy: 0.60 - ETA: 0s - loss: 0.6702 - accuracy: 0.61 - 0s 28us/step - loss: 0.6696 - accuracy: 0.6200 - val_loss: 0.6441 - val_accuracy: 0.6868\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6949 - accuracy: 0.437 - ETA: 0s - loss: 0.6883 - accuracy: 0.569 - ETA: 0s - loss: 0.6833 - accuracy: 0.58 - ETA: 0s - loss: 0.6788 - accuracy: 0.59 - ETA: 0s - loss: 0.6734 - accuracy: 0.61 - 0s 29us/step - loss: 0.6712 - accuracy: 0.6194 - val_loss: 0.6435 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6932 - accuracy: 0.656 - ETA: 0s - loss: 0.6870 - accuracy: 0.575 - ETA: 0s - loss: 0.6827 - accuracy: 0.58 - ETA: 0s - loss: 0.6782 - accuracy: 0.60 - ETA: 0s - loss: 0.6730 - accuracy: 0.61 - 0s 28us/step - loss: 0.6721 - accuracy: 0.6148 - val_loss: 0.6521 - val_accuracy: 0.7038\n",
      "2814/2814 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 19s - loss: 0.6929 - accuracy: 0.578 - ETA: 0s - loss: 0.6886 - accuracy: 0.566 - ETA: 0s - loss: 0.6824 - accuracy: 0.58 - ETA: 0s - loss: 0.6776 - accuracy: 0.60 - ETA: 0s - loss: 0.6704 - accuracy: 0.62 - 0s 28us/step - loss: 0.6685 - accuracy: 0.6226 - val_loss: 0.6413 - val_accuracy: 0.7131\n",
      "2815/2815 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6966 - accuracy: 0.484 - ETA: 0s - loss: 0.6868 - accuracy: 0.569 - ETA: 0s - loss: 0.6789 - accuracy: 0.60 - ETA: 0s - loss: 0.6723 - accuracy: 0.61 - ETA: 0s - loss: 0.6657 - accuracy: 0.62 - 0s 28us/step - loss: 0.6640 - accuracy: 0.6316 - val_loss: 0.6310 - val_accuracy: 0.7188\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 19s - loss: 0.6987 - accuracy: 0.437 - ETA: 0s - loss: 0.6891 - accuracy: 0.547 - ETA: 0s - loss: 0.6835 - accuracy: 0.57 - ETA: 0s - loss: 0.6771 - accuracy: 0.59 - ETA: 0s - loss: 0.6712 - accuracy: 0.61 - 0s 29us/step - loss: 0.6683 - accuracy: 0.6173 - val_loss: 0.6352 - val_accuracy: 0.7074\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6954 - accuracy: 0.515 - ETA: 0s - loss: 0.6906 - accuracy: 0.529 - ETA: 0s - loss: 0.6849 - accuracy: 0.56 - ETA: 0s - loss: 0.6774 - accuracy: 0.59 - ETA: 0s - loss: 0.6727 - accuracy: 0.60 - 0s 29us/step - loss: 0.6707 - accuracy: 0.6069 - val_loss: 0.6382 - val_accuracy: 0.7081\n",
      "2814/2814 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 19s - loss: 0.6958 - accuracy: 0.375 - ETA: 0s - loss: 0.6840 - accuracy: 0.550 - ETA: 0s - loss: 0.6784 - accuracy: 0.57 - ETA: 0s - loss: 0.6718 - accuracy: 0.60 - ETA: 0s - loss: 0.6659 - accuracy: 0.61 - 0s 29us/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6352 - val_accuracy: 0.7102\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 19s - loss: 0.6945 - accuracy: 0.468 - ETA: 0s - loss: 0.6899 - accuracy: 0.536 - ETA: 0s - loss: 0.6842 - accuracy: 0.57 - ETA: 0s - loss: 0.6763 - accuracy: 0.60 - ETA: 0s - loss: 0.6699 - accuracy: 0.61 - 0s 29us/step - loss: 0.6684 - accuracy: 0.6159 - val_loss: 0.6409 - val_accuracy: 0.7031\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6935 - accuracy: 0.453 - ETA: 0s - loss: 0.6911 - accuracy: 0.510 - ETA: 0s - loss: 0.6888 - accuracy: 0.51 - ETA: 0s - loss: 0.6853 - accuracy: 0.53 - ETA: 0s - loss: 0.6826 - accuracy: 0.55 - 0s 31us/step - loss: 0.6810 - accuracy: 0.5554 - val_loss: 0.6551 - val_accuracy: 0.6378\n",
      "2815/2815 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6938 - accuracy: 0.453 - ETA: 0s - loss: 0.6935 - accuracy: 0.517 - ETA: 0s - loss: 0.6927 - accuracy: 0.53 - ETA: 0s - loss: 0.6899 - accuracy: 0.54 - ETA: 0s - loss: 0.6852 - accuracy: 0.54 - 0s 31us/step - loss: 0.6836 - accuracy: 0.5462 - val_loss: 0.6441 - val_accuracy: 0.6960\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 23s - loss: 0.6939 - accuracy: 0.484 - ETA: 0s - loss: 0.6933 - accuracy: 0.523 - ETA: 0s - loss: 0.6925 - accuracy: 0.52 - ETA: 0s - loss: 0.6918 - accuracy: 0.52 - ETA: 0s - loss: 0.6904 - accuracy: 0.53 - 0s 31us/step - loss: 0.6893 - accuracy: 0.5344 - val_loss: 0.6750 - val_accuracy: 0.6911\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6924 - accuracy: 0.562 - ETA: 0s - loss: 0.6930 - accuracy: 0.525 - ETA: 0s - loss: 0.6906 - accuracy: 0.53 - ETA: 0s - loss: 0.6878 - accuracy: 0.55 - ETA: 0s - loss: 0.6836 - accuracy: 0.55 - 0s 31us/step - loss: 0.6807 - accuracy: 0.5682 - val_loss: 0.6476 - val_accuracy: 0.6939\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6942 - accuracy: 0.453 - ETA: 0s - loss: 0.6929 - accuracy: 0.496 - ETA: 0s - loss: 0.6919 - accuracy: 0.50 - ETA: 0s - loss: 0.6891 - accuracy: 0.51 - ETA: 0s - loss: 0.6861 - accuracy: 0.52 - 0s 31us/step - loss: 0.6849 - accuracy: 0.5308 - val_loss: 0.6663 - val_accuracy: 0.7060\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6939 - accuracy: 0.531 - ETA: 0s - loss: 0.6926 - accuracy: 0.525 - ETA: 0s - loss: 0.6907 - accuracy: 0.51 - ETA: 0s - loss: 0.6877 - accuracy: 0.52 - ETA: 0s - loss: 0.6828 - accuracy: 0.54 - 0s 31us/step - loss: 0.6808 - accuracy: 0.5513 - val_loss: 0.6576 - val_accuracy: 0.6932\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6940 - accuracy: 0.562 - ETA: 0s - loss: 0.6940 - accuracy: 0.486 - ETA: 0s - loss: 0.6913 - accuracy: 0.49 - ETA: 0s - loss: 0.6878 - accuracy: 0.52 - ETA: 0s - loss: 0.6819 - accuracy: 0.54 - 0s 31us/step - loss: 0.6795 - accuracy: 0.5533 - val_loss: 0.6483 - val_accuracy: 0.7045\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6940 - accuracy: 0.453 - ETA: 0s - loss: 0.6926 - accuracy: 0.536 - ETA: 0s - loss: 0.6896 - accuracy: 0.55 - ETA: 0s - loss: 0.6841 - accuracy: 0.56 - ETA: 0s - loss: 0.6759 - accuracy: 0.58 - 0s 31us/step - loss: 0.6711 - accuracy: 0.5977 - val_loss: 0.6153 - val_accuracy: 0.7081\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6939 - accuracy: 0.484 - ETA: 0s - loss: 0.6930 - accuracy: 0.543 - ETA: 0s - loss: 0.6913 - accuracy: 0.55 - ETA: 0s - loss: 0.6875 - accuracy: 0.57 - ETA: 0s - loss: 0.6816 - accuracy: 0.58 - 0s 31us/step - loss: 0.6781 - accuracy: 0.5929 - val_loss: 0.6319 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 23s - loss: 0.6944 - accuracy: 0.531 - ETA: 0s - loss: 0.6924 - accuracy: 0.528 - ETA: 0s - loss: 0.6895 - accuracy: 0.53 - ETA: 0s - loss: 0.6847 - accuracy: 0.55 - ETA: 0s - loss: 0.6785 - accuracy: 0.56 - 0s 31us/step - loss: 0.6756 - accuracy: 0.5769 - val_loss: 0.6345 - val_accuracy: 0.7081\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 23s - loss: 0.6941 - accuracy: 0.515 - ETA: 0s - loss: 0.6928 - accuracy: 0.525 - ETA: 0s - loss: 0.6910 - accuracy: 0.53 - ETA: 0s - loss: 0.6868 - accuracy: 0.55 - ETA: 0s - loss: 0.6795 - accuracy: 0.58 - 0s 31us/step - loss: 0.6738 - accuracy: 0.5889 - val_loss: 0.6205 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 23s - loss: 0.6935 - accuracy: 0.484 - ETA: 0s - loss: 0.6910 - accuracy: 0.514 - ETA: 0s - loss: 0.6890 - accuracy: 0.52 - ETA: 0s - loss: 0.6848 - accuracy: 0.55 - ETA: 0s - loss: 0.6782 - accuracy: 0.57 - 0s 31us/step - loss: 0.6728 - accuracy: 0.5816 - val_loss: 0.6276 - val_accuracy: 0.6982\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 23s - loss: 0.6951 - accuracy: 0.515 - ETA: 0s - loss: 0.6921 - accuracy: 0.531 - ETA: 0s - loss: 0.6886 - accuracy: 0.54 - ETA: 0s - loss: 0.6830 - accuracy: 0.57 - ETA: 0s - loss: 0.6755 - accuracy: 0.58 - 0s 31us/step - loss: 0.6699 - accuracy: 0.5953 - val_loss: 0.6139 - val_accuracy: 0.7038\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 23s - loss: 0.6952 - accuracy: 0.500 - ETA: 0s - loss: 0.6930 - accuracy: 0.524 - ETA: 0s - loss: 0.6887 - accuracy: 0.54 - ETA: 0s - loss: 0.6816 - accuracy: 0.57 - ETA: 0s - loss: 0.6735 - accuracy: 0.58 - 0s 31us/step - loss: 0.6678 - accuracy: 0.6024 - val_loss: 0.6103 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 23s - loss: 0.6960 - accuracy: 0.468 - ETA: 0s - loss: 0.6933 - accuracy: 0.525 - ETA: 0s - loss: 0.6893 - accuracy: 0.54 - ETA: 0s - loss: 0.6824 - accuracy: 0.57 - ETA: 0s - loss: 0.6724 - accuracy: 0.59 - 0s 31us/step - loss: 0.6670 - accuracy: 0.6015 - val_loss: 0.6099 - val_accuracy: 0.7152\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 22s - loss: 0.6973 - accuracy: 0.437 - ETA: 0s - loss: 0.6939 - accuracy: 0.522 - ETA: 0s - loss: 0.6911 - accuracy: 0.54 - ETA: 0s - loss: 0.6859 - accuracy: 0.57 - ETA: 0s - loss: 0.6778 - accuracy: 0.59 - ETA: 0s - loss: 0.6700 - accuracy: 0.60 - 0s 33us/step - loss: 0.6694 - accuracy: 0.6081 - val_loss: 0.6166 - val_accuracy: 0.7024\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 23s - loss: 0.6954 - accuracy: 0.484 - ETA: 0s - loss: 0.6929 - accuracy: 0.532 - ETA: 0s - loss: 0.6880 - accuracy: 0.55 - ETA: 0s - loss: 0.6790 - accuracy: 0.58 - ETA: 0s - loss: 0.6699 - accuracy: 0.60 - 0s 31us/step - loss: 0.6647 - accuracy: 0.6118 - val_loss: 0.6054 - val_accuracy: 0.7138\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6957 - accuracy: 0.484 - ETA: 0s - loss: 0.6939 - accuracy: 0.521 - ETA: 0s - loss: 0.6912 - accuracy: 0.54 - ETA: 0s - loss: 0.6849 - accuracy: 0.56 - ETA: 0s - loss: 0.6772 - accuracy: 0.58 - 0s 31us/step - loss: 0.6735 - accuracy: 0.5912 - val_loss: 0.6245 - val_accuracy: 0.7031\n",
      "2814/2814 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6950 - accuracy: 0.421 - ETA: 0s - loss: 0.6933 - accuracy: 0.531 - ETA: 0s - loss: 0.6927 - accuracy: 0.55 - ETA: 0s - loss: 0.6918 - accuracy: 0.57 - ETA: 0s - loss: 0.6906 - accuracy: 0.58 - 0s 28us/step - loss: 0.6902 - accuracy: 0.5877 - val_loss: 0.6823 - val_accuracy: 0.7088\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 21s - loss: 0.6938 - accuracy: 0.437 - ETA: 0s - loss: 0.6923 - accuracy: 0.505 - ETA: 0s - loss: 0.6908 - accuracy: 0.51 - ETA: 0s - loss: 0.6898 - accuracy: 0.51 - ETA: 0s - loss: 0.6890 - accuracy: 0.51 - 0s 29us/step - loss: 0.6884 - accuracy: 0.5208 - val_loss: 0.6786 - val_accuracy: 0.5682\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 21s - loss: 0.6950 - accuracy: 0.421 - ETA: 0s - loss: 0.6910 - accuracy: 0.532 - ETA: 0s - loss: 0.6895 - accuracy: 0.53 - ETA: 0s - loss: 0.6885 - accuracy: 0.52 - ETA: 0s - loss: 0.6872 - accuracy: 0.52 - 0s 29us/step - loss: 0.6863 - accuracy: 0.5362 - val_loss: 0.6736 - val_accuracy: 0.5930\n",
      "2815/2815 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 21s - loss: 0.6924 - accuracy: 0.531 - ETA: 0s - loss: 0.6914 - accuracy: 0.535 - ETA: 0s - loss: 0.6898 - accuracy: 0.55 - ETA: 0s - loss: 0.6893 - accuracy: 0.55 - ETA: 0s - loss: 0.6880 - accuracy: 0.55 - 0s 30us/step - loss: 0.6873 - accuracy: 0.5623 - val_loss: 0.6762 - val_accuracy: 0.6264\n",
      "2814/2814 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 21s - loss: 0.6951 - accuracy: 0.390 - ETA: 0s - loss: 0.6929 - accuracy: 0.493 - ETA: 0s - loss: 0.6919 - accuracy: 0.50 - ETA: 0s - loss: 0.6912 - accuracy: 0.50 - ETA: 0s - loss: 0.6898 - accuracy: 0.50 - 0s 29us/step - loss: 0.6892 - accuracy: 0.5094 - val_loss: 0.6807 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 21s - loss: 0.6936 - accuracy: 0.531 - ETA: 0s - loss: 0.6918 - accuracy: 0.525 - ETA: 0s - loss: 0.6913 - accuracy: 0.52 - ETA: 0s - loss: 0.6896 - accuracy: 0.53 - ETA: 0s - loss: 0.6880 - accuracy: 0.54 - 0s 29us/step - loss: 0.6875 - accuracy: 0.5458 - val_loss: 0.6788 - val_accuracy: 0.6889\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 21s - loss: 0.6946 - accuracy: 0.562 - ETA: 0s - loss: 0.6931 - accuracy: 0.508 - ETA: 0s - loss: 0.6911 - accuracy: 0.52 - ETA: 0s - loss: 0.6883 - accuracy: 0.53 - ETA: 0s - loss: 0.6855 - accuracy: 0.55 - 0s 29us/step - loss: 0.6837 - accuracy: 0.5636 - val_loss: 0.6623 - val_accuracy: 0.6612\n",
      "2815/2815 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 21s - loss: 0.6935 - accuracy: 0.546 - ETA: 0s - loss: 0.6922 - accuracy: 0.531 - ETA: 0s - loss: 0.6903 - accuracy: 0.55 - ETA: 0s - loss: 0.6878 - accuracy: 0.56 - ETA: 0s - loss: 0.6844 - accuracy: 0.57 - 0s 29us/step - loss: 0.6836 - accuracy: 0.5787 - val_loss: 0.6650 - val_accuracy: 0.6925\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 21s - loss: 0.6942 - accuracy: 0.562 - ETA: 0s - loss: 0.6911 - accuracy: 0.544 - ETA: 0s - loss: 0.6882 - accuracy: 0.55 - ETA: 0s - loss: 0.6858 - accuracy: 0.55 - ETA: 0s - loss: 0.6824 - accuracy: 0.57 - 0s 29us/step - loss: 0.6811 - accuracy: 0.5788 - val_loss: 0.6603 - val_accuracy: 0.6818\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 21s - loss: 0.6936 - accuracy: 0.515 - ETA: 0s - loss: 0.6909 - accuracy: 0.519 - ETA: 0s - loss: 0.6886 - accuracy: 0.53 - ETA: 0s - loss: 0.6863 - accuracy: 0.54 - ETA: 0s - loss: 0.6828 - accuracy: 0.56 - 0s 29us/step - loss: 0.6816 - accuracy: 0.5709 - val_loss: 0.6629 - val_accuracy: 0.6385\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 22s - loss: 0.6946 - accuracy: 0.453 - ETA: 0s - loss: 0.6930 - accuracy: 0.504 - ETA: 0s - loss: 0.6914 - accuracy: 0.51 - ETA: 0s - loss: 0.6896 - accuracy: 0.52 - ETA: 0s - loss: 0.6877 - accuracy: 0.53 - 0s 29us/step - loss: 0.6871 - accuracy: 0.5338 - val_loss: 0.6739 - val_accuracy: 0.5881\n",
      "2814/2814 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 21s - loss: 0.6948 - accuracy: 0.453 - ETA: 0s - loss: 0.6928 - accuracy: 0.535 - ETA: 0s - loss: 0.6920 - accuracy: 0.54 - ETA: 0s - loss: 0.6903 - accuracy: 0.55 - ETA: 0s - loss: 0.6885 - accuracy: 0.56 - ETA: 0s - loss: 0.6866 - accuracy: 0.57 - 0s 33us/step - loss: 0.6860 - accuracy: 0.5750 - val_loss: 0.6709 - val_accuracy: 0.7024\n",
      "2814/2814 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6965 - accuracy: 0.453 - ETA: 0s - loss: 0.6922 - accuracy: 0.542 - ETA: 0s - loss: 0.6895 - accuracy: 0.56 - ETA: 0s - loss: 0.6857 - accuracy: 0.58 - ETA: 0s - loss: 0.6820 - accuracy: 0.59 - 0s 29us/step - loss: 0.6799 - accuracy: 0.5994 - val_loss: 0.6524 - val_accuracy: 0.6989\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6920 - accuracy: 0.593 - ETA: 0s - loss: 0.6913 - accuracy: 0.539 - ETA: 0s - loss: 0.6881 - accuracy: 0.55 - ETA: 0s - loss: 0.6845 - accuracy: 0.56 - ETA: 0s - loss: 0.6812 - accuracy: 0.58 - 0s 29us/step - loss: 0.6791 - accuracy: 0.5880 - val_loss: 0.6532 - val_accuracy: 0.7010\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6952 - accuracy: 0.468 - ETA: 0s - loss: 0.6929 - accuracy: 0.531 - ETA: 0s - loss: 0.6904 - accuracy: 0.55 - ETA: 0s - loss: 0.6874 - accuracy: 0.57 - ETA: 0s - loss: 0.6832 - accuracy: 0.59 - 0s 29us/step - loss: 0.6814 - accuracy: 0.5967 - val_loss: 0.6579 - val_accuracy: 0.7017\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6953 - accuracy: 0.453 - ETA: 0s - loss: 0.6902 - accuracy: 0.546 - ETA: 0s - loss: 0.6866 - accuracy: 0.57 - ETA: 0s - loss: 0.6817 - accuracy: 0.58 - ETA: 0s - loss: 0.6769 - accuracy: 0.60 - 0s 29us/step - loss: 0.6749 - accuracy: 0.6088 - val_loss: 0.6471 - val_accuracy: 0.6946\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6963 - accuracy: 0.453 - ETA: 0s - loss: 0.6915 - accuracy: 0.548 - ETA: 0s - loss: 0.6881 - accuracy: 0.57 - ETA: 0s - loss: 0.6841 - accuracy: 0.59 - ETA: 0s - loss: 0.6789 - accuracy: 0.60 - 0s 30us/step - loss: 0.6755 - accuracy: 0.6157 - val_loss: 0.6450 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 21s - loss: 0.6965 - accuracy: 0.500 - ETA: 0s - loss: 0.6931 - accuracy: 0.521 - ETA: 0s - loss: 0.6905 - accuracy: 0.56 - ETA: 0s - loss: 0.6879 - accuracy: 0.57 - ETA: 0s - loss: 0.6843 - accuracy: 0.59 - 0s 30us/step - loss: 0.6824 - accuracy: 0.5989 - val_loss: 0.6640 - val_accuracy: 0.6591\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 19s - loss: 0.6947 - accuracy: 0.562 - ETA: 0s - loss: 0.6926 - accuracy: 0.517 - ETA: 0s - loss: 0.6931 - accuracy: 0.51 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - 0s 28us/step - loss: 0.6929 - accuracy: 0.5140 - val_loss: 0.6922 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 18s - loss: 0.6925 - accuracy: 0.578 - ETA: 0s - loss: 0.6929 - accuracy: 0.533 - ETA: 0s - loss: 0.6928 - accuracy: 0.52 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6927 - accuracy: 0.52 - 0s 27us/step - loss: 0.6927 - accuracy: 0.5193 - val_loss: 0.6918 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 18s - loss: 0.6942 - accuracy: 0.343 - ETA: 0s - loss: 0.6943 - accuracy: 0.468 - ETA: 0s - loss: 0.6942 - accuracy: 0.46 - ETA: 0s - loss: 0.6941 - accuracy: 0.47 - ETA: 0s - loss: 0.6940 - accuracy: 0.48 - 0s 27us/step - loss: 0.6940 - accuracy: 0.4807 - val_loss: 0.6934 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 18s - loss: 0.6935 - accuracy: 0.531 - ETA: 0s - loss: 0.6933 - accuracy: 0.507 - ETA: 0s - loss: 0.6934 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - ETA: 0s - loss: 0.6933 - accuracy: 0.51 - 0s 27us/step - loss: 0.6933 - accuracy: 0.5141 - val_loss: 0.6928 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 18s - loss: 0.6929 - accuracy: 0.593 - ETA: 0s - loss: 0.6939 - accuracy: 0.503 - ETA: 0s - loss: 0.6938 - accuracy: 0.49 - ETA: 0s - loss: 0.6938 - accuracy: 0.49 - ETA: 0s - loss: 0.6937 - accuracy: 0.50 - 0s 27us/step - loss: 0.6937 - accuracy: 0.5035 - val_loss: 0.6933 - val_accuracy: 0.5277\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 18s - loss: 0.6945 - accuracy: 0.406 - ETA: 0s - loss: 0.6940 - accuracy: 0.469 - ETA: 0s - loss: 0.6939 - accuracy: 0.47 - ETA: 0s - loss: 0.6939 - accuracy: 0.48 - ETA: 0s - loss: 0.6939 - accuracy: 0.48 - 0s 27us/step - loss: 0.6939 - accuracy: 0.4888 - val_loss: 0.6938 - val_accuracy: 0.5099\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 23s - loss: 0.6944 - accuracy: 0.500 - ETA: 0s - loss: 0.6941 - accuracy: 0.499 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6939 - accuracy: 0.51 - 0s 29us/step - loss: 0.6939 - accuracy: 0.5118 - val_loss: 0.6934 - val_accuracy: 0.5447\n",
      "2815/2815 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 18s - loss: 0.6946 - accuracy: 0.531 - ETA: 0s - loss: 0.6940 - accuracy: 0.519 - ETA: 0s - loss: 0.6938 - accuracy: 0.53 - ETA: 0s - loss: 0.6936 - accuracy: 0.53 - ETA: 0s - loss: 0.6936 - accuracy: 0.53 - 0s 27us/step - loss: 0.6936 - accuracy: 0.5314 - val_loss: 0.6930 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 18s - loss: 0.6930 - accuracy: 0.656 - ETA: 0s - loss: 0.6947 - accuracy: 0.482 - ETA: 0s - loss: 0.6946 - accuracy: 0.48 - ETA: 0s - loss: 0.6946 - accuracy: 0.49 - ETA: 0s - loss: 0.6945 - accuracy: 0.49 - 0s 28us/step - loss: 0.6945 - accuracy: 0.4998 - val_loss: 0.6939 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 18s - loss: 0.6951 - accuracy: 0.484 - ETA: 0s - loss: 0.6946 - accuracy: 0.480 - ETA: 0s - loss: 0.6946 - accuracy: 0.48 - ETA: 0s - loss: 0.6944 - accuracy: 0.49 - ETA: 0s - loss: 0.6944 - accuracy: 0.49 - 0s 27us/step - loss: 0.6944 - accuracy: 0.4958 - val_loss: 0.6935 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 18s - loss: 0.6941 - accuracy: 0.562 - ETA: 0s - loss: 0.6940 - accuracy: 0.517 - ETA: 0s - loss: 0.6937 - accuracy: 0.53 - ETA: 0s - loss: 0.6937 - accuracy: 0.53 - ETA: 0s - loss: 0.6938 - accuracy: 0.52 - 0s 27us/step - loss: 0.6937 - accuracy: 0.5281 - val_loss: 0.6932 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 18s - loss: 0.6946 - accuracy: 0.468 - ETA: 0s - loss: 0.6943 - accuracy: 0.483 - ETA: 0s - loss: 0.6942 - accuracy: 0.49 - ETA: 0s - loss: 0.6941 - accuracy: 0.50 - ETA: 0s - loss: 0.6940 - accuracy: 0.50 - 0s 28us/step - loss: 0.6940 - accuracy: 0.5045 - val_loss: 0.6938 - val_accuracy: 0.5092\n",
      "2814/2814 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 17s - loss: 0.6961 - accuracy: 0.453 - ETA: 0s - loss: 0.6944 - accuracy: 0.515 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6943 - accuracy: 0.51 - ETA: 0s - loss: 0.6942 - accuracy: 0.51 - 0s 27us/step - loss: 0.6942 - accuracy: 0.5148 - val_loss: 0.6932 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 18s - loss: 0.6962 - accuracy: 0.453 - ETA: 0s - loss: 0.6948 - accuracy: 0.496 - ETA: 0s - loss: 0.6946 - accuracy: 0.50 - ETA: 0s - loss: 0.6945 - accuracy: 0.50 - ETA: 0s - loss: 0.6944 - accuracy: 0.50 - 0s 28us/step - loss: 0.6944 - accuracy: 0.5072 - val_loss: 0.6936 - val_accuracy: 0.5298\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 18s - loss: 0.6963 - accuracy: 0.484 - ETA: 0s - loss: 0.6949 - accuracy: 0.503 - ETA: 0s - loss: 0.6948 - accuracy: 0.51 - ETA: 0s - loss: 0.6947 - accuracy: 0.51 - ETA: 0s - loss: 0.6947 - accuracy: 0.51 - 0s 28us/step - loss: 0.6947 - accuracy: 0.5114 - val_loss: 0.6941 - val_accuracy: 0.5284\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 17s - loss: 0.6942 - accuracy: 0.500 - ETA: 0s - loss: 0.6945 - accuracy: 0.518 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6945 - accuracy: 0.51 - ETA: 0s - loss: 0.6944 - accuracy: 0.51 - 0s 28us/step - loss: 0.6944 - accuracy: 0.5167 - val_loss: 0.6938 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 18s - loss: 0.6945 - accuracy: 0.500 - ETA: 0s - loss: 0.6938 - accuracy: 0.521 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6940 - accuracy: 0.51 - ETA: 0s - loss: 0.6937 - accuracy: 0.51 - 0s 28us/step - loss: 0.6937 - accuracy: 0.5184 - val_loss: 0.6926 - val_accuracy: 0.5270\n",
      "2814/2814 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 18s - loss: 0.6961 - accuracy: 0.437 - ETA: 0s - loss: 0.6949 - accuracy: 0.484 - ETA: 0s - loss: 0.6947 - accuracy: 0.51 - ETA: 0s - loss: 0.6946 - accuracy: 0.52 - ETA: 0s - loss: 0.6946 - accuracy: 0.52 - 0s 28us/step - loss: 0.6946 - accuracy: 0.5246 - val_loss: 0.6942 - val_accuracy: 0.5504\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 23s - loss: 0.6928 - accuracy: 0.546 - ETA: 0s - loss: 0.6932 - accuracy: 0.500 - ETA: 0s - loss: 0.6922 - accuracy: 0.51 - ETA: 0s - loss: 0.6908 - accuracy: 0.51 - ETA: 0s - loss: 0.6885 - accuracy: 0.53 - 0s 30us/step - loss: 0.6875 - accuracy: 0.5373 - val_loss: 0.6727 - val_accuracy: 0.6420\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 23s - loss: 0.6945 - accuracy: 0.546 - ETA: 0s - loss: 0.6936 - accuracy: 0.495 - ETA: 0s - loss: 0.6925 - accuracy: 0.50 - ETA: 0s - loss: 0.6915 - accuracy: 0.51 - ETA: 0s - loss: 0.6904 - accuracy: 0.50 - 0s 31us/step - loss: 0.6891 - accuracy: 0.5099 - val_loss: 0.6773 - val_accuracy: 0.5391\n",
      "2815/2815 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 23s - loss: 0.6942 - accuracy: 0.484 - ETA: 0s - loss: 0.6934 - accuracy: 0.509 - ETA: 0s - loss: 0.6932 - accuracy: 0.51 - ETA: 0s - loss: 0.6924 - accuracy: 0.51 - ETA: 0s - loss: 0.6920 - accuracy: 0.51 - 0s 30us/step - loss: 0.6918 - accuracy: 0.5118 - val_loss: 0.6867 - val_accuracy: 0.5270\n",
      "2815/2815 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 25s - loss: 0.6940 - accuracy: 0.390 - ETA: 1s - loss: 0.6933 - accuracy: 0.488 - ETA: 0s - loss: 0.6930 - accuracy: 0.51 - ETA: 0s - loss: 0.6920 - accuracy: 0.53 - ETA: 0s - loss: 0.6901 - accuracy: 0.54 - 0s 34us/step - loss: 0.6871 - accuracy: 0.5547 - val_loss: 0.6700 - val_accuracy: 0.6307\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6951 - accuracy: 0.406 - ETA: 0s - loss: 0.6932 - accuracy: 0.529 - ETA: 0s - loss: 0.6925 - accuracy: 0.54 - ETA: 0s - loss: 0.6914 - accuracy: 0.55 - ETA: 0s - loss: 0.6900 - accuracy: 0.55 - 0s 31us/step - loss: 0.6892 - accuracy: 0.5588 - val_loss: 0.6786 - val_accuracy: 0.6974\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6936 - accuracy: 0.562 - ETA: 0s - loss: 0.6932 - accuracy: 0.513 - ETA: 0s - loss: 0.6926 - accuracy: 0.51 - ETA: 0s - loss: 0.6914 - accuracy: 0.51 - ETA: 0s - loss: 0.6900 - accuracy: 0.51 - 0s 31us/step - loss: 0.6887 - accuracy: 0.5125 - val_loss: 0.6780 - val_accuracy: 0.5433\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12664/12664 [==============================] - ETA: 23s - loss: 0.6939 - accuracy: 0.546 - ETA: 0s - loss: 0.6936 - accuracy: 0.528 - ETA: 0s - loss: 0.6928 - accuracy: 0.53 - ETA: 0s - loss: 0.6913 - accuracy: 0.55 - ETA: 0s - loss: 0.6895 - accuracy: 0.56 - ETA: 0s - loss: 0.6869 - accuracy: 0.57 - 0s 35us/step - loss: 0.6850 - accuracy: 0.5842 - val_loss: 0.6645 - val_accuracy: 0.6726\n",
      "2815/2815 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6916 - accuracy: 0.609 - ETA: 0s - loss: 0.6939 - accuracy: 0.514 - ETA: 0s - loss: 0.6930 - accuracy: 0.53 - ETA: 0s - loss: 0.6911 - accuracy: 0.55 - ETA: 0s - loss: 0.6885 - accuracy: 0.56 - 0s 30us/step - loss: 0.6873 - accuracy: 0.5707 - val_loss: 0.6680 - val_accuracy: 0.6911\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6930 - accuracy: 0.625 - ETA: 0s - loss: 0.6934 - accuracy: 0.506 - ETA: 0s - loss: 0.6919 - accuracy: 0.53 - ETA: 0s - loss: 0.6895 - accuracy: 0.54 - ETA: 0s - loss: 0.6852 - accuracy: 0.56 - 0s 30us/step - loss: 0.6832 - accuracy: 0.5757 - val_loss: 0.6557 - val_accuracy: 0.7088\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 25s - loss: 0.6941 - accuracy: 0.515 - ETA: 0s - loss: 0.6924 - accuracy: 0.554 - ETA: 0s - loss: 0.6913 - accuracy: 0.56 - ETA: 0s - loss: 0.6892 - accuracy: 0.56 - ETA: 0s - loss: 0.6853 - accuracy: 0.58 - 0s 32us/step - loss: 0.6829 - accuracy: 0.5849 - val_loss: 0.6580 - val_accuracy: 0.7060\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6950 - accuracy: 0.468 - ETA: 0s - loss: 0.6932 - accuracy: 0.513 - ETA: 0s - loss: 0.6919 - accuracy: 0.52 - ETA: 0s - loss: 0.6902 - accuracy: 0.53 - ETA: 0s - loss: 0.6878 - accuracy: 0.55 - 0s 31us/step - loss: 0.6865 - accuracy: 0.5579 - val_loss: 0.6682 - val_accuracy: 0.7010\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 23s - loss: 0.6947 - accuracy: 0.453 - ETA: 0s - loss: 0.6927 - accuracy: 0.540 - ETA: 0s - loss: 0.6909 - accuracy: 0.54 - ETA: 0s - loss: 0.6887 - accuracy: 0.54 - ETA: 0s - loss: 0.6846 - accuracy: 0.56 - 0s 30us/step - loss: 0.6822 - accuracy: 0.5717 - val_loss: 0.6602 - val_accuracy: 0.6889\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 23s - loss: 0.6960 - accuracy: 0.421 - ETA: 0s - loss: 0.6931 - accuracy: 0.530 - ETA: 0s - loss: 0.6907 - accuracy: 0.54 - ETA: 0s - loss: 0.6878 - accuracy: 0.56 - ETA: 0s - loss: 0.6829 - accuracy: 0.58 - 0s 31us/step - loss: 0.6789 - accuracy: 0.5948 - val_loss: 0.6444 - val_accuracy: 0.6989\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 23s - loss: 0.6925 - accuracy: 0.515 - ETA: 0s - loss: 0.6915 - accuracy: 0.525 - ETA: 0s - loss: 0.6907 - accuracy: 0.52 - ETA: 0s - loss: 0.6871 - accuracy: 0.55 - ETA: 0s - loss: 0.6819 - accuracy: 0.57 - 0s 31us/step - loss: 0.6787 - accuracy: 0.5836 - val_loss: 0.6464 - val_accuracy: 0.7031\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 25s - loss: 0.6949 - accuracy: 0.546 - ETA: 0s - loss: 0.6927 - accuracy: 0.558 - ETA: 0s - loss: 0.6900 - accuracy: 0.57 - ETA: 0s - loss: 0.6864 - accuracy: 0.58 - ETA: 0s - loss: 0.6827 - accuracy: 0.59 - 0s 32us/step - loss: 0.6784 - accuracy: 0.6073 - val_loss: 0.6452 - val_accuracy: 0.7088\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6944 - accuracy: 0.546 - ETA: 0s - loss: 0.6939 - accuracy: 0.514 - ETA: 0s - loss: 0.6931 - accuracy: 0.52 - ETA: 0s - loss: 0.6909 - accuracy: 0.54 - ETA: 0s - loss: 0.6878 - accuracy: 0.56 - 0s 33us/step - loss: 0.6830 - accuracy: 0.5784 - val_loss: 0.6523 - val_accuracy: 0.6740\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6953 - accuracy: 0.515 - ETA: 0s - loss: 0.6920 - accuracy: 0.531 - ETA: 0s - loss: 0.6898 - accuracy: 0.55 - ETA: 0s - loss: 0.6853 - accuracy: 0.57 - ETA: 0s - loss: 0.6792 - accuracy: 0.59 - 0s 32us/step - loss: 0.6745 - accuracy: 0.6040 - val_loss: 0.6348 - val_accuracy: 0.7095\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 23s - loss: 0.6925 - accuracy: 0.609 - ETA: 0s - loss: 0.6938 - accuracy: 0.532 - ETA: 0s - loss: 0.6919 - accuracy: 0.55 - ETA: 0s - loss: 0.6879 - accuracy: 0.58 - ETA: 0s - loss: 0.6824 - accuracy: 0.59 - 0s 31us/step - loss: 0.6785 - accuracy: 0.6009 - val_loss: 0.6453 - val_accuracy: 0.7031\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 18s - loss: 0.6972 - accuracy: 0.484 - ETA: 0s - loss: 0.6912 - accuracy: 0.524 - ETA: 0s - loss: 0.6896 - accuracy: 0.52 - ETA: 0s - loss: 0.6880 - accuracy: 0.52 - ETA: 0s - loss: 0.6862 - accuracy: 0.52 - 0s 27us/step - loss: 0.6859 - accuracy: 0.5306 - val_loss: 0.6734 - val_accuracy: 0.6286\n",
      "2815/2815 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 19s - loss: 0.6938 - accuracy: 0.531 - ETA: 0s - loss: 0.6921 - accuracy: 0.546 - ETA: 0s - loss: 0.6909 - accuracy: 0.55 - ETA: 0s - loss: 0.6899 - accuracy: 0.56 - ETA: 0s - loss: 0.6890 - accuracy: 0.56 - 0s 29us/step - loss: 0.6882 - accuracy: 0.5680 - val_loss: 0.6823 - val_accuracy: 0.7124\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 19s - loss: 0.6925 - accuracy: 0.593 - ETA: 0s - loss: 0.6924 - accuracy: 0.506 - ETA: 0s - loss: 0.6904 - accuracy: 0.52 - ETA: 0s - loss: 0.6890 - accuracy: 0.52 - ETA: 0s - loss: 0.6870 - accuracy: 0.53 - 0s 28us/step - loss: 0.6865 - accuracy: 0.5410 - val_loss: 0.6763 - val_accuracy: 0.6641\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 19s - loss: 0.6935 - accuracy: 0.578 - ETA: 0s - loss: 0.6889 - accuracy: 0.569 - ETA: 0s - loss: 0.6867 - accuracy: 0.57 - ETA: 0s - loss: 0.6839 - accuracy: 0.58 - ETA: 0s - loss: 0.6805 - accuracy: 0.59 - 0s 29us/step - loss: 0.6800 - accuracy: 0.5919 - val_loss: 0.6652 - val_accuracy: 0.6960\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6901 - accuracy: 0.640 - ETA: 0s - loss: 0.6923 - accuracy: 0.540 - ETA: 0s - loss: 0.6897 - accuracy: 0.57 - ETA: 0s - loss: 0.6866 - accuracy: 0.58 - ETA: 0s - loss: 0.6834 - accuracy: 0.59 - 0s 30us/step - loss: 0.6821 - accuracy: 0.5988 - val_loss: 0.6649 - val_accuracy: 0.7116\n",
      "2814/2814 [==============================] - ETA:  - 0s 11us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6939 - accuracy: 0.421 - ETA: 0s - loss: 0.6913 - accuracy: 0.527 - ETA: 0s - loss: 0.6894 - accuracy: 0.54 - ETA: 0s - loss: 0.6867 - accuracy: 0.56 - ETA: 0s - loss: 0.6844 - accuracy: 0.57 - 0s 29us/step - loss: 0.6836 - accuracy: 0.5728 - val_loss: 0.6715 - val_accuracy: 0.7045\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6952 - accuracy: 0.453 - ETA: 0s - loss: 0.6916 - accuracy: 0.516 - ETA: 0s - loss: 0.6881 - accuracy: 0.54 - ETA: 0s - loss: 0.6841 - accuracy: 0.56 - ETA: 0s - loss: 0.6805 - accuracy: 0.57 - 0s 29us/step - loss: 0.6790 - accuracy: 0.5832 - val_loss: 0.6577 - val_accuracy: 0.7074\n",
      "2815/2815 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6939 - accuracy: 0.531 - ETA: 0s - loss: 0.6883 - accuracy: 0.572 - ETA: 0s - loss: 0.6821 - accuracy: 0.59 - ETA: 0s - loss: 0.6771 - accuracy: 0.60 - ETA: 0s - loss: 0.6720 - accuracy: 0.61 - 0s 29us/step - loss: 0.6701 - accuracy: 0.6222 - val_loss: 0.6426 - val_accuracy: 0.7230\n",
      "2815/2815 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6973 - accuracy: 0.406 - ETA: 0s - loss: 0.6924 - accuracy: 0.507 - ETA: 0s - loss: 0.6894 - accuracy: 0.53 - ETA: 0s - loss: 0.6857 - accuracy: 0.55 - ETA: 0s - loss: 0.6819 - accuracy: 0.56 - 0s 29us/step - loss: 0.6805 - accuracy: 0.5700 - val_loss: 0.6597 - val_accuracy: 0.7038\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6946 - accuracy: 0.531 - ETA: 0s - loss: 0.6869 - accuracy: 0.582 - ETA: 0s - loss: 0.6818 - accuracy: 0.58 - ETA: 0s - loss: 0.6753 - accuracy: 0.60 - ETA: 0s - loss: 0.6698 - accuracy: 0.62 - 0s 29us/step - loss: 0.6685 - accuracy: 0.6216 - val_loss: 0.6392 - val_accuracy: 0.7045\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6924 - accuracy: 0.640 - ETA: 0s - loss: 0.6913 - accuracy: 0.530 - ETA: 0s - loss: 0.6874 - accuracy: 0.55 - ETA: 0s - loss: 0.6837 - accuracy: 0.57 - ETA: 0s - loss: 0.6798 - accuracy: 0.58 - 0s 29us/step - loss: 0.6782 - accuracy: 0.5911 - val_loss: 0.6554 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 21s - loss: 0.6947 - accuracy: 0.437 - ETA: 0s - loss: 0.6895 - accuracy: 0.536 - ETA: 0s - loss: 0.6831 - accuracy: 0.57 - ETA: 0s - loss: 0.6785 - accuracy: 0.59 - ETA: 0s - loss: 0.6743 - accuracy: 0.60 - 0s 29us/step - loss: 0.6721 - accuracy: 0.6088 - val_loss: 0.6470 - val_accuracy: 0.7088\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6984 - accuracy: 0.343 - ETA: 0s - loss: 0.6871 - accuracy: 0.554 - ETA: 0s - loss: 0.6809 - accuracy: 0.58 - ETA: 0s - loss: 0.6745 - accuracy: 0.60 - ETA: 0s - loss: 0.6686 - accuracy: 0.61 - 0s 29us/step - loss: 0.6666 - accuracy: 0.6230 - val_loss: 0.6363 - val_accuracy: 0.7031\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 20s - loss: 0.6921 - accuracy: 0.578 - ETA: 0s - loss: 0.6842 - accuracy: 0.572 - ETA: 0s - loss: 0.6772 - accuracy: 0.60 - ETA: 0s - loss: 0.6688 - accuracy: 0.62 - ETA: 0s - loss: 0.6611 - accuracy: 0.63 - 0s 29us/step - loss: 0.6584 - accuracy: 0.6390 - val_loss: 0.6207 - val_accuracy: 0.7138\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 19s - loss: 0.6947 - accuracy: 0.546 - ETA: 0s - loss: 0.6863 - accuracy: 0.564 - ETA: 0s - loss: 0.6765 - accuracy: 0.60 - ETA: 0s - loss: 0.6688 - accuracy: 0.61 - ETA: 0s - loss: 0.6633 - accuracy: 0.62 - 0s 29us/step - loss: 0.6598 - accuracy: 0.6312 - val_loss: 0.6244 - val_accuracy: 0.7067\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 19s - loss: 0.6949 - accuracy: 0.437 - ETA: 0s - loss: 0.6818 - accuracy: 0.580 - ETA: 0s - loss: 0.6737 - accuracy: 0.59 - ETA: 0s - loss: 0.6693 - accuracy: 0.60 - ETA: 0s - loss: 0.6636 - accuracy: 0.61 - 0s 29us/step - loss: 0.6615 - accuracy: 0.6195 - val_loss: 0.6303 - val_accuracy: 0.6960\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 20s - loss: 0.6958 - accuracy: 0.453 - ETA: 0s - loss: 0.6859 - accuracy: 0.538 - ETA: 0s - loss: 0.6794 - accuracy: 0.57 - ETA: 0s - loss: 0.6735 - accuracy: 0.59 - ETA: 0s - loss: 0.6691 - accuracy: 0.60 - 0s 29us/step - loss: 0.6667 - accuracy: 0.6128 - val_loss: 0.6361 - val_accuracy: 0.7053\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 19s - loss: 0.6973 - accuracy: 0.437 - ETA: 0s - loss: 0.6863 - accuracy: 0.585 - ETA: 0s - loss: 0.6794 - accuracy: 0.60 - ETA: 0s - loss: 0.6724 - accuracy: 0.61 - ETA: 0s - loss: 0.6650 - accuracy: 0.63 - 0s 29us/step - loss: 0.6625 - accuracy: 0.6356 - val_loss: 0.6320 - val_accuracy: 0.6967\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 25s - loss: 0.6938 - accuracy: 0.484 - ETA: 0s - loss: 0.6934 - accuracy: 0.514 - ETA: 0s - loss: 0.6911 - accuracy: 0.54 - ETA: 0s - loss: 0.6870 - accuracy: 0.54 - ETA: 0s - loss: 0.6805 - accuracy: 0.55 - 0s 31us/step - loss: 0.6781 - accuracy: 0.5592 - val_loss: 0.6362 - val_accuracy: 0.7145\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6937 - accuracy: 0.468 - ETA: 0s - loss: 0.6936 - accuracy: 0.500 - ETA: 0s - loss: 0.6929 - accuracy: 0.51 - ETA: 0s - loss: 0.6912 - accuracy: 0.51 - ETA: 0s - loss: 0.6894 - accuracy: 0.51 - 0s 31us/step - loss: 0.6878 - accuracy: 0.5208 - val_loss: 0.6649 - val_accuracy: 0.5952\n",
      "2815/2815 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6941 - accuracy: 0.390 - ETA: 0s - loss: 0.6927 - accuracy: 0.510 - ETA: 0s - loss: 0.6906 - accuracy: 0.51 - ETA: 0s - loss: 0.6874 - accuracy: 0.52 - ETA: 0s - loss: 0.6827 - accuracy: 0.54 - 0s 31us/step - loss: 0.6800 - accuracy: 0.5531 - val_loss: 0.6494 - val_accuracy: 0.6371\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 25s - loss: 0.6937 - accuracy: 0.515 - ETA: 0s - loss: 0.6933 - accuracy: 0.502 - ETA: 0s - loss: 0.6921 - accuracy: 0.50 - ETA: 0s - loss: 0.6890 - accuracy: 0.52 - ETA: 0s - loss: 0.6849 - accuracy: 0.53 - 0s 31us/step - loss: 0.6828 - accuracy: 0.5436 - val_loss: 0.6507 - val_accuracy: 0.6832\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665/12665 [==============================] - ETA: 25s - loss: 0.6936 - accuracy: 0.421 - ETA: 0s - loss: 0.6937 - accuracy: 0.498 - ETA: 0s - loss: 0.6936 - accuracy: 0.50 - ETA: 0s - loss: 0.6929 - accuracy: 0.52 - ETA: 0s - loss: 0.6917 - accuracy: 0.54 - 0s 32us/step - loss: 0.6906 - accuracy: 0.5477 - val_loss: 0.6765 - val_accuracy: 0.6996\n",
      "2814/2814 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6931 - accuracy: 0.468 - ETA: 0s - loss: 0.6931 - accuracy: 0.504 - ETA: 0s - loss: 0.6918 - accuracy: 0.51 - ETA: 0s - loss: 0.6905 - accuracy: 0.51 - ETA: 0s - loss: 0.6873 - accuracy: 0.53 - 0s 31us/step - loss: 0.6857 - accuracy: 0.5373 - val_loss: 0.6640 - val_accuracy: 0.6747\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6926 - accuracy: 0.578 - ETA: 0s - loss: 0.6931 - accuracy: 0.537 - ETA: 0s - loss: 0.6903 - accuracy: 0.55 - ETA: 0s - loss: 0.6859 - accuracy: 0.56 - ETA: 0s - loss: 0.6820 - accuracy: 0.57 - 0s 32us/step - loss: 0.6768 - accuracy: 0.5846 - val_loss: 0.6328 - val_accuracy: 0.7159\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 25s - loss: 0.6942 - accuracy: 0.546 - ETA: 0s - loss: 0.6932 - accuracy: 0.530 - ETA: 0s - loss: 0.6909 - accuracy: 0.53 - ETA: 0s - loss: 0.6854 - accuracy: 0.55 - ETA: 0s - loss: 0.6770 - accuracy: 0.57 - 0s 32us/step - loss: 0.6716 - accuracy: 0.5846 - val_loss: 0.6152 - val_accuracy: 0.7003\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 24s - loss: 0.6952 - accuracy: 0.359 - ETA: 0s - loss: 0.6940 - accuracy: 0.511 - ETA: 0s - loss: 0.6923 - accuracy: 0.53 - ETA: 0s - loss: 0.6888 - accuracy: 0.55 - ETA: 0s - loss: 0.6828 - accuracy: 0.55 - 0s 31us/step - loss: 0.6799 - accuracy: 0.5636 - val_loss: 0.6346 - val_accuracy: 0.7067\n",
      "2815/2815 [==============================] - ETA:  - 0s 8us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 23s - loss: 0.6940 - accuracy: 0.500 - ETA: 0s - loss: 0.6938 - accuracy: 0.526 - ETA: 0s - loss: 0.6901 - accuracy: 0.55 - ETA: 0s - loss: 0.6840 - accuracy: 0.57 - ETA: 0s - loss: 0.6759 - accuracy: 0.58 - 0s 31us/step - loss: 0.6705 - accuracy: 0.5934 - val_loss: 0.6156 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 23s - loss: 0.6949 - accuracy: 0.468 - ETA: 0s - loss: 0.6940 - accuracy: 0.514 - ETA: 0s - loss: 0.6919 - accuracy: 0.52 - ETA: 0s - loss: 0.6863 - accuracy: 0.54 - ETA: 0s - loss: 0.6773 - accuracy: 0.56 - 0s 30us/step - loss: 0.6727 - accuracy: 0.5771 - val_loss: 0.6160 - val_accuracy: 0.6974\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6966 - accuracy: 0.437 - ETA: 0s - loss: 0.6943 - accuracy: 0.511 - ETA: 0s - loss: 0.6930 - accuracy: 0.54 - ETA: 0s - loss: 0.6910 - accuracy: 0.55 - ETA: 0s - loss: 0.6869 - accuracy: 0.56 - 0s 31us/step - loss: 0.6838 - accuracy: 0.5674 - val_loss: 0.6490 - val_accuracy: 0.6705\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 25s - loss: 0.6954 - accuracy: 0.421 - ETA: 0s - loss: 0.6920 - accuracy: 0.531 - ETA: 0s - loss: 0.6903 - accuracy: 0.54 - ETA: 0s - loss: 0.6859 - accuracy: 0.56 - ETA: 0s - loss: 0.6790 - accuracy: 0.57 - 0s 32us/step - loss: 0.6747 - accuracy: 0.5868 - val_loss: 0.6286 - val_accuracy: 0.7095\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 23s - loss: 0.6949 - accuracy: 0.500 - ETA: 0s - loss: 0.6908 - accuracy: 0.521 - ETA: 0s - loss: 0.6857 - accuracy: 0.54 - ETA: 0s - loss: 0.6793 - accuracy: 0.56 - ETA: 0s - loss: 0.6710 - accuracy: 0.58 - 0s 31us/step - loss: 0.6655 - accuracy: 0.5962 - val_loss: 0.6119 - val_accuracy: 0.7088\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12664 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12664/12664 [==============================] - ETA: 23s - loss: 0.6969 - accuracy: 0.453 - ETA: 0s - loss: 0.6930 - accuracy: 0.525 - ETA: 0s - loss: 0.6913 - accuracy: 0.53 - ETA: 0s - loss: 0.6843 - accuracy: 0.56 - ETA: 0s - loss: 0.6769 - accuracy: 0.58 - 0s 31us/step - loss: 0.6720 - accuracy: 0.5926 - val_loss: 0.6182 - val_accuracy: 0.7145\n",
      "2815/2815 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 25s - loss: 0.6962 - accuracy: 0.375 - ETA: 0s - loss: 0.6921 - accuracy: 0.519 - ETA: 0s - loss: 0.6903 - accuracy: 0.52 - ETA: 0s - loss: 0.6852 - accuracy: 0.54 - ETA: 0s - loss: 0.6768 - accuracy: 0.56 - ETA: 0s - loss: 0.6697 - accuracy: 0.58 - 0s 34us/step - loss: 0.6692 - accuracy: 0.5878 - val_loss: 0.6184 - val_accuracy: 0.6918\n",
      "2814/2814 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 25s - loss: 0.6943 - accuracy: 0.515 - ETA: 0s - loss: 0.6932 - accuracy: 0.514 - ETA: 0s - loss: 0.6895 - accuracy: 0.53 - ETA: 0s - loss: 0.6843 - accuracy: 0.55 - ETA: 0s - loss: 0.6781 - accuracy: 0.57 - 0s 32us/step - loss: 0.6734 - accuracy: 0.5814 - val_loss: 0.6219 - val_accuracy: 0.7031\n",
      "2814/2814 [==============================] - ETA:  - 0s 9us/step\n",
      "Train on 12665 samples, validate on 1408 samples\n",
      "Epoch 1/1\n",
      "12665/12665 [==============================] - ETA: 24s - loss: 0.6959 - accuracy: 0.437 - ETA: 0s - loss: 0.6940 - accuracy: 0.514 - ETA: 0s - loss: 0.6917 - accuracy: 0.52 - ETA: 0s - loss: 0.6874 - accuracy: 0.54 - ETA: 0s - loss: 0.6816 - accuracy: 0.56 - 0s 32us/step - loss: 0.6778 - accuracy: 0.5714 - val_loss: 0.6386 - val_accuracy: 0.7074\n",
      "2814/2814 [==============================] - ETA:  - 0s 10us/step\n",
      "Train on 15198 samples, validate on 1689 samples\n",
      "Epoch 1/1\n",
      "15198/15198 [==============================] - ETA: 8:29 - loss: 0.7022 - accuracy: 0.0000e+ - ETA: 13s - loss: 0.6942 - accuracy: 0.4850     - ETA: 8s - loss: 0.6936 - accuracy: 0.505 - ETA: 7s - loss: 0.6925 - accuracy: 0.53 - ETA: 6s - loss: 0.6915 - accuracy: 0.56 - ETA: 5s - loss: 0.6909 - accuracy: 0.56 - ETA: 5s - loss: 0.6896 - accuracy: 0.56 - ETA: 4s - loss: 0.6894 - accuracy: 0.56 - ETA: 4s - loss: 0.6888 - accuracy: 0.57 - ETA: 4s - loss: 0.6881 - accuracy: 0.57 - ETA: 4s - loss: 0.6860 - accuracy: 0.58 - ETA: 4s - loss: 0.6837 - accuracy: 0.59 - ETA: 4s - loss: 0.6817 - accuracy: 0.59 - ETA: 3s - loss: 0.6825 - accuracy: 0.58 - ETA: 3s - loss: 0.6801 - accuracy: 0.59 - ETA: 3s - loss: 0.6788 - accuracy: 0.59 - ETA: 3s - loss: 0.6771 - accuracy: 0.60 - ETA: 3s - loss: 0.6755 - accuracy: 0.60 - ETA: 3s - loss: 0.6744 - accuracy: 0.60 - ETA: 3s - loss: 0.6725 - accuracy: 0.61 - ETA: 3s - loss: 0.6700 - accuracy: 0.61 - ETA: 3s - loss: 0.6684 - accuracy: 0.61 - ETA: 3s - loss: 0.6673 - accuracy: 0.61 - ETA: 3s - loss: 0.6655 - accuracy: 0.62 - ETA: 3s - loss: 0.6639 - accuracy: 0.62 - ETA: 3s - loss: 0.6623 - accuracy: 0.62 - ETA: 2s - loss: 0.6607 - accuracy: 0.62 - ETA: 2s - loss: 0.6599 - accuracy: 0.62 - ETA: 2s - loss: 0.6581 - accuracy: 0.62 - ETA: 2s - loss: 0.6566 - accuracy: 0.62 - ETA: 2s - loss: 0.6536 - accuracy: 0.63 - ETA: 2s - loss: 0.6517 - accuracy: 0.63 - ETA: 2s - loss: 0.6508 - accuracy: 0.63 - ETA: 2s - loss: 0.6493 - accuracy: 0.63 - ETA: 2s - loss: 0.6490 - accuracy: 0.63 - ETA: 2s - loss: 0.6484 - accuracy: 0.63 - ETA: 2s - loss: 0.6462 - accuracy: 0.64 - ETA: 2s - loss: 0.6448 - accuracy: 0.64 - ETA: 2s - loss: 0.6442 - accuracy: 0.64 - ETA: 2s - loss: 0.6440 - accuracy: 0.64 - ETA: 2s - loss: 0.6437 - accuracy: 0.64 - ETA: 2s - loss: 0.6436 - accuracy: 0.64 - ETA: 2s - loss: 0.6435 - accuracy: 0.64 - ETA: 1s - loss: 0.6422 - accuracy: 0.64 - ETA: 1s - loss: 0.6410 - accuracy: 0.65 - ETA: 1s - loss: 0.6401 - accuracy: 0.65 - ETA: 1s - loss: 0.6394 - accuracy: 0.65 - ETA: 1s - loss: 0.6396 - accuracy: 0.65 - ETA: 1s - loss: 0.6398 - accuracy: 0.65 - ETA: 1s - loss: 0.6397 - accuracy: 0.65 - ETA: 1s - loss: 0.6393 - accuracy: 0.65 - ETA: 1s - loss: 0.6380 - accuracy: 0.65 - ETA: 1s - loss: 0.6370 - accuracy: 0.65 - ETA: 1s - loss: 0.6371 - accuracy: 0.65 - ETA: 1s - loss: 0.6372 - accuracy: 0.65 - ETA: 1s - loss: 0.6364 - accuracy: 0.65 - ETA: 1s - loss: 0.6354 - accuracy: 0.65 - ETA: 1s - loss: 0.6349 - accuracy: 0.66 - ETA: 1s - loss: 0.6347 - accuracy: 0.66 - ETA: 1s - loss: 0.6341 - accuracy: 0.66 - ETA: 0s - loss: 0.6343 - accuracy: 0.66 - ETA: 0s - loss: 0.6339 - accuracy: 0.66 - ETA: 0s - loss: 0.6336 - accuracy: 0.66 - ETA: 0s - loss: 0.6329 - accuracy: 0.66 - ETA: 0s - loss: 0.6326 - accuracy: 0.66 - ETA: 0s - loss: 0.6326 - accuracy: 0.66 - ETA: 0s - loss: 0.6329 - accuracy: 0.66 - ETA: 0s - loss: 0.6328 - accuracy: 0.66 - ETA: 0s - loss: 0.6323 - accuracy: 0.66 - ETA: 0s - loss: 0.6317 - accuracy: 0.66 - ETA: 0s - loss: 0.6314 - accuracy: 0.66 - ETA: 0s - loss: 0.6310 - accuracy: 0.66 - ETA: 0s - loss: 0.6303 - accuracy: 0.66 - ETA: 0s - loss: 0.6302 - accuracy: 0.66 - ETA: 0s - loss: 0.6301 - accuracy: 0.66 - ETA: 0s - loss: 0.6299 - accuracy: 0.66 - ETA: 0s - loss: 0.6293 - accuracy: 0.66 - ETA: 0s - loss: 0.6286 - accuracy: 0.66 - ETA: 0s - loss: 0.6277 - accuracy: 0.66 - 4s 287us/step - loss: 0.6274 - accuracy: 0.6684 - val_loss: 0.5746 - val_accuracy: 0.7211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=6, error_score='raise-deprecating',\n",
       "             estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x0000028A0E4D1518>,\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'batch_size': [4, 8, 10, 16, 32, 64],\n",
       "                         'nb_epoch': [50, 100, 150],\n",
       "                         'optimizer': ['rmsprop', 'SGD', 'adam', 'Adagrad',\n",
       "                                       'Adadelta'],\n",
       "                         'units': [4, 8, 13]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_Drop= GridSearchCV(estimator=classifierGS_Drop,\n",
    "                                 param_grid = parameters_NN,\n",
    "                                 cv=6)\n",
    "random_search_Drop.fit(x_train, y_train,validation_split=0.1,#доля обучающей выборки отдать под валидацию — validation_split.\n",
    "                    callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params_:\n",
      "{'batch_size': 4, 'nb_epoch': 150, 'optimizer': 'adam', 'units': 13}\n"
     ]
    }
   ],
   "source": [
    "print(\"best_params_:\")\n",
    "print(random_search_Drop.best_params_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16887/16887 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 0s 14us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46946546"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model=random_search_Drop.evaluate(x_train, y_train)\n",
    "y_pred=random_search_Drop.predict(x_test)\n",
    "mean_pred = y_pred.mean()\n",
    "mean_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred =(y_pred>mean_pred)\n",
    "y_pred_t=random_search_Drop.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_train=roc_auc_score(y_train, y_pred_t)\n",
    "roc_auc_test=roc_auc_score(y_test, y_pred)\n",
    "data['Neural Network'] = (roc_auc_train,roc_auc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogisticRegression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>roc_auc_train</th>\n",
       "      <td>0.796034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc_test</th>\n",
       "      <td>0.788378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.723452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.720288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.724975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               LogisticRegression\n",
       "roc_auc_train            0.796034\n",
       "roc_auc_test             0.788378\n",
       "accuracy                 0.723452\n",
       "recall                   0.720288\n",
       "precision                0.724975"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RandomForestClassifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>roc_auc_train</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc_test</th>\n",
       "      <td>0.867594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.788112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.768523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.799948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               RandomForestClassifier\n",
       "roc_auc_train                1.000000\n",
       "roc_auc_test                 0.867594\n",
       "accuracy                     0.788112\n",
       "recall                       0.768523\n",
       "precision                    0.799948"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NeuralNetwork</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>roc_auc_train</th>\n",
       "      <td>0.906375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roc_auc_test</th>\n",
       "      <td>0.817715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.817707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.787171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.838453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               NeuralNetwork\n",
       "roc_auc_train       0.906375\n",
       "roc_auc_test        0.817715\n",
       "accuracy            0.817707\n",
       "recall              0.787171\n",
       "precision           0.838453"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На кросс-валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <th>Neural Network</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.800078</td>\n",
       "      <td>0.947282</td>\n",
       "      <td>0.922647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.790990</td>\n",
       "      <td>0.911258</td>\n",
       "      <td>0.831520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       LogisticRegression  RandomForestClassifier  Neural Network\n",
       "train            0.800078                0.947282        0.922647\n",
       "test             0.790990                0.911258        0.831520"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "255px",
    "width": "270px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
